<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH-LESS NEURAL NETWORKS: TEACHING OLD MLPS NEW TRICKS VIA DISTILLATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
							<email>shichang@cs.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yozen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
							<email>nshah@snap.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Snap Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH-LESS NEURAL NETWORKS: TEACHING OLD MLPS NEW TRICKS VIA DISTILLATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146?-273? and faster than other acceleration methods by 14?-27?. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.</p><p>Published as a conference paper at ICLR 2022 However, their improvements are limited given the graph dependency is not resolved. Unlike GNNs, MLPs have no dependency on graph data and are easier to deploy than GNNs. They also enjoy the auxiliary benefit of sidestepping the cold-start problem that often happens during the online prediction of relational data , meaning MLPs can infer reasonably even when neighbor information of a new encountered node is not immediately available. On the other hand, this lack of graph dependency typically hurts for relational learning tasks, limiting MLP performance on GML tasks compared to GNNs. We thus ask: can we bridge the two worlds, enjoying the low-latency, dependency-free nature of MLPs and the graph context-awareness of GNNs at the same time?</p><p>Present work. Our key finding is that it is possible to distill knowledge from GNNs to MLPs without losing significant performance, but reducing the inference time drastically for node classification. The knowledge distillation (KD) can be done offline, coupled with model training. In other words, we can shift considerable work from the latency-constrained inference step, where time reduction in milliseconds makes a huge difference, to the less time-sensitive training step, where time cost in hours or days is often tolerable. We call our approach Graph-less Neural Network (GLNN). Specifically, GLNN is a modeling paradigm involving KD from a GNN teacher to a student MLP; the resulting GLNN is an MLP optimized through KD, so it enjoys the benefits of graph contextawareness in training but has no graph dependency in inference. Regarding speed, GLNNs have superior efficiency and are 146?-273? faster than GNNs and 14?-27? faster than other inference acceleration methods. Regarding performance, under a production setting involving both transductive and inductive predictions on 7 datasets, GLNN accuracies improve over MLPs by 12.36% on average and match GNNs on 6/7 datasets. We comprehensively study when and why GLNNs can achieve competitive results as GNNs. Our analysis suggests the critical factors for such great performance are large MLP sizes and high mutual information between node features and labels. Our observations align with recent results in vision and language, which posit that large enough (or slightly modified) MLPs can achieve similar results as CNNs and Transformers <ref type="bibr" target="#b31">(Liu et al., 2021;</ref><ref type="bibr" target="#b35">Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b32">Melas-Kyriazi, 2021;</ref><ref type="bibr" target="#b36">Touvron et al., 2021;</ref><ref type="bibr" target="#b10">Ding et al., 2021)</ref>. Our core contributions are as follows:</p><p>? We propose GLNN, which eliminates neighbor-fetching latency in GNN inference via KD to MLP. ? We show GLNNs has competitive performance as GNNs, while enjoying 146?-273? faster inference than vanilla GNNs and 14?-27? faster inference than other inference acceleration methods. ? We study GLNN properties comprehensively by investigating their performance under different settings, how they work as regularizers, their inductive bias, expressiveness, and limitations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) have recently become very popular for graph machine learning 2 RELATED WORK Graph Neural Networks. The early GNNs generalize convolution nets to graphs <ref type="bibr" target="#b1">(Bruna et al., 2014;</ref><ref type="bibr" target="#b7">Defferrard et al., 2017)</ref> and later simplified to message-passing neural net (MPNN) by <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>. Most GNNs after can be put as MPNNs. For example, GAT employs attention <ref type="bibr" target="#b38">(Veli?kovi? et al., 2017)</ref>, PPNP employs personalized PageRank <ref type="bibr" target="#b26">(Klicpera et al., 2019)</ref>, GCNII and DeeperGCN employ residual connections and dense connections <ref type="bibr" target="#b4">(Chen et al., 2020;</ref>.</p><p>Inference Acceleration. Inference acceleration have been proposed by hardware improvements <ref type="bibr" target="#b5">(Chen et al., 2016;</ref><ref type="bibr" target="#b23">Judd et al., 2016)</ref> and algorithmic improvements through pruning <ref type="bibr" target="#b15">(Han et al., 2015)</ref>, quantization <ref type="bibr" target="#b13">(Gupta et al., 2015)</ref>. For GNNs, pruning  and quantizing GNN parameters <ref type="bibr" target="#b48">(Zhao et al., 2020)</ref> have been studied. These approaches speed up GNN inference to a certain extent but do not eliminate the neighbor-fetching latency. In contrast, our cross-model KD solves this issue. Concurrently, Graph-MLP also tries to bypass GNN neighbor fetching <ref type="bibr" target="#b18">(Hu et al., 2021)</ref> by training an MLP with a neighbor contrastive loss, but it only considers transductive but not the more practical inductive setting. Some sampling works focus on speed up GNN training <ref type="bibr" target="#b50">(Zou et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2018)</ref>, which are complementary to our goal on inference acceleration. GNN distillation. Existing GNN KD works try to distill large GNNs to smaller GNNs. LSP <ref type="bibr" target="#b46">(Yang et al., 2021b)</ref> and TinyGNN <ref type="bibr" target="#b44">(Yan et al., 2020)</ref> do KD while preserving local information. Their students are GNNs with fewer parameters but not necessarily fewer layers. Thus, both designs still require latency-inducing fetching. GFKD <ref type="bibr" target="#b8">(Deng &amp; Zhang, 2021)</ref> does graph-level KD via graph generation. In GFKD, data instances are independent graphs, whereas we focus on dependent nodes within a graph. GraphSAIL <ref type="bibr" target="#b43">(Xu et al., 2020)</ref> uses KD to learn students work well on new data while preserving performance on old data. CPF <ref type="bibr" target="#b45">(Yang et al., 2021a)</ref> combines KD and label propagation (LP). The student in CPF is not a GNN, but it is still heavily graph-dependent as it uses LP. <ref type="figure" target="#fig_6">Figure 1</ref>: The number of fetches and the inference time of GNNs are both magnitudes more than MLPs and grow exponentially as functions of the number of layers. Left: neighbors need to be fetched for two GNN layers. Middle: the total number of fetches for inference. Right: the total inference time. (Inductive inference for 10 random nodes on OGB Products  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Notations. For GML tasks, the input is usually a graph and its node features, which we write as G = (V, E), with V stands for all nodes, and E stands for all edges. Let N denote the total number of nodes. We use X ? R N ?D to represent node features, with row x v being the D-dimensional feature of node v ? V. We represent edges with an adjacency matrix A, with A u,v = 1 if edge (u, v) ? E, and 0 otherwise. For node classification, one of the most important GML applications, the prediction targets are Y ? R N ?K , where row y v is a K-dim one-hot vector for node v. For a given G, usually a small portion of nodes will be labeled, which we mark using superscript L , i.e. V L , X L , and Y L . The majority of nodes will be unlabeled, and we mark using the superscript U , i.e. V U , X U , and Y U .</p><p>Graph Neural Networks. Most GNNs fit under the message-passing framework, where the representation h v of each node v is updated iteratively in each layer by collecting messages from its neighbors denoted as N (v). For the l-th layer, h </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MOTIVATION</head><p>GNNs have considerable inference latency due to graph dependency. One more GNN layer means fetching one more hop of neighbors. To infer a node with a L-layer GNN on a graph with average degree R requires O(R L ) fetches. R can be large for real-world graphs, e.g. 208 for the Twitter <ref type="bibr" target="#b6">(Ching et al., 2015)</ref>. Also, as layer fetching must be done sequentially, the total latency explodes quickly as L increases. <ref type="figure" target="#fig_6">Figure 1</ref> shows the dependency added by each GNN layer and the exponential explosion of inference time. In contrast, the MLP inference time is much smaller and grows linearly. This marked gap contributes greatly to the practicality of MLPs in industrial applications over GNNs.</p><p>The node-fetching latency is exacerbated by two factors: firstly, newer GNN architectures are getting deeper from 64 layers <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> to even 1001 layers <ref type="bibr" target="#b30">(Li et al., 2021)</ref>. Secondly, industrialscale graphs are frequently too large to fit into the memory of a single machine <ref type="bibr">(Jin et al., 2022)</ref>, necessitating sharding of the graph out of the main memory. For example, Twitter has 288M monthly active users (nodes) and an estimated 60B followers (edges) as of 3/2015. Facebook has 1.39B active users with more than 400B edges as of 12/2014 <ref type="bibr" target="#b6">(Ching et al., 2015)</ref>. Even when stored in a sparse-matrix-friendly format (often COO or CSR), these graphs are on the order of TBs and are constantly growing. Moving away from in-memory storage results in even slower neighbor-fetching.</p><p>MLPs, on the other hand, lack the means to exploit graph topology, which hurts their performance for node classification. For example, test accuracy on Products is 78.61 for GraphSAGE compared to 62.47 for an equal-sized MLP. Nonetheless, recent results in vision and language posit that large (or slightly modified) MLPs can achieve similar results as CNNs and Transformers <ref type="bibr" target="#b31">(Liu et al., 2021)</ref>. We thus also ask: Can we bridge the best of GNNs and MLPs to get high-accuracy and low-latency models? This motivates us to do cross-model KD from GNNs to MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offline Training with Distillation Online Prediction on New Nodes</head><p>Only Node Features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trained GNN Teacher</head><p>Distilled Knowledge</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Targets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deploy</head><p>No dependency on graph in grey New node/edges in dashed lines</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Student</head><p>Deployed GLNN <ref type="figure">Figure 2</ref>: The GLNN framework: In offline training, a trained GNN teacher is applied on the graph for soft targets. Then, a student MLP is trained on node features guided by the soft targets. The distilled MLP, now GLNN, is deployed for online predictions. Since graph dependency is eliminated for inference, GLNNs infer much faster than GNNs, and hence the name "Graph-less Neural Network."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GRAPH-LESS NEURAL NETWORKS</head><p>We introduce GLNN and answer exploration questions of its properties: 1) How do GLNNs compare to MLPs and GNNs? 2) Can GLNNs work well under both transductive and inductive settings? 3) How do GLNNs compare to other inference acceleration methods? 4) How do GLNNs benefit from KD? 5) Do GLNNs have sufficient model expressiveness? 6) When will GLNNs fail to work?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">THE GLNN FRAMEWORK</head><p>The idea of GLNN is straightforward, yet as we will see, extremely effective. In short, we train a "boosted" MLP via KD from a teacher GNN. KD was introduced in <ref type="bibr" target="#b16">Hinton et al. (2015)</ref>, where knowledge was transferred from a cumbersome teacher to a simpler student. In our case, we generate soft targets z v for each node v with a teacher GNN. Then we train a student MLP with both true labels y v and z v . The objective is as Equation 1, with ? being a weight parameter, L label being the cross-entropy between y v and student predictions? v , L teacher being the KL-divergence.</p><formula xml:id="formula_0">L = ?? v?V L L label (? v , y v ) + (1 ? ?)? v?V L teacher (? v , z v )<label>(1)</label></formula><p>The model after KD, i.e. GLNN, is essentially a MLP. Therefore, GLNNs have no graph dependency during inference and are as fast as MLPs. On the other hand, through offline KD, GLNN parameters are optimized to predict and generalize as well as GNNs, with the added benefit of faster inference and easier deployment. In <ref type="figure">Figure 2</ref>, we show the offline KD and online inference steps of GLNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EXPERIMENT SETTINGS</head><p>Datasets. We consider all five datasets used in the CPF paper <ref type="bibr" target="#b45">(Yang et al., 2021a)</ref>, i.e. Cora, Citeseer, Pubmed, A-computer, and A-photo. To fully evaluate our method, we also include two more larger OGB datasets , i.e. Arxiv and Products.</p><p>Model Architectures. For consistent results, we use GraphSAGE <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> with GCN aggregation as the teacher. We conduct ablation studies of other GNN teachers like <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>, GAT <ref type="bibr" target="#b38">(Veli?kovi? et al., 2017)</ref> and, APPNP <ref type="bibr" target="#b26">(Klicpera et al., 2019)</ref> in Section 6.</p><p>Evaluation Protocol. For all experiments in this section, we report the average and standard deviation over ten runs with different random seeds. Model performance is measured as accuracy, and results are reported on test data with the best model selected using validation data.</p><p>Transductive vs. Inductive. Given G, X, and Y L , we consider node classification under two settings: transductive (tran) and inductive (ind). For ind, we hold out some test data for inductive evaluation only. We first select inductive nodes V U ind ? V U , which partitions V U into the disjoint inductive subset and observed subset, i.e. V U = V U obs V U ind . Then we hold out v ? V U ind and all edges connected to v ? V U ind , which leads to two disjoint graphs G = G obs G ind with no shared nodes or  </p><formula xml:id="formula_1">. X = X L X U obs X U ind , and Y = Y L Y U obs Y U ind .</formula><p>Concretely, the input/output of both settings become:</p><formula xml:id="formula_2">? tran: train on G, X, and Y L ; evaluate on (X U , Y U ); KD uses z v for v ? V. ? ind: train on G obs , X L , X U obs , and Y L ; evaluate on (X U ind , Y U ind ); KD uses z v for v ? V L V U obs .</formula><p>Note that for tran, all the nodes in the graph including the validation and test nodes are used to generate z. A discussion of this choice along with other experiment details are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">HOW DO GLNNS COMPARE TO MLPS AND GNNS?</head><p>We start by comparing GLNNs to MLPs and GNNs with the same number of layers and hidden dimensions. We first consider the standard transductive setting, so our results in <ref type="table" target="#tab_0">Table 1</ref> are directly comparable to results reported in previous literature like <ref type="bibr" target="#b45">Yang et al. (2021a)</ref> and . <ref type="table" target="#tab_0">Table 1</ref>, the performance of all GLNNs improve over MLPs by large margins. On smaller datasets (first 5 rows), GLNNs can even outperform the teacher GNNs. In other words, for each task, with the same parameter budget, there exists a set of MLP parameters that has GNN-competitive performance (detailed discussion in Sections 5.6 and 5.7). For the larger OGB datasets (last 2 rows), the GLNN performance is improved over MLPs but still worse than the teacher GNNs. However, as we show in <ref type="table" target="#tab_1">Table 2</ref>, this gap can be mitigated by increasing MLP size to MLPwi 1 . In <ref type="figure" target="#fig_1">Figure 3</ref> (right), we visualize the trade-off between prediction accuracy and model inference time with different model sizes. We show that gradually increasing GLNN size pushes its performance to be close to SAGE. On the other hand, when we reduce the number of layers of SAGE 2 , the accuracy quickly drops to be worse than GLNNs. A detailed discussion of the rationale for increasing MLP sizes is in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CAN GLNNS WORK WELL UNDER BOTH TRANSDUCTIVE AND INDUCTIVE SETTINGS?</head><p>Although transductive is the commonly studied setting for node classification, it does not encompass prediction on unseen nodes. Therefore, it may not be the best way to evaluate a deployed model, which must often generate predictions for new data points as well as reliably maintain performance on old ones. Thus, to better understand the effectiveness of GLNN, we also consider their performance under a realistic production setting, which contains both transductive and inductive predictions.</p><p>To evaluate a model inductively, we hold out some test nodes from training to form an inductive set,</p><formula xml:id="formula_3">i.e. V U = V U obs V U ind .</formula><p>In production, a model might be re-trained periodically, e.g. weekly. The hold-out nodes in V U ind represent new nodes entered the graph between two trainings. V U ind is usually   small compared to V U obs -e.g. <ref type="bibr" target="#b12">Graham (2012)</ref> estimates 5-7% for the fastest-growing tech startups. In our case, to mitigate randomness and better evaluate generalizability, we use V U ind containing 20% of the test data. We also evaluate on V U obs containing the other 80% of the test data, representing the standard transductive prediction on observed unlabeled nodes, since inference is commonly redone on existing nodes in real-world cases. We report both results and a interpolated production (prod) results in <ref type="table" target="#tab_3">Table 3</ref>. The prod results paint a clearer picture of model generalization as well as accuracy in production. See Section 6 for an ablation study of different inductive split rates other than 20-80.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we see that GLNNs can still improve over MLP by large margins for inductive predictions. On 6/7 datasets, the GLNN prod performance are competitive to GNNs, which supports deploying GLNN as a much faster model with no or only slight performance loss. On the Arxiv dataset, the GLNN performance is notably less than GNNs -we hypothesize this is due to Arxiv having a particularly challenging data split which causes distribution shift between test nodes and training nodes, which is hard for GLNNs to capture without utilizing neighbor information like GNNs. However, we note that GLNN performance is substantially improved over MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">HOW DO GLNNS COMPARE TO OTHER INFERENCE ACCELERATION METHODS?</head><p>Common techniques of inference acceleration include pruning and quantization. These approaches can reduce model parameters and Multiplication-and-ACcumulation (MACs) operations. Still, they don't eliminate neighbor-fetching latency. Therefore, their speed gain on GNNs is less significant than on NNs. For GNNs, neighbor sampling is also used to reduce the fetching latency. We show an explicit speed comparison between vanilla SAGE, quantized SAGE from FP32 to INT8 (QSAGE), SAGE with 50% weights pruned (PSAGE), inference neighbor sampling with fan-out 15, and GLNN in <ref type="table" target="#tab_4">Table 4</ref>. With the same setting as <ref type="figure" target="#fig_6">Figure 1</ref>, we see that GLNN is considerably faster.</p><p>Two other kinds of methods considered as inference acceleration are GNN-to-GNN KD like TinyGNN <ref type="bibr" target="#b44">(Yan et al., 2020)</ref> and Graph Augmented-MLPs (GA-MLPs) like SGC <ref type="bibr" target="#b41">(Wu et al., 2019)</ref> or SIGN <ref type="bibr" target="#b11">(Frasca et al., 2020)</ref>. Inference of GNN-to-GNN KD is likely to be slower than a GNN-Li with the same i as the student, since there will usually be some extra overheads like the Peer-Aware Module (PAM) in TinyGNN. GA-MLPs precompute augmented node features and apply MLPs to them. With precomputation, their inference time will be the same as MLPs for dimension-preserving augmentation (SGC) and the same as enlarged MLPwi for augmentation involves concatenation (SIGN). Thus, for both kinds of approaches, it is sufficient to compare GLNN with GNN-Li and MLPwi, which we have already shown in <ref type="figure" target="#fig_1">Figure 3</ref> (left). We see that GNN-Lis are much slower than MLPs. For GA-MLPs, since full pre-computation cannot be done for inductive nodes, GA-MLPs still need to fetch neighbor nodes. This makes them much slower than MLPwi in the inductive setting, and even slower than pruned GNNs and TinyGNN as shown in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">HOW DOES GLNN BENEFIT FROM DISTILLATION?</head><p>We showed that GNNs are markedly better than MLPs on node classification tasks. But, with KD, GLNNs can often become competitive to GNNs. This indicates that there exist suitable MLP parameters which can well approximate the ideal prediction function from node features to labels. However, these parameters can be difficult to learn through standard stochastic gradient descent. We hypothesize that KD helps to find them through regularization and transfer of inductive bias.</p><p>First, we show that KD can help to regularize the student model. From loss curves of a directly trained MLP and the GLNN in <ref type="figure">Figure 4</ref>, we see the gap between training and validation loss is visibly larger for MLPs than GLNNs, and MLPs show obvious overfitting trends. Second, we analyze the inductive bias that makes GNNs powerful on node classification, which suggests that node inferences should be influenced by the graph topology. Whereas MLPs have less inductive bias. Similar difference exists between Transformers <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> and MLPs. <ref type="bibr" target="#b31">Liu et al. (2021)</ref> shows that the inductive bias in Transformers can be mitigated by a simple gate on large MLPs. For node classification, we hypothesize that KD helps to mitigate the inductive bias, so GLNNs can perform competitively. Soft labels from GNN teachers are heavily influenced by the graph topology due to inductive bias. They maintain nonzero probabilities on classes other than the ground truth provided by labels, which can be useful for the student to learn to complement the missing inductive bias in MLPs. To evaluate this hypothesis quantitatively, we define the cut loss L cut ? [0, 1] in Equation 2 to measure the consistency between model predictions and graph topology (details in Appendix C):</p><formula xml:id="formula_4">L cut = T r(? T A? ) T r(? T D? )<label>(2)</label></formula><p>Here? ? [0, 1] N ?K is the soft classification probability output by the model, A and D are the adjacency and degree matrices. When L cut is close to 1, it means the predictions and the graph <ref type="figure">Figure 4</ref>: Loss curves on CPF datasets show GLNN distillation can help to regularize the training.</p><p>Here the training loss of GLNN is on hard labels, only corresponding to the first term in Equation <ref type="formula" target="#formula_0">1.</ref> topology are very consistent. In our experiment, we observe that the average L cut for SAGE over five CPF datasets is 0.9221, which means high consistency. The same L cut for MLPs is only 0.7644, but for GLNNs it is 0.8986. This shows that the GLNN predictions indeed benefit from the graph topology knowledge contained in the teacher outputs (the full table of L cut values in Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">DO GLNNS HAVE ENOUGH MODEL EXPRESSIVENESS?</head><p>Intuitively, the addition of neighbor information makes GNNs more powerful than MLPs when classifying nodes. Thus, a natural question regarding KD from GNNs to MLPs is whether MLPs are expressive enough to represent graph data as well as GNNs. Many recent works studied GNN model expressiveness <ref type="bibr" target="#b42">(Xu et al., 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2021)</ref>. The latter analyzed GNNs and GA-MLPs for node classification and characterized expressiveness as the number of equivalence classes of rooted graphs induced by the model (formal definitions in Appendix D). The conclusion is that GNNs are more powerful than GA-MLPs, but in most real-world cases their expressiveness is indistinguishable.</p><p>We adopt the analysis framework from <ref type="bibr" target="#b3">Chen et al. (2021)</ref>  and |X | respectively. Here m denotes the max node degree, L denotes the number of GNN layers, and X denotes the set of all possible node features. The former is apparently larger which concludes that GNNs are more expressive. Empirically, however, the gap makes little difference when |X | is large. In real applications, node features can be high dimensional like bag-of-words, or even word embeddings, thus making |X | enormous. Like for bag-of-words, |X | is in the order of O(p D ), where D is the vocabulary size, and p is the max word frequency. The expressiveness of a L-layer GNN is lower bounded by |X |+m?2 m?1 <ref type="figure" target="#fig_6">1)</ref> ), but empirically, both MLPs and GNNs should have enough expressiveness given D is usually hundreds or bigger (see <ref type="table" target="#tab_7">Table 5</ref>).</p><formula xml:id="formula_5">2 L ?1 = O(p D(m?1)(2 L ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">WHEN WILL GLNNS FAIL TO WORK?</head><p>As discussed in Section 5.7 and Appendix D, the goal of GML node classification is to fit a function f on the rooted graph G <ref type="bibr">[i]</ref> and label y i . From the information theoretic perspective, fitting f by minimizing the commonly used cross-entropy loss is equivalent to maximizing the mutual information (MI), I(G <ref type="bibr">[i]</ref> ; y i ) as shown in <ref type="bibr" target="#b33">Qin et al. (2020)</ref>. If we consider G [i] as a joint distribution of two random variables X <ref type="bibr">[i]</ref> and E <ref type="bibr">[i]</ref> representing the node features and edges in G [i] respectively, we have . For example, when every node is labeled by its degree or whether it forms a triangle. Then MLPs won't be able to fit meaningful functions, and neither will GLNNs. However, such cases are typically rare, and unexpected in practical settings our work is mainly concerned with. For real GML tasks, node features and structural roles are often highly correlated <ref type="bibr" target="#b27">(Lerique et al., 2020)</ref>, hence MLPs can achieve reasonable results even only based on node features, and thus GLNNs can potentially achieve much better results. We study the failure case of GLNNs by creating a low MI scenario in Section 6.  More experiments can be found in Appendix including advanced GNN teachers (Appendix F), GA-MLP student (Appendix G), and non-homogeneous data (Appendix I).</p><formula xml:id="formula_6">I(G [i] ; y i ) = I(X [i] , E [i] ; y i ) = I(E [i] ; y i ) + I(X [i] ; y i |E [i] )<label>(3)</label></formula><p>Noisy node features. Following Section 5.8, we investigate failure cases of GLNN by adding different levels of Gaussian noise to node features to decrease their mutual information with labels. Specifically, we replace X withX = (1 ? ?)X + ? . is an isotropic Gaussian independent from X, and ? ? [0, 1] denotes the noise level. We show the inductive performance of MLP, GNN, and GLNN under different noise levels in <ref type="figure">Figure 5</ref> (left). We see that as ? increases, the accuracy of MLPs and GLNNs decrease faster than GNNs, while the performance of GLNNs and GNNs are still comparable for small ?s. When ? reaches 1,X and Y will become independent corresponding to the extreme case discussed in Section 5.8. A more detailed discussion is in Appendix J.</p><p>Inductive split rate. In Section 5.4, we use a 20-80 split of the test data for inductive evaluation. In <ref type="figure">Figure 5</ref> (middle), we show the results under different split rates (More detailed plots in Appendix H). We see that as the inductive portion increase, GNN and MLP performance stays roughly the same, and the GLNN inductive performance drops slightly. We only consider rates up to 50-50 since having 50% or even more inductive nodes is highly atypical in practice. When a large amount of new data are encountered, practitioners can opt to retrain the model on all the data before deployment.</p><p>Teacher GNN architecture. We used SAGE to represent GNNs so far. In <ref type="figure">Figure 5</ref> (right), we show results with other various GNN teachers, e.g. GCN, GAT, and APPNP. We see that GLNNs can learn from different teachers and improve over MLPs. The performance is similar for all four teachers, with the GLNN distilled from APPNP very slightly worse than others. In fact, a similar phenomenon has been observed in <ref type="bibr" target="#b45">Yang et al. (2021a)</ref> as well, i.e. APPNP benefits the student the least. One possible reason is that the first step of APPNP is to utilize the node's own feature for prediction (prior to propagating over the graph), which is very similar to what the student MLP is doing, and thus provides less additional information to MLPs than other teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we explored whether we can bridge the best of GNNs and MLPs to achieve accurate and fast GML models for deployment. We found that KD from GNNs to MLPs helps to eliminate inference graph dependency, which results in GLNNs that are 146?-273? faster than GNNs while enjoying competitive performance. We do a comprehensive study of GLNN properties. The promising results on 7 datasets across different domains show that GLNNs can be a handy choice for deploying latencyconstraint models. In our experiments, the current version of GLNNs on the Arxiv dataset doesn't show competitive inductive performance. More advanced distillation techniques can potentially improve the GLNN performance, and we leave this investigation as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILED EXPERIMENT SETTINGS</head><p>A.1 DATASETS Here we provide a detailed description of the datasets we used to support our argument. Out of these datasets, 4 of them are citation graphs. Cora, Citeseer, Pubmed, ogbn-arxiv with the node features being descriptions of the papers, either bag-of-word vector, TF-IDF vector, or word embedding vectors.</p><p>In <ref type="table" target="#tab_7">Table 5</ref>, we provided the basic statistics of these datasets. For all datasets, we follow the setting in the original paper to split the data. Specifically, for the five smaller datasets from the CPF paper, we use the CPF splitting strategy and each random seed corresponds to a different split. For the OGB datasets, we follow the OGB official splits based on time and popularity for Arxiv and Products respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MODEL HYPERPARAMETERS</head><p>The hyperparameters of GNN models on each dataset are taken from the best hyperparameters provided by the CPF paper and the OGB official examples. For the student MLPs and GLNN s, unless otherwise specified with -wi or -Li, we set the number of layers and the hidden dimension of each layer to be the same as the teacher GNN, so their total number of parameters stays the same as the teacher GNN.  We use the distillation method proposed in <ref type="bibr" target="#b16">Hinton et al. (2015)</ref> as in Equation 1, the hard labels are found to be helpful, so nonzero ?s was suggested. In our case, we did a little tuning for ? but didn't find nonzero ?s to be very helpful. Therefore, we report all of our results with ? = 0, i.e. only the second term involving soft labels is effective. More careful tuning of ? should further improve the results since the searching space is strictly larger. We implemented a weighted version in our code, and we leave the choice of ? as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 THE TRANSDUCTIVE SETTING AND THE INDUCTIVE SETTING</head><p>Given G, X, and Y L , the goal of node classification can be divided into two different settings, i.e. transductive and inductive. In real applications, the former can correspond to predict missing attributes of a user based on the user profile and other existing users, and the latter can correspond to predict labels of some new nodes that are only seen during inference time. To create the inductive setting on a given dataset, we hold out some nodes along with edges connected to these nodes during training and use them for inductive evaluation only. These nodes and edges are picked from the test data. Using notation defined above, we pick the inductive nodes V U ind ? V U , which partitions V U into the disjoint inductive subset and observed subset, i.e. V U = V U obs V U ind . Then we can take all the edges connected to nodes in V U ind to further partition the whole graph, so we end up with</p><formula xml:id="formula_7">G = G obs G ind , X = X L X U obs X U ind , and Y = Y L Y U obs Y U ind .</formula><p>We show the input and output of both settings using the notations below.</p><p>We visualize the difference between the inductive setting and the transductive setting in <ref type="figure" target="#fig_4">Figure 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 CHOOSING SOFT TARGETS UNDER THE TRANSDUCTIVE SETTING</head><p>For the transductive setting in Section 5.3, all the nodes in the graph, including the validation and test nodes, are used for the soft target generation. It seems less practical compared to the inductive case, but it is a necessary step to develop our argument. We now discuss the rationale behind this choice.</p><p>Firstly, the transductive setting is the most common setting for graph data and it was used in most GNN architecture works and GNN acceleration works we mentioned in related work. Therefore, to avoid any confusion and for a fair comparison with numbers from previous literature, we start our experiments with exactly the same input and output as the standard transductive setting. Under this setting, the inputs to GNNs include all the node features and the graph structure, so GLNN is set to be able to access the same input. As GLNN includes a teacher training step and a distillation step, the soft labels of all the nodes are intermediate outputs produced by the teacher training step, and thus used for the second distillation step for the best GLNN performance. This transductive setting can boil down to a sanity check when the student is sufficiently large. Therefore, we separate the setting to be GLNN and GLNN+ and report the results in <ref type="table" target="#tab_0">Table 1 and Table 2</ref> separately. In <ref type="table" target="#tab_0">Table  1</ref>, we are checking how well GLNNs can perform compared to GNNs under the equal-parameter constraint. The results can be interpreted as given a fixed parameter budget, whether there exists one set of parameters (one instantiation of the MLP) that can achieve competitive results as the GNN.</p><p>Only when this holds, should we further investigate the more interesting and challenging inductive case as in Section 5.4.</p><p>Secondly, the task we focus on is node classification, which in many cases is considered as semisupervised learning with very scarce labels. For example, Pubmed only uses 60 labeled nodes (20 per class) out of 20K nodes for training. Rather than design an advanced model that can do few-shot learning, our goal here is to leverage as much data as possible to simplify the model for more efficient inference. We thus utilize the soft pseudo-labels on all the unlabelled nodes for the best GLNN performance. In reality, when there is a large amount of separate unlabeled data, these unlabeled data can be used for GLNN distillation training and a different set of labeled data can be used for evaluation. In our case, we mimic this scenario in the inductive setting in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 IMPLEMENTATION AND HARDWARD DETAILS</head><p>The experiments on both baselines and our approach are implemented using PyTorch, the DGL  library for <ref type="bibr">GNN algorithms, and Adam (Kingma &amp; Ba, 2015)</ref> for optimization. We run all experiments on a machine with 80 Intel(R) Xeon(R) E5-2698 v4 @ 2.20GHz CPUs, and a single NVIDIA V100 GPU with 16GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SPACE AND TIME COMPLEXITY OF GNNS VS. MLPS</head><p>Compared to MLP and GNN, GLNN provides a handy tool for users to trade-off between model accuracy and time complexity, which does not directly focus on space complexity. Given the space and time complexity are related, we now provide a more detailed discussion regarding these two complexities in our experiments.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, the model comparison was between equal-sized MLPs (GLNNs) and GNNs. While fixing parameter budget to control space complexity is a standard approach when comparing models, it is not completely fair for cross-model comparison especially for MLPs vs. GNNs. To do inference with GNNs, the graph needs to be loaded in the memory either entirely or batch by batch, and may use much larger space than the model parameters. Thus, the actual space complexity of GNNs is much higher than equal-sized MLPs. From the time complexity perspective, the major inference latency of GNNs comes from the data dependency as shown in Section 4. Under the same setting as <ref type="figure" target="#fig_6">Figure 1</ref>, we show in <ref type="figure" target="#fig_1">Figure 3</ref> Left that even a 5-layer MLP with 8 times wider hidden layers still runs much faster than a single-layer SAGE. Another example of cross-model comparison is Transformers vs. RNNs. Large Transformers can have more parameters than RNNs because of the attention mechanism, but they are also faster than RNNs in general, which is an important consideration in the context of inference time minimization.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we saw that for equal-sized comparison, GLNNs are not as accurate as GNNs on the OGB datasets. Following the discussion above and given the GLNNs used in <ref type="table" target="#tab_0">Table 1</ref> are relatively small (3 layers and 256 hidden dimensions) for millions of nodes in the OGB datasets, we ask whether this gap can be mitigated by increasing the MLP and thus GLNN sizes. The answer is yes as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CONSISTENCY MEASURE OF MODEL PREDICTIONS AND GRAPH TOPOLOGY BASED ON MIN-CUT</head><p>We introduce a metric to measure the consistency between model predictions and graph topology based on the min-cut problem in Section 5.6. The K-way normalized min-cut problem, or simply min-cut, partitions N nodes in V into K disjoint subsets by removing the minimum volume of edges. According to <ref type="bibr" target="#b9">Dhillon et al. (2004)</ref>, the min-cut problem can be expressed as</p><formula xml:id="formula_8">max 1 K K k=1 C T k AC k C T k DC k (4) s.t. C ? {0, 1} N ?K , C1 K = 1 N</formula><p>with C being the node assignment matrix that partitions V, i.e. C i,j = 1 if node i is assigned to class j. A being the adjacency matrix and D being the degree matrix. This quantity we try to maximize here tells us whether the assignment is consistent with the graph topology. The bigger it is, the less edges need to be removed, and the assignment is more consistent with existing connections in the graph. In <ref type="bibr" target="#b0">Bianchi et al. (2019)</ref>, the authors show that when replacing the hard assignments C ? {0, 1} N ?K with a soft classification probability? ? [0, 1] N ?K , a cut loss L cut in Equation <ref type="formula" target="#formula_4">2</ref> can become a good approximation of Equation 4 and be used as the measuring metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLASSES OF ROOTED GRAPHS</head><p>In <ref type="bibr" target="#b3">Chen et al. (2021)</ref>, the expressiveness of GNNs and GA-MLPs were theoretically quantified in terms of induced equivalence classes of rooted graphs. We adopt their framework and perform a similar analysis for GNNs vs. MLPs. We first define rooted graphs.</p><p>Definition 1 (Rooted Graph). A rooted graph, denoted as G [i] is a graph with one node i in G <ref type="bibr">[i]</ref> designated as the root. GNNs, GA-MLPs, and MLPs can all be considered as functions on rooted graphs. The goal of a node-level task on node i with label y i is to fit a function to the input-output pairs (G <ref type="bibr">[i]</ref> , y i ).</p><p>We denote the space of rooted graphs as E. Following <ref type="bibr" target="#b3">Chen et al. (2021)</ref>, the expressive power of a model on graph data is evaluated by its ability to approximate functions on E. This is further characterized as the number of induced equivalence classes of rooted graphs on E, with the equivalence relation defined as the following. Given a family of functions F on E, we define an equivalence relation E,F among all rooted graphs such that</p><formula xml:id="formula_9">?G [i] , G [j] ? E, G [i] E,F G [j] if and only if ?f ? F, f (G [i] ) = f (G [j] )</formula><p>. We now give a proposition to characterize the GNN expressive power (proof in Appendix E). Proposition 1. With X denotes the set of all possible node features and assuming |X | ? 2, with m denotes the maximum node degree and assuming m ? 3, the total number of equivalence classes of rooted graphs induced by an L-layer GNN is lower bounded by |X |+m?2 m?1</p><formula xml:id="formula_10">2 L ?1 .</formula><p>As shown in Proposition 1, the expressive power of GNNs grows doubly-exponentially in the number of layers L, which means it grows linearly in L after taking log(log(?)). The expressive power GA-MLPs only grows exponentially in L as shown in <ref type="bibr" target="#b3">Chen et al. (2021)</ref>. Under this framework, the expressive power of MLPs, which corresponds to a 0-layer GA-MLP, is |X |. Since the former is much larger than the latter, the conclusion will be GNNs are much more expressive than MLPs. The gap between these two numbers indeed exists, but empirically this gap will only make a difference when |X | is small. As in <ref type="bibr" target="#b3">Chen et al. (2021)</ref>, both the lower bound proof and the constructed examples showing GNNs are more powerful than GA-MLPs assumed |X | = 2. In real applications and datasets considered in this work, the node features can be high dimensional vectors like bag-of-words, which makes |X | enormous. Thus, this gap doesn't matter much empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E PROOF OF THE PROPOSITION 1</head><p>To prove Proposition 1, we first define rooted aggregation trees, which is similar to but different from rooted graphs.</p><p>Definition 2 (Rooted Aggregation Tree). The depth-K rooted aggregation tree of a rooted graph G <ref type="bibr">[i]</ref> is a depth-K rooted tree with a (possibly many-to-one) mapping from every node in the tree to some node in G <ref type="bibr">[i]</ref> , where (i) the root of the tree is mapped to node i, and (ii) the children of every node j in the tree are mapped to the neighbors of the node in G <ref type="bibr">[i]</ref> to which j is mapped.</p><p>A rooted aggregation tree can be obtained by unrolling the neighborhood aggregation steps in the GNNs. An illustration of rooted graphs and rooted aggregation trees can be found in <ref type="bibr" target="#b3">Chen et al. (2021)</ref>  <ref type="figure">Figure 4</ref>. We denote the set of all rooted aggregation trees of depth L using T L . Then we use T L,X ,m to denote a subset of T L , where the node features belong to X , and all the nodes have exactly degree m (m children), and at least two nodes out of these m nodes have different features. In other words, a node can't have all identical children. With rooted aggregation trees defined, we are ready to prove Proposition 1. The proof is adapted from the proof of <ref type="bibr">Lemma 3 in Chen et al. (2021)</ref>.</p><p>Proof. Since the number of equivalence classes on E induced by the family of all depth-L GNNs consists of all rooted graphs that share the same rooted aggregation tree of depth-L <ref type="bibr" target="#b3">(Chen et al., 2021)</ref>, the lower bound problem in Proposition 1 can be reduced to lower bound |T L |, which can be further reduced to lower bound the subset |T L,X ,m |. We now show |T L,X ,m | ? |X |+m?2 Assuming the statement holds for L, we show it holds for L + 1 by constructing trees in T L+1,X ,m from T, T ? T L,X ,m . We do this by assigning node features in X to the m children of each leaf node in T and T . First note that when T and T are two non-isomorphic trees, two depth-L+1 trees constructed from T and T will be different no matter how the node features are assigned. Now we consider all the trees can be constructed from T by assign node features of children to leaf nodes.</p><p>We first consider all paths from the root to leaves in T . Each path consists of a sequence of nodes where the node features form a one-to-one mapping to an L-tuple ? ? {(x 1 , . . . , x L ) : x i ? X }. Leaf nodes are called node under ? if the path from the root to it corresponds to ? . The children of nodes under different ? s are always distinguishable, and thus any assignments lead to distinct rooted aggregation trees of depth L + 1. The assignment of children of nodes under the same ? , on the other hand, could be overcounted. Therefore, to lower bound T L+1,X ,m , we only consider a special way of assignments to avoid over counting, which is that children of all nodes under the same ? are assigned the same set of features.</p><p>Since we assumed that at least two nodes of T have different features, there are at least 2 L different ? s corresponding to the path from the root to leaves. For a leaf node j under a fixed ? , one of its children needs to have the same feature as j's parent node. This restriction is due to the definition of rooted aggregation trees. Therefore, we only pick features for the other m ? 1 nodes, which will be |X |+m?2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ADVANCED GNN ARCHITECTURES AS THE TEACHER</head><p>In our experiment, SAGE teacher is used throughout to avoid influence by model architecture. Some other GNNs like GCN are also considered in the ablation studies, but they are not the best known architecture for a specific dataset. To show GLNN has stronger performance given a stronger teacher, we consider the best teacher we can access on Products. We take <ref type="bibr">MLP+C&amp;S Huang et al. (2021)</ref> from the OGB leaderboard as a new teacher, which has reported accuracy 84.18% and ranks #8 on the leadarboard as of Nov 2021. We choose MLP+C&amp;S instead of the other top 7 because the others either rely on raw text (additional info to the given node feature), or require a large GPU with &gt;16GB memory, which we don't have access to. Also, their improvement is not super significant compared to MLP+C&amp;S, i.e. 84% to 86%. The result with MLP+C&amp;S teacher is shown in <ref type="table" target="#tab_12">Table 9</ref>. We see that with the new teacher, performance of GLNN+ improves to be even better than SAGE (78.61%), which shows GLNN can get stronger given a stronger teacher. In our main experiment, the inductive performance of GLNN on the Arxiv dataset is less desirable than others. We thus consider augment the node features with their one-hop neighbors to include more graph information. This can be seen as a middle ground between pure GLNNs and GNNs. For this new experiment, we follow the setting in <ref type="table" target="#tab_3">Table 3</ref> but with two new approaches. We explain the setting of these two approaches below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H MODEL PERFORMANCE UNDER DIFFERENT INDUCTIVE SPLIT RATE</head><p>This section is a continuation of the ablation study of inductive split rate in Section 6. It generalizes <ref type="figure">Figure 5</ref> Middle to more split rates (from 10:90 to 90:10), and explicitly show the inductive and transductive performance on each dataset. For better visualization, the training data label rate is also reduced from 20 per class to 5 per class in the following plots.  I GLNN UNDER NODE FEATURE HETEROGENEITY AND NON-HOMOPHILY Besides the 7 datasets used in the main experiments, we consider 4 more datasets from <ref type="bibr">Ivanov &amp; Prokhorenkova (2021)</ref> and  to further evaluate GLNN.</p><p>The House_class and VK_class datasets are from <ref type="bibr">Ivanov &amp; Prokhorenkova (2021)</ref>. The node features of these two graphs are based on tabular data, which have different types, scales, and meanings as the opposite of the bag-of-word node features in Cora and etc. Some basic statistics of the datasets are shown in the following table.   Using the GCN teacher, we see that the performance of GLNN is improved over MLP and becomes competitive to the teacher GCN on Penn94. However, on Pokec, the simple LINK model can achieve very good performance, and it is better than most GNNs reported in . LINK is a purely structural model which does not use node features at all. This shows that the Pokec dataset corresponds to the setting we discussed in Sec 5.8 (limitations of GLNN) -if the node labels can be largely determined by only the graph structure, then GLNN will struggle. We observe that GLNN is not as good as LINK owing to this limitation. However, we still see that for most of the non-homophilous datasets, MLPs already work quite well on them, and we can use GLNN for the other ones like Penn94. In Section 6, we conducted an ablation study to compare model performance with noisy node features, and the result is shown in the left plot in <ref type="figure">Figure 5</ref>. There are two subtle points in this plot. <ref type="formula" target="#formula_0">(1)</ref> The performance of GNN is still relatively high for high noisy features, even when ? = 1 and the features are completely random.</p><p>(2) For completely random features, the performance of GLNN is still higher than MLP. We now discuss and explain them in more detail.</p><p>GNN Performance on Random Features. GNN still performs well because nodes with the same labels are likely to be connected and GNN can overfit the training data. We explain the detail through a toy example. Suppose there is a 4-clique containing nodes A, B, C, D in the graph with only a single edge D-E connects this clique to other graph nodes. Suppose A, B, C, D all have iid random Gaussian raw features and the same class label c. Let's pick A to be the inductive test node and assume E and the triangle formed by B, C, D to be in the training graph. Let's consider a simple example for 1-layer GCN and break down message passing into feature aggregation and nonlinear transformation. During training, GNN can overfit the data by learning a nonlinear transformation which maps the aggregated features of B, C, D to class c. The aggregated features of B and C will just be the average of the raw features of B, C, D. Although E is also involved in D's feature aggregation step, the aggregated features of D will also be very close to this average. Then when test on A, the aggregated feature of A will likely be classified to the same class c by the overfitted nonlinear transformation because it is the average of raw node features of A, B, C, D. In this case, GNN can actually correctly classify A because of the overfitting. For GNNs with more layers and graphs with more neighbor nodes, the conclusion may be generalized.This is roughly sort of a "majority vote" process. For a test node A, if many nodes, which A collects features from, have the same class label and appear in the training graph, then A will be classified as this class by an overfitted classifier.</p><p>GLNN and MLP Performance on Random Features. The gap between MLP and GLNN is due to imbalanced datasets. The GLNN can learn the imbalance from soft labels, whereas MLPs can only access uniformly picked training nodes. We explain more detail using the A-computer dataset as an example, for which the gap between MLP and GLNN is obvious. The task is 10-class classification. With random node features (?=1), the inductive accuracy for MLP is 0.0652 and 0.2538 for GLNN.</p><p>If the data labels are uniform, then both models should give an accuracy around 0.1. However, the labels on the inductive dataset are actually imbalanced. We show the results in <ref type="figure" target="#fig_9">Figure 9</ref>. The hist on the left is the label distribution of the inductive test set. In particular, class 4 takes about 40%. However, given this imbalance, the standard train-test split selects training nodes uniformly among labels. In this case, 20 nodes per class. Therefore, the predictions of MLP on random features are expected to be relatively uniform because the 200 nodes we train it on are uniform. This gives the hist shown in the middle, where the largest class takes about 17.5%. Finally, for GLNN, we train it on all the 200 training nodes with hard labels, plus soft labels of other nodes in the observed graph G obs (see Section 5.2). Since these extra nodes are selected randomly, whose label distribution is actually similar to the label distribution on the whole data and the distribution on the inductive test set. Therefore, we get the GLNN predictions hist on the right. Although for each node, we can't assign a prediction correlated to its feature, on average the distribution is very close to the true label distribution on the inductive test set and has a much higher expectation. In fact, if the prediction distribution is exactly the true distribution on the inductive test set, the expectation will be 0.2169. GLNN actually does even a bit better by putting its bet more on the largest class. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>u = x u ) via an aggregation operation AGGR followed by an UPDATE operation as h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Enlarged MLPs (GLNNs) can match GNN accuracy, but infer dramatically faster. Plots are under the same setting as Figure 1. Left: inference time of MLPs vs. GNN (SAGE) for different model sizes. Right: model accuracy vs. inference time. Note: time axes are log-scaled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and show in Appendix D that the number of equivalence classes induced by GNNs and MLPs are |X |+m?2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The transductive setting and inductive setting illustrated by a 2-layer GNN. The middle shows the original graph used for training. The left shows the transductive setting, where the test node is in red and within the graph. The right shows the inductive setting, where the test node is an unseen new node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 , 1 m?</head><label>11</label><figDesc>the root of the tree can have |X | different choices. For the children nodes, we pick m features from |X | and repetitions are allowed. This leads to |X |+m?1 m cases. Therefore, T L+1,X ,m = |X | |X |+m?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>m? 1</head><label>1</label><figDesc>cases for each j. Then through this construction, the total number of depth-L+1 trees from T can be lower bounded by |X |+m?2 m?1 2 L . Finally, we have this lower bound holds for all T ? T L,X ,m , so we derive T L+1,X ,m ? |X |+m?2 m?1 2 L T L,X ,m , and T L,X ,m ? |X |+m?2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Model inductive performance comparison between MLP, GNN(SAGE), and GLNN under different inductive split rate in the production setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Model transductive performance comparison between MLP, GNN(SAGE), and GLNN under different inductive split rate in the production setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Inductive (predicted)  label distribution on the A-computer dataset. Left: true labels. Middle: predicted labels by MLP. Right: predicted labels by GLNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>GLNNs outperform MLPs by large margins and match GNNs on 5 of 7 datasets under the transductive setting. ? M LP (? GN N ) represents difference between the GLNN and a trained MLP (GNN). Results show accuracy (higher is better); ? GN N ?0 indicates GLNN outperforms GNN.</figDesc><table><row><cell>Datasets</cell><cell>SAGE</cell><cell>MLP</cell><cell>GLNN</cell><cell>? M LP</cell><cell>? GN N</cell></row><row><cell>Cora</cell><cell cols="5">80.52 ? 1.77 59.22 ? 1.31 80.54 ? 1.35 21.32 (36.00%) 0.02 (0.02%)</cell></row><row><cell>Citeseer</cell><cell cols="5">70.33 ? 1.97 59.61 ? 2.88 71.77 ? 2.01 12.16 (20.40%) 1.44 (2.05%)</cell></row><row><cell>Pubmed</cell><cell cols="4">75.39 ? 2.09 67.55 ? 2.31 75.42 ? 2.31 7.87 (11.65%)</cell><cell>0.03 (0.04%)</cell></row><row><cell cols="6">A-computer 82.97 ? 2.16 67.80 ? 1.06 83.03 ? 1.87 15.23 (22.46%) 0.06 (0.07%)</cell></row><row><cell>A-photo</cell><cell cols="5">90.90 ? 0.84 78.77 ? 1.74 92.11 ? 1.08 13.34 (16.94%) 1.21 (1.33%)</cell></row><row><cell>Arxiv</cell><cell cols="4">70.92 ? 0.17 56.05 ? 0.46 63.46 ? 0.45 7.41 (13.24%)</cell><cell>-7.46 (-10.52%)</cell></row><row><cell>Products</cell><cell cols="4">78.61 ? 0.49 62.47 ? 0.10 68.86 ? 0.46 6.39 (10.23%)</cell><cell>-9.75 (-12.4%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Enlarged GLNNs match the performance of GNNs on the OGB datasets.</figDesc><table><row><cell>For Arxiv, we use</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Datasets</cell><cell cols="2">Eval SAGE</cell><cell>MLP/MLP+</cell><cell cols="2">GLNN/GLNN+ ?MLP</cell><cell>?GNN</cell></row><row><cell cols="6">Cora 19.30 Citeseer prod 79.29 58.98 78.28 prod 68.38 59.81 69.27 9.46 (15.82%)</cell><cell>0.89 (1.30%)</cell></row><row><cell></cell><cell>ind</cell><cell cols="3">69.75 ? 3.59 60.06 ? 5.00 69.25 ? 2.25</cell><cell>9.19 (15.30%)</cell><cell>-0.5 (-0.7%)</cell></row><row><cell></cell><cell>tran</cell><cell cols="3">68.04 ? 3.34 59.75 ? 2.48 69.28 ? 3.12</cell><cell>9.63 (15.93%)</cell><cell>1.24 (1.82%)</cell></row><row><cell>Pubmed</cell><cell cols="2">prod 74.88</cell><cell>66.80</cell><cell>74.71</cell><cell>7.91 (11.83%)</cell><cell>-0.17 (-0.22%)</cell></row><row><cell></cell><cell>ind</cell><cell cols="3">75.26 ? 2.57 66.85 ? 2.96 74.30 ? 2.61</cell><cell>7.45 (11.83%)</cell><cell>-0.96 (-1.27%)</cell></row><row><cell></cell><cell>tran</cell><cell cols="3">74.78 ? 2.22 66.79 ? 2.90 74.81 ? 2.39</cell><cell>8.02 (12.01%)</cell><cell>0.03 (0.04%)</cell></row><row><cell cols="3">A-computer prod 82.14</cell><cell>67.38</cell><cell>82.29</cell><cell cols="2">14.90 (22.12%) 0.15 (0.19%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell cols="2">15.36 (22.79%) 0.48 (0.58%)</cell></row><row><cell>A-photo</cell><cell cols="2">prod 91.08</cell><cell>79.25</cell><cell>92.38</cell><cell></cell></row><row><cell>Arxiv</cell><cell cols="2">prod 70.73</cell><cell>55.30</cell><cell>65.09</cell><cell>9.79 (17.70%)</cell><cell>-5.64 (-7.97%)</cell></row><row><cell></cell><cell>ind</cell><cell cols="3">70.64 ? 0.67 55.40 ? 0.56 60.48 ? 0.46</cell><cell>4.3 (7.76%)</cell><cell>-10.94 (-15.49%)</cell></row><row><cell></cell><cell>tran</cell><cell cols="3">70.75 ? 0.27 55.28 ? 0.49 71.46 ? 0.33</cell><cell cols="2">11.16 (20.18%) -4.31 (-6.09%)</cell></row><row><cell>Products</cell><cell cols="2">prod 76.60</cell><cell>63.72</cell><cell>75.77</cell><cell cols="2">12.05 (18.91%) -0.83 (-1.09%)</cell></row><row><cell></cell><cell>ind</cell><cell cols="3">76.89 ? 0.53 63.70 ? 0.66 75.16 ? 0.34</cell><cell cols="2">11.44 (17.96%) -1.73 (-2.25%)</cell></row><row><cell></cell><cell>tran</cell><cell>76.53 ?0.55</cell><cell cols="2">63.73 ? 0.69 75.92 ? 0.61</cell><cell cols="2">12.20 (19.15%) -0.61 (-0.79%)</cell></row></table><note>GLNNs match GNN performance on a production setting with both inductive and trans- ductive predictions. We use MLP for the 5 CPF datasets, MLPw4 for Arxiv, and MLPw8 for Products. ind results on V U ind , tran results on V U obs , and the interpolated prod results are reported.(32.72%) -1.01 (-1.28%) ind 81.33 ? 2.19 59.09 ? 2.96 73.82 ? 1.93 14.73 (24.93%) -7.51 (-9.23%) tran 78.78 ? 1.92 58.95 ? 1.66 79.39 ? 1.64 20.44 (34.66%) 0.61 (0.77%)ind 82.08 ? 1.79 67.84 ? 1.78 80.92 ? 1.36 13.08 (19.28%) -1.16 (-1.41%) tran 82.15 ? 1.55 67.27 ? 1.36 82.63 ? 1.13.13 (16.57%) 1.30 (1.42%) ind 91.50 ? 0.79 79.44 ? 1.72 91.18 ? 0.81 11.74 (14.78%) -0.32 (-0.35%) tran 90.80 ? 0.77 79.20 ? 1.64 92.68 ? 0.56 13.48 (17.01%) 1.70 (1.87%)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>While other inference acceleration methods speed up SAGE, they are considerably slower than GLNNs. Numbers (in ms) are inductive inference time on 10 randomly chosen nodes.</figDesc><table><row><cell>Datasets</cell><cell>SAGE</cell><cell>QSAGE</cell><cell>PSAGE</cell><cell cols="2">Neighbor Sample GLNN+</cell></row><row><cell>Arxiv</cell><cell>489.49</cell><cell>433.90 (1.13?)</cell><cell>465.43 (1.05?)</cell><cell>91.03 (5.37?)</cell><cell>3.34 (146.55?)</cell></row><row><cell cols="5">Products 2071.30 1946.49 (1.06?) 2001.46 (1.04?) 107.71 (19.23?)</cell><cell>7.56 (273.98?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Figure 5: Left: Node feature noise. GLNN has comparable performance to GNNs only when nodes are less noisy. Adding more noise decreases GLNN performance faster than GNNs. Middle: Inductive split rate. Altering the inductive:transductive ratio in the production setting doesn't affect the accuracy much. Right: Teacher GNN architecture. GLNNs can learn from different GNN teachers to improve over MLPs and achieve comparable results. Accuracies are averaged over five CPF datasets.</figDesc><table /><note>6 ABLATION STUDIES In this section, we do ablation studies of GLNNs on node feature noise, inductive split rates, and teacher GNN architecture. Reported results are test accuracies averaged over five datasets in CPF.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Dataset Statistics.</figDesc><table><row><cell>Dataset</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="2"># Features # Classes</cell></row><row><cell>Cora</cell><cell>2,485</cell><cell>5,069</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>2,110</cell><cell>3,668</cell><cell>3,703</cell><cell>6</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>3</cell></row><row><cell cols="2">A-computer 13,381</cell><cell>245,778</cell><cell>767</cell><cell>10</cell></row><row><cell>A-photo</cell><cell>7,487</cell><cell>119,043</cell><cell>745</cell><cell>8</cell></row><row><cell>Arxiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell></row><row><cell>Products</cell><cell cols="3">2,449,029 61,859,140 100</cell><cell>47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for GNNs on five datasets from the CPF paper.</figDesc><table><row><cell></cell><cell cols="4">SAGE GCN GAT APPNP</cell></row><row><cell># layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>hidden dim</cell><cell>128</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell>learning rate</cell><cell>0.01</cell><cell>0.01</cell><cell cols="2">0.01 0.01</cell></row><row><cell>weight decay</cell><cell cols="4">0.0005 0.001 0.01 0.01</cell></row><row><cell>dropout</cell><cell>0</cell><cell>0.8</cell><cell>0.6</cell><cell>0.5</cell></row><row><cell>fan out</cell><cell>5,5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>attention heads</cell><cell>-</cell><cell>-</cell><cell>8</cell><cell>-</cell></row><row><cell cols="2">power iterations -</cell><cell>-</cell><cell>-</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for GraphSAGE on OGB datasets. For GLNN s we do a hyperparameter search of learning rate from [0.01, 0.005, 0.001], weight decay from [0, 0.001, 0.002, 0.005, 0.01], and dropout from [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6] A.3 KNOWLEDGE DISTILLATION</figDesc><table><row><cell>Dataset</cell><cell>Arxiv</cell><cell>Products</cell></row><row><cell># layers</cell><cell>3</cell><cell>3</cell></row><row><cell>hidden dim</cell><cell>256</cell><cell>256</cell></row><row><cell>learning rate</cell><cell>0.01</cell><cell>0.003</cell></row><row><cell cols="2">weight decay 0</cell><cell>0</cell></row><row><cell>dropout</cell><cell>0.2</cell><cell>0.5</cell></row><row><cell cols="2">normalization batch</cell><cell>batch</cell></row><row><cell>fan out</cell><cell cols="2">[5, 10, 15] [5, 10, 15]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>GLNN predictions are much more consistent with the graph topology than MLPs. We show the L cut values of GNNs, MLPs, and GLNN s on five CPF datasets. GLNN L cut values become pretty close to the high L cut values of GNNs, which were closely related to the GNN inductive bias.</figDesc><table><row><cell>Datasets</cell><cell>SAGE MLP</cell><cell>GLNN</cell></row><row><cell>Cora</cell><cell cols="2">0.9347 0.7026 0.8852</cell></row><row><cell>Citeseer</cell><cell cols="2">0.9485 0.7693 0.9339</cell></row><row><cell>Pubmed</cell><cell cols="2">0.9605 0.9455 0.9701</cell></row><row><cell cols="3">A-computer 0.9003 0.6976 0.8638</cell></row><row><cell>A-photo</cell><cell cols="2">0.8664 0.7069 0.8398</cell></row><row><cell>Average</cell><cell cols="2">0.9221 0.7644 0.8986</cell></row></table><note>D EXPRESSIVENESS OF GNNS VS. MLPS IN TERMS OF EQUIVALENCE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>GLNN+ with MLP+C&amp;S teacher on Products</figDesc><table><row><cell cols="3">MLP+C&amp;S MLP+ GLNN+</cell></row><row><cell>Acc 84.18</cell><cell>64.50</cell><cell>82.94</cell></row></table><note>G GLNN WITH FEATURE AUGMENTATION FROM ONE-HOP NEIGHBORS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Statistics of dataset with heterogeneous node featuresWe apply the GLNN on House_class and VK_class using the best BGNN model fromIvanov  &amp; Prokhorenkova (2021)  as the teacher. The comparison is shown in the following table.Ivanov &amp;  Prokhorenkova (2021)  also includes GAT, GCN, AGNN, and APPNP as baselines, whose performance on these two datasets are quite similar (difference &lt; 0.025). We compare with these baselines by including the best result among the 4 GNN models and refer it as GNN in the table below, i.e. GNN = max(GAT, GCN, AGNN, APPNP). From the table, we see that GLNN can improve from MLP, outperform GNN and LightGBM, and become competitive to the teacher BGNN.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Nodes # Edges # Features # Classes</cell></row><row><cell cols="2">House_class 20,640</cell><cell>182,146 6</cell><cell>5</cell></row><row><cell>VK_class</cell><cell>54,028</cell><cell>213,644 14</cell><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>GLNN on datasets with heterogeneous node features. Numbers other than GLNN are taken We further pick the non-homophilous Penn94 and Pokec datasets from. Some basic statistics of the datasets are shown in the following table.</figDesc><table><row><cell cols="2">from Ivanov &amp; Prokhorenkova (2021)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">LightGBM GNNs BGNN MLP GLNN</cell></row><row><cell cols="2">House_class 0.55</cell><cell>0.625</cell><cell>0.682</cell><cell>0.534 0.672</cell></row><row><cell>VK_class</cell><cell>0.57</cell><cell>0.577</cell><cell>0.683</cell><cell>0.567 0.641</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Statistics of non-homophilous datasets</figDesc><table><row><cell cols="2">Dataset # Nodes</cell><cell># Edges</cell><cell cols="2"># Features # Classes</cell></row><row><cell cols="2">Penn94 41,536</cell><cell>1,590,655</cell><cell>5</cell><cell>2</cell></row><row><cell>Pokec</cell><cell cols="3">1,632,803 30,622,564 65</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>GLNN on non-homophilous datasets. Numbers other than GLNN are taken from Lim et al. (2021) Dataset LINK GCN MLP GLNN Penn94 80.79 82.47 73.61 81.69 Pokec 80.54 75.45 62.37 61.32 J MODEL COMPARISON WITH NOISY NODE FEATURES</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">-wi means i-times wider hidden layers, e.g. hidden layers of MLPw4 are 4-times wider than the given MLP. 2 -Li is used to explicitly note a model with i layers, e.g. SAGE-L2 represents a 2-layer SAGE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">L ?1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(GML) research and have shown great results on node classification tasks <ref type="bibr" target="#b25">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b38">Veli?kovi? et al., 2017)</ref> like product prediction on co-purchasing graphs and paper category prediction on citation graphs. However, for large-scale industrial applications, MLPs remain the major workhorse, despite common (implicit) underlying graphs and suitability for GML formalisms. One reason for this academic-industrial gap is the challenges in scalability and deployment brought by data dependency in GNNs <ref type="bibr" target="#b21">Jia et al., 2020)</ref>, which makes GNNs hard to deploy for latency-constrained applications that require fast inference.</p><p>Neighborhood fetching caused by graph dependency is one of the major sources of GNN latency. Inference on a target node necessitates fetching topology and features of many neighbor nodes, especially on small-world graphs (detailed discussion in Section 4). Common inference acceleration techniques like pruning  and quantization <ref type="bibr" target="#b34">(Tailor et al., 2021;</ref><ref type="bibr" target="#b48">Zhao et al., 2020)</ref> can speed up GNNs to some extent by reducing Multiplication-and-ACcumulation (MAC) operations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mincut pooling in graph neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.00481" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rytstxWAW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On graph neural networks versus graph-augmented {mlp}s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tiqI7w64JG2" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/chen20v.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2016.40</idno>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One trillion edges: Graph processing at facebook-scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avery</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Kabiljo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dionysios</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sambavi</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1804" to="1815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph-free knowledge distillation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel k-means: Spectral clustering and normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1014052.1014118</idno>
		<ptr target="https://doi.org/10.1145/1014052.1014118" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;04</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Repmlp: Re-parameterizing convolutions into fully-connected layers for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sign: Scalable inception graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://www.paulgraham.com/growth.html" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritish</forename><surname>Narayanan</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v37/gupta15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/2005.00687</idno>
		<ptr target="https://arxiv.org/abs/2005.00687" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph-mlp: Node classification without message passing in graph. CoRR, abs/2106.04051</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.04051" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Benson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Boost then convolve: Gradient boosting meets graph neural networks. CoRR, abs/2101.08543, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liudmila</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2101" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Redundancy-free computation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph condensation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Proteus: Exploiting numerical precision variability in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tayler</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><forename type="middle">Enright</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Supercomputing</title>
		<meeting>the 2016 International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint embedding of structure and features via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Lerique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">Levy</forename><surname>Abitbol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?rton</forename><surname>Karsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno>abs/2104.01404</idno>
		<ptr target="https://arxiv.org/abs/2104" />
		<title level="m">New benchmarks for learning on nonhomophilous graphs. CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pay attention to mlps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rethinking softmax with cross-entropy: Neural network classifier as mutual information estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Degree-quant: Quantization-aware training for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Shyam Anil Tailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Donald</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lane</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=NSBrFgJAHg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast adaptation for cold-start collaborative filtering with meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graphsail: Graph structure aware incremental learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412754</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412754" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information; Knowledge Management, CIKM &apos;20</title>
		<meeting>the 29th ACM International Conference on Information; Knowledge Management, CIKM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2861" to="2868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tinygnn: Learning efficient graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bencheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining (KDD)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1848" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Extract the knowledge of graph neural networks and go beyond it: An effective knowledge distillation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Distilling knowledge from graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Agl: a scalable system for industrial-purpose graph machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02454</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learned low precision graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateja</forename><surname>Jamnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Accelerating large scale real-time GNN inference using channel pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<idno>abs/2105.04528</idno>
		<ptr target="https://arxiv.org/abs/2105.04528" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07323</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">GA-MLP: firstly, for each node v, we collect features of its 1-hop neighbors u to augment the raw feature of v, i.e. x v ?x v , like in SGC. Then we train an MLP on the graph withx v . Note if v is in the observed graph but u is in the inductive (unobserved during training) part</title>
		<imprint/>
	</monogr>
	<note>then v doesn&apos;t collect features from u</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">GA-GLNN: Go through the same feature augmentation step as 1-hop GA-MLP. Then train an MLP with distillation from teacher GNN</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<title level="m">summary, we compare 5 different models in the table below (a) SAGE: single model on x v (b) MLP: single model on x v (c) GLNN: SAGE teacher and MLP student on x v (d) 1-hop GA-MLP: single model onx v (e) 1-hop GA-GLNN: SAGE teacher on x v , MLP student onx v</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">We show in the table below, with 1-hop neighbor features, performance of GLNN improves a lot. This is expected as we also observe significant improvement from MLP to 1-hop GA-MLP. However, we indeed see 1-hop GA-GLNN (68.83) can further improve from 1-hop GA-MLP (66.62) and nearly match the teacher</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">GLNN with feature augmentation from one-hop neighbor on Arxiv Eval SAGE MLP GLNN 1-hop GA-MLP 1-hop GA-GLNN</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">As we have shown in Figure 3, the 1-Layer GNN in our case is roughly 4 times slower than GLNN (29.31ms vs. 7.56ms), which should be a good approximation for the speed comparison between 1-hop GA-MLP/GA-GLNN and GLNN. This result is practically beneficial, as it gives practitioners more flexibility about how much accuracy they</title>
		<imprint/>
	</monogr>
	<note>want to trade for less inference time</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
