<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPARSE FACTORIZATION OF LARGE SQUARE MATRICES A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Khalitov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPARSE FACTORIZATION OF LARGE SQUARE MATRICES A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>matrix factorization ? square matrix ? attention ? sparse</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Square matrices appear in many machine learning problems and models. Optimization over a large square matrix is expensive in memory and in time. Therefore an economic approximation is needed. Conventional approximation approaches factorize the square matrix into a number matrices of much lower ranks. However, the low-rank constraint is a performance bottleneck if the approximated matrix is intrinsically high-rank or close to full rank. In this paper, we propose to approximate a large square matrix with a product of sparse full-rank matrices. In the approximation, our method needs only N (log N ) 2 non-zero numbers for an N ? N full matrix. We present both non-parametric and parametric ways to find the factorization. In the former, we learn the factorizing matrices directly, and in the latter, we train neural networks to map input data to the non-zero matrix entries. The sparse factorization method is tested for a variety of synthetic and real-world square matrices. The experimental results demonstrate that our method gives a better approximation when the approximated matrix is sparse and high-rank. Based on this finding, we use our parametric method as a scalable attention architecture that performs strongly in learning tasks for long sequential data and defeats Transformer and its several variants. Our code is publicly available 3 .</p><p>Keywords matrix factorization ? square matrix ? attention ? sparse 1 Introduction Many machine learning models include one or more square matrices, such as the kernel matrix in Support Vector Machines <ref type="bibr" target="#b5">[Cortes and Vapnik, 1995]</ref> or Gaussian Process, the affinity matrix in graph or network representation, and the attention matrix in transformers <ref type="bibr" target="#b21">[Vaswani et al., 2017a]</ref>. In many machine learning problems, the solution space is also over square matrices, for example, graph matching and network architecture optimization.</p><p>Obtaining a full N ? N matrix can cause infeasible storage and computational cost if N is large. For example, a double-precision kernel matrix of the typical MNIST handwritten digit data set (N = 70000) requires about 36.5G memory. In another example, we need to calculate eight quintillion float numbers to fill a full attention matrix of a human DNA sequence with about 3.2 billion base pairs. Therefore we need economical surrogates to approximate the full square matrices at a large scale. Matrix factorization is a widely used approach that approximately factorizes the square matrix to some cheaper matrices. In conventional matrix factorization, the factorizing matrices must be low-rank, for example, in Truncated Singular Value Decomposition and Nystr?m approximation <ref type="bibr" target="#b24">[Williams and</ref> Seeger, 2001, Drineas and<ref type="bibr" target="#b6">Mahoney, 2005]</ref>. However, these conventional approaches do not work well if the square matrix in the approximation is not low-rank. This paper proposes a novel approximation method called Sparse Factorization (SF), where the approximated square matrix is factorized into some full-rank sparse matrices. Using the Chord protocol <ref type="bibr" target="#b18">[Stoica et al., 2001</ref>] to specify the non-zero entry positions 4 , we can match an N ? N full matrix with log N factorizing matrices, where each contains * Equal contribution ? Corresponding author, zhirong.yang@ntnu.no 3 https://github.com/RuslanKhalitov/SparseFactorization 4 Non-zero entries are those stored, including both non-zeros and explicit zeros.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Examples of low-rank matrix dense matrix factorization. Each small square represents a matrix entry. Left is a two-factor factorization and the right is a four-factor factorization. The trapezoids illustrate the "bottleneck", i.e. grouping the factorizing matrices according the place with the smallest rank.</p><p>N log N non-zero entries and thus in total N (log N ) 2 non-zeros. Therefore the approximation is economical when N is large.</p><p>We first study non-parametric learning of the approximation. That is, we directly minimize the approximation loss over the sparse factorizing matrices. The new method was tested on a variety of synthetic and real-world square matrices, with comparison to the most accurate low-rank approximation method, TSVD. We find that SF is especially suitable for approximating sparse square matrices with unknown non-zero positions, with a clear win over TSVD.</p><p>The Chord protocol also enables us to parameterize the mapping from input data to the non-zero entries in a matrix row with neural networks. This parametric form of SF thus provides a new attention architecture. We find that only one block of SF attention defeats Transformer and its several state-of-the-art variants on very long synthetic sequence classification and on the public Long Range Arena benchmark tasks.</p><p>The remaining of the paper is organized as follows. We first briefly review the matrix approximation based on low-rank factorization in Section 2. In Section 3, we present the machine learning formulation of sparse factorization of square matrices. The parametric formulation using neural networks is proposed in Section 4. In Section 5, we present the experimental settings and results. Conclusion and future work are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Low-Rank Matrix Factorization</head><p>The conventional matrix approximation is to factorize the large square matrix X into a few low-rank matrices, for example X ? X = W H or X ? W SH with W ? R N ?r , S ? R r?r , H ? R r?N and r N . Nystr?m decomposition <ref type="bibr" target="#b24">[Williams and Seeger, 2001]</ref> is of the tri-factor kind, where W and H are calculated using normal kernel function, and S is learned.</p><p>The approximation error can be measured by a certain divergence, for example the squared Frobenius norm (F-norm):</p><formula xml:id="formula_0">D(X|| X) = X ? X 2 F = ij X ij ? X ij 2 .</formula><p>Minimization of X ? W H 2 F over W and H has a closed-form solution using Truncated Singular Value Decomposition (TSVD). Denote ? the diagonal matrix with the largest r singular values. The corresponding left and right singular vectors as columns form matrices U and V , respectively. Then the minimum appears by setting W = U and H = ?V T , where we can move any scaling from W to H or vice versa.</p><p>In general X = M m=1 W (m) , where W (m) ? R rm,rm+1 with r 1 = r M +1 = N . We can always reduce a multi-factor case to the two-factor form by grouping L = m ?1 m=1 W (m) and R = M m=m W (m) , where m = arg min m {r m } M m=2 and X ? LR. It is required that r m N when approximating large square matrices. Otherwise, the factorizing matrices are still too large. This two-factor form reveals a bottleneck as illustrated in <ref type="figure">Figure 1</ref> (bottom), which shows that the approximation quality of all variants of low-rank matrix factorization cannot exceed TSVD(X, r m ) in terms of Frobenius norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sparse Factorization</head><p>As we have seen, the performance of LRMF is capped due to the low-rank constraint. Its performance is poor if the approximated square matrix is far from low-rank, i.e., the sum of the few largest eigenvalues does not dominate the matrix trace. Grayscale squares in the factorizing matrices represent stored entries (non-zeros and explicit zeros), and completely white squares represent non-stored entries (implicit zeros).</p><p>Here we propose a new approximation method that is free of the low-rank constraint. Our method implements the approximation with a number (M ) of square and sparse factorizing matrices. The approximation is still economical if the total number of non-zero entries is much smaller than N 2 .</p><p>There are many ways to specify the sparse structure (i.e. the non-zero positions). A good specification should guarantee that the product of the factorizing matrices, X, should be a full matrix. The condition prevents that the approximating matrix contains some always-zero entries. In addition, we consider a secondary requirement that each factorizing matrix has the same sparse structure, which provides better symmetry in the approximation.</p><p>We thus adopt a modified Chord protocol <ref type="bibr" target="#b18">[Stoica et al., 2001]</ref> which originates for peer-to-peer distributed hash tables. In the protocol, the indices from 1 to N are organized in a circular graph, where the ith node connects to itself and the ((i + 2 k ) mod N )-th nodes with k = 0, . . . , K ? 2. We set K = log 2 N in our work. The Chord protocol is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (top).</p><p>We use the Chord protocol to specify the non-zero positions in each factorizing matrix, where each matrix row has log 2 N non-zero entries. Thus every factorizing matrix has N log 2 N non-zeros. Note that the product of the factorizing matrices corresponds to the connections in the circular graph after multiple hops. We can set the number of factorizing matrices to M = log 2 N , which corresponds to the number of hops, and the resulting matrix product becomes a full matrix with high probability [see <ref type="bibr">Stoica et al., 2001, Theorem 2]</ref>. In total, there are N (log N ) 2 non-zero entries, still much smaller than N 2 for a large N .</p><p>The (Chord) Sparse Factorization (SF) can thus be formulated as the following optimization problem:</p><formula xml:id="formula_1">minimize W (1) ,...,W (M ) X ? M m=1 W (m) 2 F (1)</formula><p>where W (m) 's are sparse square matrices with non-zero positions specified by the Chord protocol. The approximation scheme is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (bottom). We also call the approach non-parametric SF as we directly optimize over the factorizing matrices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parametric Sparse Factorization</head><p>Besides direct optimization over the factorizing matrices, we can consider the mapping from vectorial input data to the factorizing matrix entries because each node in the Chord protocol has the same out-degree. The mapping as a component can endow the model 1) generalization to newly coming data and 2) representation learning by transforming the current embedding representation to a new embedding.</p><p>We present a transformation model as a concrete example to illustrate the idea. It is a transformer-like model (see <ref type="figure" target="#fig_1">Figure  3</ref> left), where we replace the scaled dot-product attention with a product of sparse square matrices. The matrix product provides an approximation to a full non-normalized attention matrix.</p><p>One block of such Parametric Sparse Factorization Attention (PSF-Attn) transforms data in the current embedding E to new embedding E new = PSF-Attn(E). There is a number of MLPs in the block, where the MLP f (m) with parameters ? m takes E i: (the ith row of E) as an input and returns the non-zeros in the ith row of W (m) , for i = 1, . . . , N and m = 1, . . . , M . That is, for k = 1, . . . , K,</p><formula xml:id="formula_2">W (m) ij = ? ? ? f (m) (E i: ; ? m ) 1 if j = i f (m) (E i: ; ? m ) k if j = (i + 2 k?2 ) mod N 0 otherwise.</formula><p>Similar to transformers, we use another MLP g with parameters ? to convert an E row to the corresponding V row.</p><p>The new model can work as a building block in representation learning frameworks. For example, if we define an objective function J over E new , the learning can be formulated as the following optimization problem:</p><formula xml:id="formula_3">minimize ?1,...,? M ,? J (PSF-Attn(E; ? 1 , . . . , ? M , ?)),<label>(2)</label></formula><p>where the optimization can be implemented with back-propagation and a gradient based algorithm such as Adam <ref type="bibr" target="#b10">[Kingma and Ba, 2014]</ref>. It is also straightforward to stack multiple PSF-Attn blocks to implement deeper learning.  The PSF-Attn method has two advantages. First, the original transformer and many of its variants [e.g. <ref type="bibr" target="#b3">, Choromanski et al., 2020</ref><ref type="bibr" target="#b20">, Tay et al., 2021</ref><ref type="bibr" target="#b9">, Katharopoulos et al., 2020</ref> are built upon scaled dot-product, which essentially employ low-rank matrix factorization. They may not work well if the attention is intrinsically high rank.</p><p>In contrast, all factorizing matrices in our method are full-rank, and so is their product. Therefore PSF-Attn does not suffer from the low-rank bottleneck constraint. Second, our model does not require softmax over a large square matrix, avoiding many computational difficulties. Removing the softmax also endows more freedom in mixing the V rows because the mixture can go beyond the convex hull.</p><p>Our method also differs from the previous multi-layer sparse attention approaches such as <ref type="bibr" target="#b13">[Li et al., 2019</ref><ref type="bibr" target="#b2">, Child et al., 2019</ref><ref type="bibr" target="#b4">, Correia et al., 2019</ref> because they still use scaled dot-products. PSF-Attn can give full attention in one block. For the ith element in the sequence, its attention to other elements can directly be obtained by vector-matrix product W (M ) i:</p><formula xml:id="formula_4">M ?1 m=1 W (m) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We have performed three groups of experiments. In the first group, we tried the non-parametric SF for different types of square matrices and compared it with TSVD. Next, we demonstrated the scalability of PSF-Attn on synthetic sequences up to tens of thousands positions. Finally, we tested PSF-Attn for the Long Range Arena benchmark data sets to see its performance in real-world practice. The first group of experiments was run on a standard PC with an Intel Core i9 CPU. The second and third groups were run on a Linux server with one NVIDIA-Tesla V100 GPU with 32GB of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Non-paramatric SF</head><p>There are many types of square matrices. TSVD is the best for approximating matrices close to low rank in terms of F-norm. For non-parametric SF, we performed an empirical study 1) to show that SF can supersede TSVD in F-norm by using the same number of non-zeros and 2) to identify the types of square matrices particularly suitable for SF.</p><p>Given a square matrix, we used the Matlab fminunc optimizer to solve the problem in Eq. 1, where the non-zero entries in the factorizing matrices are initialized to random numbers between [K ?1 , K ?1 + 10 ?2 ]. The total number of non-zero entries for SF and TSVD are N (log 2 N ) 2 and 2N r + r. For a fair comparison, we set the r = log 2 N ) 2 /2 in TSVD such that it has nearly the same number and no fewer non-zeros than SF.</p><p>We first used 256 ? 256 grayscale images as approximated matrices because we can directly see them. Figure 4 (top) shows six typical square images, where we provide the resulting TSVD and SF approximation errors in F-norm below each image (more examples in Appendix 1).</p><p>The chess image (matrix) is close to low-rank because the black-and-white chessboard can is two-rank. Therefore TSVD performs better as expected. TSVD also works well for the Lena and apple images because TSVD tends to preserve low-frequency information in images. In contrast, TSVD is not as good as SF for the other three images that contain rich high-frequency details such as lines and corners, which indicates a matrix type where SF can defeat LRMF.</p><p>To further verify this, we computed the gradient magnitudes of the images (shown in <ref type="figure" target="#fig_2">Figure 4</ref> bottom). In this way, the intensities in constant areas become zero, and the remaining non-zeros are mainly high-frequency details. We can see that SF gives a lower approximation error than TSVD for all such matrices, which confirms that SF is more advantageous for approximating matrices with rich high-frequency details.</p><p>In summary, the results show that the TSVD performance does not cap SF by using the same number of non-zeros for approximating square matrices. The winning cases indicate that SF is often better than LRMF when the approximated matrix is 1) sparse, 2) intrinsically high-rank, or 3) containing rich high-frequency details.</p><p>Besides square images, we have also compared TSVD and SF on several other types of square matrices. <ref type="table" target="#tab_1">Table 1</ref> shows the comparison results. The data types include affinity matrices of dense graphs (dense graph), affinity matrix of sparse networks (network), affinity matrix of surface mesh over 3D objects (surface mesh), and covariance matrix of vectorial data (covariance). We can see that for dense graph and covariance types, sometimes TSVD is better while sometimes SF can win. SF wins for most cases for surface mesh because the mesh networks probably do not have a low-rank structure. SF wins all data sets in the network type, which indicates that SF is more effective for approximating sparse matrices. We give the details of the data sets in Appendix 1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approximation to Large Attention Matrices</head><p>Here we test whether PSF-Attn is scalable to approximate large attention matrices. We have used two synthetic data sets composed of long sequences for supervised learning tasks. A similar experimental setup appeared in <ref type="bibr" target="#b8">[Hochreiter and Schmidhuber, 1997</ref>] for scalability tests. The details of the data sets and tasks are given below:</p><p>? Adding Problem. This is a sequence regression task. Each element of an input sequence is a pair of numbers</p><formula xml:id="formula_5">(a i , b i ), where a i ? U (?1, 1), b i ? {0, 1}, i = 1, .</formula><p>. . , N . We generated signals at two randomly selected  positions t 1 and t 2 such that b t1 = b t2 = 1 and b i = 0 elsewhere. The learning target is y = 0.5 + a t1 + a t2 4 .</p><p>For example, an input sequence [(0.1, 0), (?0.4, 1), (0.3, 0), (?0.2, 0), (0.7, 1)] will have the learning target y = 0.575. Unlike <ref type="bibr" target="#b8">[Hochreiter and Schmidhuber, 1997</ref>], we did not restrict the t 1 and t 2 choice to make the task more challenging. That is, the relevant signals can appear either locally or at a great distance from each other. In evaluation, a network prediction? is considered correct if |y ??| &lt; 0.04. ? Temporal Order. This is a sequence classification task. A sequence consists of randomly chosen symbols from the alphabet {a, b, c, d, X, Y }, where the first four are noise symbols. Each sequence has two signal symbols, either X or Y , which appear at two arbitrary positions. The four target classes correspond to the ordered combinations of the signal symbols (X, X), (X, Y ), (Y, X), and (Y, Y ). For example, an input sequence [b, a, c, b, X, a, a, Y, b] should be classified as Class 2.</p><p>We prepared data of different sequence lengths for each problem. We started with N = 128 and gradually increased the length by the factor of two, up to N = 2 14 . For every sequence length, we generated 200,000 training and 5,000 testing instances.</p><p>We compared PSF-Attn with several popular methods based on scaled dot-product attention (referred to as X-former architectures). The first two, Linformer  and Performer <ref type="bibr" target="#b3">[Choromanski et al., 2020]</ref> have also claimed to be scalable attention-based architectures. For completeness, we also included the original Transformer <ref type="bibr" target="#b22">[Vaswani et al., 2017b]</ref>. We have used their open-source PyTorch implementations 5 .</p><p>We followed standard cross-validation techniques to tune the main hyperparameters, such as the number of layers and heads, dimensionality of the token embedding, and query/key/value dimensions. For the Temporal Order problem, we directly fed the data instances to the embedding layers. For the Adding problem, the input data was only twodimensional, and one of them was real-valued. Directly using such a low-dimensional embedding space would lead to poor attention. We augmented the dimensionality with a linear layer to assure sufficient freedom for the scaled dot-products in the X-former architectures. All the models were optimized using the Adam optimizer [Kingma and <ref type="bibr" target="#b10">Ba, 2014]</ref> with the learning rate of 0.001 using batch size of 40.</p><p>The results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. For the Adding problem, we see that all models work fine for short sequences <ref type="figure" target="#fig_0">(100% for N ? 256)</ref>. The X-former methods, however, turn worse or even useless when the sequences are longer. Linformer has an error rate of 0.48% for N = 512 and 86.18% for N = 1024, which is nearly as bad as random guessing (92% In summary, our method has better scalability than the three attention models based on scaled dot-products in terms of lower learning errors. PSF-Attn can still achieve nearly 100% prediction accuracy for an attention matrix size up to tens of thousands.    <ref type="bibr" target="#b27">[Zhu et al., 2021]</ref> 37.38 56.12 --Linformer <ref type="bibr" target="#b25">[Xiong et al., 2021]</ref> 37.25 55.91 37.84 67.60 Reformer <ref type="bibr" target="#b19">[Tay et al., 2020]</ref> 37.27 56.10 38.07 68.50 Reformer <ref type="bibr" target="#b27">[Zhu et al., 2021]</ref> 36.44 64.88 --Reformer <ref type="bibr" target="#b25">[Xiong et al., 2021]</ref> 19.05 64.88 43.29 69.36 Performer <ref type="bibr" target="#b19">[Tay et al., 2020]</ref> 18.01 65.40 42.77 77.05 Performer <ref type="bibr" target="#b27">[Zhu et al., 2021]</ref> 32.78 65.21 --Performer <ref type="bibr" target="#b25">[Xiong et al., 2021]</ref> 18.80 63.81 37.07 69.87 BigBird <ref type="bibr" target="#b19">[Tay et al., 2020]</ref> 36.06 64.02 40.83 74.87 Linear Transformer <ref type="bibr" target="#b19">[Tay et al., 2020]</ref> 16.13 65.90 42.34 75.30 Transformer-LS <ref type="bibr" target="#b27">[Zhu et al., 2021]</ref> 38.36 68.40 --RFA-Gaussian <ref type="bibr" target="#b17">[Peng et al., 2021]</ref> 36.80 66.00 --Nystr?mformer <ref type="bibr" target="#b27">[Zhu et al., 2021]</ref> 37.34 65.75 --Nystr?mformer <ref type="bibr" target="#b25">[Xiong et al., 2021]</ref> 37.15 65.52 41.58 70.94 PSF-Attn 38.85?0.06 77.32?0.25 45.01?0.21 80.49?0.13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Long Range Arena Public Benchmark</head><p>Next we provide the experimental results on Long Range Arena (LRA), a publicly available benchmark for modeling long sequential data <ref type="bibr" target="#b19">[Tay et al., 2020]</ref>. We select four tasks from LRA that cover various data types and demand flexible reasoning abilities of tested models.</p><p>? ListOps. This is a sequence classification task for measuring the ability of models to identify and parse hierarchically constructed data <ref type="bibr" target="#b16">[Nangia and Bowman, 2018]</ref>. We used an enlarged version of the original ListOps, with a max sequence length up to 2000 and tree depth up to 10 <ref type="bibr" target="#b19">[Tay et al., 2020]</ref>. Each element in a sequence can be an operator, a digit, and a left or right bracket. The brackets define lists of items. Each operator, MAX, MIN, MED, and SUM_MOD, takes the items in a list as input and returns a digit, where MED means median and SUM_MOD means summation followed by modulo 10. An example sequence [MAX 6 [MED 3 2 2] 8 5 [MIN 8 6 2]] has ground truth answer 8. A prediction is correct if an output value of a neural network matches the ground truth label. For good predictions, a model should access all sequence elements and identify the proper parsing structure.</p><p>? Text Classification This is a binary sentiment classification task constructed from the IMDb reviews <ref type="bibr" target="#b15">[Maas et al., 2011]</ref>. Given a review as a sequence of characters, the goal is to classify it as positive or negative. Due to the character-level representation, the sequences are much longer than the word-level version used in conventional language modeling. We truncated or padded every sequence to a fixed length (N = 4000). ? Image Classification. This task is to classify images into one of ten classes. The images and class labels come from the grayscale version of CIFAR10. Each image is flattened to form a sequence of length 1024. Unlike conventional computer vision, the task requires the predictors to treat the grayscale levels (0-255) as categorical values. That is, each image becomes a sequence of symbols with an alphabet size of 256. Two example images and their class labels are shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>? Pathfinder. This is a binary classification task on synthetic images, which is motivated by cognitive psychology <ref type="bibr" target="#b14">[Linsley et al., 2018]</ref>. Each image (size 32 ? 32) contains two highlighted endpoints and some path-like patterns. Similar to the Image Classification task, the predictors must treat the pixels as categorical values and flatten the image to a sequence of length 1024. The task is to classify whether there is a path consisting of dashes between two highlighted points. Two example images and their classes are shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>We have tested PSF-Attn in the above LRA learning tasks, where the hyperparameters in PSF-Attn were again tuned by cross-validation. No external data was used for pre-training. We ran PSF-Attn four times with a different random seed for each task. The mean and standard deviation across the multiple runs are reported in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>For comparison, we quote the prediction accuracies reported for many X-former methods in the literature, including Transformer <ref type="bibr" target="#b21">[Vaswani et al., 2017a]</ref>, Sparse Transformer <ref type="bibr" target="#b2">[Child et al., 2019]</ref>, Longformer <ref type="bibr" target="#b0">[Beltagy et al., 2020]</ref>, Linformer , Reformer <ref type="bibr" target="#b11">[Kitaev et al., 2020]</ref>, Performer <ref type="bibr" target="#b3">[Choromanski et al., 2020]</ref>, BigBird <ref type="bibr" target="#b26">[Zaheer et al., 2020]</ref>, Linear transformer <ref type="bibr" target="#b9">[Katharopoulos et al., 2020]</ref>, Transformer-LS <ref type="bibr" target="#b27">[Zhu et al., 2021]</ref>, RFA-Gaussian <ref type="bibr" target="#b17">[Peng et al., 2021]</ref>, and Nystr?mformer <ref type="bibr" target="#b25">[Xiong et al., 2021]</ref>. If a method has different implementations, we quote all variants and their results. We exclude results that rely on external data for pre-training.</p><p>We see that PSF-Attn wins all tasks by giving the best classification accuracy among all compared methods. Such strong cross-task wins suggest that our method usually provides better attention approximation than those based on scaled dot-products.</p><p>Remarkably, our method has substantially improved the state-of-the-art in the Text and Pathfinder tasks. For Text, PSF-Attn achieves 77.32% accuracy, compared to the runner-up 68.4% by Transformer-LS. Our method wins by 80.49% accuracy for Pathfinder, which gains about 5% higher than the best X-former (Linear Transformer 75.3%). The significant improvement brought by PSF-Attn is probably because the two tasks involve sparse attention matrices.</p><p>We also investigated whether the approximating attention X = m W (m) is meaningful by visualization. <ref type="figure">Figure 7</ref> shows an attention vector (absolute values) of token <ref type="bibr">["CLS"]</ref>. Tokens in the review having more weight are highlighted. We see that PSF-Attn has a good performance in capturing sparse attention and identify the relevant words. <ref type="figure">Figure 8</ref> illustrates the attention values for a positive instance in Pathfinder. We calculated the attention vector of the i-th point as the i-th row in X. We then summed the attention vectors of the endpoints and their direct neighbors and reshaped the sum to 32 ? 32 for visualization, where we see that the positions with high absolute visualized values distribute around the connecting path between the endpoints.</p><p>We have proposed a new approximation method called Sparse Factorization for square matrices by using a product of sparse matrices. Given the same budget of non-zero numbers, our method has shown superior performance over conventional low-rank matrix factorization approaches, especially in cases where the approximated matrix is sparse, high-rank, or contains high-frequency details. We have also given parametric design and performed an empirical study on the classification of long sequential data. As the critical attention component, our method has demonstrated clear wins over the conventional scaled dot-product transformer and its several variants in terms of scalability and accuracy.</p><p>We have employed the Chord protocol to fix the non-zero positions in the sparse factorizing matrices. Later we could consider the other predefined protocols or even adaptively learned protocols for the sparse structure. In this work, we have considered approximating unconstrained square matrices. In the future, we could investigate the approximation to more specific matrices such as non-negative, stochastic, symmetric, or semi-definite matrices. As a result, we could extend the method for further applications such as large-scale graph matching, Gaussian Process, or network structure identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. Sparse Factorization of Large Square Matrices 1 Non-parametric Experiments</head><p>In addition to six square images from the main paper, we have studied another six square images using the previously described approach. The results are shown in <ref type="figure" target="#fig_6">Figure A1</ref>. The conclusions are aligned with those in the main paper ? There exist cases where SF performs better than TSVD, which means the SF approximation quality is not capped by TSVD by using the same number of non-zeros. ? SF is more likely to win for images where there are more high-frequency details, especially for the gradientmagnitude images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Details of Other Square Matrices</head><p>In the main paper <ref type="table" target="#tab_1">Table 1</ref>, we have present comparison between TSVD and SF for approximating different types of square matrices using the same number of non-zeros. Here we provide the sources and statistics of the square matrices.</p><p>? AuralSonar (N = 100). It is the Aural Sonar data set from <ref type="bibr" target="#b1">[Chen et al., 2009]</ref>. The original research investigated the human ability to distinguish different types of sonar signals by ear <ref type="bibr">(Philips et al., 2006). ? Protein (N = 213)</ref>. It is the Protein data set from <ref type="bibr" target="#b1">[Chen et al., 2009]</ref>, which contains the radial basis function (RBF) kernel between 213 proteins. ? Voting (N = 435). It is the Voting data set from <ref type="bibr" target="#b1">[Chen et al., 2009]</ref>, which contains dissimilarities between 435 voting records with 16 scaled voting attributes. <ref type="figure" target="#fig_0">? Yeast (N = 200)</ref>. It is the Yeast-SW-7-12 data set from the same repository in <ref type="bibr" target="#b1">[Chen et al., 2009]</ref>. The data set converts the pairwise Smith-Waterman similarities s ij <ref type="bibr" target="#b12">[Lanckriet et al., 2004</ref><ref type="bibr" target="#b25">, Xu et al., 2014</ref>   ? AntiAngiogenesis (N = 205). It is the tumorAntiAngiogenesi_1 data set in SuiteSparse Matrix Collection. The matrix was from an optimal control problem. It is a sparse matrix with 1783 non-zero entries.</p><p>Easy Medium Hard <ref type="figure" target="#fig_0">Figure A2</ref>: Example images form the Pathfinder task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Long Range Arena</head><p>In this section, we provide a detailed explanation of the LRA experiments <ref type="bibr" target="#b19">[Tay et al., 2020]</ref>. The technical information, such as memory consumption and time per epoch, is provided. In addition, we visualized attention maps for several instances from the Pathfinder task and explained their extraction process in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Description</head><p>The data set for the LRA benchmark is publicly available. The information about data and the download link can be found in the official GitHub repository: https://github.com/google-research/long-range-arena.</p><p>? ListOps The raw data for this problem is organized as three separate files basic_train.tsv, basic_test.tsv, basic_val.tsv for training, testing, and validation data, respectively. The split is fixed. In addition to the tokens described in the main paper, each sequence has "(" and ")" symbols, which should be removed. To equalize the lengths of the sequences, we used the built-in PyTorch padding functional. After the sequences are prepared, the embedding layer processes each unique value, thus mapping elements to the embedding space. The rest of the training process is straightforward. ? Text Classification The IMDB data set is downloaded using the tensorflow-dataset package. It includes 25 000 instances for training and another 25 000 for the test set. To transform the raw reviews into sequences, we first went through the whole corpus and extracted the character vocabulary. Then we mapped each sequence to a vector of indices using this vocabulary. Finally, we truncated or padded each sequence to a fixed length of 4k and started training.</p><p>? Image Classification CIFAR10 is a well-known dataset, which can be downloaded from the torchvision package. The train/test splitting is fixed. To make images gray-scaled, we used standard transformation transforms.Grayscale from the same package. An image is flattened to a sequence of length 1024. Then each element is mapped to a dictionary of size 256 (all possible intensity values) and given to the embedding layer. ? Pathfinder The problem data consists of two types of files: images and metafiles. Metafiles store information about all the images and their corresponding labels (positive or negative). There are three classes of images: curv_baseline (easy), curv_contour_length_9 (medium), curv_contour_length_14 (hard). An image class corresponds to the distance between its endpoints (curve length), thus positively correlates with the difficulty level. Three images from the different catalogs are present in <ref type="figure" target="#fig_0">Figure A2</ref>. The exact data split is not provided. To separate the data into three parts, we iterated over all metafiles from the catalogs and constructed the training/val/test (90%/5%/5%) sets such that all three types of images are present equally. The rest of the processing is similar to the Image Classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization of Attention Maps</head><p>In this subsection, we explain the process of attention values extraction in detail. Two different approaches are used to obtain and interpret the training results because PSF-Attn was trained using different pooling strategies: adding the ["CLS"] token to the sequences of the IMDB reviews and simple flattening for the Pathfinder sequences.</p><p>Having an attention matrix X = m W (m) , the attention vector of the i-th sequence element is the i-th row of X.</p><p>? Text Classification The ["CLS"] token is responsible for representing the information collected from the whole input sequence. Its attention vector corresponds to the 0-th row in the attention matrix. After normalization of the absolute matrix values to a range of [0, 1], we extracted the row and visualized its values along with the  corresponding review characters of one test instance. The example in the main paper illustrates the ability of PSF-Attn to operate on sparse attention matrices successfully and to output meaningful results reflecting high classification accuracy. ? Pathfinder Flattening as a pooling strategy requires another approach to interpret the attention values. The attention matrix X for every test instance is of size (1024x1024), while the input image is (32x32). This mismatch makes it problematic to map attention values to the corresponding input image directly. To achieve this, we identified the positions of the endpoints and considered their direct neighbors (18 pixels positions, including themselves) as the source for visualization. We extracted the respective rows of the attention matrix and, finally, took their sum. The resulting vector was reshaped to the original size and depicted in a heat map. <ref type="figure" target="#fig_1">Figure A3</ref> shows the attention values obtained using the described method. We can see that the identified attention vectors of X store the meaningful information that visually matches the target path.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of (top) the Chord protocol and (bottom) sparse factorization of a square matrix for N = 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of (top) the parametric transformation from current embedding E to new embedding E new based on Sparse Factorization and (bottom) setting of MLP output to non-zero entries in the corresponding sparse factorizing matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example square matrices: (top) original images and (bottom) the gradient magnitude images (displayed after histogram equalization for better visibility). Image names are approximation errors by using TSVD and SF with the same number of non-zeros are shown below the images. Boldface font indicates the winner for each case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Error percentage of PSF-Attn and the X-formers for (left) the Adding problem and (right) the Temporal Order problem with increasing sequence lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Example matrices: (left two) from the Image Classification and (right two) from Pathfinder tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>around Craig Morris and Jessica Tsunis were especially well cast in the leads Attention vector visualization of a positive review in Text Classification. Darker cells have larger absolute values. Visualization of (left) a positive Pathfinder instance and (right) its attention values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A1 :</head><label>A1</label><figDesc>Other example square matrices: (top) original images and (bottom) the gradient magnitude images (displayed after histogram equalization for better visibility). Image names are approximation errors by using TSVD and SF with the same number of non-zeros are shown below the images. Boldface font indicates the winner for each case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A3 :</head><label>A3</label><figDesc>Example of Pathfinder images and their corresponding attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Approximation errors in F-norm by using TSVD and SF for different types of square matrices.</figDesc><table><row><cell>Data type</cell><cell>Data name</cell><cell>TSVD</cell><cell>SF</cell></row><row><cell cols="2">dense graph AuraSonar</cell><cell cols="2">8.54e+00 8.68e+00</cell></row><row><cell cols="2">dense graph Protein</cell><cell cols="2">1.17e+01 1.09e+01</cell></row><row><cell cols="2">dense graph Voting</cell><cell cols="2">8.07e-04 1.71e+01</cell></row><row><cell cols="2">dense graph Yeast</cell><cell cols="2">3.72e+01 3.61e+01</cell></row><row><cell>network</cell><cell>Sawmill</cell><cell cols="2">3.24e+00 1.03e+00</cell></row><row><cell>network</cell><cell>Scotland</cell><cell cols="2">5.90e+00 3.76e+00</cell></row><row><cell>network</cell><cell>A99m</cell><cell cols="2">1.47e+01 1.01e+01</cell></row><row><cell>network</cell><cell>Mexican Power</cell><cell cols="2">3.85e+00 1.71e+00</cell></row><row><cell>network</cell><cell>Strike</cell><cell cols="2">2.73e+00 1.04e+00</cell></row><row><cell>network</cell><cell>Webkb Cornell</cell><cell cols="2">6.98e+00 4.80e+00</cell></row><row><cell>network</cell><cell>Worldtrade</cell><cell cols="2">8.65e+04 4.47e+04</cell></row><row><cell cols="2">surface mesh Mesh1e1</cell><cell cols="2">1.87e+01 9.82e+00</cell></row><row><cell cols="2">surface mesh Mesh2e1</cell><cell cols="2">2.48e+02 3.47e+02</cell></row><row><cell cols="2">surface mesh OrbitRaising</cell><cell cols="2">9.37e+01 8.35e+01</cell></row><row><cell cols="2">surface mesh Shuttle Entry</cell><cell cols="2">2.73e+03 1.86e+03</cell></row><row><cell cols="4">surface mesh AntiAngiogenesis 5.85e+01 3.29e+01</cell></row><row><cell>covariance</cell><cell>Phoneme</cell><cell cols="2">2.80e+01 5.27e+01</cell></row><row><cell>covariance</cell><cell>MiniBooNE</cell><cell cols="2">1.04e+00 6.36e+03</cell></row><row><cell>covariance</cell><cell>Covertype</cell><cell cols="2">8.22e-02 1.90e-02</cell></row><row><cell>covariance</cell><cell>Mfeat</cell><cell cols="2">1.11e+03 4.01e+05</cell></row><row><cell>covariance</cell><cell>OptDigits</cell><cell cols="2">3.28e+01 7.01e+01</cell></row><row><cell>covariance</cell><cell>PenDigits</cell><cell cols="2">4.00e+02 1.87e+02</cell></row><row><cell>covariance</cell><cell>Acoustic</cell><cell cols="2">1.36e-02 1.11e-02</cell></row><row><cell>covariance</cell><cell>IJCNN</cell><cell cols="2">5.24e-02 3.03e-02</cell></row><row><cell>covariance</cell><cell>Spam Ham</cell><cell cols="2">1.07e-01 4.97e-02</cell></row><row><cell>covariance</cell><cell>TIMIT</cell><cell cols="2">9.64e+01 1.56e+02</cell></row><row><cell>covariance</cell><cell>Votes</cell><cell cols="2">4.00e-01 1.70e-01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). Transformer becomes problematic (84.48% error) when N ? 2048. Performer starts to get wrong when N = 4096, giving only 71.76% accuracy, and when N = 8192, its prediction becomes almost random guessing. In contrast, PSF-Attn achieves 100% accuracy for all the tested lengths.A similar pattern holds for the Temporal Order problem. When N ? 512, all compared models give perfect or nearly perfect predictions. With longer sequences, for example when N = 4096, the error rates of Transformer, Performer, and Linformer become 2.06%, 1.76%, and 4.02%, respectively. When N = 16324, their error rates become 8.38%, 8.60%, and 64.22%, respectively. In contrast, PSF-Attn achieves 100% accuracy for N ? 8192, and 99.89% accuracy for N = 16324.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies by the compared methods for the four LRA tasks.</figDesc><table><row><cell>A dash ("-") means the result is</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>to dissimilarities by d ij = ? s ii + s jj ? s ij ? s ji .</figDesc><table><row><cell>abstract</cell><cell>bridge</cell><cell>upper</cell><cell>middle</cell><cell>bottom</cell><cell>moon</cell></row><row><cell>TSVD 820, SF 1312</cell><cell>TSVD 1008, SF 1308</cell><cell>TSVD 1671, SF 2378</cell><cell>TSVD 2623, SF 2879</cell><cell>TSVD 3121, SF 2863</cell><cell>TSVD 1655, SF 1462</cell></row><row><cell>TSVD 1079, SF 839</cell><cell>TSVD 679, SF 401</cell><cell>TSVD 1824, SF 865</cell><cell>TSVD 3612, SF 1673</cell><cell>TSVD 4344, SF 2656</cell><cell>TSVD 2743, SF 1666</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A1 :</head><label>A1</label><figDesc>Technical details of the PSF-Attn training on all experiments: time per epoch (sec.), number of training parameters, and peak memory usage (GB). Benchmarks are run on a Linux machine with one NVIDIA Tesla V100 32GB, Intel Xeon Gold 6240 CPU @ 2.60GHz processors, with 754GB of system memory.</figDesc><table><row><cell>Problem</cell><cell cols="5">N Train Size (k) Time Param. (M) Memory</cell></row><row><cell cols="2">ListOps 2000</cell><cell>96</cell><cell>431</cell><cell>1.86</cell><cell>7.90</cell></row><row><cell>Text</cell><cell>4096</cell><cell>25</cell><cell>74</cell><cell>0.21</cell><cell>6.54</cell></row><row><cell>Image</cell><cell>1024</cell><cell>50</cell><cell>21</cell><cell>0.29</cell><cell>1.73</cell></row><row><cell cols="2">Pathfinder 1024</cell><cell>540</cell><cell>374</cell><cell>0.17</cell><cell>3.58</cell></row><row><cell>Adding</cell><cell>2 7</cell><cell>200</cell><cell>34</cell><cell>0.01</cell><cell>1.40</cell></row><row><cell></cell><cell>2 8</cell><cell>200</cell><cell>36</cell><cell>0.02</cell><cell>1.43</cell></row><row><cell></cell><cell>2 9</cell><cell>200</cell><cell>39</cell><cell>0.03</cell><cell>1.52</cell></row><row><cell></cell><cell>2 10</cell><cell>200</cell><cell>45</cell><cell>0.06</cell><cell>1.73</cell></row><row><cell></cell><cell>2 11</cell><cell>200</cell><cell>87</cell><cell>0.10</cell><cell>2.12</cell></row><row><cell></cell><cell>2 12</cell><cell>200</cell><cell>175</cell><cell>0.18</cell><cell>3.08</cell></row><row><cell></cell><cell>2 13</cell><cell>200</cell><cell>377</cell><cell>0.35</cell><cell>5.12</cell></row><row><cell></cell><cell>2 14</cell><cell>200</cell><cell>845</cell><cell>0.68</cell><cell>9.97</cell></row><row><cell>Order</cell><cell>2 7</cell><cell>200</cell><cell>34</cell><cell>0.02</cell><cell>1.40</cell></row><row><cell></cell><cell>2 8</cell><cell>200</cell><cell>38</cell><cell>0.03</cell><cell>1.43</cell></row><row><cell></cell><cell>2 9</cell><cell>200</cell><cell>45</cell><cell>0.05</cell><cell>1.52</cell></row><row><cell></cell><cell>2 10</cell><cell>200</cell><cell>61</cell><cell>0.08</cell><cell>1.74</cell></row><row><cell></cell><cell>2 11</cell><cell>200</cell><cell>104</cell><cell>0.15</cell><cell>2.14</cell></row><row><cell></cell><cell>2 12</cell><cell>200</cell><cell>193</cell><cell>0.28</cell><cell>3.10</cell></row><row><cell></cell><cell>2 13</cell><cell>200</cell><cell>397</cell><cell>0.55</cell><cell>5.17</cell></row><row><cell></cell><cell>2 14</cell><cell>200</cell><cell>871</cell><cell>1.07</cell><cell>10.47</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similarity-based classification: Concepts and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">K</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cazzanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="747" to="776" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gon?alo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Ft</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00015</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Adaptively sparse transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the Nystr?m method for approximating a gram matrix for improved kernel-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2153" to="2175" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer New York Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kernel-based data fusion and its application to protein function prediction in yeast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grg Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Pacific Symposium</title>
		<imprint>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">Biocomputing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning long-range spatial dependencies with horizontal gated recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Veerabadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Windolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Listops: A diagnostic dataset for latent tree learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06028</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02143</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Random feature attention</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chord: A scalable peer-to-peer lookup service for internet applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="149" to="160" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention for transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10183" to="10192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using the nystr?m method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">omformer: A nystr\&quot; om-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nystr\ ; Weiping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03902</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3709" to="3725" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ricci flow embedding for rectifying non-euclidean dissimilarity data</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Long-short transformer: Efficient transformers for language and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02192</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">It is the Sawmill communication network from the Pajek data sets 6 . This is a sparse matrix with 124 non-zero entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Sawmill</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 36)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">It is the Scotland network from the Pajek data sets, which is about Corporate interlocks in Scotland (1904-5). The matrix is sparse with 644 non-zero entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Scotland</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 108)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">It is the GD&apos;99 -Linden strasse network from the Pajek data sets. The network is about the characters and their relations in the long-running German soap opera called &apos;Lindenstrasse&apos;. The matrix is sparse with 510 non-zero entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>A99m</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 234)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">It is the Mexican network from the Pajek data sets. The network contains the core of this political elite: the presidents and their closest collaborators. The matrix is sparse with 117 non-zero entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Mexican</surname></persName>
			<affiliation>
				<orgName type="collaboration">N = 35</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Power</surname></persName>
			<affiliation>
				<orgName type="collaboration">N = 35</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">This is a social network about informal communication within a sawmill on strike. The matrix is sparse with 38 non-zero entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Strike</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>It is the Strike network from the Pajek data sets</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">It is the Cornell subset in the LINQS WebKB data set 7 . The network is about citations among the 195 publication from Cornell. The matrix is sparse with 304 non-zero entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename><surname>? Webkb</surname></persName>
			<affiliation>
				<orgName type="collaboration">N = 195</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">It is the World_trade network in the Pajek data sets. The network is about world trade in miscellaneous manufactures of metal, 1994. The matrix is sparse with 998 non-zero entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Worldtrade</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 80)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">It is the mesh1e1 data set in SuiteSparse Matrix Collection 8 . The matrix was originally from NASA, collected by Alex Pothen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Mesh1e1</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 48). It is a sparse matrix with 306 non-zero entries</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">It is the mesh2e1 data set in SuiteSparse Matrix Collection. The matrix was originally from NASA, collected by Alex Pothen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Mesh2e1</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 306). It is a sparse matrix with 2018 non-zero entries</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">It is the orbitRaising_1 data set in SuiteSparse Matrix Collection. The matrix was from an optimal control problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Orbitraising</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 442). It is a sparse matrix with 2906 non-zero entries</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">It is the spaceShuttleEntry_1 data set in SuiteSparse Matrix Collection. The matrix was from an optimal control problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Shuttle Entry</surname></persName>
			<affiliation>
				<orgName type="collaboration">N = 560</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
	<note>It is a sparse matrix with 6891 non-zero entries</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">It is the covariance matrix of the Phoneme data set accompanied with the Elements of Machine Learning book</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Phoneme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The original data 9 has 4508 instances of 256 dimensions</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>N = 256)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the MiniBooNE particle identification data set in the UCI Repository 10 . The original data has 130064 instances of 50 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Miniboone</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 50)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the Covertype data set in the UCI Repository. The original data has 581012 instances of 54 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Covertype</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 54)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the Multiple Features data set in the UCI Repository. The original data has 2000 instances of 649 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Mfeat</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 649)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the Optical Recognition of Handwritten Digits data set in the UCI Repository. The original data has 5620 instances of 64 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Optdigits</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 64)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the Pen-Based Recognition of Handwritten Digits data set in the UCI Repository. The original data has 10992 instances of 16 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Pendigits</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 16)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the SensIT Vehicle (acoustic) data set in the LIBSVM Classification (Multi-class) data collection 11 . The original data has 98528 instances of 50 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Acoustic</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 50)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the ijcnn1 data set in the LIBSVM Classification (Binary class) data collection 12 . The original data has 126701 instances of 22 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Ijcnn (n = 22</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of a data set for email classification practice. The original data has 10000 instances of 448 features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? Spam</forename><surname>Ham</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 448)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the TIMIT speech recognition data 13 . There are 151290 instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Timit (n = 390</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>each contains 390 features (concatenating MFCC features in 10 consecutive 30ms windows</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">It is the covariance matrix of the Congressional Voting Records data set in the UCI Repository. The original data has 435 instances of 16 dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Votes</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>N = 16)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
