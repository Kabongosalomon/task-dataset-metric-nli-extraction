<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CiteSum: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
							<email>yuningm2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
							<email>mingz5@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CiteSum: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="sl">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>arXiv:2205.06207v2 [cs.CL]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scientific extreme summarization (TLDR) aims to form ultra-short summaries of scientific papers. Previous efforts on curating scientific TLDR datasets failed to scale up due to the heavy human annotation and domain expertise required. In this paper, we propose a simple yet effective approach to automatically extracting TLDR summaries for scientific papers from their citation texts. Based on the proposed approach, we create a new benchmark CiteSum without human annotation, which is around 30 times larger than the previous human-curated dataset SciTLDR. We conduct a comprehensive analysis of CiteSum, examining its data characteristics and establishing strong baselines. We further demonstrate the usefulness of CiteSum by adapting models pretrained on CiteSum (named CITES) to new tasks and domains with limited supervision. For scientific extreme summarization, CITES outperforms most fully-supervised methods on SciTLDR without any fine-tuning and obtains state-of-the-art results with only 128 examples. For news extreme summarization, CITES achieves significant gains on XSum over its base model (not pre-trained on Cite-Sum), e.g., +7.2 ROUGE-1 zero-shot performance and state-of-the-art few-shot performance. For news headline generation, CITES performs the best among unsupervised and zero-shot methods on Gigaword. 1 1 Introduction Scientific summarization typically regards paper abstract as the ground-truth summary, as it is written by the authors themselves with relatively high quality and readily available in most scientific documents. However, paper abstract may not always be the ideal summary because it often involves certain details such as task description, background information, and experiment results (cf. the abstract of Paper Abstract: We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T , we would like to learn a generative function G that maps an input sample from S to the domain T , such that the output of a given function f , which accepts inputs in either domains, would remain unchanged. Other than the function f , the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f -constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.</p><p>Citation Text: Taigman et al. <ref type="bibr">[8]</ref> proposed the Domain Transfer Network (DTN) to map a sample from one domain to an analog sample in another domain and achieved favorable performance on small resolution face and digit images. <ref type="table">Table 1</ref>: An example showing that the citation texts of a paper can often be used as its ultra-short summary. this paper). As a result, recent work <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref> has studied the problem of scientific extreme summarization, which aims at forming ultra-short summaries (usually one sentence) of the papers, namely the TLDR 2 summaries.</p><p>However, unlike paper abstracts, ultra-short paper summaries are far from being universally available. Only certain scientific venues such as Open-Review.net support a TLDR field during paper submission, which is completely optional, and not all submitted papers provide such information. In addition, human-annotated summaries of scientific documents are rather costly and require domain expertise. As a consequence, the previous SciTLDR dataset <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>, using a combination of author-provided TLDR and human-annotated TLDR (rephrased from paper reviews on OpenReview), only collected around 2,000 examples for training and 600 for testing.</p><p>In this paper, we argue that citation texts can often serve as high-quality short summaries of the cited papers. In <ref type="table">Table 1</ref>, we show the abstract of one paper and its citation sentence in a follow-up paper. We observe that the citation sentence introduces the cited method and its contributions in a concise and accurate manner. Motivated by such observations, we propose a simple yet effective approach to locating, extracting, and filtering citation texts from scientific papers. We then treat the processed citation texts as ground-truth summaries of the cited papers. Based on the proposed approach, we create a large-scale scientific extreme summarization benchmark, CiteSum, which is automatically derived from citation texts and around 30 times larger than the previous human-annotated dataset SciTLDR <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>.</p><p>We conduct a comprehensive analysis of Cite-Sum regarding its data characteristics and quality, meanwhile establishing strong baselines as the reference for future studies. We further verify the usefulness of CiteSum by demonstrating that models pre-trained on CiteSum, which we name as CITES (Citation Text-guided Summarizer), exhibit superior generalizability during low-resource adaptation to new tasks and domains.</p><p>On the human-annotated scientific extreme summarization dataset SciTLDR <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>, our zero-shot BART-based <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> CITES, without any fine-tuning, performs better than most fully-supervised baselines, including the fully-supervised BART model (without pre-training on CiteSum). Our few-shot CITES achieves state-of-the-art performance with only 128 labeled examples from SciTLDR. In addition, CITES outperforms its base model (BART) on two more diverse scientific tasks -discipline classification and title generation. When transferring to news extreme summarization, despite the domain mismatch, CITES achieves significantly better zero-shot performance than BART and PEGA-SUS <ref type="bibr" target="#b36">(Zhang et al., 2020</ref>) (e.g., +7.2 ROUGE-1) and state-of-the-art few-shot performance on the XSum dataset <ref type="bibr" target="#b23">(Narayan et al., 2018)</ref>. Furthermore, CITES performs the best among unsupervised and zero-shot methods on the Gigaword news headline generation dataset <ref type="bibr" target="#b26">(Rush et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>(1) We propose a simple yet ef-Citation Example 1: We take the publicly available Semantic Scholar Open Research Corpus (S2ORC)  as the source for data creation.</p><p>Citation Example 2: Unlike WikiTransfer <ref type="bibr" target="#b8">(Fabbri et al., 2021)</ref>, CITES does not involve any downstream task-specific data selection or model tuning. fective approach to automatically extracting ultrashort paper summaries from citation texts. (2) Based on the proposed approach, we create a largescale scientific extreme summarization benchmark CiteSum and conduct a comprehensive analysis of it.</p><p>(3) We further verify the quality and usefulness of CiteSum by demonstrating that models pretrained on CiteSum perform very well on new tasks and domains such as news extreme summarization and headline generation with limited training.   as the source for data creation. In the latest version of S2ORC, there are 136M scientific papers from different academic disciplines and the number of papers with full-text access is 12M. We further remove papers without citation information, resulting in 9M papers as the candidates.</p><p>Quality Control Not all citation texts are of high quality and can be used as summaries of the cited papers. In <ref type="table" target="#tab_0">Table 2</ref>, we show two examples (in our paper) where the citation sentence simply (1) describes the data source or (2) introduces the difference of the citing paper from the cited paper. We note that prior studies on citation text generation <ref type="bibr" target="#b9">Ge et al., 2021)</ref> often do not filter these citation texts and simply treat all paragraphs/sentences with citations as the ground-truth labels, as their goals are not on paper summarization but writing assistance.</p><p>To ensure data quality, we carefully locate, extract, and filter the citation texts of papers in the following manner. First, we only take citation texts in the Related Work section of a paper, which largely ensures that they describe the content of the cited Dataset Train / Val / Test len src len summ Automatic?</p><p>Gigaword <ref type="bibr" target="#b26">(Rush et al., 2015)</ref> 3,803,957 / 189,651 / 1,951 32 9 XSum <ref type="bibr" target="#b23">(Narayan et al., 2018)</ref> 204,045 / 11,332 / 11,334 431 23 arXiv <ref type="bibr" target="#b6">(Cohan et al., 2018)</ref> 203,037 / 6,436 / 6,440 4.9K 220 SciSummNet <ref type="bibr">(Yasunaga et al., 2019)</ref> 1,000 / -/ -4.7K 150 TalkSumm <ref type="bibr" target="#b12">(Lev et al., 2019)</ref> 1,716 / -/ -4.8K 150 SciTLDR <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref> 1,992 / 619 / 618 159 21 CiteSum 83,304 / 4,721 / 4,921 255 23 <ref type="table">Table 3</ref>: Statistics of relevant summarization datasets showing the number of samples per data split, the average number of words in the source document (src) and reference summary (summ), and whether dataset creation is automatic without human annotation. SciSummNet <ref type="bibr">(Yasunaga et al., 2019)</ref> and TalkSumm <ref type="bibr" target="#b12">(Lev et al., 2019)</ref> do not contain validation/test set as their model evaluation was done on another dataset <ref type="bibr" target="#b11">(Jaidka et al., 2016)</ref>.</p><p>paper instead of irrelevant information, such as task background in the Introduction section or the implementation details in the Experiment section. After filtering papers without a Related Work section, there are around 288K papers left. Second, we only keep citation sentences that cite a single paper, since those with multiple citations typically discuss one line of work and cannot be used as the summary of a specific paper. In total, we obtain about 426K citation sentences.</p><p>Next, we measure the similarity between the citation texts and the cited papers and filter dissimilar pairs. Intuitively, if a citation sentence can serve as a high-quality summary, certain amount of its content should be from the cited paper. Prior work  also showed that authors tend to cite a paper using the information in the abstract of the cited paper. We thus calculate the overlap between paper abstracts and their citation sentences, and filter those below a threshold T . We set T to 50/20/40 for ROUGE-1/2/L recall through manual examination, resulting in a ROUGE-1/2/L recall of 73.1/39.4/58.5 after filtering. 3 As a reference, the ROUGE-1/2/L recall between paper abstracts and reference summaries on SciTLDR <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref> is 81.1/38.9/62.0 and 65.2/17.9/45.7 for author-provided (SciTLDR-Auth) and peer reviewderived (SciTLDR-PR) TLDR, respectively. That is, the abstraction level of CiteSum is between SciTLDR-Auth and SciTLDR-PR. This filtering step is rather strict as we prefer quality to quantity of the data and only 93K of the 426K examples (21.8%) are kept.</p><p>We further replace each citation span (e.g., "Taigman et al.</p><p>[8]") with a special token "REF" as they vary in different papers but essentially have the same meaning (i.e., referring to a cited paper).</p><p>Dataset Split After data filtering and preprocessing, there are 92,946 examples in the final citation text-guided summarization dataset, which we name as CiteSum. We take about 5% of the data as the validation and test sets respectively, and the remaining 90% as the training set. As one paper may be cited multiple times in different papers, we ensure that there is no label leakage by excluding papers used for evaluation from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Analysis</head><p>Dataset Statistics In <ref type="table">Table 3</ref>, we show the data statistics of CiteSum and other relevant summarization datasets. In terms of data size, CiteSum is about half the size of other automatically constructed datasets like XSum <ref type="bibr" target="#b23">(Narayan et al., 2018)</ref> and arXiv <ref type="bibr" target="#b6">(Cohan et al., 2018)</ref> due to the availability of citation texts and our strict quality control. On the other hand, the size of CiteSum is much larger than human-annotated datasets on paper summarization <ref type="bibr">(Yasunaga et al., 2019;</ref><ref type="bibr" target="#b2">Cachola et al., 2020</ref>) -almost 30 times larger than the Sc-iTLDR dataset <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>.</p><p>When compared to SciTLDR, the average length of source documents in CiteSum is longer, while that of the reference summaries is similar as the majority of summaries in SciTLDR also involve one sentence. When compared to XSum, the summary length in CiteSum is also quite similar. However, the inputs in XSum are news articles instead of scientific papers and the input lengths also vary. As for Gigaword <ref type="bibr" target="#b26">(Rush et al., 2015)</ref>, a news headline generation dataset, both its source input and target output are much shorter than CiteSum. Despite such differences, we observe that our models pre-trained on CiteSum transfer very well to these datasets in zero-shot and few-shot settings (Sec. 4).   Discipline Analysis In <ref type="figure" target="#fig_1">Fig. 1</ref>, we show the discipline distribution of papers in CiteSum. The discipline information is derived from the field of study in Microsoft Academic Graph (MAG) <ref type="bibr" target="#b28">(Shen et al., 2018)</ref>. We take the top field of study for each paper if there are multiple. We note that the discipline distribution in CiteSum is quite different from its data source S2ORC  where medicine and biology dominate. In contrast, most papers in CiteSum are in computer science. The shift in discipline distribution is because we explicitly keep papers with a Related Work section, where around 82.8% are computer science papers. We then take the citation texts in the above papers, which largely lead to papers in similar disciplines. As a result, most papers in CiteSum are from computer science, mathematics, and engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discipline distribution of papers</head><p>Citation Analysis In <ref type="figure">Fig. 2</ref>, we show the average number of citations for papers in CiteSum. Note that the citation count shown does NOT reflect the total number of citations due to data filtering, but how many times a paper appears in CiteSum as examples (with the same input and different citation sentences as target output). In total, there are 59,707 unique papers in CiteSum with an average citation of 1.56, and 98% of the papers have fewer than 5 citations. Compared to prior work, we do not only target popularly cited papers <ref type="bibr">(Yasunaga et al., 2019)</ref> and use different citation texts as different training examples instead of multiple reference summaries <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>We randomly sample 50 examples from CiteSum and ask two human annotators with a background in computer science to examine whether the citation sentences can serve  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments on CiteSum</head><p>In this section, we experiment on CiteSum with state-of-the-art baselines and analyze their performance under different setups to provide references for future studies. Implementation and training details are provided in App. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Examined Methods</head><p>We use BART-large <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> and PEGASUS-large <ref type="bibr" target="#b36">(Zhang et al., 2020)</ref> as the base models as they are the state-of-the-art methods on multiple summarization datasets. We examine the base models with different inputs such as paper abstract (Abs), abstract+introduction+conclusion (AIC), and abstract+title. In addition to using the TLDR (citation text) as the only generation target, we evaluate two multi-task settings with paper title and discipline (Disci) 4 as the targets, where different prefix tokens are added to the input such that the model can generate different targets given the same paper abstract as input <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>. We further evaluate the following extractive baselines. EXT-LEAD: a method that takes the first sentence of the paper abstract, which performs fairly well in news summarization. EXT-HEURISTIC: a heuristic method that looks for the first sentence containing "propose", "introduce", or "in this paper", as such sentences likely reflect the contribution of the paper. It falls back to EXT-LEAD if no such sentences are found. EXT-ORACLE: an upper bound that matches each sentence in the paper abstract with the reference summary and takes the sentence with the highest ROUGE-2 F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>In <ref type="table" target="#tab_6">Table 5</ref>, we show the results of various baseline methods on CiteSum. When given paper abstract as the source document, PEGASUS performs worse than BART and we thus use BART as the major model in the following experiments. Further adding paper introduction and conclusion to the model input slightly improves model performance, at the expense of longer training time and increased memory usage. The gains brought by adding title and discipline information to model input are marginal, while using them for multi-task learning does not lead to clearly better results. The fact that methods proposed by recent studies such as multi-task learning <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref> perform ineffectively on CiteSum indicates that CiteSum remains an unresolved and challenging scenario.</p><p>For the extractive baselines, EXT-LEAD performs significantly worse than that in the news domain <ref type="bibr" target="#b20">(Mao et al., 2020a)</ref>. EXT-HEURISTIC improves upon EXT-LEAD drastically and yet lags behind state-of-the-art methods by a large margin. EXT-ORACLE performs the best, the performance of which is generally consistent with the numbers on the human-annotated SciTLDR dataset <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>. On the other hand, the fact that abstractive methods have approached the extractive upper bound indicates that more abstraction is needed to further improve model performance on  BART-large is used as the base model if not otherwise specified. "/" indicates multi-task learning. R stands for ROUGE <ref type="bibr" target="#b14">(Lin, 2004)</ref> in all the tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CiteSum.</head><p>We believe that CiteSum provides a wellestablished testbed for future studies on (scientific) extreme summarization. The following future directions may be worth exploring: 1) how to better understand the structure and content of scientific papers with domain knowledge (via relevant papers, terminology, taxonomies, etc); 2) how to better capture the differences in writing styles across various domains; and 3) how to improve the saliency, factual correctness, and explainability of TLDR summaries given their conciseness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transferring to New Tasks and Domains with CITES</head><p>To further verify the quality and usefulness of Cite-Sum, we adapt models pre-trained on CiteSum to new tasks and domains, some of which are rather different from CiteSum and make model transfer with limited supervision very challenging. Specifically, we name our pre-trained model as CITES (Citation Text-guided Summarizer). CITES uses the simplest form in Sec. 3 with paper abstract as input and TLDR as target output. We evaluate CITES on various downstream tasks with no fine-tuning (zero-shot) or limited training examples (few-shot), including scientific extreme summarization on SciTLDR <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>, news extreme summarization on XSum <ref type="bibr" target="#b23">(Narayan et al., 2018)</ref>, and news headline generation on Gigaword <ref type="bibr" target="#b26">(Rush et al., 2015)</ref>. Additionally, we evaluate CITES on two more diverse tasks in the scientific domain, namely discipline classification and title generation, in a fully-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scientific Extreme Summarization</head><p>Setup SciTLDR <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>, the human-annotated scientific extreme summarization dataset, is an ideal testbed for further verifying the quality and usefulness of CiteSum since they both target extreme summarization, belong to the scientific domain (though CiteSum involves more disciplines), and share similar input/output formats (though CiteSum has slightly longer inputs). One noticeable difference, however, is the point of view of the summaries -in SciTLDR the reference summaries typically start with "We" or "This paper", while in CiteSum they often begin with "Author-Name et al." (replaced by a special token "REF" during preprocessing).</p><p>We propose two simple techniques to tackle such subtle style differences when adapting CITES to SciTLDR in a zero-shot setting without fine-tuning. The first technique is post-processing: we replace "REF" with "This paper" if the summary begins with "REF" and remove all other "REF" within the summary. The second technique is prompting: we use "This paper" as a prompt in the model decoder such that the summary always starts with "This paper". Similarly, in the few-shot setting, we replace the leading "We" with "This paper REF" in the reference summaries of SciTLDR (on the training set only) to alleviate the style mismatch.</p><p>We use BART-large <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> as the base model of CITES since most baselines on Sc-iTLDR, including the state-of-the-art methods, use the same base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot Results</head><p>In <ref type="table" target="#tab_9">Table 6</ref>, we show the performance comparison of different methods on SciTLDR. In the zero-shot setting, CITES (post-processing) outperforms competitive fullysupervised baselines such as BERTSum <ref type="bibr" target="#b15">(Liu and Lapata, 2019)</ref>. CITES (prompting) performs even better than CITES (post-processing), outperforming the fully-supervised BART model it is based upon. Such results demonstrate the benefits of pretraining on CiteSum. CITES (prompting), without any fine-tuning, is also on par with the state-of-theart method CATTS <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref>, while slightly worse than CATTS XSUM , which pre-trains on the XSum dataset <ref type="bibr" target="#b23">(Narayan et al., 2018)</ref> first.</p><p>We additionally test a zero-shot upper bound for CITES by providing our prompting model with the first 3 tokens in the reference summary (the most common ones are "We propose a" and "We present a") such that it knows how to start to sum-  <ref type="bibr" target="#b15">(Liu and Lapata, 2019)</ref> 38.5 16.6 30.5 MatchSum <ref type="bibr" target="#b39">(Zhong et al., 2020)</ref> 42.7 20.0 34.0 BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> 43.3 20.8 35.0 BART XSUM <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> 42.5 21.1 34.9 CATTS <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref> 43.8 20.9 35.5 CATTS XSUM <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref> 44  Data Overlap To ensure that the superior generalizability of CITES does not merely come from data leakage, we detect the overlap between Cite-Sum and SciTLDR. We consider two papers (near) identical if their TF-IDF cosine similarity is greater than 0.9 and find that only 9.7% papers in the test set of SciTLDR appear in the training set of Cite-Sum. Also, note that the training labels in CiteSum are automatically extracted citation sentences and different from SciTLDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scientific Discipline Classification and Title Generation</head><p>We have demonstrated the effectiveness of CITES on the task of scientific extreme summarization.  Next, we explore the feasibility of transferring CITES to more diverse tasks.</p><p>Setup We evaluate CITES with the task of scientific discipline classification and title generation. Similar to the multi-task experiments in Sec. 3, we use the same dataset split and model input, while replacing the generation target from summaries (citation texts) to the discipline or title of the papers. Examples with unavailable discipline or title are removed. We use BART-large as the base model for this experiment and compare BART with CITES in an apple-to-apple comparison.</p><p>Results In <ref type="table" target="#tab_11">Table 7</ref>, we show the performance comparison on title generation and discipline classification. CITES consistently outperforms BART on both tasks, although the differences are not as significant as in other low-resource transfer experiments. The moderate gains are possibly because there is abundant training data for the two tasks and continuous pre-training thus does not help much. As evidence, the (unweighted) Macro-F1 of CITES is considerably better than BART, which we found is because CITES performs well on those disciplines with fewer examples. Regarding the Weighted-F1, CITES is only slightly better as most papers belong to a single discipline (computer science) that dominates the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">News Extreme Summarization</head><p>Setup With the success on different tasks in the scientific domain, we next evaluate CITES on a more difficult setting where the domain is significantly different while the task is still extreme summarization. We take the XSum dataset <ref type="bibr" target="#b23">(Narayan et al., 2018)</ref> in the news domain for this purpose. We mainly use PEGASUS-large <ref type="bibr" target="#b36">(Zhang et al., 2020)</ref> as the base model of CITES as its fullysupervised version holds the state-of-the-art results on XSum. We additionally evaluate CITES Title in the zero-shot setting, which is the variant used for title generation in Sec. 4.2. <ref type="bibr" target="#b27">(See et al., 2017)</ref> 29.70 9.21 23.24 BERTSum <ref type="bibr">Lapata, 2019) 38.81 16.50 31.27 BART (Lewis et al., 2020)</ref> 45.14 22.27 37.25 PEGASUS <ref type="bibr" target="#b36">(Zhang et al., 2020)</ref> 47.21 24.56 39.25</p><formula xml:id="formula_0">Method R-1 R-2 R-L Fully-supervised PTGEN</formula><p>Zero-shot BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> 15.40 2.63 10.74 PEGASUS <ref type="bibr" target="#b36">(Zhang et al., 2020)</ref> 19.27 3.00 12.72 T5-LB <ref type="bibr" target="#b41">(Zhu et al., 2021a)</ref> 26.06 6.77 20.47 BART-LB <ref type="bibr" target="#b41">(Zhu et al., 2021a)</ref> 26.18 7.60 20.92 WikiTransfer <ref type="bibr" target="#b8">(Fabbri et al., 2021)</ref>   <ref type="bibr" target="#b36">(Zhang et al., 2020)</ref> 39.07 16.44 31.27 WikiTransfer <ref type="bibr">(Fabbri et al., 2021) 37.26 14.20 28.85 CITES 41.45 18.74 33.29</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot Results</head><p>In <ref type="table" target="#tab_13">Table 8</ref>, we show the results on XSum with various training data sizes. In the zero-shot setting, CITES significantly improves over its base model PEGASUS (+7.2 ROUGE-1).</p><p>In addition, CITES is on par with other pre-trained models such as BART-LB and T5-LB <ref type="bibr" target="#b41">(Zhu et al., 2021a)</ref>, which are specifically designed to leverage the lead bias in the news domain and require much more resources (32 vs. 1 GPU, 21.4M vs. 83K training examples) for summarization pre-training. CITES Title further improves over CITES and outperforms most zero-shot baselines (+8.9 ROUGE-1 over PEGASUS). CITES Title does not outperform WikiTransfer <ref type="bibr" target="#b8">(Fabbri et al., 2021)</ref>, which is somewhat expected as WikiTransfer carefully prepares its pre-training data to specific downstream tasks given, e.g., summary length and its level of abstraction. Unlike WikiTransfer <ref type="bibr" target="#b8">(Fabbri et al., 2021)</ref>, CITES does not involve any downstream taskspecific data selection or model tuning -we use the same CiteSum corpus in all the experiments.</p><p>Few-shot Results When given a few examples for fine-tuning, CITES quickly adapts to the new task despite the domain mismatch during pretraining. We observe that CITES consistently outperforms not only its base model but all other base-  <ref type="bibr" target="#b30">(Wang and Lee, 2018)</ref> 21.26 5.60 18.89 TED <ref type="bibr" target="#b33">(Yang et al., 2020)</ref> 25.58 8.94 22.83</p><p>Zero-shot T5 <ref type="bibr" target="#b25">(Raffel et al., 2020)</ref> 15.67 4.86 14.38 BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> 22.07 7.47 20.02 PEGASUS <ref type="bibr" target="#b36">(Zhang et al., 2020</ref><ref type="bibr">) 23.39 7.59 20.20 T5-LB (Zhu et al., 2021a</ref> 24.00 8.19 21.62 BART-LB <ref type="bibr" target="#b41">(Zhu et al., 2021a)</ref> 25  <ref type="table">Table 9</ref>: Performance comparison on the Gigaword news headline generation dataset <ref type="bibr" target="#b26">(Rush et al., 2015)</ref>.</p><p>line methods, including WikiTransfer, and achieves state-of-the-art few-shot performance on XSum.</p><p>In particular, CITES performs better than fullysupervised methods such as BERTSum <ref type="bibr" target="#b15">(Liu and Lapata, 2019)</ref> with only 100 examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">News Headline Generation</head><p>Setup To take a step further, we study the transfer performance of CITES to news headline generation. We use the Gigaword headline generation dataset <ref type="bibr" target="#b26">(Rush et al., 2015)</ref> for this evaluation. We again consider two variants of CITES, one pretrained with citation texts as the generation target and the other further pretrained with paper titles as in Sec. 4.2. We use BART-large <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> as the base model in this evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In <ref type="table">Table 9</ref>, we show the results of various methods on news headline generation. CITES again outperforms its base model (BART) significantly and achieves competitive performance with most unsupervised and zero-shot methods designed for news summarization <ref type="bibr" target="#b36">(Zhang et al., 2020;</ref><ref type="bibr" target="#b41">Zhu et al., 2021a)</ref>. CITES Title further achieves state-ofthe-art zero-shot performance despite pre-training on the scientific domain, demonstrating the generalizability and usefulness of CiteSum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Citation Text Generation There have been prior studies utilizing citation texts for different purposes. One popular line of work focuses on the generation of the citation texts for writing assistance or paper comparison <ref type="bibr" target="#b32">(Xing et al., 2020;</ref><ref type="bibr" target="#b18">Luu et al., 2021;</ref><ref type="bibr" target="#b9">Ge et al., 2021)</ref>. However, they typically do not distinguish the citation texts that can serve as summaries of the cited paper from those used for other purposes, e.g., background or result comparison <ref type="bibr" target="#b5">(Cohan et al., 2019)</ref>. For example,  treat citation text generation as a multi-document summarization task, where the target output is a paragraph with more than two citations and the model input is the abstracts of all cited papers. There is no filtering regarding the citation texts and all the paragraphs with enough citations are included. Besides including citation texts with various intents and the lack of quality control, prior studies differ from CiteSum in that they target longer outputs, e.g., multiple sentences <ref type="bibr" target="#b32">(Xing et al., 2020)</ref> or the entire Related Work section .</p><p>Citation Text for Paper Summarization Another line of work does not generate but extracts the citation texts and either uses them to form a summary directly <ref type="bibr" target="#b22">(Nakov et al., 2004;</ref><ref type="bibr" target="#b0">Abu-Jbara and Radev, 2011;</ref><ref type="bibr" target="#b24">Qazvinian et al., 2013)</ref> or treats them as a bridge to the cited paper <ref type="bibr" target="#b7">(Cohan and Goharian, 2015;</ref><ref type="bibr">Yasunaga et al., 2019)</ref>. Specifically, the citation texts in the latter studies are used to find relevant contexts in the cited paper (called citation contexts). Then, a long summary is formulated primarily using the cited paper, e.g., by selecting sentences from the citation contexts <ref type="bibr" target="#b7">(Cohan and Goharian, 2015)</ref>. Unlike CITES, prior citationbased summarization methods require (often multiple) citation texts of a paper as input, which are unavailable for new papers. In addition, they do not target ultra-short but abstract-long summaries.</p><p>Extreme Summarization Extreme summarization aims to form ultra-short summaries of the documents. Notable benchmarks in this direction include XSum <ref type="bibr" target="#b23">(Narayan et al., 2018)</ref> and New-SHead <ref type="bibr" target="#b10">(Gu et al., 2020)</ref> in the news domain, Sc-iTLDR <ref type="bibr" target="#b2">(Cachola et al., 2020)</ref> in the scientific domain, and Webis-TLDR-17 <ref type="bibr" target="#b29">(V?lske et al., 2017)</ref> for social media summarization. Compared to Sc-iTLDR, our CiteSum dataset is significantly larger in scale, from more venues than OpenReview, and composed of various disciplines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization with Limited Supervision</head><p>Our work is also related to unsupervised and zero/few-shot summarization that constructs weakly supervised guidance signals using e.g., data characteristics <ref type="bibr" target="#b4">(Chu and Liu, 2019;</ref><ref type="bibr" target="#b21">Mao et al., 2020b)</ref>, domain knowledge <ref type="bibr" target="#b42">(Zhu et al., 2021b)</ref>, or pseudo labeled data <ref type="bibr" target="#b33">(Yang et al., 2020;</ref><ref type="bibr" target="#b40">Zhong et al., 2022)</ref>. Compared to prior studies, CITES shows great cross-domain capability that has not been well explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a simple yet effective approach to automatically extracting ultra-short paper summaries from citation texts. Based on the proposed approach, we create a large-scale, highquality benchmark for scientific extreme summarization. We conduct a comprehensive analysis on the created benchmark and further demonstrate that models pre-trained on it exhibit superior generalizability to new tasks and domains such as news extreme summarization and headline generation with limited supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Regarding data collection, while we have taken multiple steps to improve data quality, as in all automatically created datasets, there are still lowquality examples. We show some examples of low quality in App. A. Limiting citation texts to the Related Work section improves data quality, but also excludes the majority of available citation sentences and makes CiteSum concentrated in the field of computer science and engineering. Regarding model performance, our transfer experiments are performed in scientific and news domains. While promising, there is no guarantee that CITES works well in other domains. Also, with abundant in-domain training data, pre-training on CiteSum may not lead to significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data Examples in CiteSum</head><p>In <ref type="table" target="#tab_16">Tables 10 and 11, we show four data examples</ref> in CiteSum corresponding to different ratings in the human evaluation. While some of the examples are still of low quality after quality control, most of the filtered citation texts can serve as high-quality summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>Official results of the baselines are taken from prior studies when possible. Model checkpoint selection is done on the validation set for every task. The special token "REF" (used to indicate citation span) is removed from model output for all transfer experiments. Paper abstract is used as input for all compared methods on SciTLDR. We experimented with other prompts such as "We" and "In REF" but found "This paper" works the best. FP16 is used in most experiments for training efficiency except for pre-training PEGASUS on CiteSum, with which it failed to learn. We use a batch size of 8. Hyperparameters like min/max generation length are generally set following prior work <ref type="bibr" target="#b36">(Zhang et al., 2020)</ref>.</p><p>All the experiments are conducted with 1 Nvidia RTX A6000 GPU. Pre-training on CiteSum only takes about 6.5h for BART and 10h for PEGASUS. The transfer experiments typically take less than 1h (time mostly spent on evaluation) as we use very few labeled data for training. The codebase is based on Huggingface transformers <ref type="bibr" target="#b31">(Wolf et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;Rating 1&gt;</head><p>Paper Title: Congested traffic states in empirical observations and microscopic simulations Paper Abstract: We present data from several German freeways showing different kinds of congested traffic forming near road inhomogeneities, specifically lane closings, intersections, or uphill gradients. The states are localized or extended, homogeneous or oscillating. Combined states are observed as well, like the coexistence of moving localized clusters and clusters pinned at road inhomogeneities, or regions of oscillating congested traffic upstream of nearly homogeneous congested traffic. The experimental findings are consistent with a recently proposed theoretical phase diagram for traffic near on-ramps [D. <ref type="bibr">Helbing, A. Hennecke, and M. Treiber, Phys. Rev. Lett. 82, 4360 (1999)</ref>]. We simulate these situations with a novel continuous microscopic single-lane model, the "intelligent driver model" (IDM), using the empirical boundary conditions. All observations, including the coexistence of states, are qualitatively reproduced by describing inhomogeneities with local variations of one model parameter. We show that the results of the microscopic model can be understood by formulating the theoretical phase diagram for bottlenecks in a more general way. In particular, a local drop of the road capacity induced by parameter variations has practically the same effect as an on-ramp. Citation Text: In a first approach, we use the well-known "intelligent driver model" (IDM) REF to show that the method works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;Rating 2&gt;</head><p>Paper Title: Probabilistic Model-Agnostic Meta-Learning Paper Abstract: Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. Citation Text: They extended their approach by incorporating a probabilistic component such that for a new task, the model is sampled from a distribution of models to guarantee a higher model diversification for ambiguous tasks REF. &lt;Rating 3&gt; Paper Title: A Generic Multi-Projection-Center Model and Calibration Method for Light Field Cameras Paper Abstract: Light field cameras can capture both spatial and angular information of light rays, enabling 3D reconstruction by a single exposure. The geometry of 3D reconstruction is affected by intrinsic parameters of a light field camera significantly. In the paper, we propose a multiprojection-center (MPC) model with 6 intrinsic parameters to characterize light field cameras based on traditional twoparallel-plane (TPP) representation. The MPC model can generally parameterize light field in different imaging formations, including conventional and focused light field cameras. By the constraints of 4D ray and 3D geometry, a 3D projective transformation is deduced to describe the relationship between geometric structure and the MPC coordinates. Based on the MPC model and projective transformation, we propose a calibration algorithm to verify our light field camera model. Our calibration method includes a close-form solution and a non-linear optimization by minimizing re-projection errors. Experimental results on both simulated and real scene data have verified the performance of our algorithm. &lt;Rating 4&gt; Paper Title: Advancing Research Infrastructure Using OpenStack Paper Abstract: Abstract-Cloud computing, which evolved from grid computing, virtualisation and automation, has a potential to deliver a variety of services to the end user via the Internet. Using the Web to deliver Infrastructure, Software and Platform as a Service (SaaS/PaaS) has benefits of reducing the cost of investment in internal resources of an organisation. It also provides greater flexibility and scalability in the utilisation of the resources. There are different cloud deployment models -public, private, community and hybrid clouds. This paper presents the results of research and development work in deploying a private cloud using OpenStack at the University of Huddersfield, UK, integrated into the University campus Grid QGG. The aim of our research is to use a private cloud to improve the High Performance Computing (HPC) research infrastructure. This will lead to a flexible and scalable resource for research, teaching and assessment. As a result of our work we have deployed private QGG-cloud and devised a decision matrix and mechanisms required to expand HPC clusters into the cloud maximising the resource utilisation efficiency of the cloud. As part of teaching and assessment of computing courses an Automated Formative Assessment (AFA) system was implemented in the QGG-Cloud. The system utilises the cloud's flexibility and scalability to assign and reconfigure required resources for different tasks in the AFA. Furthermore, the throughput characteristics of assessment workflows were investigated and analysed so that the requirements for cloud-based provisioning can be adequately made. Citation Text: In REF , the authors focus on the use of a private cloud environment in order to improve the High Performance Computing (HPC) research infrastructure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Discipline distribution of papers in CiteSum.Log scale is used for clearer illustration. Disciplines with lower than 0.1% distribution are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Citation Text: Zhang et al REF proposed a multi-projection-center (MPC) model with six intrinsic parameters to characterize both conventional and focused LF cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Examples (in our paper) showing that citation texts have different intents and cannot always be used as summaries of the cited paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>of the cited papers. On the other hand, there are still 10% misleading summaries, which we argue is quite common in automatically created summarization datasets<ref type="bibr" target="#b20">(Mao et al., 2020a)</ref>. We show 4 examples corresponding to each rating in App. A. We will further verify the quality of CiteSum by adapting models pre-trained on it to new tasks and domains (Sec. 4).</figDesc><table><row><cell></cell><cell>80 100</cell><cell cols="4">72.8% Aggregated Citation distribution of papers 89.2% 94.5% 96.9% 98.0% 98.6% 99.0%</cell></row><row><cell>Percent</cell><cell>40 60</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell>16.4%</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell cols="2">3 5.3% 2.4% 1.2% 0.6% 0.4% 4 5 6 Citation Count 7</cell></row><row><cell cols="6">Figure 2: Citation distribution of papers in CiteSum.</cell></row><row><cell cols="6">as high-quality summaries of the papers. Similar</cell></row><row><cell cols="6">to Cachola et al. (2020), we use a 4-point scale for</cell></row><row><cell cols="6">evaluation with 1 -false or misleading, 2 -partially</cell></row><row><cell cols="6">accurate, 3 -mostly accurate, and 4 -accurate. The</cell></row><row><cell cols="6">rating distribution is listed in Table 4. 80% citation</cell></row><row><cell cols="6">sentences are considered (mostly) accurate to be</cell></row><row><cell cols="4">used as summaries Rating</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell cols="4">Percentage 10% 20% 28% 42%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ratings of citation sentences in CiteSum regarding whether they can serve as high-quality summaries of the cited papers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance of different methods on CiteSum.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Performance comparison on SciTLDR (Ca-</cell></row><row><cell>chola et al., 2020) using its official evaluation script.</cell></row><row><cell>marize and (hopefully) which aspect to focus on.</cell></row><row><cell>CITES (prompting, gold 3 tokens) achieves com-</cell></row><row><cell>petitive ROUGE-1 and significantly better ROUGE-</cell></row><row><cell>2/L than the extractive upper bound EXT-ORACLE</cell></row><row><cell>that has access to the entire reference summary.</cell></row><row><cell>Few-shot Results In the few-shot setting, CITES</cell></row><row><cell>with 32 examples improves over its zero-shot coun-</cell></row><row><cell>terpart. Furthermore, 128-shot CITES outperforms</cell></row><row><cell>all fully-supervised methods and achieves new</cell></row><row><cell>state-of-the-art results on SciTLDR. In contrast,</cell></row><row><cell>a 128-shot BART model without first pre-training</cell></row><row><cell>on CiteSum largely lags behind, performing even</cell></row><row><cell>worse than our zero-shot CITES. Such results again</cell></row><row><cell>show the effectiveness of our pre-training strategy</cell></row><row><cell>and the quality of CiteSum despite being automati-</cell></row><row><cell>cally created thanks to our quality control.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison of CITES and its base model (BART) on title generation and discipline classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Performance comparison on the XSum dataset. Our few-shot results are averaged over 3 runs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Zhang et al., 2020) 39.12 19.86 36.24    Unsupervised SEQ 3<ref type="bibr" target="#b1">(Baziotis et al., 2019)</ref> 25.39 8.21 22.68 Brief</figDesc><table><row><cell>Method</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>Fully-supervised</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PEGASUS (</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Examples in CiteSum with different quality ratings.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our dataset and code can be found at https:// github.com/morningmoni/CiteSum.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">"TLDR" (or "TL;DR") is short for "too long; didn't read", often used in online discussions about scientific papers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also experimented with semantic metrics such as BERTScore<ref type="bibr" target="#b37">(Zhang et al., 2019)</ref> but they did not function as well as ROUGE-based metrics in our human evaluation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Here, we cast discipline classification as a seq2seq task<ref type="bibr" target="#b19">(Mao et al., 2021)</ref>. We found that all generated outputs are valid discipline names in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table 11: Examples in CiteSum with different quality ratings.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coherent citation-based summarization of scientific papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Abu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jbara</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SEQ?3: Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1071</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="673" to="681" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, Ioannis Konstas, and Alexandros Potamianos</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TLDR: Extreme summarization of scientific documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Cachola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.428</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4766" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Capturing relations between scientific papers: An abstractive model for related work section generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hind</forename><surname>Alamro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.473</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6068" to="6077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meansum: a neural model for unsupervised multi-document abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1223" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural scaffolds for citation intent classification in scientific publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Field</forename><surname>Cady</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1361</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3586" to="3596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scientific article summarization using citation-context and article&apos;s discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1045</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="390" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving zero and few-shot abstractive summarization with intermediate fine-tuning and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.57</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="704" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BACO: A background knowledge-and content-based framework for citing sentence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ly</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ante</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Diesner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1466" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating Representative Headlines for News Stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Finnie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Zukoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the the Web Conf</title>
		<meeting>of the the Web Conf</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the CL-SciSumm 2016 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthu</forename><surname>Kumar Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajal</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL)</title>
		<meeting>the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Talk-Summ: A dataset and scalable annotation method for scientific paper summarization based on conference talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achiya</forename><surname>Jerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Konopnicki</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2125" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-XScience: A large-scale dataset for extreme multidocument summarization of scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.648</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8068" to="8074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explaining relationships between scientific documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Cachola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2130" to="2144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reader-guided passage reranking for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.29</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="344" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facet-aware evaluation for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.445</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4941" to="4957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-document summarization with maximal marginal relevance-guided reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1737" to="1751" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Citances: Citation sentences for semantic analysis of bioscience text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><forename type="middle">S</forename><surname>Preslav I Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR</title>
		<meeting>the SIGIR</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating extractive summaries of scientific paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Whidby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="165" to="201" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A web-scale system for scientific knowledge exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-4015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TL;DR: Mining Reddit to learn automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>V?lske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4508</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to encode text as human-readable summaries using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaushian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4187" to="4195" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic generation of citation texts in scholarly papers: A pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosheng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6181" to="6190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TED: A pretrained unsupervised summarization model with theme modeling and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gmyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Darve</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.168</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1865" to="1874" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<publisher>Dan Friedman,</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7386" to="7393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sentence centrality revisited for unsupervised summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1628</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6236" to="6247" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extractive summarization as text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.552</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised summarization with customized granularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Leveraging lead bias for zero-shot abstractive news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gmyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sumdocs: Surrounding-aware unsupervised multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2021 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
