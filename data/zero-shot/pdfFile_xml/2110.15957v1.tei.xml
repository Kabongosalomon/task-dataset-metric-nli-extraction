<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Keyword Spotting with Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Prajwal</surname></persName>
							<email>prajwal@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Momeni</surname></persName>
							<email>liliane@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
							<email>afourast@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Keyword Spotting with Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>PRAJWAL ET AL.: VISUAL KEYWORD SPOTTING WITH ATTENTION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider the task of spotting spoken keywords in silent video sequences -also known as visual keyword spotting. To this end, we investigate Transformerbased models that ingest two streams, a visual encoding of the video and a phonetic encoding of the keyword, and output the temporal location of the keyword if present. Our contributions are as follows: <ref type="formula">(1)</ref> We propose a novel architecture, the Transpotter, that uses full cross-modal attention between the visual and phonetic streams; (2) We show through extensive evaluations that our model outperforms the prior state-of-the-art visual keyword spotting and lip reading methods on the challenging LRW, LRS2, LRS3 datasets by a large margin; (3) We demonstrate the ability of our model to spot words under the extreme conditions of isolated mouthings in sign language videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been significant progress in automatic visual speech recognition (VSR) due to the availability of large-scale annotated datasets and the development of powerful neural network-based learners <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref>. These methods are continually improving and becoming more sophisticated, by incorporating better visual models, stronger language modelling and training on larger datasets. Indeed the best industrial grade lip reading models today are far superior to humans, and achieve error rates approaching Automatic Speech Recognition (ASR) performance <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>However, for many applications it is not necessary to transcribe every word that is spoken in a silent video (the task of VSR), rather only specific utterances or keywords need to be recognised. This is for example the case in "wake word" recognition, where only particular keywords need to be spotted over long input sequences. A further drawback of VSR methods is that they are heavily reliant on language modelling; in general, their performance decreases significantly when context is limited (e.g. short utterances) or parts of the input are occluded, e.g. from the speaker's hands or a microphone. In this work, we focus instead on the task of Visual Keyword Spotting (KWS), where the the goal is to detect and localise a given keyword in (silent) spoken videos.</p><p>Automatic visual KWS enables a diverse range of practical applications: indexing archival silent videos by keyword to enable content-based search; helping virtual assistants (e.g. Alexa and Siri) and smart home technologies respond to wake words and phrases; assisting people with speech impairment (e.g. amyotrophic lateral sclerosis patients) or aphonia in communication <ref type="bibr" target="#b55">[56]</ref>; and detecting mouthings in sign language videos <ref type="bibr" target="#b4">[5]</ref>.</p><p>KWS differs in complexity from VSR primarily because in KWS we are armed with the keyword we need to recognise, whereas VSR has the harder task of recognising every word from scratch. The core hypothesis motivating this work is that this additional knowledge renders visual KWS an easier task than VSR; and it is therefore expected that KWS should achieve a higher performance than VSR, and generally be more robust to challenging and adversarial situations. Nevertheless, visual KWS remains a very difficult task and shares similar challenges to VSR methods: first, some words sound different but involve identical lip movements ('man', 'pan', 'ban'), these homopheme words cannot be distinguished using only visual information. Second, speech variations such as accents, speed, and mumbling can alter lip movements significantly for the same word. Third, co-articulation of the lips between preceding and subsequent words in continuous speech also affects lip appearance and motion.</p><p>In this paper, we make the following three contributions: (i) We propose a novel Transformer-based architecture, the Transpotter (a portmanteau of Transformer and Spotter), that is tailored to the visual KWS task. The model takes as input two streams, one encoding visual information from a video and the other providing a phonetic encoding of the keyword; the heterogeneous inputs are then fused using full cross-modal attention.</p><p>(ii) Through extensive evaluations, we show that our Transpotter model outperforms the prior state-of-the-art visual KWS and VSR methods on the challenging LRW, LRS2 and LRS3 lip reading datasets by a large margin. (iii) We test our best model under extreme conditions: finding words in mouthings of people communicating using sign language. Signers sometimes mouth words as they sign as an additional non-manual signal to disambiguate and help understanding <ref type="bibr" target="#b60">[61]</ref>. This new task is extremely challenging as there is a significant domain shift between full spoken sentences (in our training and test sets) and mouthings, where the context is sporadic and phonemes of the keyword may be missing -as sometimes only parts of words are mouthed <ref type="bibr" target="#b10">[11]</ref>. Our approach outperforms previous KWS models in this challenging, practical use-case. Video examples are available at the project's webpage: www.robots.ox.ac.uk/~vgg/research/transpotter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our work relates to prior work on KWS, lip reading, visual grounding, and applications of Transformers for text and video. We present a brief discussion of these topics below. KWS. KWS in audio (speech) is a well studied problem with a long history, spanning several decades. Prior to the establishment of deep learning models, KWS methods were based on Hidden Markov Models <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b64">65]</ref>, dynamic time warping <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b74">75]</ref> or indexing of ASR lattices <ref type="bibr" target="#b12">[13]</ref>. A number of works have since used deep architectures suitable for sequence modelling (e.g. RNNs, CNNs, or graph convolutional networks) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b75">76]</ref>, including encoder-decoder approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b77">78]</ref>. Berg et al. <ref type="bibr" target="#b8">[9]</ref> recently proposed using a Transformer model for the same task. Different from ours, this work uses a single input stream (audio) and only learns to spot a fixed vocabulary of keywords. In contrast, we use Transformers to temporally process, then fuse the multi-modal inputs, building a model that can eventually perform open-set KWS. Visual KWS has also received attention recently. The proposed methods include query-by-example <ref type="bibr" target="#b32">[33]</ref> approaches, sliding window classification <ref type="bibr" target="#b68">[69]</ref>, or looking up phonetic queries in lip reading feature sequences <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b58">59]</ref>, while audio-visual methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b66">67]</ref> that fuse the two modalities to improve robustness to noise have also been proposed. Our method builds upon these approaches: we address various weaknesses and propose superior video-text modelling as well as explicit keyword localization, resulting in significantly improved performance. Lip reading. Early works in lip reading usually relied on hand-crafted pipelines and features <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b76">77]</ref>. The availability of large scale lip reading datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref> and the development of deep neural network models resulted in major performance improvements, initially in word-level lip reading <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b57">58]</ref> and constrained sentences <ref type="bibr" target="#b6">[7]</ref>. Sentence level models were subsequently developed, using sequence-to-sequence architectures based on RNNs <ref type="bibr" target="#b17">[18]</ref>, CTC-based <ref type="bibr" target="#b55">[56]</ref> approaches, or a hybrid of the two <ref type="bibr" target="#b46">[47]</ref>. Replacing RNNs with Transformers resulted in better performing architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b73">74]</ref>. Joint audio-visual training and cross-modal distillation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b69">70]</ref> have also been investigated. The current state-of-the-art model uses Transformers in the visual front-end and achieves remarkable results with word error rates reaching as low as 30.7% <ref type="bibr" target="#b33">[34]</ref>. Visual grounding. Our work is also related to tasks such as natural language grounding in videos <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref> and subtitle alignment in sign language clips <ref type="bibr" target="#b11">[12]</ref>. Transformers. Since their introduction for machine translation, Transformers <ref type="bibr" target="#b62">[63]</ref> have become ubiquitous and are used today in a wide range of applications from natural language processing <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b48">49]</ref> and speech recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref> to visual representation learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b65">66]</ref>. In this work, we rely on Transformers as our building blocks for their strong sequence modelling capability and inherent potential for localisation through attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual KWS with Attention</head><p>In this section, we describe our proposed method shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We outline the architecture of our model (Section 3.1), our training procedure (Section 3.2) and differences to prior work (Section 3.3). We refer the reader to the Appendix for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Transpotter Architecture</head><p>Our model ingests two input streams: (i) a textual keyword q = (q 1 , q 2 ? ? ? , q n p ), and (ii) a silent video clip v ? R T ?H?W ?3 in which we need to spot the keyword. For each of the inputs, we have separate encoders that learn initial modality-specific representations. This is followed by a joint multi-modal Transformer that learns cross-modal relationships between the video and text features. The joint transformer predicts two outputs: (i) a sequence-level probability of the keyword occurring in the video and (ii) frame-level probabilities indicating the location of the keyword in the video if present. We describe each of the modules next. Text Representations. Our textual input is a phonetic representation of the keyword, obtained using a pronunciation dictionary. The input phoneme sequence of length n p is mapped to a sequence of learnable embedding vectors Q ? R n p ?d . Sinusoidal positional encodings are added to the input phoneme feature vectors, and the result is passed through a Transformer Encoder <ref type="bibr" target="#b62">[63]</ref> with N t layers to capture temporal information across the phoneme sequence:</p><p>Q enc = encoder q (Q + PE 1:n p ) ? R n p ?d .</p><p>Video Representations. We use a pre-trained visual front-end (either a CNN <ref type="bibr" target="#b1">[2]</ref> or VTP <ref type="bibr" target="#b33">[34]</ref>) to extract a feature vector for each input video frame, V ? R T ?d . Similar to the text encoder token embedding (such as in BERT <ref type="bibr" target="#b18">[19]</ref> and ViT <ref type="bibr" target="#b21">[22]</ref>) is then prepended to the result:</p><formula xml:id="formula_0">J = ([CLS];V enc ; Q enc ) ? R (1+T +n p )?d .</formula><p>We use a Transformer encoder with N m layers to jointly learn the relationships across video and phoneme vectors:</p><formula xml:id="formula_1">Z = encoder vq (J + PE 1:(1+T +n p ) ) ? R (1+T )?d . 1</formula><p>Prediction heads. The [CLS] output feature vector Z 1 serves as a joint aggregate representation for the video-text pair. An MLP head for binary classification, f c is attached to Z 1 to predict the probability of the keyword being present in the video:</p><formula xml:id="formula_2">y cls = ? ( f c (Z 1 )) ? R 1 ,</formula><p>where ? denotes a sigmoid activation. To localise the keyword, we attach a second MLP head f l that is shared across all the video output states from the multi-modal joint Transformer:</p><formula xml:id="formula_3">y loc = ? ( f l (Z 2:(T +1) )) ? R T .</formula><p>The output y loc t at each video frame time-step t ? T indicates the probability of the frame t being a part of the keyword utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Optimisation objectives. Given a training dataset D consisting of tuples (v, q, y cls , y loc ) of silent video clips, text queries, class labels and location labels (indicating the position of the keyword within the clip), we define the following objectives:</p><formula xml:id="formula_4">L cls = ?E (v,q,y cls )?D BCE(y cls ,? cls ) (1) L loc = ?E (v,q,y cls ,y loc )?D y cls 1 T T ? t=1 BCE(y loc t ,? loc t ) (2) BCE(y,?) = y log? + (1 ? y) log(1 ??),<label>(3)</label></formula><p>where BCE stands for the binary cross-entropy loss. The labels y cls are set to 1 when the given keyword occurs in the video and 0 otherwise; the frame labels y loc are set to 1 for the frames where the keyword is uttered and 0 otherwise. We train the model the optimise the total loss L = ? L cls + (1 ? ? )L loc , where ? is a balancing hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Compared to prior approaches, the design of our model offers several important advantages. Stronger Visual Representations. Previous works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b58">59]</ref> model temporal relationships between video frames using RNNs. In contrast, we employ Transformers <ref type="bibr" target="#b62">[63]</ref>, which are far more effective in modeling temporal relationships <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. Joint Video-text Modeling. Prior works such as KWS-Net <ref type="bibr" target="#b42">[43]</ref> follow a late-fusion strategy.</p><p>In our model each frame-wise video feature can attend to any keyword token (phoneme) and vice-versa. The information exchange across the modalities occurs at every layer, without restrictions on the receptive field for either modality. Stronger keyword localisation. Fine-grained localisation of the keyword in the video can be important for applications such as sign spotting <ref type="bibr" target="#b4">[5]</ref>. Existing methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b58">59]</ref> "weakly" localise the keyword by taking the sequence-level prediction to be the maximum probability over all the video time-steps. We instead provide stronger frame-level supervision, by enforcing the model to predict the exact temporal extent of the keyword in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation details</head><p>Pre-training the visual front-end. We explore two different visual front-end architectures for the Transpotter: (1) a CNN, highly similar in architecture to TM-seq2seq <ref type="bibr" target="#b1">[2]</ref> and (2) VTP <ref type="bibr" target="#b33">[34]</ref>, the current state-of-the-art for lip reading (trained only on public data). Both models are trained end-to-end on two-word video clips of LRS2 <ref type="bibr" target="#b17">[18]</ref> and LRS3 <ref type="bibr" target="#b0">[1]</ref> for lip reading. We refer the reader to the Appendix for the exact CNN architecture and training hyper-parameters. We refer the reader to <ref type="bibr" target="#b33">[34]</ref> for architectural hyper-parameters and training protocols for VTP. We pre-compute the visual features for each backbone for both datasets and then train directly on them for faster training. All our models and ablations use the pre-trained CNN features, unless otherwise stated.</p><p>Sampling. We form the training dataset D by randomly sampling with 50% probability a positive or negative video clip v for each query q. Each video v contains word boundary annotations, which allows (i) performing data augmentation by randomly cropping video segments during training, and (ii) creating frame labels y loc , as described in 3.2.</p><p>Misc. The keyword q is mapped to a phoneme sequence using the CMU dictionary <ref type="bibr" target="#b56">[57]</ref>; words not present in the dictionary are discarded from training D. We set ? = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section is structured as follows: We first present the datasets used as well the evaluation protocols that we follow in our experiments (Section 4.1). Next, we compare the performance of our proposed Transpotter model against strong baselines (Section 4.2) and then present a comprehensive study ablating our design choices (Section 4.3). Finally, we perform further performance analysis and provide qualitative results (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Protocol</head><p>Datasets. All models and baselines are trained and evaluated on LRS2 <ref type="bibr" target="#b17">[18]</ref>  Metrics. Given ground truth video-keyword samples, we assess the performance of our model in two ways. First, we assess classification performance, i.e. whether the model can accurately predict whether the keyword occurs in the video or not. We compute accuracy (Acc Cls @k ) and mean average precision (mAP Cls ) metrics, where Acc Cls @k measures how often a given keyword occurs in any of the top-k retrievals, and mAP Cls is obtained with the above criterion (where every word in the test keyword vocabulary is considered as a separate class).</p><p>Second, we assess the model's localisation capability, i.e. whether the model can accurately localise the keyword in the video clip. We follow common practice from the detection literature: we consider a keyword accurately detected when the intersection-over-union (IOU) between the prediction? loc and ground truth label y loc is above a certain threshold, and calculate the mean average precision mAP Loc . To calculate the IOU, we binarise the model's predictions using a threshold ? = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to baselines</head><p>We compare our model's performance against a state-of-the-art VSR model and KWS-Net <ref type="bibr" target="#b42">[43]</ref>, the previous state-of-the-art visual KWS model. VSR baseline. We use an improved version of the TM-seq2seq <ref type="bibr" target="#b1">[2]</ref> VSR model, with the same pre-trained CNN backbone (Section 3.4) that we use for the KWS models. The model is trained with the curriculum training strategy of <ref type="bibr" target="#b1">[2]</ref> (details in the Appendix). The VSR model achieves state-of-the-art Word Error Rate (WER) performance of 36.9% and 48.0% on the LRS2, LRS3 test sets respectively. Since the VSR model only produces text transcriptions of a given video, but no localisation prediction, we can only evaluate its classification performance (Acc Cls @k ,mAP Cls ). We follow the method detailed in <ref type="bibr" target="#b27">[28]</ref> to estimate the posterior probability that the keyword occurs in a video clip. KWS-Net. As a KWS baseline we use the state-of-the-art model of Momeni et al. <ref type="bibr" target="#b42">[43]</ref>. For fair comparison, here too we use the same CNN backbone that is also used for our model. State-of-the-art KWS. We report our model's performance and compare it with strong baselines in <ref type="table" target="#tab_2">Table 1</ref>. It is clear that our model outperforms both baselines. On the last row, LRS2 LRS3   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Architecture ablations</head><p>To assess our design choices for the Transformer skeleton, we perform a number of ablations considering variations of the model architecture. We briefly explain the alternative approaches below; more details can be found in the Appendix. In particular we consider two alternative encoder-decoder architectures, with the video input at the encoder side and the text query at the decoder (Enc vid -Dec text ) and vice versa (Enc text -Dec vid ). Since the latter model outputs at the temporal resolution of the video input, it can explicitly localise the keyword (in the same way as the Transpotter), while the former can only perform classification. We also consider a variant of the Transpotter, where the model does not output localisation predictions (hence no L loc is used for its training). We show the results in <ref type="table">Table 3</ref>. The selected Transpotter architecture outperforms all variants. In particular, by comparing rows 2 and 4, we observe that training with a localisation head and loss L loc also improves classification (e.g. 64.0 vs 69.2 mAP Cls ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transpotter performance analysis</head><p>In this section, we analyse the performance of our proposed method when varying the keyword length and the size of the surrounding visual context. Keyword length. In <ref type="figure" target="#fig_1">Figure 2a</ref>, we plot the model's performance on the LRS2 test set against the minimum keyword length in phonemes n p . As expected, longer keywords are easier  <ref type="table">Table 3</ref>: Model ablations: Our approach of jointly modeling text and video sequences with a localisation head for stronger supervision outperforms other architectural designs. to spot and therefore result in better retrieval performance. Indeed for long 7-phoneme keywords, mAP Loc reaches as high as 82.5. We note however that even for very challenging short keywords with only 2 phonemes (such as "my", "to", "at"), mAP Loc stays high at 67.5.</p><p>Context. The visual appearance of spoken words can be highly ambiguous <ref type="bibr" target="#b1">[2]</ref>, therefore recognising isolated words from visual input alone may be very challenging. Current lip reading models utilise the surrounding visual context to resolve this ambiguity. In <ref type="figure" target="#fig_1">Figure 2b</ref>, we illustrate how the performances of our Transpotter KWS model and our VSR baseline vary based on the amount of contextual information available. We plot the mAP Cls against the number of words in the video clip. We observe that both models benefit from larger surrounding context, with the Transpotter outperforming the VSR baseline consistently. Qualitative analysis. In <ref type="figure" target="#fig_2">Figure 3</ref>, we show qualitative examples from the LRS2 and LRS3 test sets. It is clear that the model produces smooth predictions that precisely indicate the full location of the word. In the bottom right corner we observe a failure case where the model's confidence is low -the keyword "that's" in this case is short. Model response to homophemes. We further probe our Transpotter model for failure cases.</p><p>In visual-only keyword spotting, a common failure case is due to homophemes, i.e. words with identical lip movements. To investigate the response of our model to such cases, we construct a list of keywords from the LRS2 test set sentences that are known to have homopheme counterparts (e.g. mark, which has two matching homophemes, bark and park) and then for each test set clip that contains one of the keywords, we query that keyword along with its corresponding homophemes and plot the model's outputs. We illustrate several examples in <ref type="figure">Figure 4</ref>. We observe that in such cases, the model spots the keyword as well as its homophemes at the same (ground truth) location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mouthing Spotting in Sign Language videos</head><p>In this section, we investigate the application of our method for spotting mouthed words in sign language videos. This is an important application of visual KWS, as it has enabled an entire line of work on sign language recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b61">62]</ref>.  <ref type="figure">Figure 4</ref>: Model's response to homophemes: We query words and their corresponding homophemes for LRS2 test set clips. We observe that the model spots the words and their homophemes at the same (ground truth) location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data description &amp; evaluation protocol.</head><p>Here, we use a subset of BSL Corpus <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> as a test set. BSL Corpus is a large public dataset containing videos of conversations conducted in sign language by deaf signers, from various regions across the UK. We extend the dataset's annotations by adding a Mouthing tier and asking a deaf annotator to identify and localise mouthing occurrences that correspond to visible signs. We obtain 383 mouthing instances, from 29 different signers, over a keyword vocabulary size of 187. We use a pre-processing pipeline similar to <ref type="bibr" target="#b17">[18]</ref> to obtain face-cropped tracks around the faces of the signers. To evaluate KWS performance, we take 8-second video clips centered around the annotated mouthings and follow the same evaluation protocol described in Section 4.</p><p>Results. We summarise the evaluation results in <ref type="table">Table 4</ref>. The Transpotter model is far superior to the prior state-of-the-art KWS baseline, achieving a great improvement in performance (e.g. 29.6 vs 15.6 mAP Cls score). To complete this analysis, we also show qualitative examples of the spotted mouthings in <ref type="figure">Figure 5</ref>.</p><p>Discussion. We note that sign language mouthings are often very different from equivalent spoken words. Words may be partially mouthed and can be occluded by the signing hands. There is therefore a significant domain gap between the BSL-Corpus signing videos and our lip reading training videos. However, we note that our proposed model greatly outperforms the KWS-Net baseline -a variant of which has been successfully deployed for detecting mouthings in order to bootstrap learning of sign spotting methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b61">62]</ref>. This indicates the potential of our proposed method to greatly improve these pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc Cls</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@1</head><p>Acc  <ref type="table">Table 4</ref>: Spotting mouthings in BSL-Corpus: The Transpotter is far more accurate than the current state-of-the-art in spotting keywords in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Localisation probability</head><p>Time frames <ref type="figure">Figure 5</ref>: Qualitative results on BSL-Corpus: Despite the large domain shift from our training examples and additional challenges such as partially mouthed words and hand occlusions, the Transpotter succeeds in correctly spotting mouthings in these challenging conditions. We observe a failure case (bottom right) where the localisation is incorrect. We note that contrary to LRS2 and LRS3, where word boundaries are obtained through robust audio-based forced alignment, the annotations for BSL-Corpus are noisier as they are performed manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented the Transpotter, a cross-modal attention based architecture for visual keyword spotting. Our method surpasses the performance of the previous best visual keyword spotting approach by a large margin, as well as that of a state-of-the-art lip reading baseline. We demonstrate the ability of our model to generalise to sign language videos where it can be used to spot mouthings, enabling automatic annotation of sign instances. In future work, we plan to further improve our method's performance by incorporating keyword semantics and context of the surrounding words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Application to silent films</head><p>Our Transpotter model has been trained to spot words in silent talking face videos. Naturally, the next step is to apply it on scenes from actual silent films to detect if a word is spoken and if so, also localize it in time. On the project website, we show qualitative examples of applying the Transpotter on scenes from the silent film, The Artist. Silent films often use title cards to convey character dialogues. Although audio was not recorded, actors would still talk during the shooting, following the script, as it enabled them to act more naturally. We therefore used the film's screenplay (available online) to identify scenes where title cards appear, which indicates the presence of dialogue, and queried words out of the title card content of these scenes. This application is particularly challenging because the title cards are only a guideline for the dialogue, therefore the lines that the actors end up using may differ substantially. Moreover we note that in this particular example, some of the actors are not native English speakers, therefore the model also has to deal with a domain shift in terms of accent. Despite those challenges, the Transpotter is still able to detect and properly localize words. Another application with a similar domain shift is spotting mouthings in sign language, which we have already discussed in the main paper.</p><p>The rest of this PDF is organized as follows. Section B describes the datasets we used for training and evaluation in detail. Next, we elaborate on the training process for the VSR baseline in Section C. In Section D, we describe the hyper-parameters choices for the Transpotter architecture. In Section E, we evaluate the model for longer query sequences containing multiple words. In Section F, we provide details for the architecture variants that were used in the main paper. In Section G, we conduct two more ablation studies, on the choice of the localization head and on the need for modality embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Statistics</head><p>We conduct most of our experiments on the Lip Reading LRS2 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref> and LRS3 <ref type="bibr" target="#b0">[1]</ref> datasets. Each dataset contains three splits: (i) Pre-train, (ii) Train-val, and (iii) Test. The utterances in the pre-train set correspond to part-sentences as well as multiple sentences, whereas the training set only consists of single full sentences or phrases. The number of utterances, words and vocabulary for each of the splits in both datasets is reported in  Our models are first pre-trained on both the LRS2, LRS3 pre-train splits. Before testing on the test set of each dataset, we fine-tune the model on the train-val split of that particular dataset. Training hyper-parameters are detailed in the next sections.</p><p>Similar to the spotting models, we fine-tune on the train-val set of LRS2/LRS3 before evaluation. During inference, we decode text sequences with a left-to-right beam search with a beam width of B = 20. The beam hypotheses scores and textual outputs are used for computing the keyword spotting scores. More details of the evaluation mechanism can be found in this paper <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Transpotter hyperparameters</head><p>Model hyper-parameters. We set the number of text encoder layers N t = 3, the number of video encoder layers N v = 6, and the number of joint multimodal transformer layers T m = 6. The embedding dimension d is set to 512, and each multi-headed attention layer uses 8 heads.</p><p>Training hyper-parameters. The Transpotter is trained on the extracted visual features that we obtain after Stage 1 training (described above) of the VSR model. We use the Adam optimizer <ref type="bibr" target="#b36">[37]</ref> with an initial learning rate of 5e ?5 , which is decreased by a factor of 5 every time the validation loss plateaus for 15 epochs. The minimum learning rate is 1e ?6 . The model is trained for 280 epochs on a single Ampere A40 GPU, which takes about 4 ? 5 days. We fine-tune on the train-val set of LRS2/LRS3 before evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Using the Transpotter to spot phrases</head><p>The Transpotter architecture can be used to spot multiple words (phrases) without any change in the architecture. Instead of feeding a phoneme sequence for a single word, we simply feed a phoneme sequence for a phrase. This is obtained by concatenating the phoneme sequence of each word in the phrase. We fine-tune the word-level model for a slight boost in performance.</p><p>In order to evaluate this model, we form our query set by using n-grams from the LRS2 and LRS3 test sets. We evaluate for n = 1, 2, 3. Note that n = 1 corresponds to single words. As is done for our single word evaluation in the main paper, we only use words with number of phonemes n p ? 3. The scores are reported in <ref type="table" target="#tab_9">Table 7</ref>. We know that Transpotter becomes increasingly accurate with longer query lengths from <ref type="figure" target="#fig_1">Figure 2</ref> (a) of the main paper, and the same trend continues when we try to spot phrases instead of single words. Spotting phrases is clearly far easier than spotting single words.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Details for the Architecture Ablations</head><p>In <ref type="table" target="#tab_3">Table 2</ref> (Section 4.3) of the main paper, we experiment with alternative Transformer architectures for this task. We explain each of the architectures below and also illustrate them in <ref type="figure">Figure 6</ref>. Enc vid -Dec text : This is the standard Transformer Encoder-Decoder architecture, which has also been used with great success <ref type="bibr" target="#b1">[2]</ref> in related tasks such as VSR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLS</head><p>FC+Sig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V'</head><p>Trans. Dec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q' y cl?</head><p>Trans. Enc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC+Sig. FC+Sig.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V'</head><p>Trans. Dec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q'</head><p>y cl? y lo? <ref type="figure">Figure 6</ref>: Architecture ablations: We illustrate each of the architecture variants tried for this task. The models are compared in <ref type="table" target="#tab_3">Table 2</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Choice of Localization Head</head><p>In <ref type="table">Table G</ref>.1, we compare our proposed localization head with an alternative start-end span prediction head. For this, we have two MLP layers that predict start and end probabilities at each video frame indicating the span of the keyword. The frame-wise probabilities are softmax-normalized and optimized with cross-entropy loss during training. We observe that our proposed localization head consistently outperforms this alternative technique.   <ref type="table">Table 9</ref>: Are modality embeddings needed in Transpotter? Presence/absence of modality embeddings to explicitly specify the text and video time-steps does not affect the performance. Our dedicated video and text encoders encode sufficient modality specific information.</p><p>Since we concatenate our video and text embeddings together into a single sequence for feeding into the joint multi-modal transformer, the question arises if we need to explicitly specify which time-steps are video/text. We argue that this is not necessary because we have separate text and video encoders before the joint transformer. In order to empirically show this, we train a model with learnable modality tokens that are added to the text and video time-steps. We use two learnable vectors (one each for video/text) of embedding dimension d and add them to each of the video/text time-step accordingly. We perform this addition just before feeding them into the joint transformer. In <ref type="table">Table G</ref>.2, we show that this performs comparably to our Transpotter model without any modality tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Error analysis on LRW</head><p>In this section, we illustrate some of the common errors made by our model. We conduct this analysis on the LRW test set using the same retrieval protocol that we have used in all our experiments, i.e. given a query word, we retrieve the top-K videos. Since LRW videos correspond to a single word label, we can clearly see which word labels are wrongly retrieved.</p><p>In <ref type="table">Table H</ref>, we show the word labels of the incorrectly retrieved videos (among the top 10 retrievals) for some of the query words. We observe that often errors occur due to some of the phonemes of the query word appearing in the video. For example, the words "president" and "prison" share several phonemes. We also observe that compound words are a source of error: for example, in the second row, where "every" appears entirely in "everybody" and "everything". Finally, as discussed in Section 4.4, another failure case are homophemes. For example, in the last row, we see an example of the model retrieving the homopheme "million" for the query word "billion".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query word</head><p>Mistakes among the top 10 retrievals president press, prison, pressure every everybody, everything allowed allow, cloud, announced example couple, happen billion million, building <ref type="table" target="#tab_2">Table 10</ref>: Error analysis on LRW: For each query word, we report examples of incorrectly retrieved word instances among the top 10 retrievals. We can see clear patterns in mistakes such as phoneme overlap between the query word and the mistaken word, compound words and homophemes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 :</head><label>1</label><figDesc>The Transpotter architecture: Video frames are inputted to a visual front-end (CNN<ref type="bibr" target="#b1">[2]</ref> or VTP<ref type="bibr" target="#b33">[34]</ref>) to extract low-level visual features, which are then passed to N v Transformer layers to encode temporal information. The keyword in the form of a phoneme sequence is encoded using N t Transformer layers. The text and visual features are finally concatenated in time and processed using a joint multi-modal Transformer which predicts: (i) the probability the keyword occurs in the video, (ii) frame-level probabilities indicating the location of the word. PE corresponds to positional encoding. Q enc , we pass the visual features through a Transformer Encoder with N v layers to capture temporal information, after adding positional encodings:V enc = encoder v (V + PE 1:T ) ? R T ?d .Joint Video-Text Representations. The uni-modal representations V and Q are concatenated along the time dimension to produce a single sequence of feature vectors. A learnable [CLS]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Transpotter's performance increases with the keyword length; (b) Transpotter performs far better than VSR with limited context. Both methods improve with more context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on LRS2 and LRS3: The Transpotter accurately localises the keyword in most examples. In the bottom right example, the model's confidence is low, most likely because it is a short word. The IOU is zero since we threshold at ? = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and LRS3<ref type="bibr" target="#b0">[1]</ref> lip reading datasets. LRS2 contains BBC broadcast footage from British television and LRS3 is based on TED/TEDx videos downloaded from YouTube (refer to the Appendix for detailed statistics). The video clips for both datasets are tightly cropped face-tracks of active speakers only. For each clip, a full transcription of the utterance as well as word boundary alignments are provided. The number of videos, number of keyword instances and keyword vocabulary for each of the test sets is shown inTable 1. Evaluation Protocol. Evaluation is performed for every test dataset as follows: First, the vocabulary of test keywords is determined, by considering all the words occurring in the test set transcriptions with above a certain phoneme length n p . If not specified, we use n p ? 3. Every word in the query vocabulary is then searched for in all the test set videos.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1.2K vids. / 4.3K inst. / 1.6K vocab. 1.3K vids. / 6.1K inst. / 1.9K vocab.</figDesc><table><row><cell>Model</cell><cell>Acc Cls @1</cell><cell>Acc Cls @5</cell><cell>mAP Cls</cell><cell>mAP Loc</cell><cell>Acc Cls @1</cell><cell>Acc Cls @5</cell><cell>mAP Cls</cell><cell>mAP Loc</cell></row><row><cell>KWS-Net [43]</cell><cell>36.1</cell><cell>61.2</cell><cell>41.0</cell><cell>36.2</cell><cell>29.8</cell><cell>54.6</cell><cell>34.3</cell><cell>29.2</cell></row><row><cell>VSR</cell><cell>63.7</cell><cell>76.3</cell><cell>64.3</cell><cell>-</cell><cell>52.3</cell><cell>66.0</cell><cell>50.3</cell><cell>-</cell></row><row><cell>Transpotter</cell><cell>65.0</cell><cell>87.1</cell><cell>69.2</cell><cell>68.3</cell><cell>52.0</cell><cell>77.1</cell><cell>55.4</cell><cell>53.6</cell></row><row><cell>Transpotter (VTP)</cell><cell>68.7</cell><cell>90.7</cell><cell>72.5</cell><cell>71.6</cell><cell>55.7</cell><cell>78.5</cell><cell>58.2</cell><cell>56.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison to baselines: We outperform the current state-of-the-art KWS and VSR methods by a large margin. Our Transpotter model is particularly effective in localising the keyword in the video. Moreover, by using the recently proposed VTP<ref type="bibr" target="#b33">[34]</ref> architecture as the Transpotter's visual backbone instead of a CNN, we achieve even better performance. we show the boost in performance by replacing the CNN with the recently proposed VTP backbone<ref type="bibr" target="#b33">[34]</ref>, resulting in state-of-the art performance on both the LRS2 and LRS3 datasets.</figDesc><table><row><cell>Model</cell><cell>Acc Cls @1</cell><cell>Acc Cls @5</cell><cell>mAP Cls</cell></row><row><cell>KWS-Net [43]</cell><cell>66.6</cell><cell>89.0</cell><cell>33.0</cell></row><row><cell>Transpotter</cell><cell>85.8</cell><cell>99.6</cell><cell>64.1</cell></row></table><note>Evaluation on LRW. We also compare the performance of KWS-Net [43] with our proposed Transpotter model on the LRW [16] test set following the same evaluation protocol. The test set contains 25K single-word video clips spanning a vocabulary of 500 words (50 instances per word). Note that KWS-Net has been pretrained on the LRW training split, but the Transpotter has only been trained on LRS2 and LRS3. As we can see in Table 2, the Transpotter outperforms the previous state-of-the-art baseline KWS-Net by a large margin. We refer the reader to the Appendix for a qualitative error analysis in this setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison on LRW [16]: The Transpotter outperforms the previous state-of-the- art KWS model on the LRW test set, despite not having been trained on LRW data. The localization metric mAP Loc is not reported as the input videos are single-word clips.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Split</cell><cell>#Utterances</cell><cell>#Words</cell><cell>#Hours</cell><cell>Vocabulary</cell></row><row><cell></cell><cell>Pre-train</cell><cell>96k</cell><cell>2M</cell><cell>195</cell><cell>41k</cell></row><row><cell>LRS2 [2, 17]</cell><cell>Train-val</cell><cell>47k</cell><cell>336k</cell><cell>29</cell><cell>18k</cell></row><row><cell></cell><cell>Test</cell><cell>1.2k</cell><cell>6k</cell><cell>0.5</cell><cell>1.7k</cell></row><row><cell></cell><cell>Pre-train</cell><cell>132k</cell><cell>3.9M</cell><cell>444</cell><cell>51k</cell></row><row><cell>LRS3 [1]</cell><cell>Train-val</cell><cell>32k</cell><cell>358k</cell><cell>30</cell><cell>17k</cell></row><row><cell></cell><cell>Test</cell><cell>1.3k</cell><cell>10k</cell><cell>1</cell><cell>2k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Audio-visual datasets: Statistics of LRS2 and LRS3 datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Spotting phrases: Transpotter can be trained to spot multi-word queries (phrases) with no change in the architecture. We report the scores for spotting unigrams (single words), bigrams and trigrams. Longer queries can be spotted far more easily.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>and I thought this would be great fun"Trans. Enc. Trans. Enc. CLS FC+Sig. FC+Sig.Trans. Enc. Trans. Enc. CLS FC+Sig.</head><label></label><figDesc>The video encoder contains 6 Transformer layers encoding the temporal information for the video input. A text decoder consists of 6 Transformer decoder layers, each with (i) self-attention across the phoneme feature vectors, (ii) cross-attention between the phoneme and video feature vectors. The text input to the decoder is preprended with a [CLS] vector, where the probability of the keyword occurring in the video is predicted. Note that this model cannot explicitly localize the keyword in the video, as the output time-steps correspond to the text sequence and not the video.Enc text -Dec vid : This architecture is similar to the previous one, except the inputs are swapped. The encoder processes the text sequence and the decoder inputs the video frames (prepended with a [CLS] vector). Since the output time-steps of the decoder correspond to the video, the exact same localization loss used in Transpotter can be applied here for localizing the keyword. Transpotter w/o localization head: We also train our Transpotter model without the localization head. We only optimize for the presence/absence of the keyword at the [CLS] time-step.All the above variants are trained using the exact same data pipeline and optimizer hyper-parameters.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Transpotter</cell><cell></cell><cell cols="2">Transpotter w/o loc.</cell></row><row><cell>y cl?</cell><cell>y lo?</cell><cell></cell><cell></cell><cell>y cl?</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>V'</cell></row><row><cell></cell><cell>Venc V'</cell><cell cols="2">Trans. Enc.</cell><cell>Qenc Q'</cell><cell>Venc V'</cell><cell>Trans. Enc.</cell><cell>Qenc Q'</cell><cell>3D/2D CNN GT: "PE V</cell></row><row><cell></cell><cell></cell><cell cols="3">Encvid-Dectext</cell><cell></cell><cell cols="2">Enctext-Decvid</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q'</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PE</cell></row><row><cell></cell><cell></cell><cell>y cl?</cell><cell></cell><cell>y Q</cell><cell></cell><cell></cell><cell>Q</cell></row><row><cell></cell><cell></cell><cell>FC+Sig.</cell><cell cols="2">FC+Softmax</cell><cell></cell><cell></cell><cell>Embedding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>G</cell><cell>R</cell><cell>EY1</cell><cell>T</cell></row><row><cell></cell><cell>Trans. Enc.</cell><cell></cell><cell cols="2">Trans. Dec.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Keyword: "great"</cell></row></table><note>Trans. Enc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Ablations on the Localization head: Per-frame sigmoid prediction of the keyword location outperforms the softmax-based span prediction.</figDesc><table><row><cell cols="5">G.2 Are modality embeddings needed?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>LRS2</cell><cell></cell><cell></cell><cell></cell><cell>LRS3</cell><cell></cell></row><row><cell>Modality Tokens</cell><cell>Acc Cls @1</cell><cell>Acc Cls @5</cell><cell>mAP Cls</cell><cell>mAP Loc</cell><cell>Acc Cls @1</cell><cell>Acc Cls @5</cell><cell>mAP Cls</cell><cell>mAP Loc</cell></row><row><cell></cell><cell>64.6</cell><cell>87.3</cell><cell>68.9</cell><cell>68.3</cell><cell>52.9</cell><cell>77.0</cell><cell>55.4</cell><cell>53.5</cell></row><row><cell>?</cell><cell>65.0</cell><cell>87.1</cell><cell>69.2</cell><cell>68.3</cell><cell>52.0</cell><cell>77.1</cell><cell>55.4</cell><cell>53.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">the n p outputs corresponding to the phonetic embeddings are dropped.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Funding for this research is provided by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems, the Oxford-Google DeepMind Graduate Scholarship, the EPSRC Programme Grant VisualAI (EP/T028572/1) and the Royal Society Research Professorships 2019 RP/R1/191132. We thank Samuel Albanie for his invaluable help in applying our method to signer mouthings.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C VSR baseline</head><p>In this section, we describe the exact architectural details and training hyper-parameters for the VSR baseline. Architecture details. The architecture closely resembles the TM-seq2seq architecture <ref type="bibr" target="#b1">[2]</ref>. The only minor changes are all in the visual backbone, which we describe below in <ref type="table">Table 6</ref>  Training hyper-parameters. We train the VSR baseline on LRS2, LRS3. The training consists of two stages, similar to <ref type="bibr" target="#b1">[2]</ref>. In the first stage, we train the visual backbone end-to-end with the transformer layers on all two-word video clips (obtained using the available LRS2, LRS3 word alignments). Both the CNN and Transformer layers use the Adam optimizer <ref type="bibr" target="#b36">[37]</ref>, but with different learning rate schedules. For the CNN, we start with an initial learning rate of 1e ?4 and reduce it by a factor of 2 every time the validation loss plateaus for 3 epochs. The minimum learning rate for the CNN is 1e ?5 . The transformer layers start with an initial learning rate of 5e ?5 . We do not reduce this learning rate in the first stage. The first stage takes approximately 6 days on four Tesla v100 GPUs.</p><p>After the visual backbone is trained, we extract the features for all the video clips in our datasets, and use these fixed features for all further training. We follow the curriculum strategy of <ref type="bibr" target="#b1">[2]</ref> and train the transformer layers for longer video segments (9 words). We halve the learning rate every time the validation loss plateaus for 10 epochs. The minimum learning rate for the transformer layers is 1e ?6 . The second stage takes about 2 days on one Tesla v100 GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">LRS3-TED: a largescale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asr is all you need: Cross-modal distillation for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Characterlevel language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3159" to="3166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Samuel Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: Sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End to-end asr-free keyword search from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Bhuvana Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Keyword transformer: A selfattention model for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel Tairum</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cruz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00769</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Hands Are The Head of The Mouth. The Mouth as Articulator in Sign Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boyes Braem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Sutton-Spence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Signum Press</publisher>
			<biblScope unit="volume">3927731838</biblScope>
			<pubPlace>Hamburg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Aligning subtitles in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lattice indexing for spoken term detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogan</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sara?lar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Smallfootprint keyword spotting with graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Signs in time: Encoding human motion as a temporal image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Brave New Ideas for Motion Representations, ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio-visual keyword spotting based on multidimensional convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech-transformer: A no-recurrence sequenceto-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An application of recurrent neural networks to discriminative keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Excl: Extractive clip localization using natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuva</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dbn based multi-stream models for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>John N Gowdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 IEEE International conference on acoustics, speech, and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">993</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Streaming small-footprint keyword spotting using sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcgraw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ASRU</title>
		<meeting>IEEE ASRU</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Online keyword spotting with a character-level recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuyeon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonyong</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08903</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Minimum Prediction Residual Principle Applied to Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumitada</forename><surname>Itakura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="volume">1558601244</biblScope>
			<biblScope unit="page" from="154" to="158" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word spotting in silent lip videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sub-word level lip reading with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K R</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07603</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Temporal feedback convolutional recurrent neural networks for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taejun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01803</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An end-to-end architecture for keyword spotting and voice activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 End-to-End Learning for Speech and Audio Processing Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving audio-visual speech recognition performance with cross-modal student-teacher training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6560" to="6564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attentive moment retrieval in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent neural network transducer for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaki</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basilio</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otavio</forename><surname>Braga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Siohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<title level="m">Transformers with convolutional context for asr</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Themos Stafylakis, Samuel Albanie, and Andrew Zisserman. Seeing wake words: Audio-visual keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Triantafyllos Afouras, and Andrew Zisserman. Watch, read and lookup: Learning to spot signs from multiple supervisors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Jointly learning to locate and classify words using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Palaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanassios</forename><surname>Katsamanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="435" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Vassilis Pitsikalis, and Petros Maragos</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Audio-Visual Speech Recognition with a Hybrid CTC/Attention Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerasimos</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chalapathy</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A hidden markov model based keyword recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas B</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">End-to-end speech recognition and keyword search on low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bhuvana Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picheny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seibi</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Schembri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Fenlon</surname></persName>
		</author>
		<title level="m">Ramas Rentelis, Sally Reynolds, and Kearsy Cormier. Building the British Sign Language Corpus. Language Documentation &amp; Conservation</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="136" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">British Sign Language Corpus Project: A corpus of digital video data and annotations of British Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Schembri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Fenlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramas</forename><surname>Rentelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kearsy</forename><surname>Cormier</surname></persName>
		</author>
		<ptr target="http://www.bslcorpusproject.org" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Third Edition</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Senior, and Nando de Freitas. Large-Scale Visual Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?an</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsav</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrayne</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<meeting><address><addrLine>Marie Mulville, Ben Coppin, Ben Laurie, Andrew</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<ptr target="http://www.speech.cs.cmu.edu/cgi-bin/cmudict" />
		<title level="m">CMU pronouncing dictionary</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Speech Group at Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Combining residual networks with lstms for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Zero-shot keyword spotting for visual speech recognition in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Maxpooling loss training of long short-term memory networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaran</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikko</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Amsterdam studies in the theory and history of linguistic science, series 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Sutton-Spence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page">147</biblScope>
		</imprint>
	</monogr>
	<note>Mouthings and simultaneity in british sign language</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Triantafyllos Afouras, and Andrew Zisserman. Read and attend: Temporal localisation in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Trainable frontend for robust and far-field keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Getreuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><surname>Saurous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Application of hidden markov models for recognition of a limited set of words in unconstrained speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Wilpon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.1989.266413</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1989-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="254" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A novel lip descriptor for audio-visual keyword spotting based on adaptive decision fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multilevel language and vision integration for text-to-clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Spotting visual keywords from temporal sliding windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mandarin Audio-Visual Speech Recognition Challenge</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Audio-visual recognition of overlapped speech for the lrs2 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shansong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP40776.2020.9054127</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="6984" to="6988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">To find where you talk: Temporal sentence localization in video with attention based location regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dense regression network for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence models for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00348</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Spatio-temporal fusion based convolutional sequence learning for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="713" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU.2009.5372931</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="398" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Hello edge: Keyword spotting on microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unrestricted Vocabulary Keyword Spotting Using LSTM-CTC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimeng</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
