<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Region Proposal by Guided Anchoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Rekognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<email>dhlin@ie.cuhk.edu.hkshuoy@amazon.comccloy@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Region Proposal by Guided Anchoring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Region anchors are the cornerstone of modern object detection techniques. State-of-the-art detectors mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the spatial domain with a predefined set of scales and aspect ratios. In this paper, we revisit this foundational stage. Our study shows that it can be done much more effectively and efficiently. Specifically, we present an alternative scheme, named Guided Anchoring, which leverages semantic features to guide the anchoring. The proposed method jointly predicts the locations where the center of objects of interest are likely to exist as well as the scales and aspect ratios at different locations. On top of predicted anchor shapes, we mitigate the feature inconsistency with a feature adaption module. We also study the use of high-quality proposals to improve detection performance. The anchoring scheme can be seamlessly integrated into proposal methods and detectors. With Guided Anchoring, we achieve 9.1% higher recall on MS COCO with 90% fewer anchors than the RPN baseline. We also adopt Guided Anchoring in Fast R-CNN, Faster R-CNN and RetinaNet, respectively improving the detection mAP by 2.2%, 2.7% and 1.2%. Code will be available at https: //github.com/open-mmlab/mmdetection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anchors are regression references and classification candidates to predict proposals (for two-stage detectors) or final bounding boxes (for single-stage detectors). Modern object detection pipelines usually begin with a large set of densely distributed anchors. Take Faster RCNN <ref type="bibr" target="#b26">[27]</ref>, a popular object detection framework, for instance, it first generates region proposals from a dense set of anchors and then classifies them into specific classes and refines their locations via bounding box regression.</p><p>There are two general rules for a reasonable anchor de- * Equal contribution. sign: alignment and consistency. Firstly, to use convolutional features as anchor representations, anchor centers need to be well aligned with feature map pixels. Secondly, the receptive field and semantic scope should be consistent with the scale and shape of anchors on different locations of a feature map. The sliding window is a simple and widely adopted anchoring scheme following the rules. For most detection methods, the anchors are defined by such a uniform scheme, where every location in a feature map is associated with k anchors with predefined scales and aspect ratios. Anchor-based detection pipelines have been shown effective in both benchmarks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5]</ref> and real-world systems. However, the uniform anchoring scheme described above is not necessarily the optimal way to prepare the anchors. This scheme can lead to two difficulties: (1) A neat set of anchors of fixed aspect ratios has to be predefined for different problems. A wrong design may hamper the speed and accuracy of the detector. <ref type="bibr" target="#b1">(2)</ref> To maintain a sufficiently high recall for proposals, a large number of anchors are needed, while most of them correspond to false candidates that are irrelevant to the object of interests. Meanwhile, a large number of anchors can lead to significant computational cost especially when the pipeline involves a heavy classifier in the proposal stage.</p><p>In this work, we present a more effective method to prepare anchors, with the aim to mitigate the issues of handpicked priors. Our method is motivated by the observation that objects are not distributed evenly over the image. The scale of an object is also closely related to the imagery content, its location and geometry of the scene. Following this intuition, our method generates sparse anchors in two steps: first identifying sub-regions that may contain objects and then determining the shapes at different locations.</p><p>Learnable anchor shapes are promising, but it breaks the aforementioned rule of consistency, thus presents a new challenge for learning anchor representations for accurate classification and regression. Scales and aspect ratios of anchors are now variable instead of fixed, so different feature map pixels have to learn adaptive representations that fit the corresponding anchors. To solve this problem, we introduce an effective module to adapt the features based on anchor geometry.</p><p>We formulate a Guided Anchoring Region Proposal Network (GA-RPN) with the aforementioned guided anchoring and feature adaptation scheme. Thanks to the dynamically predicted anchors, our approach achieves 9.1% higher recall with 90% substantially fewer anchors than the RPN baseline that adopts dense anchoring scheme. By predicting the scales and aspect ratios instead of fixing them based on a predefined list, our scheme handles tall or wide objects more effectively. Besides region proposals, the guided anchoring scheme can be easily integrated into any detectors that depend on anchors. Consistent performance gains can be achieved with our scheme. For instance, GA-Fast-RCNN, GA-Faster-RCNN and GA-RetinaNet improve overall mAP by 2.2%, 2.7% and 1.2% respectively on COCO dataset over their baselines with sliding window anchoring. Furthermore, we explore the use of high-quality proposals, and propose a fine-tuning schedule using GA-RPN proposals, which can improve the performance of any trained models, e.g., it improves a fully converged Faster R-CNN model from 37.4% to 39.6%, in only 3 epochs.</p><p>The main contributions of this work lie in several aspects. (1) We propose a new anchoring scheme with the ability to predict non-uniform and arbitrary shaped anchors other than dense and predefined ones. <ref type="bibr" target="#b1">(2)</ref> We formulate the joint anchor distribution with two factorized conditional distributions, and design two modules to model them respectively. <ref type="bibr" target="#b2">(3)</ref> We study the importance of aligning features with the corresponding anchors and design a feature adaption module to refine features based on the underlying anchor shapes. (4) We investigate the use of high-quality proposals for two-stage detectors and propose a scheme to improve the performance of trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sliding window anchors in object detection. Generating anchors with the sliding window manner in feature maps has been widely adopted by anchor-based various detectors. The two-stage approach has been the leading paradigm in the modern era of object detection. Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> proposes the Region Proposal Network (RPN) to generates object proposals. It uses a small fully convolutional network to map each sliding window anchor to a low-dimensional feature. This design is also adopted in later two-stage methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>. MetaAnchor <ref type="bibr" target="#b31">[32]</ref> introduces meta-learning to anchor generation. There have been attempts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref> that apply cascade architecture to reject easy samples at early layers or stages, and regress bounding boxes iteratively for progressive refinement. Compared to two-stage approaches, the single-stage pipeline skips object proposal generation and predicts bounding boxes and class scores in one evaluation.</p><p>Although the proposal step is omitted, single-stage methods still use anchor boxes produced by the sliding window. For instance, SSD <ref type="bibr" target="#b20">[21]</ref> and DenseBox <ref type="bibr" target="#b13">[14]</ref> generate anchors densely from feature maps and evaluate them like a multiclass RPN. RetinaNet <ref type="bibr" target="#b18">[19]</ref> introduces focal loss to address the foreground-background class imbalance. YOLOv2 <ref type="bibr" target="#b25">[26]</ref> adopt sliding window anchors for classification and spatial location prediction so as to achieve a higher recall than its precedent. Comparison and difference. We summarize the differences between the proposed method and conventional methods as follows. (i) Primarily, previous methods (singlestage, two-stage and multi-stage) still rely on dense and uniform anchors by sliding window. We discard the sliding window scheme and propose a better counterpart to guide the anchoring and generate sparse anchors, which has not been explored before. (ii) Cascade detectors adopt more than one stage to refine detection bounding boxes progressively, which usually leads to more model parameters and a decrease in inference speed. These methods adopt RoI Pooling or RoI Align to extract aligned features for bounding boxes, which is too expensive for proposal generation or single-stage detectors. (iii) Anchor-free methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref> usually have simple pipelines and produce final detection results within a single stage. Due to the absence of anchors and further anchor-based refinement, they lack the ability to deal with complex scenes and cases. Our focus is the sparse and non-uniform anchoring scheme and use of high-quality proposals to boost the detection performance. Towards this goal, we have to solve the misalignment and inconsistency issues which are specific to anchorbased methods. (iv) Some single-shot detectors <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b29">30]</ref> refine anchors by multiple regression and classification. Our method differs from them significantly. We do not refine anchors progressively, instead, we predict the distribution of anchors, which is factorized as locations and shapes. Conventional methods fail to consider the alignment between anchors and features so they regress anchors (represented by [x, y, w, h]) for multiple times and breaks the alignment as well as consistency. On the contrary, we emphasize the importance of the two rules, so we only predict anchor shapes but fix anchor centers and adapt features based on the predicted shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Guided Anchoring</head><p>Anchors are the basis in modern object detection pipelines. Mainstream frameworks, including two-stage and single-stage methods, mostly rely on a uniform arrangement of anchors. Specifically, a set of anchors with predefined scales and aspect ratios will be deployed over a feature map of size W ? H, with a stride of s. This scheme is inefficient, as many of the anchors are placed in regions where the objects of interest are unlikely to exist. In addi-  <ref type="figure">Figure 1</ref>: An illustration of our framework. For each output feature map in the feature pyramid, we use an anchor generation module with two branches to predict the anchor location and shape, respectively. Then a feature adaption module is applied to the original feature map to make the new feature map aware of anchor shapes. tion, such hand-picked priors unrealistically assume a set of fixed shape (i.e., scale and aspect ratio) for objects.</p><p>In this work, we aim to develop a more efficient anchoring scheme to arrange the anchors with learnable shapes, considering the non-uniform distribution of objects' locations and shapes. The guided anchoring scheme works as follows. The location and the shape of an object can be characterized by a 4-tuple in the form of (x, y, w, h), where (x, y) is the spatial coordinate of the center, w the width, and h the height. Suppose we draw an object from a given image I, then its location and shape can be considered to follow a distribution conditioned on I, as follows:</p><p>p(x, y, w, h|I) = p(x, y|I)p(w, h|x, y, I).</p><p>(</p><p>This factorization captures two important intuitions: <ref type="bibr" target="#b0">(1)</ref> given an image, objects may only exist in certain regions; and (2) the shape, i.e., scale and aspect ratio, of an object closely relates to its location. Following this formulation, we devise an anchor generation module as shown in the red dashed box of <ref type="figure">Figure 1</ref>. This module is a network comprised of two branches for location and shape prediction, respectively. Given an image I, we first derive a feature map F I . On top of F I , the location prediction branch yields a probability map that indicates the possible locations of the objects, while the shape prediction branch predicts location-dependent shapes. Given the outputs from both branches, we generate a set of anchors by choosing the locations whose predicted probabilities are above a certain threshold and the most probable shape at each of the chosen locations. As the anchor shapes can vary, the features at different locations should capture the visual content within different ranges. With this taken into consideration, we further introduce a feature adaptation module, which adapts the feature according to the anchor shape. The anchor generation process described above is based on a single feature map. Recent advances in object detec-tion <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> show that it is often helpful to operate on multiple feature maps at different levels. Hence, we develop a multi-level anchor generation scheme, which collects anchors at multiple feature maps, following the FPN architecture <ref type="bibr" target="#b17">[18]</ref>. Note that in our design, the anchor generation parameters are shared across all involved feature levels thus the scheme is parameter-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Anchor Location Prediction</head><p>As shown in <ref type="figure">Figure 1</ref>, the anchor location prediction branch yields a probability map p(?|F I ) of the same size as the input feature map F I , where each entry p(i, j|F I ) corresponds to the location with coordinate ((i + 1 2 )s, (j + 1 2 )s) on I, where s is stride of the feature map, i.e., the distance between neighboring anchors. The entry's value indicates the probability of an object's center existing at that location.</p><p>In our formulation, the probability map p(i, j|F I ) is predicted using a sub-network N L . This network applies a 1 ? 1 convolution to the base feature map F I to obtain a map of objectness scores, which are then converted to probability values via an element-wise sigmoid function. While a deeper sub-network can make more accurate predictions, we found empirically that a convolutional layer followed by a sigmoid transform strikes a good balance between efficiency and accuracy.</p><p>Based on the resultant probability map, we then determine the active regions where objects may possibly exist by selecting those locations whose corresponding probability values are above a predefined threshold L . This process can filter out 90% of the regions while still maintaining the same recall. As illustrated in <ref type="figure">Figure 4</ref>(b), regions like sky and ocean are excluded, while anchors concentrate densely around persons and surfboards. Since there is no need to consider those excluded regions, we replace the ensuing convolutional layers by masked convolution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> for more efficient inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Anchor Shape Prediction</head><p>After identifying the possible locations for objects, our next step is to determine the shape of the object that may exist at each location. This is accomplished by the anchor shape prediction branch, as shown in <ref type="figure">Figure 1</ref>. This branch is very different from conventional bounding box regression, since it does not change the anchor positions and will not cause misalignment between anchors and anchor features. Concretely, given a feature map F I , this branch will predict the best shape (w, h) for each location, i.e., the shape that may lead to the highest coverage with the nearest ground-truth bounding box.</p><p>While our goal is to predict the values of the width w and the height h, we found empirically that directly predicting these two numbers is not stable, due to their large range. Instead, we adopt the following transformation:</p><formula xml:id="formula_1">w = ? ? s ? e dw , h = ? ? s ? e dh .<label>(2)</label></formula><p>The shape prediction branch will output dw and dh , which will then be mapped to (w, h) as above, where s is the stride and ? is an empirical scale factor (? = 8 in our experiments). This nonlinear transformation projects the output space from approximate [0, 1000] to [?1, 1], leading to an easier and stable learning target. In our design, we use a sub-network N S for shape prediction, which comprises a 1 ? 1 convolutional layer that yields a two-channel map that contains the values of dw and dh, and an element-wise transform layer that implements Eq.(2). Note that this design differs essentially from the conventional anchoring schemes in that every location is associated with just one anchor of the dynamically predicted shape instead of a set of anchors of predefined shapes. Our experiments show that due to the close relations between locations and shapes, our scheme can achieve much higher recall than the baseline scheme. Since it allows arbitrary aspect ratios, our scheme can better capture those extremely tall or wide objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Anchor-Guided Feature Adaptation</head><p>In the conventional RPN or single stage detectors where the sliding window scheme is adopted, anchors are uniform on the whole feature map, i.e., they share the same shape and scale in each position. Thus the feature map can learn consistent representation. In our scheme, however, the shape of anchors varies across locations. Under this condition, we find that it may not be a good choice to follow the previous convention <ref type="bibr" target="#b26">[27]</ref>, in which a fully convolutional classifier is applied uniformly over the feature map. Ideally, the feature for a large anchor should encode the content over a large region, while those for small anchors should have smaller scopes accordingly. Following this intuition, we further devise an anchor-guided feature adaptation compo-nent, which will transform the feature at each individual location based on the underlying anchor shape, as</p><formula xml:id="formula_2">f i = N T (f i , w i , h i ),<label>(3)</label></formula><p>where f i is the feature at the i-th location, (w i , h i ) is the corresponding anchor shape. For such a location-dependent transformation, we adopt a 3 ? 3 deformable convolutional layer <ref type="bibr" target="#b3">[4]</ref> to implement N T . As shown in <ref type="figure">Figure 1</ref>, we first predict an offset field from the output of anchor shape prediction branch, and then apply deformable convolution to the original feature map with the offsets to obtain f I . On top of the adapted features, we can then perform further classification and bounding-box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Joint objective. The proposed framework is optimized in an end-to-end fashion using a multi-task loss. Apart from the conventional classification loss L cls and regression loss L reg , we introduce two additional losses for the anchor localization L loc and anchor shape prediction L shape . They are jointly optimized with the following loss.</p><formula xml:id="formula_3">L = ? 1 L loc + ? 2 L shape + L cls + L reg .<label>(4)</label></formula><p>Anchor location targets. To train the anchor localization branch, for each image we need a binary label map where 1 represents a valid location to place an anchor and 0 otherwise. In this work, we employ ground-truth bounding boxes for guiding the binary label map generation. In particular, we wish to place more anchors around the vicinity of an object's center, while fewer of them far from the center. Firstly, we map the ground-truth bounding box (x g , y g , w g , h g ) to the corresponding feature map scale, and obtain (x g , y g , w g , h g ). We denote R(x, y, w, h) as the rectangular region whose center is (x, y) and the size of w ?h. Anchors are expected to be placed close to the center of ground truth objects to obtain larger initial IoU, thus we define three types of regions for each box.</p><p>(1) The center region CR = R(x g , y g , ? 1 w , ? 1 h ) defines the center area of the box. Pixels in CR are assigned as positive samples.</p><p>(2) The ignore region IR = R(x g , y g , ? 2 w , ? 2 h )\CR is a larger (? 2 &gt; ? 1 ) region excluding CR. Pixels in IR are marked as "ignore" and excluded during training.</p><p>(3) The outside region OR is the feature map excluding CR and IR. Pixels in OR are regarded as negative samples. Previous work <ref type="bibr" target="#b13">[14]</ref> proposed the "gray zone" for balanced sampling, which has a similar definition to our location targets but only works on a single feature map. Since we use multiple feature levels from FPN, we also consider the influence of adjacent feature maps. Specifically, each level of feature map should only target objects of a specific scale range, so we assign CR on a feature map only if the  feature map matches the scale range of the targeted object. The same regions of adjacent levels are set as IR, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. When multiple objects overlap, CR can suppress IR, and IR can suppress OR. Since CR usually accounts for a small portion of the whole feature map, we use Focal Loss <ref type="bibr" target="#b18">[19]</ref> to train the location branch. Anchor shape targets. There are two steps to determine the best shape target for each anchor. First, we need to match the anchor to a ground-truth bounding box. Next, we will predict the anchor's width and height which can best cover the matched ground-truth.</p><p>Previous work <ref type="bibr" target="#b26">[27]</ref> assign a candidate anchor to the ground truth bounding box that yields the largest IoU value with the anchor. However, this process is not applicable in our case, since w and h of our anchors are not predefined but variables. To overcome this problem, we define the IoU between a variable anchor a wh = {(x 0 , y 0 , w, h)|w &gt; 0, h &gt; 0} and a ground truth bounding box gt = (x g , y g , w g , h g ) as follows, denoted as vIoU.</p><p>vIoU(a wh , gt) = max w&gt;0,h&gt;0</p><p>IoU normal (a wh , gt), <ref type="bibr" target="#b4">(5)</ref> where IoU normal is the typical definition of IoU and w and h are variables. Note that for an arbitrary anchor location (x 0 , y 0 ) and ground-truth gt, the analytic expression of vIoU(a wh , gt) is complicated, and hard to be implemented efficiently in an end-to-end network. Therefore we use an alternative way to approximate it. Given (x 0 , y 0 ), we sample some common values of w and h to simulate the enumeration of all w and h. Then we calculate the IoU of these sampled anchors with gt, and use the maximum as an approximation of vIoU(a wh , gt). In our experiments, we sample 9 pairs of (w, h) to estimate vIoU during training. Specifically, we adopt the 9 pairs of different scales and aspect ratios used in RetinaNet <ref type="bibr" target="#b18">[19]</ref>. Theoretically, the more pairs we sample, the more accurate the approximation is, while the computational cost is heavier. We adopt a variant of bounded iou loss <ref type="bibr" target="#b28">[29]</ref> to optimize the shape prediction, without computing the target. The loss is defined in Eq. (6), where (w, h) and (w g , h g ) denote the predicted anchor shape and the shape of the corresponding ground-truth bounding box. L 1 is the smooth L1 loss.</p><formula xml:id="formula_4">L shape = L 1 (1 ? min( w wg , wg w )) + L 1 (1 ? min( h hg , hg h )). (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">The Use of High-quality Proposals</head><p>RPN enhanced by the proposed guided anchoring scheme (GA-RPN) can generate much higher quality proposals than the conventional RPN. We explore how to boost the performance of conventional two-stage detectors, through the use of such high-quality proposals. Firstly, we study the IoU distribution of proposals generated by RPN and GA-RPN, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. There are two significant advantages of GA-RPN proposals over RPN proposals: (1) the number of positive proposals is larger, and (2) the ratio of high-IoU proposals is more significant. A straightforward idea is to replace RPN in existing models with the proposed GA-RPN and train the model end-to-end. However, this problem is non-trivial and adopting exactly the same settings as before can only bring limited gain (e.g., less than 1 point). From our observation, the pre-requisite of using high-quality proposals is to adapt the distribution of training samples in accordance to the proposal distribution. Consequently, we set a higher positive/negative threshold and use fewer samples when training detectors end-to-end with GA-RPN compared to RPN.</p><p>Besides end-to-end training, we find that GA-RPN proposals are capable of boosting a trained two-stage detector by a fine-tuning schedule. Specifically, given a trained model, we discard the proposal generation component, e.g., RPN, and use pre-computed GA-RPN proposals to finetune it for several epochs (3 epochs by default). GA-RPN proposals are also used for inference. This simple fine-tuning scheme can further improve the performance by a large margin, with only a time cost of a few epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Dataset. We perform experiments on the challenging MS COCO 2017 benchmark <ref type="bibr" target="#b19">[20]</ref>. We use the train split for  training and report the performance on val split. Detection results are reported on test-dev split. Implementation details. We use ResNet-50 <ref type="bibr" target="#b12">[13]</ref> with FPN <ref type="bibr" target="#b17">[18]</ref> as the backbone network, if not otherwise specified. As a common convention, we resize images to the scale of 1333 ? 800, without changing the aspect ratio. We set ? 1 = 0.2, ? 2 = 0.5. In the multi-task loss function, we simply use ? 1 = 1, ? 2 = 0.1 to balance the location and shape prediction branches. We use synchronized SGD over 8 GPUs with 2 images per GPU. We train 12 epochs in total with an initial learning rate of 0.02, and decrease the learning rate by 0.1 at epoch 8 and 11. The runtime is measured on TITAN X GPUs. Evaluation metrics. The results of RPN are measured with Average Recall (AR), which is the average of recalls at different IoU thresholds (from 0.5 to 0.95). AR for 100, 300, and 1000 proposals per image are denoted as AR 100 , AR 300 and AR 1000 . The AR for small, medium, and large objects (AR S , AR M , AR L ) are computed for 100 proposals. Detection results are evaluated with the standard COCO metric, which averages mAP of IoUs from 0.5 to 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We first evaluate our anchoring scheme by comparing the recall of GA-RPN with the RPN baseline and previ- ous state-of-the-art region proposal methods. Meanwhile, we compare some variants of RPN. "RPN+9 anchors" denotes using 3 scales and 3 aspect ratios in each feature level, while baselines use only 1 scale and 3 aspect ratios, following <ref type="bibr" target="#b17">[18]</ref>. "RPN+Focal Loss" and "RPN+Bounded IoU Loss" denotes adopting focal loss <ref type="bibr" target="#b18">[19]</ref> and bounded IoU Loss <ref type="bibr" target="#b28">[29]</ref> to RPN by substituting binary cross-entropy loss and smooth l1 loss, respectively. "RPN+Iterative" denotes applying two RPN heads consecutively, with an additional 3 ? 3 convolution between them. "RefineRPN" denotes a similar structure to <ref type="bibr" target="#b32">[33]</ref>, where anchors are regressed and classified twice with features before and after FPN.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method outperforms the RPN baseline by a large margin. Specifically, it improves AR 300 by 10.5% and AR 1000 by 9.1% respectively. Notably, GA-RPN with a small backbone can achieve a much higher recall than RPN with larger backbones. Our encouraging results are supported by the qualitative results shown in <ref type="figure">Figure 4</ref>, where we show the sparse and arbitrary shaped anchors and visualize the outputs of two branches. It is observed that the anchors concentrate more on objects and provides a good basis for the ensuing object proposal. In <ref type="figure">Figure 5</ref>, we show some examples of proposals generated upon sliding window anchoring and guided anchoring.</p><p>Iterative regression and classification ("RPN+Iterative" and "RefineRPN") only brings limited gain to RPN, which proves the importance of the aforementioned rule of alignment and consistency, and simply refining anchors multiple times is not effective enough. Keeping the center of anchors fixed and adapt features based on anchor shapes are crucial.</p><p>To investigate the generalization ability of guided anchoring and its power to boost the detection performance, we integrate it into both two-stage and single-stage detection pipelines, including Fast R-CNN <ref type="bibr" target="#b10">[11]</ref>, Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> and RetinaNet <ref type="bibr" target="#b18">[19]</ref>. For two-stage detectors, we replace the original RPN with GA-RPN, and for singlestage detectors, the sliding window anchoring scheme is replaced with the proposed guided anchoring. Results in <ref type="table" target="#tab_1">Table 2</ref> show that guided anchoring not only increases the proposal recall of RPN, but also improves the detection performance by a large margin. With guided anchoring, the mAP of these detectors are improved by 2.3%, 2.7% and 1.2% respectively.</p><p>To further study the effectiveness of high-quality proposals and investigate the fine-tuning scheme, we take a fully converged Faster R-CNN model and finetune it with precomputed RPN or GA-RPN proposals. We finetune the detector for 3 epochs, with the learning rate of 0.02, 0.002 and 0.0002 respectively. The results are in <ref type="table" target="#tab_2">Table 3</ref> illustrate that RPN proposals cannot bring any gain, while the highquality GA-RPN proposals bring 2.2% mAP improvement to the trained model with only a time cost of 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Model design. We omit different components in our design to investigate the effectiveness of each component, including location prediction, shape prediction and feature adaption. Results are shown in <ref type="table" target="#tab_3">Table 4</ref>. The shape prediction branch is shown effective which leads to a gain of 4.2%.   The location prediction branch brings marginal improvement. Nevertheless, the importance of this branch is reflected in its usefulness of obtaining sparse anchors leading to more efficient inference. The obvious gain brought by the feature adaption module suggests the necessity of rearranging the feature map according to predicted anchor shapes. This module helps to capture information corresponding to anchor scopes, especially for large objects. Anchor location. The location threshold L controls the sparsity of anchor distribution. Adopting different thresholds will yield different numbers of anchors. To reveal the influence of L on efficiency and performance, we vary the threshold and compare the following results: the average number of anchors per image, recall of final proposals and the inference runtime. From <ref type="table" target="#tab_4">Table 5</ref> we can observe that the objectness scores of most background regions are close to 0, so a small L can greatly reduce the number of anchors by more than 90%, with only a minor decrease on recall rate. It is noteworthy that the head in RPN is just one convolutional layer, so the speedup is not apparent. Nevertheless, a significant reduction in the number of anchors offers a possibility to perform more efficient inference with a heavier head. Anchor shape. We compare the set of generated anchors of our method with sliding window anchors of pre-defined shapes. Since our method predicts only one anchor at each location of the feature map instead of k (k = 3 in our baseline) anchors of different scales and aspect ratios, the total anchor number is reduced by 1 k . We present the scale and aspect ratio distribution of our anchors with sliding window anchors in <ref type="figure" target="#fig_5">Figure 6</ref>. The results show great advantages of the guided anchoring scheme over predefined anchor scales and shapes. The predicted anchors cover a much wider range of scales and aspect ratios, which have a similar distribution to ground truth objects and provide a pool of initial anchors with higher coverage on objects. Feature adaption. The feature adaption module improves the recall by a large margin, proving that a remedy of features consistency is essential. We claim that the improvement not only comes from adopting deformable convolution, but also results from our design of using anchor shape predictions to predict the offset of the deformable convolution layer. If we simply add a deformable convolution layer after anchor generation, the results of AR100/AR300/AR1000 are 56.1%/62.4%/66.1%, which are inferior to results from our design. Alignment and consistency rule. We verify the necessity of the two proposed rules. The alignment rule suggests that we should keep the anchor centers aligned with feature map pixels. According to the consistency rule, we design the feature adaption module to refine the features. Results in <ref type="table" target="#tab_5">Table 6</ref> show the importance of these rules. 1) From row 1 and 2, or row 3 and 4, we learn that predicting both the shape and center offset instead of just predicting the shape harms the performance. 2) The comparison between row 1 and 3, or row 2 and 4 shows the impact of consistency. The use of high-quality proposals. Despite with highquality proposals, training a good detector remains a nontrivial problem. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, GA-RPN proposals provide more candidates of high IoU. This suggests that we can use fewer proposals for training detectors. We test different numbers of proposals and different IoU thresholds to assign labels for foreground/background on Fast R-CNN.</p><p>From the results in <ref type="table" target="#tab_6">Table 7</ref>, we observe that: (1) Larger IoU threshold is important for taking advantage of highquality proposals. By focusing on positive samples of higher IoU, there will be fewer false positives and the features for classification are more discriminative. Since we assign negative labels to proposals with IoU less than 0.6 during training, AP 0.5 will decrease while AP of high IoUs will increase by a large margin, and the overall AP is much higher.</p><p>(2) Using fewer proposals during training and testing can benefit the learning if the recall is high enough.</p><p>Fewer proposals lead to a lower recall, but will simplify the learning process, since there are more hard samples in low-score proposals. When training with RPN proposals, the performance will decrease if we use only 300 proposals, because the recall is not sufficient and many objects get missed. However, GA-RPN guarantees high recall even with fewer proposals, thus training with 300 proposals could still boost the final mAP.</p><p>Hyper-parameters. Our method is insensitive to hyperparameters.</p><p>(1) As we sample 3, 9, 15 pairs to approximate Eq.(5), we respectively obtain AR@1000 68.3%, 68.5%, 68.5%. (2) We set ? 2 = 0.1 to balance the loss terms by default. We obtain 68.4% with ? 2 = 0.2 or 0.05 and 68.3% with ? 2 = 0.02. (3) We vary ? 1 within [0.1, 0.5] and ? 2 within [0.2, 1.0], and the performance remains comparable (between 68.1% and 68.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed the Guided Anchoring scheme, which leverages semantic features to guide the anchoring. It generates non-uniform anchors of arbitrary shapes by jointly predicting the locations and anchor shapes dependent on locations. The proposed method achieves 9.1% higher recall with 90% fewer anchors than the RPN baseline using the sliding window scheme. It can also be applied to various anchor-based detectors to improve the performance by as much as 2.7%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Anchor location target for multi-level features. We assign ground truth objects to different feature levels according to their scales, and define CR, IR and OR respectively. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>IoU distribution of RPN and GA-RPN proposals. We show the accumulated proposal number with decreasing IoUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Anchor prediction results. (a) input image and predict anchors; (b) predicted anchor location probability map; (c) predicted anchor aspect ratio. Examples of RPN proposals (top row) and GA-RPN proposals (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(a) Anchor scale and (b) aspect ratio distributions of different anchoring schemes. The x-axis is reduced to log-space by apply log 2 (?) operator. GT, GA, SW indicates ground truth, guided anchoring, sliding window, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Region proposal results on MS COCO.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="7">AR100 AR300 AR1000 ARS ARM ARL runtime (s/img)</cell></row><row><cell>SharpMask [24]</cell><cell>ResNet-50</cell><cell>36.4</cell><cell>-</cell><cell>48.2</cell><cell>6.0</cell><cell>51.0</cell><cell>66.5</cell><cell>0.76 (unfair)</cell></row><row><cell>GCN-NS [22]</cell><cell>VGG-16 (SyncBN)</cell><cell>31.6</cell><cell>-</cell><cell>60.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.10</cell></row><row><cell>AttractioNet [10]</cell><cell>VGG-16</cell><cell>53.3</cell><cell>-</cell><cell>66.2</cell><cell>31.5</cell><cell>62.2</cell><cell>77.7</cell><cell>4.00</cell></row><row><cell>ZIP [16]</cell><cell>BN-inception</cell><cell>53.9</cell><cell>-</cell><cell>67.0</cell><cell>31.9</cell><cell>63.0</cell><cell>78.5</cell><cell>1.13</cell></row><row><cell></cell><cell>ResNet-50-FPN</cell><cell>47.5</cell><cell>54.7</cell><cell>59.4</cell><cell>31.7</cell><cell>55.1</cell><cell>64.6</cell><cell>0.09</cell></row><row><cell>RPN</cell><cell>ResNet-152-FPN</cell><cell>51.9</cell><cell>58.0</cell><cell>62.0</cell><cell>36.3</cell><cell>59.8</cell><cell>68.1</cell><cell>0.16</cell></row><row><cell></cell><cell>ResNeXt-101-FPN</cell><cell>52.8</cell><cell>58.7</cell><cell>62.6</cell><cell>37.3</cell><cell>60.8</cell><cell>68.6</cell><cell>0.26</cell></row><row><cell>RPN+9 anchors</cell><cell>ResNet-50-FPN</cell><cell>46.8</cell><cell>54.6</cell><cell>60.3</cell><cell>29.5</cell><cell>54.9</cell><cell>65.6</cell><cell>0.09</cell></row><row><cell>RPN+Focal Loss [19]</cell><cell>ResNet-50-FPN</cell><cell>50.2</cell><cell>56.6</cell><cell>60.9</cell><cell>33.9</cell><cell>58.2</cell><cell>67.5</cell><cell>0.09</cell></row><row><cell>RPN+Bounded IoU Loss [29]</cell><cell>ResNet-50-FPN</cell><cell>48.3</cell><cell>55.1</cell><cell>59.6</cell><cell>33.0</cell><cell>56.0</cell><cell>64.3</cell><cell>0.09</cell></row><row><cell>RPN+Iterative</cell><cell>ResNet-50-FPN</cell><cell>49.7</cell><cell>56.0</cell><cell>60.0</cell><cell>34.7</cell><cell>58.2</cell><cell>64.0</cell><cell>0.10</cell></row><row><cell>RefineRPN</cell><cell>ResNet-50-FPN</cell><cell>50.2</cell><cell>56.3</cell><cell>60.6</cell><cell>33.5</cell><cell>59.1</cell><cell>66.9</cell><cell>0.11</cell></row><row><cell>GA-RPN</cell><cell>ResNet-50-FPN</cell><cell>59.2</cell><cell>65.2</cell><cell>68.5</cell><cell>40.9</cell><cell>67.8</cell><cell>79.0</cell><cell>0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detection results on MS COCO 2017 test-dev.</figDesc><table><row><cell>Method</cell><cell>AP AP50 AP75 APS APM APL</cell></row><row><cell>Fast R-CNN</cell><cell>37.1 59.6 39.7 20.7 39.5 47.1</cell></row><row><cell cols="2">GA-Fast-RCNN 39.4 59.4 42.8 21.6 41.9 50.4</cell></row><row><cell>Faster R-CNN</cell><cell>37.1 59.1 40.1 21.3 39.8 46.5</cell></row><row><cell cols="2">GA-Faster-RCNN 39.8 59.2 43.5 21.8 42.6 50.7</cell></row><row><cell>RetinaNet</cell><cell>35.9 55.4 38.8 19.4 38.9 46.5</cell></row><row><cell>GA-RetinaNet</cell><cell>37.1 56.9 40.0 20.1 40.1 48.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Fine-tuning results on a trained Faster R-CNN.</figDesc><table><row><cell cols="2">proposals AP AP50 AP75 APS APM APL</cell></row><row><cell>-</cell><cell>37.4 58.9 40.3 20.8 41.1 49.5</cell></row><row><cell>RPN</cell><cell>37.3 58.6 40.1 20.4 40.6 49.8</cell></row><row><cell cols="2">GA-RPN 39.6 59.3 43.0 22.0 42.8 52.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The effects of each module in our design. L., S., and F.A. denote location, shape, and feature adaptation, respectively. L. S. F.A. AR100 AR300 AR1000 ARS ARM ARL</figDesc><table><row><cell>47.5</cell><cell>54.7</cell><cell>59.4</cell><cell>31.7 55.1 64.6</cell></row><row><cell>48.0</cell><cell>54.8</cell><cell>59.5</cell><cell>32.3 55.6 64.8</cell></row><row><cell>53.8</cell><cell>59.9</cell><cell>63.6</cell><cell>36.4 62.9 71.7</cell></row><row><cell>54.0</cell><cell>60.1</cell><cell>63.8</cell><cell>36.7 63.1 71.5</cell></row><row><cell>59.2</cell><cell>65.2</cell><cell>68.5</cell><cell>40.9 67.8 79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of different location threshold L.</figDesc><table><row><cell></cell><cell>L</cell><cell cols="5">#anchors/image AR100 AR300 AR1000 fps</cell></row><row><cell></cell><cell>0</cell><cell cols="2">75583 (100.0%) 59.2</cell><cell>65.2</cell><cell>68.5</cell><cell>7.8</cell></row><row><cell></cell><cell cols="2">0.01 22274 (29.4%)</cell><cell>59.2</cell><cell>65.2</cell><cell>68.5</cell><cell>8.0</cell></row><row><cell></cell><cell>0.05</cell><cell>5251 (6.5%)</cell><cell>59.1</cell><cell>65.1</cell><cell>68.2</cell><cell>8.2</cell></row><row><cell></cell><cell>0.1</cell><cell>2375 (3.2%)</cell><cell>59.0</cell><cell>64.7</cell><cell>67.2</cell><cell>8.2</cell></row><row><cell>0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6</cell><cell cols="3">2 3 4 5 6 7 8 9 scale (sqrt(w*h)) GT GA SW</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The effects of alignment and consistency rules. C.A. and F.A. denote center alignment (alignment rule) and feature adaption (consistency rule) respectively. C.A. F.A. AR100 AR300 AR1000 ARS ARM ARL</figDesc><table><row><cell>51.7</cell><cell>58.0</cell><cell>61.6</cell><cell>33.8 60.9 70.0</cell></row><row><cell>54.0</cell><cell>60.1</cell><cell>63.8</cell><cell>36.7 63.1 71.5</cell></row><row><cell>57.2</cell><cell>63.6</cell><cell>66.8</cell><cell>38.3 66.1 77.8</cell></row><row><cell>59.2</cell><cell>65.2</cell><cell>68.5</cell><cell>40.9 67.8 79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Exploration of utilizing high-quality proposals.proposal num IoU thr AP AP50 AP75</figDesc><table><row><cell></cell><cell>1000</cell><cell>0.5</cell><cell>36.7 58.8 39.3</cell></row><row><cell>RPN</cell><cell>1000 300</cell><cell>0.6 0.5</cell><cell>37.2 57.1 40.5 36.1 57.6 39.0</cell></row><row><cell></cell><cell>300</cell><cell>0.6</cell><cell>37.0 56.3 39.5</cell></row><row><cell></cell><cell>1000</cell><cell>0.5</cell><cell>37.4 59.9 40.0</cell></row><row><cell>GA-RPN</cell><cell>1000 300</cell><cell>0.6 0.5</cell><cell>38.9 59.0 42.4 37.5 59.6 40.4</cell></row><row><cell></cell><cell>300</cell><cell>0.6</cell><cell>39.4 59.3 43.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun. R-Fcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepproposal: Hunting objects by cascading deep convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scale-aware pixelwise object proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><forename type="middle">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng Hock Francis</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4525" to="4539" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zoom out-and-in network with map attention decision for region proposal and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Not all pixels are equal: difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward scale-invariance and position-sensitive region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Fu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Lin</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">G-cnn: an iterative grid based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2369" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Beyond trade-off: Accelerate fcn-based face detector with higher accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Leng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Single-shot bidirectional pyramid networks for highquality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Craft objects from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cascade region proposal and global context for deep object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10749</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
