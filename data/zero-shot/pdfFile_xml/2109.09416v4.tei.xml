<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ElasticFace: Elastic Margin Loss for Deep Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadi</forename><surname>Boutros</surname></persName>
							<email>fadi.boutros@igd.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Computer Graphics Research IGD</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mathematical and Applied Visual Computing</orgName>
								<address>
									<settlement>Darmstadt, Darmstadt</settlement>
									<region>TU</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Computer Graphics Research IGD</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mathematical and Applied Visual Computing</orgName>
								<address>
									<settlement>Darmstadt, Darmstadt</settlement>
									<region>TU</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Computer Graphics Research IGD</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Computer Graphics Research IGD</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mathematical and Applied Visual Computing</orgName>
								<address>
									<settlement>Darmstadt, Darmstadt</settlement>
									<region>TU</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ElasticFace: Elastic Margin Loss for Deep Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning discriminative face features plays a major role in building high-performing face recognition models. The recent state-of-the-art face recognition solutions proposed to incorporate a fixed penalty margin on commonly used classification loss function, softmax loss, in the normalized hypersphere to increase the discriminative power of face recognition models, by minimizing the intra-class variation and maximizing the inter-class variation. Marginal penalty softmax losses, such as ArcFace and CosFace, assume that the geodesic distance between and within the different identities can be equally learned using a fixed penalty margin. However, such a learning objective is not realistic for real data with inconsistent inter-and intra-class variation, which might limit the discriminative and generalizability of the face recognition model. In this paper, we relax the fixed penalty margin constrain by proposing elastic penalty margin loss (ElasticFace) that allows flexibility in the push for class separability. The main idea is to utilize random margin values drawn from a normal distribution in each training iteration. This aims at giving the decision boundary chances to extract and retract to allow space for flexible class separability learning. We demonstrate the superiority of our ElasticFace loss over ArcFace and CosFace losses, using the same geometric transformation, on a large set of mainstream benchmarks. From a wider perspective, our ElasticFace has advanced the state-of-the-art face recognition performance on seven out of nine mainstream benchmarks. All training codes, pre-trained models, training logs are publicly released 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Face recognition technologies are increasingly deployed to enhance the security and convenience of processes involving identity verification, such as border control and financial services. The typical pipeline of a face recognition system involves mapping the face image (after detection and alignment <ref type="bibr" target="#b32">[31]</ref>) into a feature vector (embedding) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Two face images are then compared by comparing their relative embeddings and therefore, measuring the degree of identity similarity between both faces. Knowing that it is intuitive that such embeddings should ideally have small intra-class and large inter-class variation, with the class here being an identity. This corresponds to a face recognition system that still makes correct genuine decisions (same identity) even when face images are largely varied (pose, age, expression, etc.), and make correct imposter (not same identity) decision even when the appearance of the face image pair of different identities is very similar. To achieve that, different solutions opted to train deep neural networks by either directly learning the embedding (e.g. Triplet loss <ref type="bibr" target="#b24">[23]</ref>) or by learning an 1 https://github.com/fdbtrs/ElasticFace identity classification problem (e.g. Softmax loss <ref type="bibr" target="#b1">[2]</ref>). One of the main challenges for training with metric-based learning such as Triple <ref type="bibr" target="#b24">[23]</ref>, n-pair <ref type="bibr" target="#b26">[25]</ref>, or contrastive <ref type="bibr" target="#b2">[3]</ref> losses, is training the model with a large-scale dataset as the number of possible triplets explodes with the number of samples. Alternatively, classification-based losses such as softmax loss can be easily adopted for training a face recognition model as it does not pose that issue. However, the softmax loss does not directly optimize the feature embedding needed for face verification. Liu et al. <ref type="bibr" target="#b17">[17]</ref> proposed a large-margin softmax (L-Softmax) by incorporating angular margin constraints on softmax loss to encourage intra-class compactness and inter-class separability between learned features. SphereFace <ref type="bibr" target="#b16">[16]</ref> extended L-Softmax by normalizing the weights of the last full-connected layer and deploying multiplicative angular penalty margin between the deep features and their corresponding weights. Different from SphereFace, CosFace <ref type="bibr" target="#b28">[27]</ref> proposed additive cosine margin on the cosine angle between the deep features and their corresponding weights. CosFace also proposed to fix the norm of the deep features and their corresponding weights to 1, then scaling the deep feature norm to a constant s, achieving better performance on mainstream face recognition benchmarks. Later, ArcFace <ref type="bibr" target="#b3">[4]</ref> proposed additive angular margin by deploying angular penalty margin on the angle between the deep features and their corresponding weights. The great success of softmax loss with penalty margin motivated several works to propose a novel variant of softmax loss <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b0">[1]</ref>. All these solutions achieved notable accuracies on mainstream benchmarks <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b18">[18]</ref> for face recognition. Huang et al. <ref type="bibr" target="#b10">[10]</ref> proposed an Adaptive Curriculum Learning loss based on margin-based softmax loss. The proposed loss targets the easy samples at an early stage of training and the hard ones at a later stage of training. Jiao et al. <ref type="bibr" target="#b12">[12]</ref> proposed Dyn-arcface based on ArcFace loss <ref type="bibr" target="#b3">[4]</ref> by replacing the fixed margin value of ArcFace with an adaptive one. The margin value of Dyn-arcface is adjusted based on the distance between each class center and the other class centers. However, this might not reflect the real properties of the class separability, but rather their separability in the current stage of the model training. Kim et al. <ref type="bibr" target="#b14">[14]</ref> proposed to enrich the feature representation learned by ArcFace loss with group-aware representations. UniformFace <ref type="bibr" target="#b4">[5]</ref> suggested to equalize distances between all the classes centers by adding a new loss function to SphereFace loss <ref type="bibr" target="#b16">[16]</ref>. A recent work by An et al. <ref type="bibr" target="#b0">[1]</ref> presented an efficient distributed sampling algorithm (Partial-FC). The Partial-FC method is based on randomly sampling a small subset of the complete training set of classes for the softmax-based loss function. Thus, it enables the training of the face recognition model on a massive number of identities. The authors experimentally proved that training with only 10% of training samples using CosFace <ref type="bibr" target="#b28">[27]</ref> and ArcFace <ref type="bibr" target="#b3">[4]</ref> can achieve comparable results on mainstream benchmarks to the case when training is performed on a complete set of classes. MagFace <ref type="bibr" target="#b19">[19]</ref> deployed magnitude-aware margin on ArcFace loss to enhance intra-class compactness by pulling high-quality samples close to class centers while pushing low-quality samples away. However, this is based on the weak assumption of optimal face quality (utility) estimation. Moreover, this might prevent the model from convergence when the most of training samples in the training dataset are of low quality.</p><p>The main challenge for the majority of the previously listed works is the fine selection of the ideal margin penalty value. In this work, we propose the ElasticFace loss that relaxes the fixed single margin value by deploying a random margin drawn from a normal distribution. We additionally extended this concept by guiding the assignment of the drawn margin values to put more attention on hardly classified samples. We provided a simple toy example with an 8-class classification problem to demonstrate the enhanced separability and robustness induced by our ElasticFace loss. To experimentally demonstrate the effect of our ElasticFace loss on face recognition accuracy, we report the results on nine different benchmarks. The achieved results are compared to the results reported in the recent state-of-the-art. In a detailed comparison, compared to fixed margin penalties and recent state-of-the-art, our ElasticFace loss enhanced the face recognition accuracy on most of the considered benchmarks, consequently extending state-of-the-art face recognition performance on seven out of nine benchmarks and scoring close to the state-of-the-art in the remaining two. This is especially the case in the benchmarks where the intra-class variation is extremely high, such as frontal-to-profile face verification (CFP-FP <ref type="bibr" target="#b25">[24]</ref>) and large age gap face verification (AgeDB-30 <ref type="bibr" target="#b21">[20]</ref>), which points to the generalizability induced by the proposed ElasticFace.</p><p>In the rest of this paper, we will first introduce our proposed ElasticFace loss by building up to its definition starting from the basic softmax loss. This rationalization will include an experimental toy example demonstrating the effect of the proposed loss. Later on, the experimental setup and implementation details are introduced. This is followed by a detailed comparative discussion of the achieved results and a final conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ELASTICFACE LOSS</head><p>We propose in this work a novel learning loss strategy, ElasticFace loss, aiming at improving the accuracy of face recognition by targeting enhanced intra-class compactness and inter-class discrepancy in a flexible manner. Unlike previous works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b28">[27]</ref> that utilize a fixed penalty margin value, our proposed ElasticFace loss accommodates flexibility through relaxing this constraint by randomly drawing the margin value from a Gaussian distribution. Our proposed ElasticFace loss targets giving the model flexibility in optimizing the separability between and within the classes as it incorporates random margin values for each sample in each training iteration. The randomized margin penalty can be easily integrated into any of the angular margin-based softmax losses, which we demonstrate on two state-of-theart margin-based softmax losses. The angular margin-based losses and our ElasticFace loss extend over the softmax loss by manipulating the decision boundary to enhance intra-class compactness and inter-class discrepancy. Therefore, in this section, we first revisit the conventional softmax loss. Then, we present the modified version of softmax loss and the angular margin-based softmax loss. This leads up to presenting our proposed ElasticFace loss and an extended definition, the ElasticFace+, where the assignment of the drawn margins to training samples is linked to their proximity to their class centers.</p><p>a) Softmax Loss: The widely used multi-class classification loss, softmax loss <ref type="bibr" target="#b17">[17]</ref>, refers to applying crossentropy loss between the output of the logistic function (softmax activation function) and the ground-truth. Assume x i ? R d is a feature representation of the i-th sample z i and y i is its corresponding class label (y i integer in the range <ref type="bibr">[1, c]</ref>). Given that c is the number of classes (identities), the output of the softmax activation function is defined as follows: </p><p>where f yi is the activation of the last fully-connected layer with weight vector W yi and bias b yi . W yi is the y i -th column of weights W ? R d c and b yi is the corresponding bias offset. The output of the softmax activation function is the probability of x i being correctly classified as y i . Given a mini-batch of size N, the cross-entropy loss function that measures the divergence between the model output and the ground-truth labels can be defined as follows:</p><formula xml:id="formula_1">L CE = 1 N i?N ?log e xiW T y i +by i c j=1 e xiW T j +bj .<label>(2)</label></formula><p>In a simple binary class classification, assuming that the input z i belong to class 1, the model will correctly classify z i if W T 1 x i +b1 &gt; W T 2 x i +b2 and z i will be classified as class 2 if</p><formula xml:id="formula_2">W T 2 x i + b2 &gt; W T 1 x i + b1. Therefore, the decision boundary of softmax loss is x(W T 1 ? W T 2 ) + b1 ? b2 = 0.</formula><p>One of the main limitations of using softmax loss for learning face embeddings is that softmax loss does not explicitly optimize the feature representation needed for face verification as there is no restriction on the minimum distance between the class centers. Thus, training with softmax loss is less than optimal for achieving the maximum inter-class distances and the minimum intra-class distances. To mitigate this limitation, margin penalty-based cosine softmax loss was proposed as an alternative to the conventional softmax loss and it became a popular loss function for training face recognition models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b16">[16]</ref>. To get there, <ref type="bibr" target="#b16">[16]</ref> has proposed a modified softmax loss (Cosine softmax loss) that optimized the angle cosine between features and the weights cos(?) and then, incorporates a margin penalty on cos(?). b) Cosine Softmax Loss: Following <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, the bias offset, for simplicity, can be fixed to b yi = 0. The logit f yi , in this case, can be reformulated as:</p><formula xml:id="formula_3">x i W T yi = x i W yi cos(? yi ),</formula><p>where ? yi is the angle between the weights of the last fully-connected layer W yi and the feature representation x i . By fixing the weights norm and the feature norm to W yi = 1 and x i = 1, respectively, and rescaling the x i to the constant s <ref type="bibr" target="#b28">[27]</ref>, the output of the softmax activation function is subject to the cosine of the angle ? yi . The modified softmax loss (L M L ) can be defined, as stated in <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b16">[16]</ref>, as follows:</p><formula xml:id="formula_4">L M L = 1 N i?N ?log e s(cos(?y i )) e s(cos(?y i )) + c j=1,j =yi e s(cos(?j ))</formula><p>. <ref type="formula">(3)</ref> In the previous binary example, assume that the input z i belong to the class 1, z i will be correctly classified if cos(?1) &gt; cos(?2). The decision boundary, in this case, is cos(?1)?cos(?2) = 0. Therefore, training with the modified (cosine) softmax loss emphasizes that the model prediction depends on the angle cosine between the features and the weights. However, and similar to conventional softmax loss, modified softmax loss does not explicitly optimize the feature representation needed for face verification. This motivated the introduction of the angular margin penalty-based losses <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b16">[16]</ref>. c) Angular Margin Penalty-based Loss: In recent works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b16">[16]</ref>, different types of margin penalty are proposed to push the decision boundary of softmax, and thus enhance intra-class compactness and inter-class discrepancy aiming at improving the accuracy of face recognition. The general angular margin penalty-based loss (L AM L ) is defined as follows:</p><formula xml:id="formula_5">L AM L = 1 N i?N ?log e s(cos(m 1 ?y i +m 2 )?m 3 ) e s(cos(m 1 ?y i +m 2 )?m 3 ) + c j=1,j =y i e s(cos(? j )) ,<label>(4)</label></formula><p>where m 1 , m 2 and m3 are the margin penalty parameters proposed by SphereFace <ref type="bibr" target="#b16">[16]</ref>, ArcFace <ref type="bibr" target="#b3">[4]</ref> and CosFace <ref type="bibr" target="#b28">[27]</ref>, respectively. In SphereFace <ref type="bibr" target="#b16">[16]</ref>, multiplicative angular margin penalty is deployed by multiplying ? by m 1 = ? and setting m 2 = 0 and m 3 = 0 ( ? &gt; 1.0). The decision boundary of SphereFace is then cos(m 1 ? yi ) ? cos(? j ) = 0. Differently, CosFace <ref type="bibr" target="#b28">[27]</ref> proposed additive cosine margin penalty by setting m 1 = 1, m 2 = 0 and m 3 = ? (0 &lt; ? &lt; 1 ? cos( ? 4 )). The decision boundary of CosFace is then cos(? yi ) ? cos(? j ) ? m3 = 0. Later, ArcFace <ref type="bibr" target="#b3">[4]</ref> proposed additive angular margin penalty by setting up m 1 = 1, m 2 = ? and m 3 = 0 (0 &lt; ? &lt; 1.0). The decision boundary of ArcFace is then cos(? yi + m2) ? cos(? j ) = 0.</p><p>Even though, ArcFace <ref type="bibr" target="#b3">[4]</ref>, CosFace <ref type="bibr" target="#b28">[27]</ref> and SphereFace <ref type="bibr" target="#b16">[16]</ref> introduced the important concept of angular margin penalty on softmax loss, selecting a single optimal margin value (?) is a critical issue in these works. By setting up m 1 = 1, m 2 = 0 and m 3 = 0, ArcFace, CosFace and SphereFace are equivalent to the modified softmax loss. A reasonable choice could be selecting a large margin value that is close to the margin upper bound to enable higher separability between the classes. However, when the margin value is too large, the model fails to converge, as stated in <ref type="bibr" target="#b28">[27]</ref>. ArcFace, CosFace, and SphereFace selected the margin value through trial and error assuming that the samples are equally distributed in geodesic space around the class centers. However, this assumption could not be held when there are largely different intra-class variations leading to less than optimal discriminative feature learning, especially when there are large variations between the samples/classes in the training dataset. This motivated us to propose ElasitcFace loss by utilizing random margin penalty values drawn from a Gaussian distribution aiming at giving the model space for flexible class separability learning.</p><p>d) Elastic Angular Margin Penalty-based Loss (Elas-ticFace): The proposed ElasticFace loss is extended over the angular margin penalty-based loss by deploying random margin penalty values drawn from a Gaussian distribution. Formally, the probability density function of a normal distribution is defined as follows:</p><formula xml:id="formula_6">f (x) = 1 ? ? 2? e ? 1 2 ( x?? ? ) 2 ,<label>(5)</label></formula><p>where ? is the mean of the distribution and ? is its standard deviation. To demonstrate and prove our proposed elastic margin, we chose to integrate the randomized margin penalty in ArcFace (noted as ElasticFace-Arc) and CosFace (noted as ElasticFace-Cos) as they proved to have clearer geometric interpretation and achieved higher accuracy on mainstream benchmarks than the earlier SphereFace. ElasticFace-Arc (L EArc ) can be defined as follows:</p><formula xml:id="formula_7">L EArc = 1 N i?N ?log e s(cos(? y i +E(m,?))) e s(cos(? y i +E(m,?))) + c j=1,j =y i e s(cos(? j )) ,<label>(6)</label></formula><p>and ElasticFace-Cos (L ECos ) can be defined as follows:</p><formula xml:id="formula_8">L ECos = 1 N i?N ?log e s(cos(? y i )?E(m,?)) e s(cos(? y i )?E(m,?)) + c j=1,j =y i e s(cos(? j )) ,<label>(7)</label></formula><p>where E(m, ?) is a normal function that return a random value from a Gaussian distribution with the mean m and the standard deviation ?.</p><p>The decision boundaries of ElasticFace-Arc and ElasticFace-Cos are cos(? yi + E(m, ?)) ? cos(? j ) = 0 and cos(? yi ) ? cos(? j ) ? E(m, ?) = 0, respectively. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the decision boundary of ArcFace, ElasticFace-Arc, CosFace and ElasticFace-Cos. The sample push towards its center during training using ElasticFace-Arc and ElasticFace-Cos varies between training samples, based on the margin penalty drawn from E(m, ?). During the training phase, a new random margin is generated for each sample in each training iteration. This aims at giving the model flexibility in the push for class separability. When ? is 0, our ElasticFace-Arc and ElasticFace-Cos are equivalent to ArcFace and CosFace, respectively. e) ElasticFace+: We propose an extension to our Elas-ticFace, the ElasticFace+, that observes the intra-class variation during each training iteration and use this observation to assign a margin value to each sample based on its proximity to its class center. This causes the samples that are relatively far from their class center to be pushed with a larger penalty margin to their class center. This aims at giving the model space to push the samples that are relatively far from their class center to be closer to their centers while giving less penalty attention to the samples that are already close to their center. To achieve that, we sort (descending) the output of the Gaussian distribution function (Equation 5) based on cos(? yi ) value. Thus, the sample with small cos(? yi ) will be pushed with large value from E(m, ?) function, and vice versa.   to 0.45, 0.50 and 0.55, respectively, to assure the advised margin in <ref type="bibr" target="#b3">[4]</ref>. Then, based on the sum of the performance ranking Borda count on LFW <ref type="bibr" target="#b9">[9]</ref>, AgeDB-30 <ref type="bibr" target="#b21">[20]</ref>, CALFW <ref type="bibr" target="#b35">[34]</ref>, CPLFW <ref type="bibr" target="#b34">[33]</ref>, and CFP-FP <ref type="bibr" target="#b25">[24]</ref>, we select the margin that achieved the highest Borda count sum and set it as m for E(m, ?) function, where our goal is to use the most optimal margin. The best margin observed in our experiment, in this case, is 0.5 <ref type="table" target="#tab_1">(Table I)</ref>. To select the ? value for E(m, ?) function, we conducted additional experiments on four instances of ResNet-50 trained on CASIA <ref type="bibr" target="#b31">[30]</ref> with our proposed ElasticFace-Arc by setting up the ? to one of these values 0.0125, 0.015, 0.025 and 0.05. Then, we rank these models based on the sum of the performance ranking Borda count across all datasets. Finally, the ? value is chosen based on the highest Borda count sum. The best ? observed in our experiment, in this case, is 0.05 <ref type="table" target="#tab_1">(Table I)</ref>. Similarly, we follow the same procedure to select the parameters (m and ?) for ElasticFace-Cos. We first choose the best margin value by training three different instances of ResNet-50 on CASIA <ref type="bibr" target="#b31">[30]</ref> with CosFace using a margin equal to 0.3, 0.35, and 0.40. The best m observed in our experiment based on the sum of the performance ranking Borda count across all evaluated datasets, in this case, is 0.035 <ref type="table" target="#tab_1">(Table II)</ref>. Similar to ? selection approach of ElasticFace-Arc, we train four instance of ElasticFace-Cos to choose the best ? for E(m, ?) function. The best observed ? in our experiment, in this case, is 0.05 <ref type="table" target="#tab_1">(Table II)</ref>. For ElasticFace-Cos+ and ElasticFace-Arc+, we followed the exact approach of parameter selection  Thus, the feature embeddings are allocated around the class centers in the arc space with a fixed radius. The numbers next to each class center indicate the mean of the standard deviation of each class feature embeddings. The angle in degree are calculated between each two consecutive classes to illustrate the decision margin between the classes. One can noticed that feature produced by ElasticFace and ElasticFace+ are more equally distributed around the class centers than ArcFace, in the arc space. Same colors always indicates same class across plots.</p><p>for ElasticFace-Arc and ElasticFace-Cos. The best observed ? for ElasticFace-Arc+ is 0.0175 and the best observed one for ElasticFace-Cos+ is 0.025 <ref type="table" target="#tab_1">(Table I and II)</ref>. These selected parameters are used to train our solutions (training details in Section III) evaluated in Section IV. g) Toy example: To demonstrate the robustness and the class separability induced by our proposed ElasticFace and ElasticFace+, we present a simple toy example by training three ResNet-18 networks <ref type="bibr" target="#b8">[8]</ref> to classify eight different identities and produce 2-D feature embeddings. All the networks are trained with a small batch size of 128 for 11200 iterations with stochastic gradient descent (SGD) and an initial learning rate of 0.1. The learning rate is reduced by a factor of 10 after 1680, 2800, 3360, and 8400 training iterations. To demonstrate a classification case where the classes are not identically varied, these eight identities are selected to have four identities with small intra-class variation and another four identities with a large intra-class variation (measured as the average of all intra-class comparison scores for each identity). These identities were chosen from all the identities with more than 400 images per identity in the MS1MV2 dataset <ref type="bibr" target="#b3">[4]</ref>, we note this selected subset as MS1MV2-400. From these identities, we select the four identities with the highest intra-class variation and the four with the lowest intra-class variation. The features for this selection were extracted using an open-source 2 ResNet-100 <ref type="bibr" target="#b8">[8]</ref> model trained with ArcFace loss <ref type="bibr" target="#b3">[4]</ref>, and the comparison is performed by a cosine similarity. The set of the selected eight identities is noted as MS1MV2-8. We use MS1MV2-8 to train the toy networks with ArcFace (m=0.5), ElasticArcFace (m=0.5, ?=0.05), and ElasticArcFace+ (m=0.5, ?=0.0175), based on our parameter selection. <ref type="figure" target="#fig_2">Figure 2</ref> shows the classification 2 https://github.com/deepinsight/insightface of MS1MV2-8 for each of the experimental settings. In each of the plots in <ref type="figure" target="#fig_2">Figure 2a, 2b and 2c</ref>, we calculate the angle between each consecutive identities to demonstrate the separability between the identities in the arc space (inter-class discrepancy). The optimal inter-class discrepancy may be achieved if the angle, in degree, between each of consecutive identities is close to 45 degrees i.e. 360 / 8. Also, we calculate the mean of the standard deviation of each class feature embeddings to illustrate intra-class compactness induced by ArcFace, ElasticFace, and ElasticFace+. The smaller standard deviation (shown at the edge of each class in <ref type="figure" target="#fig_2">Figure 2</ref>), in this case, indicates higher intra-class compactness. It can be noticed that our EalsticFace and EalsticFace+ achieved better intra-class compactness and inter-class discrepancy than ArcFace, while the differences in inter-class variation between EalsticFace and EalsticFace+ are minor <ref type="figure" target="#fig_2">(Figures 2a 2c, and 2b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL SETUP</head><p>a) Training settings:: The network architecture we used to demonstrate our ElasticFace is the ReseNet-100 <ref type="bibr" target="#b8">[8]</ref>. This was motivated by the wide use of this architecture in the state-of-the-art face recognition solutions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b10">[10]</ref>. We follow the common setting <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[10]</ref> to set the scale parameter s to 64. We set the mini-batch size to 512 and train our model on one Linux machine (Ubuntu 20.04.2 LTS) with Intel(R) Xeon(R) Gold 5218 CPU 2.30GHz, 512 G RAM, and 4 Nvidia GeForce RTX 6000 GPUs. The proposed models in this paper are implemented using Pytorch <ref type="bibr" target="#b22">[21]</ref>. All models are trained with Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 1e-1. We set the momentum to 0.9 and the weight decay to 5e-4. The learning rate is divided by 10 at 80k, 140k, 210k, and 280k training iterations. The total number of training iteration is  295K, which corresponds to the number of margin sampling from the normal distribution. During the training, we use random horizontal flipping with a probability of 0.5 for data augmentation. The networks are trained (and evaluated) on images of the size 112 ? 112 ? 3 to produce 512 ? d feature embeddings. These images are aligned and cropped using the Multi-task Cascaded Convolutional Networks (MTCNN) <ref type="bibr" target="#b32">[31]</ref> following <ref type="bibr" target="#b3">[4]</ref>. All the training and testing images are normalized to have pixel values between -1 and 1. b) Training dataset:: We follow the trend in recent works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b19">[19]</ref> in using the MS1MV2 dataset <ref type="bibr" target="#b3">[4]</ref> to train the investigated models with the proposed ElasticFace loss. This enables a direct comparison with the state-of-theart as will be shown in Section IV. The MS1MV2 is a refined version <ref type="bibr" target="#b3">[4]</ref> of the MS-Celeb-1M [7] containing 5.8M images of 85K identities. c) Evaluation benchmarks and metrics:: To demonstrate the effect of our proposed ElasticFace on face recognition accuracy and enable a wide comparison to stateof-the-art, we report the achieved results on nine benchmarks. These benchmarks are of a diverse nature, where some represent a special vulnerabilities of face recognition. The nine benchmarks are 1) Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b9">[9]</ref>, 2) AgeDB-30 <ref type="bibr" target="#b21">[20]</ref>, 3) Cross-age LFW (CALFW) <ref type="bibr" target="#b35">[34]</ref>, 4) Cross-Pose LFW (CPLFW) <ref type="bibr" target="#b34">[33]</ref>, 5) Celebrities in Frontal-Profile in the Wild (CFP-FP) <ref type="bibr" target="#b25">[24]</ref>, 6) IARPA Janus Benchmark-B (IJB-B) <ref type="bibr" target="#b29">[28]</ref>, 7) IARPA Janus Benchmark-C (IJB-C) <ref type="bibr" target="#b18">[18]</ref>, 8) MegaFace <ref type="bibr" target="#b13">[13]</ref>, and 9) MegaFace (R) <ref type="bibr" target="#b3">[4]</ref>. The face recognition performance on LFW, AgeDB-30, CALFW, CPLFW, and CFP-FP is reported as verification accuracy, following their evaluation protocol. The performance on IJB-C and IJB-B is reported (as defined in <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b18">[18]</ref>) as true acceptance rates (TAR) at false acceptance rates (FAR) of 1e-4. The MegaFace and MegaFace(R) benchmarks report the face recognition performance as Rank-1 correct identification rate and as TAR at FAR=1e-6 verification accuracy.</p><p>We acknowledge the verification and identification performance evaluation metrics reported in ISO/IEC 19795-1 <ref type="bibr" target="#b11">[11]</ref>. However, to enhance the reproducibility and comparability, we follow the evaluation protocols and metrics used in each of the benchmarks as listed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>Tables III and IV presents the achieved results on the nine considered benchmarks. The main observation is that our proposed ElasticFace solutions scored beyond the state-ofthe-art in seven out of the nine benchmarks, and very close to the state-of-the-art in the remaining two. When possible, and to build a fair comparison, the results of previous works are reported when trained on the MS1MV2 <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b3">[4]</ref> (or a refined variant of MS1M <ref type="bibr" target="#b6">[7]</ref>) as the ElasticFace results are based on training on this dataset. The proposed ElasticFace ranked first in comparison to the state-of-the-art on the benchmarks AgeDB-30, CPLFW, CFP-FP, IJB-B, IJB-C, MegaFace (R), and MegaFace (verification). In the remaining benchmarks, ElasticFace solutions ranked second on CALFW, third on LFW, and fourth on MegaFace (identification).</p><p>A main outcome of the evaluation is concerning the databases with very large intra-user variations. These are the large age gape benchmark (AgeDB-30) and the frontal-toprofile face verification benchmark (CFP-FP). On AgeDB-30, our ElasticFace-Arc solution scored an accuracy of 98.35%, while the top state-of-the-art performance was 98.32% scored by the CurricularFace <ref type="bibr" target="#b10">[10]</ref>. On CFP-FP, our ElasticFace-Arc+ solution scored an accuracy of 98.73% and our ElasticFace-Arc scored an accuracy of 98.67%, while the top state-of-the-art performances were 98.51% scored by the Partial-FC-CosFace <ref type="bibr" target="#b0">[1]</ref> solution and 98.46% scored by the MagFace <ref type="bibr" target="#b19">[19]</ref>. This significantly enhanced performance in the extreme intra-class variation scenarios points out the generalizability induced by the ElasticFace loss. CALFW and CPLFW also considered age gaps and pose variation, however, with a lower variation than AgeDB-30 and CFP-   <ref type="bibr" target="#b28">[27]</ref>, noting that CosFace was trained on a private dataset. On the same benchmark (MegaFace), our ElasticFace-Arc+ ranked first with 97.44% TAR at FAR1e-6, while the top state-ofthe-art performances were 97.35% scored by the GroupFace <ref type="bibr" target="#b14">[14]</ref>. It must be mentioned that the MegaFace benchmark has been refined in <ref type="bibr" target="#b3">[4]</ref> to MegaFace (R) as it contains many face images with wrong labels as reported in <ref type="bibr" target="#b3">[4]</ref>.</p><p>In comparison to the closely defined losses in ArcFace <ref type="bibr" target="#b3">[4]</ref>, CosFace <ref type="bibr" target="#b28">[27]</ref>, and Partial-FC <ref type="bibr" target="#b0">[1]</ref> solutions, our ElasticFace models did prove to provide a strong performance edge by scoring higher recognition performance on most benchmarks. When it comes to comparing ElasticFace and Elastic-Face+, the ElasticFace-Arc and ElasticFace-Arc+ did achieve very close performances when considering all benchmarks. On the other hand, the ElasticFace-Cos+ did outperform ElasticFace-Cos on most benchmarks.</p><p>We acknowledge that the Partial-FC <ref type="bibr" target="#b0">[1]</ref> solution reported additional performance rates when trained on their new collected database, the Glint360K <ref type="bibr" target="#b0">[1]</ref>. However, we could not acquire this database as it requires an account on a cloud platform, that in itself requires a SIM card registered in a specific country, which is very restrictive and we do not have access to. Therefore, and for a fair comparison, we opted to compare our results with the Partial-FC results when trained on the same dataset that our ElasticFace solution is using, the MS1MV2 <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b3">[4]</ref> dataset.</p><p>The slightly increased training computational cost is a minor limitation of our proposed ElasticFace. Training the ResNet-100 model on MS1MV2 dataset with CosFace or ArcFace using the specified machine and training details described in Section III requires around 57 hours. This training time is increased by around one minute for ElasticFace and by 11 hours for ElasticFace+. The minor increase in the ElasticFace training time is caused by the sampling of the margin values, while the larger increase in ElasticFace+ training time is additionally caused by the sorting algorithms.</p><p>On a less technical note, we stress that our efforts in the advancement of face recognition are aimed at enhancing the security, convenience, and life quality of the members of society, e.g. enabling convenient access to financial and health services <ref type="bibr" target="#b5">[6]</ref> and enhancing the security of border checks within clear legal frameworks and users consent. We acknowledge and reject the possible malicious or illegal use of this and other technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose an elastic margin penalty loss (ElasticFace) that avoids setting a single constant penalty margin. Our motivation considers that real training data is inconsistent in terms of inter and intra-class variation, and thus the assumption made by many margin softmax losses that the geodesic distance between and within the different identities can be equally learned using a fixed margin is less than optimal. We, therefore, relax this fixed margin constrain by using a random margin value drawn from a normal distribution in each training iteration. In an extended definition, the assignment of these margin values to training samples corresponds to their proximity to their class centers. We evaluated our ElasticFace loss, in comparison to state-of-the-art face recognition approaches, on nine different benchmarks. This evaluation demonstrated that our ElasticFace solution consistently extended state-of-theart face recognition performance on most benchmarks (seven out of nine). This was specifically apparent in the challenging benchmarks with large intra-class variations, such as large age gaps and frontal-to-profile face comparisons. Our code, trained models, and training details will be released under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>sof tmax(x i , y i ) = e fy i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Decision boundary of (a) ArcFace, (b) ElasticFace-Arc, (c) CosFace, and (d) ElasticFace-Cos for binary classification. The dashed blue line is the decision boundary. The gray area illustrates the decision margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Toy example of 3 ResNet-18 networks trained under different experimental settings. The 2-D features are normalized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Parameter selection for ElasticFace-Arc and ElasticFace-Arc+. The Borda count (BC) is reported separately for each of training settings (ArcFace, ElasticFace-Arc and ElasticFace-Arc+) and each of the evaluation benchmarks. The final ? and m parameters are selected based on the highest BC sum. In all settings, the used architecture is ResNet-50 trained on CASIA<ref type="bibr" target="#b31">[30]</ref>.</figDesc><table><row><cell>Loss</cell><cell cols="2">LFW Acc (%) BC</cell><cell cols="3">AgeDB-30 Acc (%) BC Acc (%) CALFW</cell><cell cols="2">CPLFW BC Acc (%)</cell><cell cols="2">CFP-FP BC Acc (%)</cell><cell cols="2">-BC Sum BC</cell></row><row><cell>CosFace (m=0.4)</cell><cell>99.42</cell><cell>1</cell><cell>94.65</cell><cell>3</cell><cell>93.45</cell><cell>1</cell><cell>90.38</cell><cell>3</cell><cell>95.30</cell><cell>1</cell><cell>9</cell></row><row><cell>CosFace (m=0.35)</cell><cell>99.55</cell><cell>3</cell><cell>94.55</cell><cell>2</cell><cell>93.78</cell><cell>3</cell><cell>89.95</cell><cell>1</cell><cell>95.31</cell><cell>2</cell><cell>11</cell></row><row><cell>CosFace (m=0.3)</cell><cell>99.45</cell><cell>2</cell><cell>94.45</cell><cell>1</cell><cell>93.46</cell><cell>2</cell><cell>90.12</cell><cell>2</cell><cell>95.39</cell><cell>3</cell><cell>10</cell></row><row><cell>ElasticFace-Cos (m=0.35,?=0.0125)</cell><cell>99.45</cell><cell>2</cell><cell>94.72</cell><cell>1</cell><cell>93.83</cell><cell>1</cell><cell>90.12</cell><cell>2</cell><cell>95.47</cell><cell>3</cell><cell>9</cell></row><row><cell>ElasticFace-Cos (m=0.35,?=0.0175)</cell><cell>99.50</cell><cell>3</cell><cell>94.77</cell><cell>3</cell><cell>93.97</cell><cell>4</cell><cell>90.10</cell><cell>1</cell><cell>95.30</cell><cell>2</cell><cell>13</cell></row><row><cell>ElasticFace-Cos (m=0.35,?=0.025)</cell><cell>99.42</cell><cell>1</cell><cell>94.85</cell><cell>4</cell><cell>93.88</cell><cell>2</cell><cell>90.20</cell><cell>3</cell><cell>95.21</cell><cell>1</cell><cell>11</cell></row><row><cell>ElasticFace-Cos (m=0.35,?=0.05)</cell><cell>99.52</cell><cell>4</cell><cell>94.77</cell><cell>3</cell><cell>93.93</cell><cell>3</cell><cell>90.38</cell><cell>4</cell><cell>95.52</cell><cell>4</cell><cell>18</cell></row><row><cell>ElasticFace-Cos+(m=035, ?=0.0125</cell><cell>99.38</cell><cell>1</cell><cell>94.50</cell><cell>2</cell><cell>93.67</cell><cell>3</cell><cell>89.85</cell><cell>1</cell><cell>95.20</cell><cell>1</cell><cell>8</cell></row><row><cell cols="2">ElasticFace-Cos+(m=035, ?=0.0175) 99.45</cell><cell>2</cell><cell>94.97</cell><cell>4</cell><cell>93.48</cell><cell>1</cell><cell>89.98</cell><cell>2</cell><cell>95.23</cell><cell>2</cell><cell>11</cell></row><row><cell>ElasticFace-Cos+(m=035, ?=0.025)</cell><cell>99.55</cell><cell>4</cell><cell>94.63</cell><cell>3</cell><cell>93.65</cell><cell>2</cell><cell>90.28</cell><cell>4</cell><cell>95.47</cell><cell>4</cell><cell>17</cell></row><row><cell>ElasticFace-Cos+(m=035, ?=0.05)</cell><cell>99.48</cell><cell>3</cell><cell>94.45</cell><cell>1</cell><cell>93.77</cell><cell>4</cell><cell>90.01</cell><cell>3</cell><cell>95.26</cell><cell>3</cell><cell>14</cell></row></table><note>f) Parameter Selection: The probability density func- tion has its peak around m [22]. Thus, when ElasticFace is integrated into ArcFace [4], we select the best margin value (as a single value) by training three instances of ResNet-50 [8] on CASIA [30] with ArcFace loss using margins equal</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table /><note>: Parameter selection for ElasticFace-Cos and ElasticFace-Cos+. The Borda count (BC) is reported sep- arately for each of training settings (ArcFace, ElasticFace- Cos and ElasticFace-Cos+) and each of the evaluation bench- marks. The final ? and m parameters are selected based on the highest BC sum. In all settings, the used architecture is ResNet-50 trained on CASIA [30].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>The achieved results on the LFW, AgeDB-30, CALFW, CPLFW, and CFP-FP benchmarks. On large age gape (AgeDB-30) and frontal-to-profile face comparisons (CFP-FP), the ElasticFace solutions consistently extend state-of-the-art performances. ElasticFace scores very close to the state-of-the-art on LFW and CALFW. All decimal points are provided as reported in the respective works. The top performance in each benchmark is in bold. The top three performances in each benchmark are noted with rank number between parentheses (1,2 or 3).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>The achieved results on the IJB-B, IJB-C, MegaFace (R), and MegaFace benchmarks. On the earlier three, and the verification accuracy of the fourth, the ElasticFace solutions consistently extend state-of-the-art performances. ElasticFace scores very close to the state-of-the-art on MegaFace. MegaFace has been refined in<ref type="bibr" target="#b3">[4]</ref> to MegaFace (R) as it contains many face images with wrong labels. All decimal points are provided as reported in the respective works. The top performance in each benchmark is in bold. The top three performances in each benchmark are noted with rank number between parentheses (1,2 or 3).FP. In CALFW, ElasticFace-Cos+ scored a close second with 96.18% accuracy, with the lead going to the CurricularFace<ref type="bibr" target="#b10">[10]</ref> with 96.20% accuracy. In CPLFW, our ElasticFace-Arc+ is ranked first with 93.28% accuracy, while the top stateof-the-art performance was 93.17% accuracy scored by the GroupFace<ref type="bibr" target="#b14">[14]</ref>. On the LFW benchmark<ref type="bibr" target="#b9">[9]</ref>, which is one of the oldest and nearly saturated benchmarks reported in the recent works, our ElasticFace-Cos and ElasticFace-Arc+ solutions scored an accuracy of 98.82%, very close behind the GroupFace<ref type="bibr" target="#b14">[14]</ref> with 99.85%.InTable IV, on IJB-B benchmark, our ElasticFace-Cos+ scored a TAR at FAR1e-4 of 95.43%, far ahead of the Partial-FC-CosFace<ref type="bibr" target="#b0">[1]</ref> and the GroupFace<ref type="bibr" target="#b14">[14]</ref> with 95.0% and 94.93%, respectively. Similarly, on the IJB-C benchmark, our ElasticFace-Cos+ scored a TAR at FAR1e-4 of 96.65%, ahead of the Partial-FC-CosFace<ref type="bibr" target="#b0">[1]</ref> and the GroupFace<ref type="bibr" target="#b14">[14]</ref> with 96.4% and 96.36% respectively. On the MegaFace (R), our ElasticFace-Arc scored 98.81% Rank-1 identification rate and 98.92% TAR at FAR1e-6, ahead of the previous lead solution, the GroupFace<ref type="bibr" target="#b14">[14]</ref> with 98.74% and 98.79%, respectively. On the MegaFace benchmark, our ElasticFace-Cos scored Rank-1 identification rate of 81.01%, close to the state-of-the-art 82.72% score by CosFace</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Partial fc: Training 10 million identities on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="1445" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition, FG 2018</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uniformface: Learning deep equidistributed representation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="https://eaadhaar.uidai.gov.in/" />
		<title level="m">Unique Identification Authority of India</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<biblScope unit="page" from="2016" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings, Part III</title>
		<meeting>Part III<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Curricularface: Adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5900" to="5909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">ISO/IEC JTC1 SC37 Biometrics. ISO/IEC 19795-1:2021 Information technology -Biometric performance testing and reporting -Part 1: Principles and framework. International Organization for Standardization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dyn-arcface: dynamic additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multim. Tools Appl</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Groupface: Learning latent groups and constructing group-based representations for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5620" to="5629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptiveface: Adaptive margin and sampling for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11947" to="11956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>M. Balcan and K. Q. Weinberger</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">IARPA janus benchmark -C: face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics</title>
		<meeting><address><addrLine>Gold Coast, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-02-20" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Magface: A universal representation for face recognition and quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR Virtual</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Agedb: The first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1997" to="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Probability, random variables, and random signal principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Z</forename><surname>Peebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>McGraw Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016</title>
		<meeting><address><addrLine>Lake Placid, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1849" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6397" to="6406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">IARPA janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A light CNN for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2884" to="2896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adacos: Adaptively scaling cosine logits for effectively learning deep face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10823" to="10832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<idno>18-01</idno>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-age LFW: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1708.08197</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
