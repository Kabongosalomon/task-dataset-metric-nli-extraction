<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting People in Artwork with CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Westlake</surname></persName>
							<email>n.westlake@bath.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Unversity of Bath</orgName>
								<address>
									<settlement>Bath</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongping</forename><surname>Cai</surname></persName>
							<email>hongping.cai@bristol.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hall</surname></persName>
							<email>p.m.hall@bath.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Unversity of Bath</orgName>
								<address>
									<settlement>Bath</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting People in Artwork with CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNNs</term>
					<term>cross-depiction problem</term>
					<term>object recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CNNs have massively improved performance in object detection in photographs. However research into object detection in artwork remains limited. We show state-of-the-art performance on a challenging dataset, People-Art, which contains people from photos, cartoons and 41 different artwork movements. We achieve this high performance by fine-tuning a CNN for this task, thus also demonstrating that training CNNs on photos results in overfitting for photos: only the first three or four layers transfer from photos to artwork. Although the CNN's performance is the highest yet, it remains less than 60% AP, suggesting further work is needed for the cross-depiction problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection has improved significantly in recent years, especially as a result of the resurgence of convolutional neural networks (CNNs) and the increase in performance and memory of GPUs. However, in spite of the successes in photobased recognition and detection, research into recognition within styles of images other than natural images (photos) remains limited <ref type="bibr" target="#b0">[1]</ref>. We refer to this as the cross-depiction problem: detecting objects regardless of how they are depicted (photographed, painted, drawn, etc.).</p><p>We believe that cross-depiction recognition is an interesting and open problem. It is interesting because it forces researchers to look beyond the surface appearance of object classes. By analogy, just as a person retains their identity no matter what clothes they wear, so an object retains its class identity no matter how it is depicted: a dog is a dog whether photographed, painted in oils, or drawn with a stick in the sand.</p><p>Cross-depiction is a practical problem too: an example is an image search. The world contains images in all sorts of depictions. Any recognition solution that does not generalise across these depictions is of limited power. Yet most current computer vision methods tacitly assume a photographic input, either by design or training. Any model premised on a single depictive style e.g. photos will lack sufficient descriptive power for cross-depiction recognition. Therefore, an image search using methods will limit its results to photos and photo-like depictions.</p><p>In our paper, we talk about natural images (photos) and non-natural images (artwork) as a linguistic convenience. We would argue that this is a false dichotomy: the universe of all images includes images in all possible depictive styles, and there is no particular reason to privilege any one style. Nevertheless, we acknowledge that the distribution of styles is not uniform: photos may be more abundant and certainly are in computer vision datasets such as Ima-geNet <ref type="bibr" target="#b1">[2]</ref>. This creates problems for generalisation: training a detector on photos alone constrains it not only in terms its ability to handle denotational varieties, but projective and pose varieties too, as we discuss later.</p><p>We present a new dataset, People-Art, which contains photos, cartoons and images from 41 different artwork movements. Unlike the Photo-Art dataset <ref type="bibr" target="#b2">[3]</ref>, which had 50 classes, this dataset has a single class: people. We labelled people since we observe that people occur far more frequently across the wide spectrum of depictive styles than other classes, thus allowing a far greater variety. Detecting people within this dataset is a challenging task because of the huge range of ways artists depict people: from Picasso's cubism to Disney's Sleeping Beauty. The best performance on a pre-release of the dataset is 45% average precision (AP), from a CNN that was neither trained nor fine-tuned for this task. By fine-tuning a state-of-the-art CNN for this task <ref type="bibr" target="#b3">[4]</ref>, we achieved 58% AP, a substantial improvement.</p><p>As well as achieving state-of-art performance on our People-Art dataset, we make the following contributions, in order of strength:</p><p>1. We show that a simple tweak for the "Fast Region-based Convolutional Network" method (Fast R-CNN) <ref type="bibr" target="#b3">[4]</ref>, changing the criteria for negative training exemplars compared to default configuration, is key to higher performance on artwork. 2. We show the extent to which fine-tuning a CNN on artwork improves performance when detecting people in artwork on our dataset (Section 5.2) and the Picasso dataset <ref type="bibr" target="#b4">[5]</ref> (Section 5.4). We show that this alone is not a solution: the performance is still less than 60% AP after fine tuning, suggesting the need for futher work. 3. Consistent with earlier work <ref type="bibr" target="#b5">[6]</ref>, we show that the lower convolutional layers of a CNN generalise to artwork: others benefit from fine-tuning (Section 5.1).</p><p>We begin by presenting related work and our People-Art dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We use a state-of-the-art CNN to improve performance on a cross-depiction dataset, thereby contributing towards cross-depiction object recognition. We first explore related work on deep learning for object detection and localisation (largely in photos), followed by previous work on the cross-depiction problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning for Object Detection and Localisation</head><p>Deep learning has been around for a few decades <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. After a period of limited use within computer vision, <ref type="bibr" target="#b9">Krizhevsky et al. (2012)</ref> [10] demonstrated a vast performance improvement for image classification over previous state-of-the-art methods, using a deep CNN. As a result, the use of CNNs surged within computer vision.</p><p>Early CNN based approaches for object localisation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> used the same sliding-window approach used by previous state-of-the-art detection systems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. As CNNs became larger, and with an increased number of layers, this approach became intractable. However, <ref type="bibr" target="#b16">Sermanet et al. (2014)</ref>  <ref type="bibr" target="#b16">[17]</ref> demonstrated that few windows are required, provided the CNN is fully convolutional. Furthermore, as the size of their receptive fields increased, CNNs either became or were trained to be less sensitive to precise location and scale the input. As a result, obtaining a precise bounding box using sliding window and non-maximal suppression became difficult. One early approach attempted to solve this issue by training a separate CNN for precise localisation <ref type="bibr" target="#b17">[18]</ref>. <ref type="bibr" target="#b18">Szegedy et al. (2013)</ref>  <ref type="bibr" target="#b18">[19]</ref> modified the architecture of Krizhevsky et al. (2012) <ref type="bibr" target="#b9">[10]</ref> for localisation by replacing the final layer of the CNN with a regression layer. This layer produces a binary mask indicating whether a given pixel lies within the bounding box of an object. <ref type="bibr" target="#b19">Schulz and Behnke (2011)</ref>  <ref type="bibr" target="#b19">[20]</ref> previously used a similar approach with a much smaller network for object segmentation. <ref type="bibr" target="#b20">Girshick et al. (2014)</ref>  <ref type="bibr" target="#b20">[21]</ref> introduced "regions with CNN features" (R-CNN), which surpassed previous approaches. The authors used selective search <ref type="bibr" target="#b21">[22]</ref>, a hierarchical segmentation method, to generate region proposals: possible object locations within an image. Next, a CNN obtains features from each region and a support vector machine (SVM) classifies each region. In addition, they used a regression model to improve the accuracy of the bounding box output by learning bounding box adjustments for each class-agnostic region proposal.   <ref type="bibr" target="#b22">[23]</ref> improved the run-time performance by introducing SPP-net, which uses a spatial pyramid pooling (SPP) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> layer after the final convolutional layer. The convolutional layers operate on the whole image, while the SPP layer pools based on the region proposal to obtain a fixed length feature vector for the fully connected layers. <ref type="bibr" target="#b3">Girshick (2015)</ref>  <ref type="bibr" target="#b3">[4]</ref> later introduced Fast R-CNN which improves upon R-CNN and SPP-net and allows the CNN to output a location of the bounding box (relative to the region proposal) directly, along with class detection score, thus replacing the SVM. Furthermore, this work enables end-to-end training of the whole CNN for both detection and bounding box regression. We use this approach to achieve state-of-the-art performance on our People-Art dataset and detail the method in Section 4.</p><p>To make Fast R-CNN even faster and less dependent on selective search <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">Lenc and Vedaldi (2015)</ref>  <ref type="bibr" target="#b25">[26]</ref> used a static set of region proposals. <ref type="bibr" target="#b26">Ren et al. (2015)</ref>  <ref type="bibr" target="#b26">[27]</ref> instead used the output of the existing convolutional layers plus additional convolutional layers to predict regions, resulting in a further increase in accuracy and efficiency.</p><p>Redmon et al. (2015) <ref type="bibr" target="#b27">[28]</ref> proposed "You Only Look Once" (YOLO), which operates quicker though with less accuracy than other state-of-art approaches. A single CNN operates on an entire image, divided in a grid of rectangular cells, without region proposals. Each cell outputs bounding box predictions and class probabilities; unlike previous work, this occurs simultaneously. <ref type="bibr" target="#b28">Huang et al. (2015)</ref> [29] proposed a similar system, introducing up-sampling layers to ensure the model performs better with very small and overlapping objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Depiction Detection and Matching</head><p>Early work relating to non-photographic images focused on matching handdrawn sketches. <ref type="bibr" target="#b29">Jacobs et al. (1995)</ref>  <ref type="bibr" target="#b29">[30]</ref> used wavelet decomposition of image colour channels to allow matching between a rough colour image sketch and a more detailed colour image. <ref type="bibr" target="#b30">Funkhouser et al. (2003)</ref> [31] used a distance transform of a binary line drawing, followed by fourier analysis of the distance transforms at fixed radii from the centre of the drawing, to match 2D sketches and 3D projections, with limited performance. Hu and Collomosse (2013) <ref type="bibr" target="#b31">[32]</ref> used a modified version of Histograms of Oriented Gradients (HOG) <ref type="bibr" target="#b14">[15]</ref> to extract descriptors at interest-points in the image: for photographs, these are at Canny edges <ref type="bibr" target="#b32">[33]</ref> pixels; for sketches, these are sketch strokes. <ref type="bibr" target="#b33">Wang et al. (2015)</ref>  <ref type="bibr" target="#b33">[34]</ref> used a siamese CNN configuration to match sketches and 3D model projections, optimising the CNN to minimise the distances between sketches and 3D model projections of the same class.</p><p>Another cross-depiction matching approach, by Crowley et al. (2015) <ref type="bibr" target="#b34">[35]</ref>, uses CNN generated features to match faces between photos and artwork. This relies on the success of a general face detector <ref type="bibr" target="#b35">[36]</ref>, which succeeds on artwork which is "largely photo-realistic in nature" but has not been verified on more abstract artwork styles such as cubism.</p><p>Other work has sought to use self-similarity to detect patterns across different depictions such as Shechtman and Irani (2007) <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b37">Chatfield et al. (2009)</ref>  <ref type="bibr" target="#b37">[38]</ref> who used self-similarity descriptors formed by convolving small regions within in image over a larger region. This approach is not suitable for identifying (most) objects as a whole: for example, the results show effective matching of people forming a very specific pose, not of matching people as an object class in general.</p><p>Recent work has focused on cross-depiction object classification and detection. <ref type="bibr">Wu</ref>   <ref type="bibr" target="#b15">[16]</ref> to perform cross-depiction matching between photographs and "artwork", (including "clip-art", cartoons and paintings). Instead of using root and part-based filters and a latent SVM, the authors learnt a fully connected graph to better model object structure between depictions, using the structured support vector machine (SSVM) formulation of <ref type="bibr" target="#b38">Cho et al. (2013)</ref>  <ref type="bibr" target="#b38">[39]</ref>. In addition, each model has separate "attributes" for photographs and "artwork": at test-time, the detector uses the maximum response from either of "attribute" set, to achieve depiction invariance. This work improved performance for detecting objects in artwork, but depended on a high performing DPM to bootstrap the model. Our dataset is more challenging than the one used, leading to a low accuracy using DPM and hence this is approach is also not suitable.</p><p>Zissermann et al. (2014) <ref type="bibr" target="#b39">[40]</ref> evaluate the performance of CNNs learnt on photos for classifying objects in paintings, showing strong performance in spite of the different domain. Their evaluation excludes people as a class, as people appear frequently in their paintings without labels. Our People-Art dataset addresses this issue: all people are labelled and hence we provide a new benchmark. We also believe our dataset contains more variety in terms of artwork styles and presents a more challenging problem. Furthermore, we advance their findings: we show the performance improvement when a CNN is fine-tuned for this task rather than simply fine-tuned on photos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The People-Art Dataset and its Challenges</head><p>Our People-Art dataset 3 contains images divided into 43 depiction styles. Images from 41 of these styles came from WikiArt.org while the photos came from PASCAL VOC 2012 <ref type="bibr" target="#b40">[41]</ref> and the cartoons from google searches. We labelled people since, according to our empirical observations, people are drawn or painted more often than other objects. Consequently, this increases the total number of individual instances and thus the range of depictive styles represented. <ref type="figure">Figure 2</ref> shows one painting from each style represented in our People-Art dataset.</p><p>The 41 depictive styles from WikiArt.org are catagorised based on art movements. These depiction styles cover the full range of projective and denotational styles, as defined by Willats <ref type="bibr" target="#b41">[42]</ref>. In addition, we propose that these styles cover many poses, a factor which Willats did not consider.</p><p>We believe that our dataset is challenging for the following reasons:</p><p>range of denotational styles This is the style with which primitive marks are made (brush strokes, pencil lines, etc.) <ref type="bibr" target="#b41">[42]</ref>. We consider photos to be a depictive style in its own right. range of projective style This includes linear camera projection, orthogonal projection, inverse perspective, and in fact a range of ad-hoc projections <ref type="bibr" target="#b41">[42]</ref>. An extreme form is shown in cubism, in which it is common for the view of a person from many different viewpoints to be drawn or painted on the 2D canvas <ref type="bibr" target="#b4">[5]</ref>.</p><p>range of poses Though pose is handled by previous computer vision algorithms <ref type="bibr" target="#b15">[16]</ref>, we have observed that artwork, in general, exhibits a wider variety of poses than photos. overlapping, occluded and truncated people This occurs in artwork as in photos, and perhaps to a greater extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CNN architecture</head><p>We use the same architecture as Fast R-CNN <ref type="bibr" target="#b3">[4]</ref>, which is built around a modified version of the Caffe library <ref type="bibr" target="#b42">[43]</ref>. The CNN has two inputs: an image and a set of class-agnostic rectangular region proposals. Many algorithms exist for generating region proposals; we use selective search <ref type="bibr" target="#b21">[22]</ref> with the default configuration.</p><p>The first stage of the CNN operates on the entire image (having been resized to a fixed dimension while preserving aspect ratio). This stage consists of convolutional layers, rectified linear units (ReLUs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b43">44]</ref>, max-pooling layers and, in some cases, local response normalisation layers <ref type="bibr" target="#b9">[10]</ref>. The final layer is a region of interest (ROI) pooling layer which is novel to Fast R-CNN: as well as the input from the previous convolutional or ReLU layer, this layer receives another input, a region proposal or ROI; the output is a fixed-length feature vector formed by Fine tune Learn from random <ref type="figure">Fig. 3</ref>. We use a network pre-trained on ImageNet and fine-tuned on our People-Art dataset (training and validation sets): we fix the weights for the first F layers, selected by validation.</p><p>max-pooling of the convolution features. In order to preserve information about the global structure of the ROI, i.e. at a scale within an order of magnitude of the ROI size, the max-pooling happens over a uniformly spaced rectangular grid, size H ? W . As a result, the layer outputs feature vector with CHW dimensions where C is the number of channels of the previous convolutional layer. This feature vector is the input to the second stage of the CNN, which is fully connected. It consists of inner product and ReLU layers, as well as dropout layers (training only) aimed at preventing overfitting <ref type="bibr" target="#b44">[45]</ref>. The output for each class is a score and a set of four co-ordinate which indicate the bounding box co-ordinates relative to the ROI. We modified the final layer to output a score and bounding box prediction for only one class: person.</p><p>We use the same approach for training as Fast R-CNN, which uses stochastic gradient descent (SGD) with momentum <ref type="bibr" target="#b9">[10]</ref>, initialising the network with weights from the pre-trained models, in our case, trained on ImageNet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. We fix the weights of the first F convolutional layers to those in the pre-trained model; this parameter is selected by validation. We experiment with different criteria for the region proposals to use as training ROI, as detailed in Section 5.1. Since the final inner product layers have a different size output as we only detect one class, we use random (Gaussian) initialisation. <ref type="figure">Figure 3</ref> shows our network architecture in detail.</p><p>We fine-tune the models (pre-trained on ImageNet) using our People-Art dataset (training and validation sets). We test three different models: CaffeNet, which is a reproduction of AlexNet <ref type="bibr" target="#b9">[10]</ref> with some minor changes, Oxford VGG's "CNN M 1024" (VGG1024) <ref type="bibr" target="#b45">[46]</ref> and Oxford VGG's "Net D" (VGG16) <ref type="bibr" target="#b46">[47]</ref>. Both CaffeNet and VGG1024 have five convolutional layers and local response normalisation layers and vary slightly: in particular VGG1024 has more weights and channels. VGG16 is much a larger network, with thirteen convolutional layers and no local response normalisation. Except for the number of dimensions, all three networks have the same ROI pooling layer and fully connected network structure: each CNN's fully connected network structure consists of two inner product layers, each followed by ReLU and dropout layers (training only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>For both validation and testing, our benchmark is average precision (AP): we calculate this using the same method as PASCAL Visual Object Classes (VOC) detection task <ref type="bibr" target="#b47">[48]</ref>. A positive detection is one whose intersection over union (IoU) overlap with a ground-truth bounding box is greater than 50%; duplicate detections are considered false. Annotations marked as difficult are excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ROI Selection and Layer Fixing for CNN Fine-Tuning</head><p>Although we used the default selective search settings to generate region proposals, we experimented with different criteria to specify which region proposals to use in training. The default configuration of Fast-RCNN <ref type="bibr" target="#b3">[4]</ref> defines positive ROI be region proposals whose IoU overlap with a ground-truth bounding box is at least 0.5, and defines negative ROI to be those whose overlap lies in the interval [0.1, 0.5). The cutoff between positive and negative ROI matches the definition of positive detection according the VOC detection task <ref type="bibr" target="#b47">[48]</ref>. <ref type="bibr" target="#b3">Girshick (2015)</ref> states that the lower cut-off (0.1) for negative ROI appears to act as a heuristic to mine hard examples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>We experimented with two alternative configurations for fine tuning:</p><p>gap We discarded ROI whose IoU overlap with a ground-truth bounding box lies in the interval [0.4, 0.6): we hypothesised that ROI lying in this interval are ambiguous and hamper training performance. all-neg We removed the lower bound for negative ROI. We hypothesised that this would improve performance on our People-Art dataset for two reasons: 1. This results in the inclusion of ROI containing classes which appear similar to people, for example animals with faces. 2. This permits the inclusion of more artwork examples, for example images without any people present. We hypothesised that this would make the CNN better able to discern between features caused by the presence of people and features resulting from a particular depiction style.</p><p>We fixed all other hyper-parameters of the CNN except for F , the number of convolutional layers whose weights we fix to those learnt from ImageNet, which we select based validation performance. <ref type="table">Table 1</ref> shows the validation performance for the different criteria, i.e. from testing on the validation set after fine-tuning on the People-Art training set. Removing the lower bound on negative ROI (all-neg) results in a significant increase in performance, around a 9 percentage point increase in average precision <ref type="table">Table 1</ref>. Validation performance using different criteria for positive and negative ROI: we use CNNs pre-trained on ImageNet, fine-tune on the training set and then test on the validation set; we select the best configuration for each CNN (bold). in the best performing case. Indeed, it appears that what is not a person is as important as what is a person for training. Discarding ROI with an IoU overlap in the interval [0.4, 0.6) yields mixed results: it was marginally beneficial in one case, and detrimental in all others. We note that the optimal number of convolutional layers for which to fix weights to the pre-trained model, F , varies across the different training configurations, even for the same CNN. The variation in performance could be explained by stochastic variation caused by the use of SGD. The performance falls rapidly for F ? 5; we therefore conclude that the first three or four convolutional layers transfer well from photos to artwork. Fine-tuning these layers yields no significant improvement nor detriment in performance. In this respect, we show similar results to <ref type="bibr" target="#b5">Yosinski et al. (2014)</ref>  <ref type="bibr" target="#b5">[6]</ref> for our task: i.e. the first three or four convolutional layers are more transferable than later layers, in our case from photos to artwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROI IoU</head><p>For all later experiments, including the performance benchmarks, we select the configuration which maximises performance on the validation set (bold in <ref type="table">Table 1</ref>) and re-train (fine-tune) using the combined train and validation sets. <ref type="table" target="#tab_3">Table 2</ref> shows how each CNN model and other methods perform on the People-Art test set. The best performing CNN, VGG16, scores 58% AP, an improvement of 13 percentage points on the best previous result 45% <ref type="bibr" target="#b27">[28]</ref>. The results demonstrate the benefits of fine-tuning the CNN (on the training and validation sets of People-Art) for the task. We also conclude that training and fine-tuning a CNN on photos yields a model which overfits to photographic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Benchmarks on the People-Art Dataset</head><p>As noted in Section 4, Fast R-CNN (unlike YOLO) relies on an external algorithm, here selective search <ref type="bibr" target="#b21">[22]</ref>, to generate region proposals. We used the default settings, which are tuned to photos. Selective Search achieves a recall   rate of 98% on the People-Art test set. As such, this does not appear to be a limiting factor for the performance. We attempted to fine-tune YOLO <ref type="bibr" target="#b27">[28]</ref> on People-Art. The default configuration results in an exploding gradient, perhaps due to the sparsity of regions containing objects (only people in this case) compared to other datasets. We expect that a brute-force search over the parameters or heuristic may solve this problem in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detection Performance on People-Art</head><p>We used the tools of <ref type="bibr" target="#b48">Hoiem et al. (2012)</ref>  <ref type="bibr" target="#b48">[49]</ref> to analyse the detection performance of the best performing CNN. Since we only have a single class (person), detections have three types based on their IoU with a ground truth labelling:</p><p>Cor correct i.e. IoU ? 0.5 Loc false positive caused by poor localisation, 0.1 ? IoU &lt; 0.5 BG a background region, IoU &lt; 0.1 <ref type="figure" target="#fig_2">Figure 4</ref> shows the detection trend: the proportion of detection types as the number of detections increases, i.e. from reducing the threshold. At higher thresholds, the majority of incorrect detections are caused by poor localisation; at lower thresholds, background regions dominate. In total, there are 1088 people labelled in the test set, and that are not labelled difficult. The graph in <ref type="figure" target="#fig_2">Figure  4</ref> shows a grey dashed line corresponding to this number detections and <ref type="figure" target="#fig_2">Figure  4</ref> shows a separate pie chart for this threshold. This threshold corresponding to this number of detections is significant: with perfect detection, there would be no false positives or false negatives. This shows that poor localisation is the bigger cause of false positives, though only slightly more so than background regions. <ref type="figure">Figure 5</ref> shows false positives caused by background regions. Some are caused by mammals which is understandable given these, like people, have faces and bodies. Others detections have less clear causes. <ref type="figure">Figure 6</ref> show the false positives caused by poor localisation. In some of the cases, the poor localisation is caused by the presence of more than one person, which leads to the bounding box covering multiple people. In other cases, the bounding box does not cover the full extent of the person, i.e. it misses limbs or the lower torso. We believe that this shows the extent to which the range of poses makes detecting people in artwork a challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Benchmarks on the Picasso Dataset</head><p>In addition to the results on People-Art, we show results on the Picasso Dataset <ref type="bibr" target="#b4">[5]</ref>. The dataset contains a set of Picasso paintings and labellings for people which are based on the median of the labellings given by multiple human participants. <ref type="table" target="#tab_4">Table 3</ref> shows how each CNN and other methods perform. As before, each CNN performed better if it was fine-tuned on People-Art rather than VOC 2007 ; moreover, DPM performs better than CNNs fine-tuned on VOC 2007 but worse  <ref type="bibr" target="#b15">[16]</ref> VOC 2007 N/A 38% YOLO <ref type="bibr" target="#b27">[28]</ref> ImageNet VOC 2012 53% than those fine-tuned on People-Art. This confirms our earlier findings: CNNs fine-tuned on photos overfit to photo. In addition, we show that our fine-tuning results in a model which is not just better for People-Art but a dataset containing artwork which we did not train on. Interestingly, the best performing CNN is the smallest (CaffeNet), suggesting that the CNNs may still be overfitting to less abstract artwork. Furthermore, the best performing method is YOLO despite being fine-tuned on photos (VOC 2012 ). Selective Search achieved a recall rate of 99% on the Picasso Dataset, so this is unlikely to be the reason that Fast R-CNN performs worse than YOLO. We therefore believe that YOLO's design is more robust to abstract forms of art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">The Importance of Global Structure</head><p>Earlier work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> suggests that structure is invariant across depictive styles, and therefore useful for cross-depiction detection. As described in Section 4, Fast R-CNN includes an ROI pooling layer, which carries out max-pooling over H ? W uniformly spaced rectangular grid. Therefore, the ROI pooling layer captures the global structure of the person, while earlier convolutional layers only pick up the local structure.  To test whether the global structure is useful for detecting and localising people in artwork, we replaced the ROI pooling layer replaced with a single cell max-pooling layer. This is equivalent to setting W = 1 and H = 1 for the ROI pooling layer (see <ref type="figure" target="#fig_4">Figure 7</ref>). This is similar to "bag of visual word" algorithms: with W = H = 1, the fully connected layers have no information about the location the previous layer's output. We fine-tuned as before. <ref type="table" target="#tab_5">Table 4</ref> shows the results. In all cases, replacing the default ROI pooling layer with a single cell max-pooling layer results in worse performance. On top of this, the performance is worse than when fine-tuned on VOC 2007 with the default configuration. This supports the claim of earlier work, that structure is invariant across depictive styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated state-of-the-art cross-depiction detection performance on our challenge dataset, People-Art, by fine-tuning a CNN for this task. In doing so, we have shown that a CNN trained on photograph alone overfits to photos, while fine-turning on artwork allows the CNN to better generalise to other styles of artwork. We have also made other observations, including the importance of negative exemplars from artwork.</p><p>The performance on our People-Art dataset, though the best so far, is still less than 60% AP. We have demonstrated that the CNN often detects other mammals instead of people or makes other spurious detections and often fails to localise people correctly. We propose further work to address these issues.</p><p>In addition, the dataset only covers a subset of possible images containing people. Our dataset does not include African, Babylonian, Chinese or Egyptian art, the Bayeux Tapestry, stained glass windows, photos of sculptures and all kinds of other possibilities. Therefore, we are only beginning to examine the cross-depiction problem, which provides a huge scope for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Detecting people across different depictive styles a challenge: here we show some successful detections. arXiv:1610.08871v1 [cs.CV] 27 Oct 2016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Left: The proportion of detections by type as the threshold decreases: either correct, a background region (BG) or poor localisation (LOC); Right: the proportion for D=1088, the actual number of people, marked as a grey dashed line on the left plot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>False positive detections on background regions from the best performing CNN False positive detections due to poor localisation from the best performing CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Two pooling layers and their resulting feature vectors from a two channel input; Left: An ROI pooling layer (red grid) takes the maximum for each channel in each cell of an ROI (blue grid) resulting in an 8 dimensional vector; Right: A global max-pooling layer simply takes the maximum yielding a 2 dimensional vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>et al. (2014) [3] improved upon Felzenszwalb et al.'s Deformable Part-based Model (DPM)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance of different methods on the test set of our People-Art dataset: the best performance is achieved using a CNN (Fast R-CNN) fine-tuned on People-Art</figDesc><table><row><cell></cell><cell></cell><cell>datasets</cell><cell></cell></row><row><cell>method</cell><cell>pre-train</cell><cell>fine tuning</cell><cell>average precision</cell></row><row><cell cols="3">Fast R-CNN (CaffeNet) ImageNet People-Art (train+val)</cell><cell>46%</cell></row><row><cell cols="3">Fast R-CNN (VGG1024) ImageNet People-Art (train+val)</cell><cell>51%</cell></row><row><cell>Fast R-CNN (VGG16)</cell><cell cols="2">ImageNet People-Art (train+val)</cell><cell>59%</cell></row><row><cell cols="2">Fast R-CNN (CaffeNet) ImageNet</cell><cell>VOC 2007</cell><cell>36%</cell></row><row><cell cols="2">Fast R-CNN (VGG1024) ImageNet</cell><cell>VOC 2007</cell><cell>36%</cell></row><row><cell>Fast R-CNN (VGG16)</cell><cell>ImageNet</cell><cell>VOC 2007</cell><cell>43%</cell></row><row><cell>DPM [16]</cell><cell>People-Art</cell><cell>N/A</cell><cell>33%</cell></row><row><cell>YOLO [28]</cell><cell>ImageNet</cell><cell>VOC 2010</cell><cell>45%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance of different methods on the Picasso dataset</figDesc><table><row><cell>method</cell><cell cols="3">training fine tuning average precision</cell></row><row><cell cols="3">Fast R-CNN (CaffeNet) ImageNet People-Art</cell><cell>45%</cell></row><row><cell cols="3">Fast R-CNN (VGG1024) ImageNet People-Art</cell><cell>44%</cell></row><row><cell>Fast R-CNN (VGG16)</cell><cell cols="2">ImageNet People-Art</cell><cell>44%</cell></row><row><cell cols="2">Fast R-CNN (CaffeNet) ImageNet</cell><cell>VOC 2007</cell><cell>29%</cell></row><row><cell cols="2">Fast R-CNN (VGG1024) ImageNet</cell><cell>VOC 2007</cell><cell>37%</cell></row><row><cell>Fast R-CNN (VGG16)</cell><cell>ImageNet</cell><cell>VOC 2007</cell><cell>33%</cell></row><row><cell>DPM</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Replacing the ROI pooling layer (default) with a single cell max-pooling layer yields a performance drop greater than not fine tuning People-Art</figDesc><table><row><cell>Fine-Tuning</cell><cell cols="2">People-Art</cell><cell>VOC 2007</cell></row><row><cell cols="3">ROI Pooling default single cell</cell><cell>default</cell></row><row><cell>CaffeNet</cell><cell>46%</cell><cell>34%</cell><cell>36%</cell></row><row><cell>VGG1024</cell><cell>51%</cell><cell>35%</cell><cell>36%</cell></row><row><cell>VGG16</cell><cell>59%</cell><cell>40%</cell><cell>43%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/BathVisArtData/PeopleArt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was funded in part by EPSRC grant reference EP/K015966/1. This research made use of the Balena High Performance Computing Service at the University of Bath.</p><p>Detecting People in Artwork with CNNs</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-depiction problem: Recognition and synthesis of photographs and artwork</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Corradi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="103" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning graphs to model visual objects across different depictive styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="313" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting people in cubist art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In: Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature extraction and recognition of handwritten characters by homogeneous layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Giebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zeichenerkennung durch biologische und technische Systeme/Pattern Recognition in Biological and Technical Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1971" />
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading handwritten digits: A zip code recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Pednault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Satterfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Stenard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional neural network hand tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="901" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural network-based face detection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="250" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>IEE Proceedings-Vision</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object-class segmentation using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DAGM Workshop on New Challenges in Neural Computation</title>
		<meeting>the DAGM Workshop on New Challenges in Neural Computation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="58" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1458" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06981</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">R-cnn minus r. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast multiresolution image querying</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 22nd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A search engine for 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A performance evaluation of gradient field hog descriptor for sketch based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="790" to="806" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.03504</idno>
		<title level="m">Sketch-based 3d shape retrieval using convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face painting: querying art with photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient retrieval of deformable shape classes using local self-similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
	<note>IEEE 12th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning graphs to match</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Workshop at the European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
	<note>search of art</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Art and representation: New principles in the analysis of pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willats</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ofs the 27th International Conference on Machine Learning (ICML-10</title>
		<meeting>s the 27th International Conference on Machine Learning (ICML-10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge 2007 (voc2007) development kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>University of Leeds</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Structure is a visual class invariant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning invariant structure for object identification by using graph methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi-Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1023" to="1031" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
