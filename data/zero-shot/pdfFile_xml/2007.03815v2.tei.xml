<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Semantic Segmentation with Fast Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
						</author>
						<title level="a" type="main">Real-time Semantic Segmentation with Fast Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In deep CNN based models for semantic segmentation, high accuracy relies on rich spatial context (large receptive fields) and fine spatial details (high resolution), both of which incur high computational costs. In this paper, we propose a novel architecture that addresses both challenges and achieves state-of-the-art performance for semantic segmentation of high-resolution images and videos in real-time. The proposed architecture relies on our fast spatial attention, which is a simple yet efficient modification of the popular self-attention mechanism and captures the same rich spatial context at a small fraction of the computational cost, by changing the order of operations. Moreover, to efficiently process high-resolution input, we apply an additional spatial reduction to intermediate feature stages of the network with minimal loss in accuracy thanks to the use of the fast attention module to fuse features. We validate our method with a series of experiments, and show that results on multiple datasets demonstrate superior performance with better accuracy and speed compared to existing approaches for real-time semantic segmentation. On Cityscapes, our network achieves 74.4% mIoU at 72 FPS and 75.5% mIoU at 58 FPS on a single Titan X GPU, which is ?50% faster than the state-of-the-art while retaining the same accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic segmentation is a fundamental task in robotic sensing and computer vision, aiming to predict dense semantic labels for given images <ref type="bibr">[?]</ref>, <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. With the ability to extract scene contexts such as category, location, and shape of objects and stuff (everything else), semantic segmentation can be widely applied to many important applications like robots <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref> and autonomous driving <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. For many of these applications, efficiency is critical, especially in realtime (?30FPS) scenarios. To achieve high accuracy semantic segmentation, previous methods rely on features enhanced with rich contextual cues <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[17]</ref> and high-resolution spatial details <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, rich contextual cues are typically captured via very deep networks with sizable receptive fields <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref> that require high computational costs; and detailed spatial information demand for inputs of high resolution <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which incur high FLOPs during inference.</p><p>Recent efforts have been made to accelerate models for real-time applications <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b21">[22]</ref>. These efforts can be roughly grouped into two types. The first strategy is to adopt compact and shallow model architectures <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>. However, this approach may decrease the model capacity and limit the size of the receptive field for features, therefore decreasing the model's discriminative ability. Another technique is to restrict the input to be lowresolution <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Though greatly decreasing the computational complexity, low-resolution images may lose important details like object boundaries or small objects. As a result, both types of methods sacrifice effectiveness for speed, limiting their practical applicability.</p><p>In this work, we address these challenges by proposing the Fast Attention Network (FANet) for real-time semantic segmentation. To capture rich spatial contextual information, we introduce an efficient fast attention module. The original self-attention mechanism has been shown to be beneficial for various vision tasks <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> due to its ability to capture non-local context from the input feature maps. However, given c channels, the original self-attention <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> has a computational complexity of O(n 2 c), which is quadratic with respect to the feature's spatial size n = height?width. In the task of semantic segmentation, where high-resolution feature maps are required, this is costly and limits the model's efficiency and applications to real-time scenarios. Instead, in our fast attention module, we replace the Softmax normalization used in self-attention with cosine similarity, thus converting the computation process to a series of matrix multiplication upon which the matrix-multiplication associativity can be applied to reduce the computational complexity to the linear O(nc 2 ), without loss of spatial information. The proposed fast attention is n c times more efficient than the standard self-attention, given n c in semantic segmentation (e.g. n=128?256 and c=512) .</p><p>FANet works by first extracting different stages of feature maps, which are then enhanced by fast attention modules and finally merged from deep to shallow stages in a cascaded way for class label prediction. Moreover, to process highresolution inputs at real-time speed, we apply additional spatial reduction into FANet. Rather than directly down-scaling the input images, which loses spatial details, we opt for down-sampling intermediate feature maps. This strategy not only reduces computations but also enables lower layers to learn to extract features from high-resolution spatial details, enhancing FANet effectiveness. As a result, with very low computational cost, FANet makes use of both rich contextual information and full-resolution spatial details. We conduct extensive experiments to validate our proposed approach, and the results on multiple datasets demonstrate that FANet can achieve the fastest speed with state-of-the-art accuracy when compared to previous approaches for real-time semantic segmentation. Furthermore, in pursuit of better performance for video streams, we generalize the fast attention module to spatial-temporal contexts, and show (in Sec. 4) that this has the same computational cost as the single-frame model and does not increase with the length of the temporal range. This allows us to add rich spatial-temporal context to video semantic segmentation while avoiding an increase in computation.</p><p>In summary, we contribute the following: (1)We introduce the fast attention module for non-local context aggregation for efficient semantic segmentation, and further generalize it to a spatial-temporal version for video semantic segmentation. <ref type="bibr" target="#b1">(2)</ref> We empirically show that applying extra spatial reduction to intermediate feature stages of the network effectively decreases computational costs while enhancing the model rich spatial details. (3) We present a Fast Attention Network for real-time semantic segmentation of images and videos with state-of-the-art accuracy and much higher efficiency over previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Extracting rich context information is key for high-quality semantic segmentation <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>. To this end, dilated convolutions <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> are proposed as an effective tool to enlarge receptive field without shrinking spatial resolution <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>. DeepLab <ref type="bibr" target="#b13">[14]</ref> and PSPNet <ref type="bibr" target="#b12">[13]</ref> capture multi-scale spatial context. The encoder-decoder architecture is an effective way for extracting spatial context. Early works like SegNet <ref type="bibr" target="#b33">[34]</ref> and U-net <ref type="bibr" target="#b34">[35]</ref> adopt the symmetric structures for encoder and decoder. RefineNet <ref type="bibr" target="#b35">[36]</ref> designs the multi-path refinement module to enhance the feature maps from deep to shallow. GCN <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> explicitly refine predictions with large-kernel filters at different stages. Recently, Deeplab-v3+ <ref type="bibr" target="#b38">[39]</ref> integrates dilated convolution and spatial pyramid pooling into an encoder-decoder network to further boost the effectiveness. The self-attention <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> mechanism has been applied in semantic segmentation <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> with the superior ability to capture long-range dependency, which, however, may incur intensive computation. To achieve better efficiency, Zhu et al. <ref type="bibr" target="#b41">[42]</ref> propose to sample sparse anchor pixel locations for saving computation. Huang et al. <ref type="bibr" target="#b42">[43]</ref> only consider the pixels on the same column and row. Although these methods reduce computation, they all take an approximation of the self-attention model and only partially collect the spatial information. In contrast, our fast attention does not only greatly save computation, but also capture full information from the feature map without loss of spatial information.</p><p>We also notice that there are several works on bilinear feature pooling <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> that are related to our fast attention. Yet, our work differentiates from them in three aspects. (1) <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> approximate the affinity between pixels, while our fast attention is derived in a strictly equivalent form to built accurate affinity. (2) Unlike <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> that focus on recognition tasks, our fast attention effectively tackles the dense semantic segmentation task. (3) As we will show later, in contrast to <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, our fast attention allows for very efficient feature reuse in the video scenario, which can benefit video semantic segmentation with extra temporal context without increasing computation.</p><p>Existing methods for tackling video semantic segmentation can be grouped into two types. The first one <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b52">[53]</ref> takes advantage of the redundant information in video frames, and reduce computation by reusing the high-level feature computed at keyframes. These methods run very efficiently, while always struggling with the spatial misalignment between frames, which leads to a decreased accuracy. Differently, the other type of methods ignore the redundancy and focus on capturing the temporal context information from neighboring frames for better effectiveness <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b55">[56]</ref>, which, however, incurs extra computation to sharply decrease the efficiency. In contrast to these methods, our FANet can be easily extended to also aggregate temporal context and allow for efficient feature reuse, achieving both high effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FAST ATTENTION NETWORK</head><p>In this section, we describe the Fast Attention Network (FANet) for real-time image semantic segmentation. We start by presenting the fast attention module and analyzing its computational advantages over original self-attention. Then we introduce the architecture of FANet. Last, we show that extra spatial reduction at intermediate feature stages of the model enables us to extract rich spatial details from highresolution inputs while keeping a low computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fast Attention Module</head><p>The self-attention module <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> aims to capture nonlocal contextual information for each pixel location as a weighted sum of features at all positions in the feature map. Given a flattened input feature map X ? R n?c where c is the channel size and n = height ? width is the spatial size, the self-attention model <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> applies 1?1 convolutions to encode the feature maps into a Value map V ? R n?c that contains the semantic information of each pixel, a Query map Q ? R n?c together with a Key map K ? R n?c that are used to build correlations between pixel positions. Then the self-attention is calculated as</p><formula xml:id="formula_0">Y = f (Q, K) ? V where f (?, ?) : R n?c ? R n?c ?</formula><p>? R n?n is the Affinity operation modeling the pairwise relations between all spatial locations. The Softmax function is typically used to model affinity f (?, ?), resulting in the popular self-attention response <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>,</p><formula xml:id="formula_1">Y = Sof tmax(Q ? K ) ? V<label>(1)</label></formula><p>Due to the existence of the normalization term in the Softmax function, the computation of Eq. (1) needs first compute the inner matrix multiplication Q ? K , and then the one outside. This results in a computational complexity of O(n 2 c). In semantic segmentation, feature maps have high spatial resolution. As complexity is quadratic with respect to the spatial size, this incurs high computational and memory costs, thus limiting the applications to scenarios especially those requiring real-time speed.  We tackle this challenge by first removing the Softmax affinity. As indicated in <ref type="bibr" target="#b24">[25]</ref>, there are a number of other affinity functions possible that can be used instead. One example, the dot product affinity can be computed simply as: f (Q, K) = Q ? K . However, directly adopting the dot product may lead to affinity with unbounded values, and can be arbitrarily large. To avoid this, we instead use normalized cosine similarity for affinity computation,</p><formula xml:id="formula_2">Y = 1 n (Q ?K ) ? V<label>(2)</label></formula><p>whereQ andK are the results of Q and K after L2normalization along the channel dimension. Unlike Eq. 1, we observe that Eq. 2 can be represented as a series of matrix multiplications, which means that we can apply standard matrix-multiplication associativity to change the order of computation to achieve our fast attention as follows,</p><formula xml:id="formula_3">Y = 1 nQ ? (K ? V )<label>(3)</label></formula><p>where n = height ? width is the spatial size, andK ? V is computed first. Without loss of generality, this fast attention module can be computed with a computational complexity of O(nc 2 ), which is only about c n of the computational requirement of Eq. 1 (note that n is typically much larger than c in semantic segmentation). An illustration of fast attention module is shown in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>. We noticed that channel attention (CA) <ref type="bibr" target="#b15">[16]</ref> has similar computation to our FA, yet CA only aggregates feature along the channel dimension for each pixel, while our Fast Attention aggregates contextual information over the spatial domain thus being more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>We describe our architecture for image semantic segmentation <ref type="figure" target="#fig_1">Fig 1 (a)</ref>. The network is an encoder-decoder architecture with three components: encoder (left), context aggregation (middle), and decoder (right). We use a lightweight backbone (ResNet18 <ref type="bibr" target="#b56">[57]</ref> without last fully connected layers) as the encoder to extract features from the input at different semantic levels. Given an input with resolution h ? w, the first res-block ("Res-1") in the encoder produces feature maps of h 4 ? w 4 resolution. The other blocks sequentially output feature maps with resolution downsampled by a factor of 2. Our network applies the fast attention modules at each stage. As shown in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>, the fast attention module is composed of three 1 ? 1 convolutional layers for embedding the input features to be Query, Key, and Value maps respectively. When generating the Query and Key, we remove the ReLU layer to allow for a wider range of correlation between pixels. The L2-normalization along the channel dimension makes sure the affinity is between -1 to +1. After the feature pyramid is processed by the fast attention modules, the decoder gradually merges and upsamples the features in a sequential fashion from deep feature maps to shallow ones. To enhance the decoded features with a high-level context, we further connect the middle features via a skip connection. An output with h 4 ? w 4 resolution is predicted based on the enhanced feature output by the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extra Spatial Reduction for Real-time Speed</head><p>Being able to generate semantic segmentation for high resolution inputs efficiently is challenging. Typically, highresolution inputs provide rich spatial details that help achieve better accuracy, but dramatically reduce efficiency <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>. On the other hand, using smaller input resolution saves computational costs, but generates worse results due to the loss of spatial details <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>To alleviate this, we adopt a simple yet effective strategy, which is to apply additional down-sample operations to the intermediate feature stages of the network rather than directly down-sampling the input images. We conduct an additional experiment where we use different types of spatial reduction operations, such as pooling and strided convolution at different feature stages, and evaluate how this impacts the resulting quality and speed trade-off. When applying an extra spatial reduction operator to our model, a similar upsampling operation is added to the same stage of the decoder to keep the output resolution. We select the best choice of these, which we show in Section IV-C, not only reduces computation for upper layers, but also allows lower layers to learn to extract rich spatial details from high-resolution inputs and enhance performance. Thus allowing for both realtime efficiency and effectiveness with full-resolution input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extending to Video Semantic Segmentation</head><p>In many real-world applications of semantic segmentation, such as self-driving and robotics, video streams are the natural input for vision systems to understand the physical world. Nevertheless, most existing approaches for semantic segmentation focus on processing static images, and pay less attention to video data. In addition to spatial context from individual frames, video sequences also contain important temporal context derived from dynamics in the camera and scene. To take advantage of such temporal context for better accuracy, in this section we extend our fast attention module to spatial-temporal contexts, and show that it improves video semantic segmentation without increasing computational costs.</p><p>Given {Q T , K T , V T } extracted from the target frame T ,</p><formula xml:id="formula_4">and {Q T ?i , K T ?i , V T ?i } with i ? {1, 2, .</formula><p>., t ? 1} from the previous t ? 1 frames respectively, the spatial-temporal context within such a t-frame window can be aggregated via the traditional self-attention <ref type="bibr" target="#b24">[25]</ref> as,</p><formula xml:id="formula_5">Y T = t?1 i=0 f (Q T , K T ?i ) ? V T ?i .<label>(4)</label></formula><p>This has a computational complexity of O(tn 2 c), t times higher than the single-frame spatial attention in Eq. 1. By replacing the original self-attention with our fast attention, the spatial-temporal context for the target frame T can be computed as</p><formula xml:id="formula_6">Y T = t?1 i=0 1 nQ T ? (K T ?i ? V T ?i ) (5) = 1 nQ T ? (K T ? V T + t?1 i=1K T ?i ? V T ?i )<label>(6)</label></formula><p>where n is the spatial size,Q andK indicate the L2normalized Q and K respectively. At time step T , the results forK T ?i ? V T ?i with i ? {1, 2, ..., t ? 1} have already been computed and simply can be reused. We can see in Eq. 6, that we only need to compute and store the termK T ? V T , add it to those of the previous frames' (this matrix addition's cost is negligible), and multiply it byQ T . Therefore, given a t-frame window our spatial-temporal fast attention has a computational complexity of O nc 2 , which is as efficient as the single-frame fast attention, and free of t. Therefore, our fast attention is able to aggregate spatial-temporal context without increasing computational cost. An illustration of the spatial-temporal FA is shown in <ref type="figure">Fig. 2</ref>. By replacing the fast attention modules with this spatial-temporal version, FANet is able to sequentially segment video frames with feature enhanced with spatial-temporal context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets and Evaluation</head><p>Cityscapes <ref type="bibr" target="#b9">[10]</ref> is a large benchmark containing 19 semantic classes for urban scene understanding with 2975/500/1525  </p><formula xml:id="formula_7">K V L2 K V L2 Q L2 Frame T Frame T-1 X Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We use ResNet-18/34 <ref type="bibr" target="#b56">[57]</ref> pretrained on Imagenet as the encoder in FANet, and randomly initialize parameters in fast attention modules as well as the decoder network. We train using mini-batch stochastic gradient descent (SGD) with batchsize 16, weight decay 5e ?4 , and momentum 0.9. The learning rate is initialized as 1e ?2 , and multiplied with (1 ? iter max iter ) 0.9 after each iteration. We apply data augmentation including random horizontal flipping, random scaling (from 0.75 to 2), random cropping and color jittering in the training process. During testing, we input images at full resolution, and resize the output to the original size for calculating the accuracy. All the evaluation experiments are conducted with batchsize 1 on a single Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Method Analysis</head><p>Fast Attention. We first show the advantage in efficiency due to our fast attention. In <ref type="table" target="#tab_1">Table I</ref>, we compare GFLOPs between a single original self-attention module and our fast attention module. Note that our fast attention runs significantly more efficiently for different size input features with more than 94% less computation.   We also compare our fast attention to the original selfattention module <ref type="bibr" target="#b25">[26]</ref> in our FANet. As shown in <ref type="table" target="#tab_1">Table III</ref>, compared to the model without attention (denoted as "w/o Att."), applying the original self-attention module to the network increases mIoU by 2.4% while decreasing the speed from 83 fps to 8 fps. In contrast to the original self-attention module, our fast attention (denoted as "FA with L2-norm") can achieve only slightly worse quality performance while greatly saving the computation cost. To further analyze our cosine-similarity based fast attention, we also train without the L2-normalization for both Query and Key features (denoted as "FA w/o L2-norm") and achieve 74.1% mIoU on the Cityscapes val, which is lower than 75.0% mIoU of our full model. This validates the necessity of cosine similarity to ensure bounded values for affinity computation.</p><p>In <ref type="table" target="#tab_1">Table II</ref>, we analyze the influence of channel numbers for Key and Query maps in our fast attention module. As we can see, too few channels such as c'=8 or c'=16 saves computation, but limits the representing capacity of the feature and leading to lower accuracy. On the other hand, when increasing the channel number from 32 to 128, the accuracy becomes stable, yet the speed drops. As a result, we adopt c'=32 in our experiments. Spatial Reduction Next, we analyze the effect of applying the extra spatial reduction at different feature stages of FANet. The effects of additionally down-sampling different blocks are presented in <ref type="figure" target="#fig_3">Fig. 3</ref>. As we can see, down-sampling before "Conv-0" (down-scaling the input image), reduces the computation of all the subsequent layers, but loses critical spatial details which reduces the result quality. "Res-1" indicates that we reduce the spatial size at the stage of the first Res-block in FANet. Extra spatial reduction at higher stages like "Res-2", "Res-3", and "Res-4" do not increase speed significantly. Interestingly enough, we observe that applying down-sampling to "Res-4" actually performs better than "None", no additional downsampling. We hypothesize that this may be because that the block "Res-4" process high-level features, and adding extra down-sampling helps to enlarge the receptive field thus benefiting with rich contextual information. Based on these observations and with an aim of real-time semantic segmentation, we choose to apply extra down-sampling to "Res-1" and denote the model as FANet-18/34 based on the ResNet encoder used.</p><p>In additional to doubling the stride of convolutional layers None</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-4</head><p>Res-3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-2</head><p>Res-1 None</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-4</head><p>Res-3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-2</head><p>Res-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-0</head><p>Real-time to achieve 75.0% mIoU, we also experiment with other forms of down-sampling including average pooling (72.9% mIoU) and max pooling (74.2% mIoU). Enlarging stride for Conv layers performs the best. This may be because that stride convolution helps to capture more spatial details while keeping sizable receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Image Semantic Segmentation</head><p>We compare our final method to the recent state-of-the-art efficient approaches for real-time semantic segmentation. For fair comparisons, we evaluate the speed for different methods with PyTorch on the same Titan X GPU. Please check our supplementary material for details. On benchmarks including Cityscapes <ref type="bibr" target="#b9">[10]</ref>, CamVid <ref type="bibr" target="#b57">[58]</ref>, and COCO-Stuff <ref type="bibr" target="#b1">[2]</ref>, our FANet achieves accuracy comparable to the state-of-the-art with the highest efficiency. Cityscapes. In <ref type="table" target="#tab_1">Table IV</ref>, we present the speed-accuracy comparison. FANet-34 achieves mIoU 76.3% for validation and 75.5% for testing at a speed of 58 fps with full-resolution (1024?2048) inputs. To our best knowledge, FANet-34 outperforms existing approaches for real-time semantic segmentation with better speed and state-of-the-art accuracy. By adopting a lighter-weight encoder ResNet-18, our FANet-18 further accelerates the speed to 72 fps, which is nearly two times faster than the recent methods like ShelfNet <ref type="bibr" target="#b59">[60]</ref> and SwiftNet <ref type="bibr" target="#b18">[19]</ref>. Although the accuracy drops to mIoU 75.0% for validation and 74.4% for testing, it is still much better than many previous methods like SegNet <ref type="bibr" target="#b33">[34]</ref> and ICNet <ref type="bibr" target="#b17">[18]</ref>, and comparable to the most recent methods like BiseNet <ref type="bibr" target="#b20">[21]</ref> and ShelfNet <ref type="bibr" target="#b59">[60]</ref>. The performance achieved by our models demonstrates the superior ability to better balance the accuracy and speed for real-time semantic segmentation. Some visual results of our method are shown in <ref type="figure">Fig. 4</ref>. CamVid. Results for this dataset are reported in <ref type="table" target="#tab_6">Table V</ref>. As we can see, our FANet outperforms previous methods with better accuracy and much faster speed. Comparing to BiseNet <ref type="bibr" target="#b20">[21]</ref>, our FANet-18 runs 2? efficient, and our FANet-34 outperforms with 1.4% mIoU and a faster speed. COCO-Stuff. To be consistent with previous methods <ref type="bibr" target="#b17">[18]</ref>, we evaluate at resolution 640?640 for segmenting the 182 categories. As shown in   understanding task with this dataset, our FANet is also able to achieve satisfying accuracy with much faster speed than previous methods. Compared to the state-of-the-art realtime model ICNet <ref type="bibr" target="#b17">[18]</ref>, our FANet-34 achieves both better accuracy and speed, and FANet-18 can further accelerate the speed with a comparable mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Video Semantic Segmentation</head><p>In this part, we evaluate our method for video semantic segmentation on the challenging dataset Cityscapes <ref type="bibr" target="#b9">[10]</ref>. Without significantly increasing the computational cost, our method can effectively capture both spatial and temporal contextual information to achieve better accuracy, and outperforms previous methods with much lower latency. In <ref type="table" target="#tab_1">Table VI</ref>, we compare our method with recent state-of-theart approaches for video semantic segmentation. Compared to the image segmentation baseline models FANet18 and FANet34, both our spatial-temporal version FANet18+Temp and FANet34+Temp help to improve the accuracy at the  indicates FANet with spatial-temporal attention (t=2). Avg RT is the average per-frame running time, and MaxLatency is the maximum per-frame running time.</p><p>same computational costs. We also see that most of the existing methods fail to achieve real-time speed (? 30fps), apart from DVSNet which has much lower accuracy than ours. Methods like Clockwork <ref type="bibr" target="#b48">[49]</ref> and DFF <ref type="bibr" target="#b49">[50]</ref> save the overall computation while suffering from high latency due to the heavy computation at keyframes. PEARL <ref type="bibr" target="#b54">[55]</ref> and Networp <ref type="bibr" target="#b53">[54]</ref> achieves state-of-the-art accuracy at the cost of very low speed and high latency. In contrast, FANet18+Temp and FANet34+Temp achieve state-of-the-art accuracy with a much faster speed. FANet18+Temp achieves more than 200? better efficiency than Netwarp <ref type="bibr" target="#b53">[54]</ref>. FANet34+Temp outperforms PEARL <ref type="bibr" target="#b54">[55]</ref> with 40? faster speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have proposed a novel Fast Attention Network for realtime semantic segmentation. In the network, we introduce fast attention to efficiently capture contextual information from feature maps. We further extend the fast attention to spatial-temporal context, and apply our models to achieve low-latency video semantic segmentation. To ensure highresolution input with high efficiency, we also propose to apply spatial reduction to the intermediate feature stages. As a result, our model is enhanced with both rich contextual information and high-resolution details, while keeping a real-time speed. Extensive experiments on multiple datasets demonstrate the efficiency and effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Architecture of Fast Attention Network (FANet). (b) Structure of Fast Attention (FA). (c) Structure of "FuseUp" module. Distinct from channel attention(CA) which only aggregates feature along the channel dimension for each pixel independently, our Fast Attention aggregates contextual information over the spatial domain thus achieving better effectiveness</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Accuracy and speed analysis on Cityscapes val for adding an additional down-sampling operation (rate=2) to different stages of the encoder in FANet. "Conv-0" means to directly down-sample the input image. "Res-n" indicates double the stride of the first Conv layer in the n-th Res-block. "None" means no additional downsampling operation is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2: Visualization of our fast attention for spatial-temporal context aggregation (t=2). The red arrows indicate the feature stored and reused by future frames.</figDesc><table><row><cell>C=</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell></row><row><cell>Self-Att.[26]</cell><cell>68</cell><cell>103</cell><cell>173</cell><cell>313</cell><cell>602</cell><cell>1203</cell></row><row><cell>Ours</cell><cell>0.2</cell><cell>0.6</cell><cell>1.7</cell><cell>5</cell><cell>19</cell><cell>73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>GFLOPs for non-local module<ref type="bibr" target="#b24">[25]</ref> and our fast attention module with C?128?256 features as input.</figDesc><table><row><cell>This</cell></row><row><cell>dataset has 9,000 densely annotated images for training and</cell></row><row><cell>1,000 for testing. Following previous work[18], we adopt the</cell></row><row><cell>resolution 640?640 and evaluate on 182 classes including 91</cell></row><row><cell>for things and 91 for stuff. We evaluate our method on image</cell></row><row><cell>semantic segmentation for all four datasets, and additionally</cell></row><row><cell>evaluate on Cityscapes for video semantic segmentation.</cell></row><row><cell>The mIoU (mean Intersection over Union) is reported for</cell></row><row><cell>evaluation.</cell></row></table><note>scenes for train/validation/test respectively. CamVid[58] is another street-view dataset with 11 classes. The an- notated frames are divided into 367/101/233 for train- ing/validation/testing. COCO-Stuff [2] contains both diverse indoor and outdoor scenes for semantic segmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Performance on Cityscapes val for different channel numbers (c') in fast attention in FANet-18.</figDesc><table><row><cell></cell><cell cols="3">mIoU(%) Speed(fps) GFLOPs</cell></row><row><cell>w/o Att.</cell><cell>72.7</cell><cell>83</cell><cell>48</cell></row><row><cell>Self-Att.[26]</cell><cell>75.1</cell><cell>8</cell><cell>121</cell></row><row><cell>Channel-Att.[16]</cell><cell>74.6</cell><cell>70</cell><cell>51</cell></row><row><cell>FA w/o L2-norm</cell><cell>74.1</cell><cell>72</cell><cell>49</cell></row><row><cell>FA with L2-norm</cell><cell>75.0</cell><cell>72</cell><cell>49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance on Cityscapes for different attention mechanisms for FANet-18. "FA" denotes our fast attention.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V</head><label>V</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>mIoU(%) val</cell><cell>test</cell><cell>Speed(fps)</cell><cell>GFLOPs</cell><cell>GFLOPs@1Mpx</cell><cell>Input Resolution</cell></row><row><cell>SegNet [34]</cell><cell>-</cell><cell>56.1</cell><cell>36</cell><cell>143</cell><cell>650</cell><cell>360?640</cell></row><row><cell>ICNet [18]</cell><cell>67.7</cell><cell>69.5</cell><cell>38</cell><cell>30</cell><cell>15</cell><cell>1024?2048</cell></row><row><cell>ERFNet [59]</cell><cell>71.5</cell><cell>69.7</cell><cell>48</cell><cell>103</cell><cell>206</cell><cell>512?1024</cell></row><row><cell>BiseNet [21]</cell><cell>74.8</cell><cell>74.7</cell><cell>47</cell><cell>67</cell><cell>59.5</cell><cell>768?1536</cell></row><row><cell>ShelfNet [60]</cell><cell>-</cell><cell>74.8</cell><cell>39</cell><cell>95</cell><cell>47.5</cell><cell>1024?2048</cell></row><row><cell>SwiftNet [19]</cell><cell>75.4</cell><cell>75.5</cell><cell>40</cell><cell>106</cell><cell>53</cell><cell>1024?2048</cell></row><row><cell>FANet-34</cell><cell>76.3</cell><cell>75.5</cell><cell>58</cell><cell>65</cell><cell>32.5</cell><cell>1024?2048</cell></row><row><cell>FANet-18</cell><cell>75.0</cell><cell>74.4</cell><cell>72</cell><cell>49</cell><cell>24.5</cell><cell>1024?2048</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, for the general scene</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Image semantic segmentation performance comparison with recent state-of-the-art real-time methods on Cityscapes dataset."GFLOPs@1Mpx" shows the GFLOPs for input with resolution 1M pixels.</figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FANet-18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FANet-34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Fig. 4: Image semantic segmentation results on Cityscapes.</cell></row><row><cell>Method</cell><cell cols="2">mIoU Speed (%) (fps)</cell><cell>Method</cell><cell cols="2">mIoU Speed (%) (fps)</cell></row><row><cell>SegNet [34]</cell><cell>55.6</cell><cell>12</cell><cell>FCN [27]</cell><cell>22.7</cell><cell>9</cell></row><row><cell>ENet [24]</cell><cell>51.3</cell><cell>46</cell><cell>DeepLab [14]</cell><cell>26.9</cell><cell>14</cell></row><row><cell>ICNet [18]</cell><cell>67.1</cell><cell>82</cell><cell>ICNet [18]</cell><cell>29.1</cell><cell>110</cell></row><row><cell>BiseNet [21]</cell><cell>68.7</cell><cell>75</cell><cell>BiseNet [21]</cell><cell>25.6</cell><cell>113</cell></row><row><cell>FANet-34</cell><cell>70.1</cell><cell>121</cell><cell>FANet-34</cell><cell>29.5</cell><cell>142</cell></row><row><cell>FANet-18</cell><cell>69.0</cell><cell>154</cell><cell>FANet-18</cell><cell>27.8</cell><cell>191</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Image semantic segmentation performance on Camvid</figDesc><table /><note>(left) and COCO-Stuff (right).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Video semantic segmentation on Cityscapes. "+Temp"</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Ping Hu, Kate Saenko, and Stan Sclaroff are with Department of Computer Science, Boston University. 1 Fabian Caba Heilbron, Oliver Wang, Zhe Lin, and Federico Perazzi are with Adobe Research.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic segmentation from limited training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Erskine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grinover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised CNN segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Real-time joint semantic segmentation and depth estimation using asymmetric annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<editor>ICRA.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Image segmentation using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic mapping for mobile robotics tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostavelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term visual localization using semantically segmented images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Joint learning of instance and semantic segmentation for robotic pick-and-place with heavy occlusions in clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<editor>ICRA.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated process for incorporating drivable path into real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zyner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nebot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep semantic lane segmentation for mapless driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Salscheider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Orzechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dfnet: Semantic segmentation on panoramic images with dynamic loss weights and residual fusion block</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<editor>ICRA.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">In defense of pretrained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient segmentation: Learning downsampling near semantic boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic downsampling for cost-adjustable inference and improved regularization in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qun Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Seeing behind things: Extending semantic segmentation to occluded regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Purkait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<editor>IROS.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MICCAI.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Expectationmaximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A?2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Accel: A corrective fusion network for efficient semantic segmentation on video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic video segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient ladder-style densenets for semantic segmentation of large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on ITS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Semantic video cnns through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE IVS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Shelfnet for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
