<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Dual Consecutive Network for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
							<email>liuzhenguang2008@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Chen</surname></persName>
							<email>chenhaomingbob@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyang</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
							<email>wushuang@outlook.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
							<email>xwang@zjgsu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Dual Consecutive Network for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-frame human pose estimation in complicated situations is challenging. Although state-of-the-art human joints detectors have demonstrated remarkable results for static images, their performances come short when we apply these models to video sequences. Prevalent shortcomings include the failure to handle motion blur, video defocus, or pose occlusions, arising from the inability in capturing the temporal dependency among video frames. On the other hand, directly employing conventional recurrent neural networks incurs empirical difficulties in modeling spatial contexts, especially for dealing with pose occlusions. In this paper, we propose a novel multi-frame human pose estimation framework, leveraging abundant temporal cues between video frames to facilitate keypoint detection. Three modular components are designed in our framework. A Pose Temporal Merger encodes keypoint spatiotemporal context to generate effective searching scopes while a Pose Residual Fusion module computes weighted pose residuals in dual directions. These are then processed via our Pose Correction Network for efficient refining of pose estimations. Our method ranks No.1 in the Multi-frame Person Pose Estimation Challenge on the large-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have released our code, hoping to inspire future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is a fundamental problem in computer vision, which aims at locating anatomical key-* Corresponding Authors points (e.g., wrist, ankle, etc.) or body parts. It has enormous applications in diverse domains such as security, violence detection, crowd riot scene identification, human behavior understanding, and action recognition <ref type="bibr" target="#b21">[22]</ref>. Earlier methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b28">29]</ref> adopt the probabilistic graphical model or the pictorial structure model. Recent methods have built upon the success of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, achieving outstanding performance in this task. Unfortunately, most of the recent state-of-the-art methods are designed for static images, with greatly diminished performance when handling video input.</p><p>In this paper, we focus on the problem of multi-person pose estimation in video sequences. Conventional imagebased approaches disregard the temporal dependency and geometric consistency across video frames. Dissevering these additional cues results in failure cases when dealing with challenging situations that inherently occurs in video sequences such as motion blur, video defocus, or pose occlusions. Effectively leveraging the temporal information in video sequences is of great significance to facilitate pose estimation and often plays an indispensable role for detecting heavily occluded or blurry joints.</p><p>A direct and intuitive approach to tackle this issue is to employ recurrent neural networks (RNNs) such as Long-Short Term Memory (LSTM), Gate Recurrent Unit (GRU) or 3DCNNs to model geometric consistency as well as temporal dependency across video frames. <ref type="bibr" target="#b24">[25]</ref> uses convolutional LSTM to capture temporal and spatial cues, and directly predicts the keypoint heatmap sequences for videos. This RNN based approach is more effective when the human subjects are spatially sparse such as single-person scenes with minimal occlusion. However, performance is severely hindered in the case of occlusion commonly occurring in multi-person pose estimation and even selfocclusion in the single-person case. <ref type="bibr" target="#b38">[39]</ref> proposes a 3DHR-Net (extension of HRNet <ref type="bibr" target="#b32">[33]</ref> to include a temporal dimension) for extracting spatial and temporal features across video frames to estimate pose sequences. This model has shown excellent results particularly for adequately long duration single-person sequences. Another line of work considers fine-tuning the primary prediction with high confidence keypoints from the adjacent frames. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28]</ref> propose to compute the dense optical flow between every two frames, and leverage the additional flow based representations to align the predictions. This approach is promising when the optical flow can be computed precisely. However in cases involving motion blur or defocus, the poor image qualities lead to imprecise optical flows which translates to performance drops.</p><p>To address the shortcomings of existing methods, we propose to incorporate consecutive frames from dual temporal directions to improve pose estimation in videos. Our framework, termed Dual Consecutive network for pose estimation (DCPose), first encodes the spatial-temporal key-point context into localized search scopes, computes pose residuals, and subsequently refines the keypoint heatmap estimations. Specifically, we design three task-specific modules within the DCPose pipeline. 1) As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, a Pose Temporal Merger (PTM) network performs keypoints aggregation over a continuous video segment (e.g., three consecutive frames) with group convolution, thereby localizing the search range for the keypoint.</p><p>2) A Pose Residual Fusion (PRF) network is introduced to efficiently obtain the pose residuals between the current frame and adjacent frames. PRF computes inter-frame keypoint offsets by explicitly utilizing the temporal distance.</p><p>3) Finally, a Pose Correction Network (PCN) comprising five parallel convolution layers with different dilation rates is proposed for resampling keypoint heatmaps in the localized search range.</p><p>It is worth mentioning that the architecture of our network extends the successful PoseWarper architecture <ref type="bibr" target="#b2">[3]</ref> in three ways. (1) PoseWarper focuses on enabling effective label propagation between frames, while we aim to refine the pose estimation of current frame using the motion context and temporal information from unlabeled neighboring frames. (2) Information from two directions are utilized and we explicitly consider weighted residuals between frames.</p><p>(3) Instead of applying the learned warping operation to a heatmap from one adjacent frame, the new network fuses together heatmaps from the adjacent frames and the current frame.</p><p>To summarize, our key contributions are: 1) A novel dual consecutive pose estimation framework is proposed. DC-Pose effectively incorporates bidirectional temporal cues across frames to facilitate the multi-person pose estimation task in videos. 2) We design 3 modular networks within DCPose to effectively utilise the temporal context: i) a novel Pose Temporal Merger network for effectively aggregating keypoint across frames and identifying a search scope, ii) a Pose Residual Fusion network to efficiently compute weighted pose residuals across frames, and iii) a Pose Correction Network that updates the pose estimation with the refined search scope and pose residual information. 3) Our method achieves state-of-the-art results on PoseTrack2017 and PoseTrack2018 Multi-frame Person Pose Estimation Challenge. To facilitate future research, our source code is released at https://github.com/ Pose-Group/DCPose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Imaged Based Multi Person Pose Estimation</head><p>Earlier image-based human pose estimation works generally fall within a pictorial structure model paradigm, in which the human body is represented as a tree-structured model <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b28">29]</ref> or a forest model <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10]</ref>. Despite allowing for efficient inference, these approaches tend to be insufficient in modeling complex relationships between body parts, and this weakness is accentuated when temporal information enters the picture. Recently, neural networks based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">50]</ref> have been in the spotlight due to their superior performance in various fields. One line of work <ref type="bibr" target="#b5">[6]</ref> outputs skeletal joints coordinates directly by regressing image features. Another approach <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> utilises probability heatmaps to represent joints locations. Due to the reduced difficulty for optimization, heatmap based pose estimation have since been widely adopted. In general, these methods can be classified into part-based framework (bottom-up) and two-step framework (top-down). The bottom-up approach <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> first detects individual body parts, then assembles these constituent parts into the entire person. <ref type="bibr" target="#b4">[5]</ref> builds a bottom-up pipeline and utilizes part affinity fields to capture pairwise relationships between different body parts. Conversely, the top-down approach <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref> first performs person detection, then proceeds with single-person pose estimation on each individual. <ref type="bibr" target="#b40">[41]</ref> proposes a sequential architecture of convolutional pose machines, which follows a strategy of iteratively refining the output of each network stage. <ref type="bibr" target="#b11">[12]</ref> designs a symmetric spatial transformer network for extracting a high-quality single person region from an inaccurate bounding box. A recent work in <ref type="bibr" target="#b32">[33]</ref> proposes a HRNet that performs multi-scale fusion to retain high resolution feature maps. This improves spatial precision in keypoint heatmaps and achieves the state-of-the-art on several image-based benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Based Multi Person Pose Estimation</head><p>Directly applying the existing image-level methods to video sequences produces unsatisfactory predictions, primarily due to the failure to capture temporal dependency among video frames. Consequently, these models fail to handle motion blur, video defocus, or pose occlusions which are frequently encountered in video inputs. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref> compute the dense optical flow between every consecutive frames with the flow representations providing additional cues for aligning predictions. However, motion blur, defocus or occlusion occurrences hinder optical flow computation and affect performance. <ref type="bibr" target="#b24">[25]</ref> replaces the convolutional pose machines in <ref type="bibr" target="#b40">[41]</ref> with convolutional LSTMs for modeling temporal information in addition to spatial contexts. A principal shortcoming of such an approach is being severely impacted by occlusion. <ref type="bibr" target="#b2">[3]</ref> proposes to learn an effective video pose detector from sparsely labeled videos through a warping mechanism and has turned out to be very successful, dominating the PoseTrack leaderboard for a long time. <ref type="bibr" target="#b38">[39]</ref> extends HRNet <ref type="bibr" target="#b32">[33]</ref> with temporal convolutions and proposes 3DHRNet, which is successful in handling pose estimation and tracking jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>The pipeline of our proposed DCPose is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. To improve keypoint detection for the current frame F c , we make use of additional temporal information from a previous frame F p and a future frame F n . F p and F n are selected within a frame window [c ? T, c + T ], where p ? [c ? T, c) and n ? (c, c + T ] respectively denote the frame indices. The bounding boxes for individual persons in F c are first obtained by a human detector. Each bounding box is enlarged by 25% and is further used to crop the same person in F p and F n . Individual i in the video will thus be composed of a cropped video segment, which we denote as Clip i (p, c, n). Clip i (p, c, n) is then fed into a backbone network that serves to output preliminary keypoint heatmap estimations h i (p, c, n). The pose heatmaps h i (p, c, n) is then processed in parallel through two modular networks, Pose Temporal Merger (PTM) and Pose Residual Fusion (PRF). PTM outputs ? i (p, c, n), which encodes the spatial aggregation, and PRF computes ? i (p, c, n), which captures pose residuals in two directions. Both feature tensors ? i (p, c, n) and ? i (p, c, n) are then simultaneously fed into our Pose Correction Network (PCN) to refine and improve upon the initial pose estimations. In what follows, we introduce the three key components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose Temporal Merger</head><p>The motivation for our Pose Temporal Merger (PTM) comes from the following observations and heuristics. 1) Although existing pose estimation methods such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b11">12]</ref> suffer from performance deterioration on videos, we observe that their predictions do still provide useful information for approximating the keypoint spatial positions.</p><p>2) Another heuristic is on temporal consistency, i.e., the pose of an individual does not undergo dramatic and abrupt changes across a very few frame intervals (typically 1/60 to 1/25 of a second per frame). Therefore, we design PTM to encode the keypoint spatial contexts based on initial predictions (from a backbone network), providing a compressed search scope that facilitates refinement and correction of pose prediction within a confined range.</p><p>For person i, the backbone network returns initial keypoint heatmaps h i (p), h i (c), h i (n). Naively, we could merge them through a direct summation H i (p, c, n) = h i (p) + h i (c) + h i (n). However, we expect that the additional information that may be extracted from F p and F n is inversely proportional to their temporal distances from current frame F c . We formalize this intuition as:</p><formula xml:id="formula_0">H(p, c, n) = n ? c n ? p h i (p) + h i (c) + c ? p n ? p h i (n). (1)</formula><p>Recall that p, c, n are frame indices. We explicitly assign higher weights to the frames that are temporally nearer to the current frame. Based on the important fact that convolution operations serve to adjust (feature) weights, we utilise convolutional neural networks to practically implement the idea of Eq. 1. However, including all joint channels in the computation for the merged keypoint heatmap of a single joint will result in redundancy. For example, when encoding the spatial context of the left wrist, involving other joints such as the head and ankle at different times will likely not to have any bearing and may even breed confusion. Therefore, for each joint, we only include its own specific temporal information for computing its merged keypoint heatmap. This is implemented via a group convolution. We regroup the keypoint heatmaps h i (p), h i (c), h i (n) according to joint, and stack them to a feature tensor ? i , which can be expressed as:</p><formula xml:id="formula_1">? i (p, c, n) = N j=1 n ? c n ? p h j i (p) ? h j i (c) ? c ? p n ? p h j i (n)</formula><p>(2) where ? denotes the concatenate operation and the superscript j index the j-th joint for a total of N joints. Subsequently, the feature tensor ? i is fed into a stack of 3 ? 3 residual blocks (adapted from the residual steps block in RSN <ref type="bibr" target="#b3">[4]</ref>), producing the merged keypoint heatmaps ? i (p, c, n):</p><formula xml:id="formula_2">? i (p, c, n) stack of 3?3 ???????? residual blocks ? i (p, c, n).<label>(3)</label></formula><p>This group convolution not only eliminates the disturbance of irrelevant joints, but also removes redundancy and shrinks the amount of model parameters required. It is also advantageous to directly summing keypoint heatmaps in Eq. 1 since the group CNN operation allow different weights at the pixel level and benefits learning an end-toend model. Visual results of aggregated keypoint heatmaps following our PTM is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose Residual Fusion</head><p>Parallel to spatial aggregation of keypoint heatmaps in PTM, our Pose Residual Fusion (PRF) branch aims to compute the pose residuals which will serve as additional favorable temporal cues. Given keypoint heatmaps h i (p), h i (c), h i (n), we compute the pose residual features as follows:</p><formula xml:id="formula_3">? i (p, c) = h i (c) ? h i (p) ? i (c, n) = h i (n) ? h i (c) ? i = ? i (p, c) ? ? i (c, n) ? n ? c n ? p ? i (p, c) ? c ? p n ? p ? i (c, n).<label>(4)</label></formula><p>? i concatenates the original pose residuals ? i (p, c), ? i (c, n), and their weighted versions, where the weights are obtained according to the temporal distances. Similar to PTM, ? i is then processed via a stack of 3 ? 3 residual blocks to give the final pose residual feature ? i (p, c, n):</p><formula xml:id="formula_4">? i (p, c, n) stack of 3?3 ???????? residual blocks ? i (p, c, n).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pose Correction Network</head><p>Given the merged keypoint heatmaps ? i (p, c, n) and pose residual feature tensor ? i (p, c, n), our Pose Correction Network is employed to refine the initial keypoint heatmap estimation h i (c), yielding adjusted final keypoint heatmaps. Primarily, the pose residual feature tensor ? i (p, c, n) is used as the input for five parallel 3 ? 3 convolution layers with different dilation rates d ? {3, 6, 9, 12, 15}. This computation gives five groups of offsets for the five kernels of the subsequent deformable convolution layer. Formally, the offsets are computed as:</p><formula xml:id="formula_5">? i (p, c, n) ? ? i (p, c, n) stack of 3?3 ???????? residual blocks dilation rate d ? ???????? ? convolution layers O i,d .<label>(6)</label></formula><p>Different dilation rates correspond to varying the size of the effective receptive field whereby enlarging the dilation rate <ref type="bibr" target="#b44">[45]</ref> increases the scope of the receptive field. A smaller dilation rate focuses on local appearance, which is more sensitive for capturing subtle motion contexts. Conversely, using a large dilation rate allows us to encode global representations and capture relevant information of a larger spatial scope. In addition to the offset computation, we feed the merged keypoint heatmaps to similar convolution layers and obtain five sets of masks M d as:</p><formula xml:id="formula_6">? i (p, c, n) ? ? i (p, c, n) stack of 3?3 ???????? residual blocks dilation rate d ? ???????? ? convolution layers M i,d .<label>(7)</label></formula><p>The parameters of two dilation convolution structures for offset O and mask M computation are independent. A mask M d can be considered as the weight matrix for a convolution kernel.</p><p>We implement the pose correction module through the deformable convolution V 2 network (DCN v2 <ref type="bibr" target="#b48">[49]</ref>) at various dilation rates d. DCN v2 takes the following inputs: 1) the merged keypoint heatmaps ? i (p, c, n), 2) the kernel offsets O i,d , and 3) the masks M i,d , and outputs a pose   heatmap for person i at dilation rate d:</p><formula xml:id="formula_7">(? i (p, c, n), O i,d , M i,d ) dilation rate d ? ?????? ? DCN v2 H i,d (c).<label>(8)</label></formula><p>The five outputs for five dilation rates are summarized and normalized to yield the final pose prediction for person i: d?{3,6,9,12,15}</p><formula xml:id="formula_8">H i,d (c) normalization ? ?????? ? H i (c).<label>(9)</label></formula><p>Ultimately, the above procedure is performed for each individual i. By effectively utilising the additional cues from F p and F n in our DCPose framework, the final pose heatmaps are enhanced and improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Backbone Model Our network is highly adaptable and we can seamlessly integrate any image based pose estimation architecture as our backbone. We employ the state-ofthe-art Deep High Resolution Network (HRNet-W48 <ref type="bibr" target="#b32">[33]</ref>) as our backbone joints detector, since its superior performance for single image pose estimation will be beneficial for our approach.</p><p>Training Our Deep Dual Consecutive Network is implemented in PyTorch. During training, we use the ground truth person bounding boxes to generate the Clip i (p, c, n) for person i as the input sequence to our model. For boundary cases, we apply same padding. In other words, if there are no frames to extend forward and backward from F c , F p or F n will be replaced by F c . We utilize the HRNet-W48 pretrained on the PoseTrack dataset as our backbone, and freeze the backbone parameters throughout training, only backpropagating through the subsequent components in DCPose.</p><p>Loss function We employ the standard pose estimation loss function as our cost function. Training aims to minimize the total Euclidean or L2 distance between prediction and ground truth heatmaps for all joints. The cost function is defined as:</p><formula xml:id="formula_9">L = 1 N * N j=1 v j * ||G(j) ? P (j)|| 2<label>(10)</label></formula><p>Where G(j), P (j) and v j respectively denote the ground truth heatmap, prediction heatmap and visibility for joint j.</p><p>During training, the total number of joints is set to N = 15.</p><p>The ground truth heatmaps are generated via a 2D Gaussian centered at the joint location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present our experimental results on two large-scale benchmark datasets: Posetrack2017 and PoseTrack2018 Multi-frame Person Pose Estimation Challenge datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Datasets PoseTrack is a large-scale public dataset for human pose estimation and articulated tracking in video and includes challenging situations with complicated movement of highly occluded people in crowded environments. The PoseTrack2017 dataset <ref type="bibr" target="#b14">[15]</ref> contains 514 video clips and 16,219 pose annotations, with 250 clips for training, 50 clips for validation, and 214 clips for testing. The Pose-Track2018 dataset greatly increased the number of video clips to a total of 1,138 clips and 153,615 pose annotations. The training, validation, and testing splits consist of 593, 170, and 375 clips respectively. The 30 frames in the center of the training video clips are densely annotated. For the validation clips, annotations are provided every four frames. Both PoseTrack2017 and PoseTrack2018 identify 15 joints, with an additional annotation label for joint visibility. We evaluate our model only for visible joints with the average precision (AP) metric <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Parameter Settings During training, we incorporate data augmentation including random rotations, scaling, truncating, and horizontal flipping to increase variation. Input image size is fixed to 384 ? 288. The default interval between F c and F p or F n is set to 1. Backbone parameters are fixed to the pretrained HRNet-W48 model weights. All subsequent weight parameters are initialized from a Gaussian distribution with ? = 0 and ? = 0.001, while bias parameters are initialized to 0. We employ the Adam optimizer with a base learning rate of 0.0001 that decays by 10% every 4 epochs. We train our model for a batch size of 32 for 20 epochs with 2 Nvidia GeForce Titan X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Approaches</head><p>Results on the PoseTrack2017 Dataset We evaluate our approach on PoseTrack2017 validation set and full test set using the widely adopted average precision (AP) metric <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b46">47]</ref>. <ref type="table" target="#tab_0">Table 1</ref> presents the quantitative results of different approaches in terms of AP on PoseTrack2017 validation set. We benchmark our DCPose model against eight existing methods <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b2">[3]</ref>. In <ref type="table" target="#tab_0">Table 1</ref>, the APs of key joints, such as Head, Shoulder, Knee, and Elbow, are reported, as well as the mAP (mean AP) for all joints.</p><p>Results on the test set are provided in <ref type="table" target="#tab_1">Table 2</ref>. These results are obtained by uploading our prediction results to the PoseTrack evaluation server:https://posetrack. net/leaderboard.php because the annotations for test set are not public. Our DCPose network achieves stateof-the-art results for multi-frame person pose estimation challenge for both the validation and test sets. DCPose consistently outperforms existing methods and achieves a mAP of 79.2. The performance boost for relatively difficult joints is also encouraging: we obtain an mAP of 76.1 for the wrist and an mAP of 71.2 for the ankle. Some sample results are displayed in <ref type="figure" target="#fig_2">Fig. 3</ref>, which are indicative of the effectiveness of our method in complex scenes. More visualized results can be found in https://github.com/Pose-Group/DCPose.</p><p>Results on the PoseTrack2018 Dataset We also evaluate our model on the PoseTrack2018 dataset. The validation and test set AP results are tabulated in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 4</ref>, respectively. As shown in the tables, our approach once again delivers the state-of-the-art results. We achieve an mAP of 79.0 on the test set, and obtain an mAP of 77.2 for the difficult wrist joint and 72.3 for the ankle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Experiments</head><p>Extensive ablation experiments are performed on the PoseTrack2017 dataset to study the effectiveness of various components in our DCPose framework.  <ref type="table">Table 5</ref>. Ablation study of different components in our DCPose performed on PoseTrack2017 validation set. "r/m X" refers to removing X module in our network. The complete DCPose consistently achieves the best results which are highlighted. rection Network, we evaluate their contributions towards the overall performance. We also investigate the efficacy of including information from both temporal directions as well as the impact of modifying the temporal distance, i.e., frame intervals between F c and F p , F n . The results are reported in <ref type="table">Table 5</ref>.</p><p>Pose Temporal Merger For this ablation setting, we remove the PTM and instead obtain the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Correction Network</head><p>We study the effects of adopting different sets of dilation rates for the convolutions in PCN. This corresponds to different effective receptive fields. We experiment with four different dilation settings: d = 3, d ? {3, 6}, d ? {3, 6, 9} and d ? {3, 6, 9, 12} whereas the complete DCPose framework setting has d ? {3, 6, 9, 12, 15}. From the results in <ref type="table">Table 5</ref>, we observe the gradual improvement of the mAP with increasing levels of dilation rates, from 81.7 ? 81.9 ? 82.7 ? 82.6 ? 82.8. This is in line with our intuitions that increasing the depth and scope of the effective receptive fields, i.e., by varying the range of dilation rates, the Pose Correction Network is able to model local and global contexts more efficiently, leading to more accurate estimations of the joint locations.</p><p>Bidirectional Temporal Input In addition, we examine the effects of removing a single temporal direction, i.e., removal of either F p or F n from the input to PTM and PRF. In removing the previous (respectively next) frame F p (respectively F n ), the mAP drops 0.6 (respectively 0.5). This highlights the importance of leveraging dual temporal directions as each direction allows access to useful information that are beneficial in improving pose estimation for videos.</p><p>Time Interval T The time interval T described in Section 3 is a hyper-parameter whose default value is set to 1. In other words, DCPose looks at 3 consecutive frames as the shorter time/frame interval ensures the validity of our assumptions of temporal consistency. We experiment with enlarging the interval between frames, where T is set to 2, 3, 4. Indices p and n are randomly selected with p ? [c ? T, c) and n ? (c, c + T ]. The results reflect a performance drop with an increase in T , whereby the mAP decreases from 82.8 for T = 1 to 82.6, 82.1, 81.7 as T goes to 2, 3, 4 respectively. This is in accordance with our expectations, since frame-wise difference is small for shorter time intervals so that F p and F n can provide more abundant and effective temporal and spatial cues for our PTM and PRF. Conversely, a large time interval would invalidate our assumption of time consistency as the frame differences would be too great and the additional cues would lack efficacy. This ablation experiment explains our choice of modeling con-secutive frames. The continuous frames of a video clip facilitate a precise aggregation of spatiotemporal context cues and improve the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison of Visual Results</head><p>In order to evaluate the capability of our model to adapt to sophisticated scenarios, we illustrate in <ref type="figure" target="#fig_4">Fig. 4</ref> the sideby-side comparisons of our DCPose network against stateof-the-art approaches. Each column depicts different scenario complications including rapid motion, nearby-person, occlusions and video defocus, whereas each row displays the joint detection results from different methods. We compare our a) DCPose against 3 state-of-the-art methods, namely b) SimpleBaseline <ref type="bibr" target="#b42">[43]</ref>, c) HRNet-W48 <ref type="bibr" target="#b32">[33]</ref> and d) PoseWarper <ref type="bibr" target="#b2">[3]</ref>. It is observed that our method yields more robust detection for such challenging cases. SimpleBaseline and HRNet-W48 are trained on static images and fail to capture temporal dependency among video frames, resulting in suboptimal joint detection. On the other hand, PoseWarper leverages the temporal information between video frames to warp the initial pose heatmaps but only employing one adjacent video frame as the auxiliary frame. Our DCPose network makes full use of the temporal context by processing consecutive frames in dual directions. Through our principled design of PTM and PRF modules for better encoding of these information as well as PCN for refining pose detection, our method achieves new state-of-the-art both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a dual consecutive network for multi-frame person pose estimation which significantly outperforms existing state-of-the-art methods on the benchmark datasets. We design a Pose Temporal Merger and a Pose Residual Fusion module that allows abundant auxiliary information to be drawn from the adjacent frames, providing a localized and pose residual corrected search range for location keypoints. Our Pose Correction Network employs multiple effective receptive fields to refine pose estimation in this search range, achieving notable improvements and is able to handle complex scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of our Pose Temporal Merger (PTM) network. (a): Original video sequence in the datasets, and we aim to detect poses in the current frame Fc. (b): Each person in the original video sequence is assembled to a cropped clip and a single-person joints detector gives preliminary estimations of keypoint heatmaps (illustrated for right wrist). (c)-left: Merged keypoint heatmaps for the right wrist, generated by our PTM network through encoding the keypoint spatial contexts. The color intensity encodes spatial aggregation. (c)-right: Zoomed-in view of the merged keypoint heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall pipeline of our DCPose framework. The goal is to locate the keypoint positions for the current frame Fc. First, an individual person i is assembled into an input sequence Clip i (p, c, n), and a HRNet backbone predicts initial keypoint heatmaps hi(p), hi(c), hi(n). Our Pose Temporal Merger (PTM) and Pose Residual Fusion (PRF) networks work concurrently to obtain an effective search scope ?i(p, c, n) and pose residuals ?i(p, c, n), respectively. These are then fed into our Pose Correction Network (PCN) which refines the keypoint estimation for person i in Fc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visual results of our model on the PoseTrack2017 and PoseTrack2018 datasets comprising complex scenes: high-speed movement, occlusion, and multiple persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>merged pose heatmaps ?(p, c, n) as follows: i) h(p) + h(c) + h(n) 3?3 ? ???????? ? convolution layer ?(p, c, n); ii) h(c) ? ? ?(p, c, n). The mAP falls from 82.8 to 82.0 for (i) and 82.2 for (ii). This significant performance degradation upon removal of the PTM module can be attributed to the failure of obtaining an effective search range for the joints, which lead to decreased accuracy in locating the joint in the subsequent pose correction stage. Pose Residual Fusion We investigate removing the PRF and compute the pose residual maps ?(p, c, n) with the following scheme: h(c)?h(p)?h(n)?h(c) 3?3 ? ???????? ? convolution layer ?(p, c, n). This results in the mAP dropping 0.7 to 82.1. This significant reduction in performance highlights the important role of PRF in providing accurate pose residual cues for computing offsets and guiding the keypoint localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the pose predictions of our DCPose model(a), SimpleBaseline(b), HRNet(c), and PoseWarper(d) on the challenging cases from the P oseT rack2017 and P oseT rack2018 datasets. Each column from left to right represents the Rapid-Motion, Nearby-Person, Occlusions, and Video-Defocus scene, respectively. Inaccurate predictions are highlighted with the red solid circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative Results (AP) on PoseTrack2017 validation set.</figDesc><table><row><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Mean</cell></row><row><cell>PoseFlow[44]</cell><cell>66.7</cell><cell>73.3</cell><cell>68.3</cell><cell>61.1</cell><cell cols="2">67.5 67.0</cell><cell>61.3</cell><cell>66.5</cell></row><row><cell>JointFlow[11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.3</cell></row><row><cell>FastPose[47]</cell><cell>80.0</cell><cell>80.3</cell><cell>69.5</cell><cell>59.1</cell><cell cols="2">71.4 67.5</cell><cell>59.4</cell><cell>70.3</cell></row><row><cell cols="2">SimpleBaseline[43] 81.7</cell><cell>83.4</cell><cell>80.0</cell><cell>72.4</cell><cell cols="2">75.3 74.8</cell><cell>67.1</cell><cell>76.7</cell></row><row><cell>STEmbedding[16]</cell><cell>83.8</cell><cell>81.6</cell><cell>77.1</cell><cell>70.0</cell><cell cols="2">77.4 74.5</cell><cell>70.8</cell><cell>77.0</cell></row><row><cell>HRNet[33]</cell><cell>82.1</cell><cell>83.6</cell><cell>80.4</cell><cell>73.3</cell><cell cols="2">75.5 75.3</cell><cell>68.5</cell><cell>77.3</cell></row><row><cell>MDPN[13]</cell><cell>85.2</cell><cell>88.5</cell><cell>83.9</cell><cell>77.5</cell><cell cols="2">79.0 77.0</cell><cell>71.4</cell><cell>80.7</cell></row><row><cell>PoseWarper[3]</cell><cell>81.4</cell><cell>88.3</cell><cell>83.9</cell><cell>78.0</cell><cell cols="2">82.4 80.5</cell><cell>73.6</cell><cell>81.2</cell></row><row><cell>DCPose</cell><cell>88.0</cell><cell>88.7</cell><cell>84.1</cell><cell cols="4">78.4 83.0 81.4 74.2</cell><cell>82.8</cell></row><row><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>PoseFlow[44]</cell><cell>64.9</cell><cell>67.5</cell><cell>65.0</cell><cell>59.0</cell><cell cols="2">62.5 62.8</cell><cell>57.9</cell><cell>63.0</cell></row><row><cell>JointFlow[11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.1</cell><cell>-</cell><cell>-</cell><cell>50.4</cell><cell>63.4</cell></row><row><cell>KeyTrack[30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.9</cell><cell>-</cell><cell>-</cell><cell>65.0</cell><cell>74.0</cell></row><row><cell>DetTrack[39]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.8</cell><cell>-</cell><cell>-</cell><cell>65.9</cell><cell>74.1</cell></row><row><cell cols="2">SimpleBaseline[43] 80.1</cell><cell>80.2</cell><cell>76.9</cell><cell>71.5</cell><cell cols="2">72.5 72.4</cell><cell>65.7</cell><cell>74.6</cell></row><row><cell>HRNet[33]</cell><cell>80.1</cell><cell>80.2</cell><cell>76.9</cell><cell>72.0</cell><cell cols="2">73.4 72.5</cell><cell>67.0</cell><cell>74.9</cell></row><row><cell>PoseWarper[3]</cell><cell>79.5</cell><cell>84.3</cell><cell>80.1</cell><cell>75.8</cell><cell cols="2">77.6 76.8</cell><cell>70.8</cell><cell>77.9</cell></row><row><cell>DCPose</cell><cell>84.3</cell><cell>84.9</cell><cell>80.5</cell><cell cols="4">76.1 77.9 77.1 71.2</cell><cell>79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparisons on the PoseTrack2017 test set.</figDesc><table><row><cell>Method</cell><cell cols="7">Head Shoulder Elbow Wrist Hip Knee Ankle Mean</cell></row><row><cell cols="2">AlphaPose[12] 63.9</cell><cell>78.7</cell><cell>77.4</cell><cell>71.0</cell><cell>73.7 73.0</cell><cell>69.7</cell><cell>71.9</cell></row><row><cell>MDPN[13]</cell><cell>75.4</cell><cell>81.2</cell><cell>79.0</cell><cell>74.1</cell><cell>72.4 73.0</cell><cell>69.9</cell><cell>75.0</cell></row><row><cell cols="2">PoseWarper[3] 79.9</cell><cell>86.3</cell><cell>82.4</cell><cell>77.5</cell><cell>79.8 78.8</cell><cell>73.2</cell><cell>79.7</cell></row><row><cell>DCPose</cell><cell>84.0</cell><cell>86.6</cell><cell>82.7</cell><cell cols="3">78.0 80.4 79.3 73.8</cell><cell>80.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Quantitative Results(AP) on PoseTrack2018 validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 4. Performance comparisons on the PoseTrack2018 test set.</figDesc><table><row><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>AlphaPose++[12, 13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.2</cell><cell>-</cell><cell>-</cell><cell>65.0</cell><cell>67.6</cell></row><row><cell>DetTrack [39]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.8</cell><cell>-</cell><cell>-</cell><cell>67.1</cell><cell>73.5</cell></row><row><cell>MDPN[13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.5</cell><cell>-</cell><cell>-</cell><cell>69.0</cell><cell>76.4</cell></row><row><cell>PoseWarper[3]</cell><cell>78.9</cell><cell>84.4</cell><cell>80.9</cell><cell>76.8</cell><cell cols="2">75.6 77.5</cell><cell>71.8</cell><cell>78.0</cell></row><row><cell>DCPose</cell><cell>82.8</cell><cell>84.0</cell><cell>80.8</cell><cell cols="4">77.2 76.1 77.6 72.3</cell><cell>79.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unipose: Unified human pose estimation in single images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Artacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7035" to="7044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning temporal pose estimation from sparsely-labeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3027" to="3038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04030</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A?3ncf: An adaptive aspect attention model for rating prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3748" to="3754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04596</idno>
		<title level="m">Joint flow: Temporal flow fields for multi person tracking</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-domain pose network for multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfu</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5700" to="5709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-person articulated tracking with spatial and temporal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10863" to="10872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning a disentangled embedding for monocular 3d shape retrieval and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Kyaw Zaw Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09899</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph-based characteristic view set extraction and matching for 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="442" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical clustering multi-task learning for joint human action grouping and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Zhi</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="102" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Aggregated multi-gans for controlled 3d human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards natural and accurate future motion prediction of humans and animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10004" to="10012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lstm pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7773" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded models for articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="406" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">15 keypoints is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Snower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6738" to="6748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thin-slicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with enhanced channelwise and spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5674" to="5682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional regression forests for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3394" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mixture dense regression for object detection and human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Varamesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13086" to="13095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="596" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Combining detection and tracking for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Modolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple tree models for occlusion and spatial constraints in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="710" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00977</idno>
		<title level="m">Pose flow: Efficient online pose tracking</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fastpose: Towards real-time pose estimation and tracking via scale-normalized multi-task networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05593</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation via parsing a tree structure based human model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1349" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Smart contract vulnerability detection using graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3283" to="3290" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
