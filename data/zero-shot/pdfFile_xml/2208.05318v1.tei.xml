<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Supervised Training for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Supervised Training for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based action recognition has drawn a lot of attention for its computation efficiency and robustness to lighting conditions. Existing skeleton-based action recognition methods are typically formulated as a one-hot classification task without fully utilizing the semantic relations between actions. For example, "make victory sign" and "thumb up" are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encoding of action classes but could be unveiled in the language description of actions. Therefore, utilizing action language descriptions in training could potentially benefit representation learning. In this work, we propose a Language Supervised Training (LST) approach for skeleton-based action recognition. More specifically, we employ a large-scale language model as the knowledge engine to provide text descriptions for body parts movements of actions, and propose a multi-modal training scheme by utilizing the text encoder to generate feature vectors for different body parts and supervise the skeleton encoder for action representation learning. Experiments show that our proposed LST method achieves noticeable improvements over various baseline models without extra computation cost at inference. LST achieves new state-of-the-arts on popular skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The code can be found at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Action recognition has been an active research topic due to its wide range of applications in human-computer interaction, sports and health analysis, entertainment, etc. In recent years, with the emergence of depth sensors, such as Kinect <ref type="bibr" target="#b38">(Zhang 2012)</ref> and RealSense <ref type="bibr" target="#b11">(Keselman et al. 2017)</ref>, human body joints can be easily acquired. The action recognition approach utilizing body joints, i.e., the socalled skeleton-based action recognition, has drawn a lot of attentions due to its computation efficiency and robustness to lighting conditions, viewpoint variations and background noise.</p><p>Most of the previous methods in skeleton-based action recognition focus on modeling the relation of human joints, following a unimodal training scheme with a sequence of skeleton coordinates as inputs <ref type="bibr" target="#b12">Lee et al. 2017;</ref><ref type="bibr" target="#b22">Si et al. 2019;</ref><ref type="bibr" target="#b8">Du, Wang, and Wang 2015;</ref><ref type="bibr" target="#b23">Song et al. 2017;</ref><ref type="bibr" target="#b20">Shi et al. 2019;</ref><ref type="bibr" target="#b33">Ye et al. 2020;</ref><ref type="bibr" target="#b24">Song et al. 2022;</ref><ref type="bibr" target="#b30">Xia and Gao 2021;</ref><ref type="bibr" target="#b17">Plizzari, Cannici, and Matteucci 2021)</ref>. With the recent success of multi-modal training of image and language <ref type="bibr" target="#b18">(Radford et al. 2021;</ref><ref type="bibr" target="#b0">Brown et al. 2020)</ref>, in this work we investigate an interesting question: whether language supervised training could unveil the fine-grained semantic relations and benefit skeleton-based action recognition? Unfortunately, as there lacks a large-scale dataset of skeleton-text pairs, the training scheme in <ref type="bibr" target="#b18">(Radford et al. 2021;</ref><ref type="bibr" target="#b0">Brown et al. 2020)</ref> could not be directly applied to skeleton-based action recognition. Therefore, new multi-modal training paradigms are demanded to tackle this problem.</p><p>We propose to leverage the category-level language description for actions. The language definition of an action contains rich prior knowledge. For example, different actions focus on the movement of different body parts: "make victory sign" and "thumb up" describe the gesture of hands; "arm circles" and "tennis bat swing" describe the movement of arms; "nod head" and "shake head" are the motions of head; "jump up" and "side kick" rely on movements of foot and leg. Some actions describe the interaction of multiple body parts, e.g., "put on a hat" and "put on a shoe" involve actions of hand and head, hand and foot, respectively. These prior knowledge about actions could provide fine-grained guidance for representation learning.</p><p>In this work, we develop a new training paradigm, which employs language supervision for skeleton-based action recognition. In <ref type="figure">Figure 1</ref>, we compare our proposed frameworks (b)(c) with traditional single encoder skeleton-based action recognition framework (a). In our framework, a multimodal training scheme is developed, which contains a skeleton encoder and a text encoder. The skeleton encoder takes skeleton coordinates as inputs and generates both part feature vectors and global feature representations. The text encoder transforms global action description or body part descriptions into text features for the whole action or each body part. Instead of using expensive manual annotations, we take advantages of the large-scale language models <ref type="bibr" target="#b0">(Brown et al. 2020)</ref> as our knowledge engine to generate meaningful text descriptions for actions. With elaborately designed text prompts, detailed text descriptions for the whole action and each body part can be produced. A multi-part contrastive loss (single contrastive loss for (b)) is used to align the text part features and skeleton part features, and the crossentropy loss is applied on the global features. With this new training paradigm, we could improve the performance of various skeleton-based action recognition methods without additional cost at the inference stage.</p><p>Our contributions are summarized as follow:</p><p>-As far as we known, this is the first work to apply the language model as the knowledge engine and elaborately employ text prompt to generate detailed text descriptions of the whole action and body parts movements for different actions automatically. -We propose a new multi-modal training paradigm that utilizes language supervision to guide skeleton-based action recognition, which enhances the representation by using knowledge about actions and human body parts. -With our proposed training paradigm, we achieve stateof-the-art performance on several popular skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and NW-UCLA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton-based Action Recognition</head><p>In recent years, various methods have been proposed for skeleton-based action recognition by designing efficient and effective model architecture. RNNs were applied to handle the sequence of human joints in <ref type="bibr" target="#b8">(Du, Wang, and Wang 2015;</ref><ref type="bibr" target="#b23">Song et al. 2017;</ref><ref type="bibr" target="#b34">Zhang et al. 2017)</ref>. HBRNN <ref type="bibr" target="#b8">(Du, Wang, and Wang 2015)</ref> proposed an end-to-end hierarchical RNN to model long-term contextual information of temporal skeleton sequences. VA-LSTM ) designed a view adaptive RNN, which enables the network to adapt to the most suitable observation viewpoints from end to end. Due to the success of CNN in image tasks, CNNbased methods <ref type="bibr" target="#b31">Xu et al. 2021)</ref> have been utilized to model joints relations. TA-CNN <ref type="bibr" target="#b31">(Xu et al. 2021</ref>) proposed a pure CNN architecture named Topologyaware CNN. As human joints can be naturally presented as graph nodes and joints connections can be described by adjacent matrix, GCN-based methods <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b20">Shi et al. 2019;</ref><ref type="bibr" target="#b24">Song et al. 2022)</ref> have drawn a lot of attentions. For example, ST-GCN (Yan, Xiong, and Lin 2018) applied spatialtemporal GCN to model human joints relations in both spatial and temporal dimension. CTR-GCN  proposed a channel-wise graph convolution for fine-grained relation modeling. With recent popularity of vision transformer <ref type="bibr" target="#b7">(Dosovitskiy et al. 2021)</ref>, transformer-based methods <ref type="bibr" target="#b17">(Plizzari, Cannici, and Matteucci 2021;</ref><ref type="bibr" target="#b21">Shi et al. 2020;</ref>) have also been investigated for skeleton data. All the previous methods adopt a unimodal training scheme. Our work in this paper, as far as we know, is the first to apply a multi-modal training scheme for skeletonbased action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Part Prior</head><p>Human part prior for skeleton-based action recognition has been used by designing special model architectures in previous works <ref type="bibr" target="#b25">(Thakkar and Narayanan 2018;</ref><ref type="bibr" target="#b23">Song et al. 2020;</ref><ref type="bibr" target="#b9">Huang et al. 2020</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Representation Learning</head><p>Multimodal representation learning methods, such as CLIP <ref type="bibr" target="#b18">(Radford et al. 2021</ref>) and ALIGN <ref type="bibr" target="#b10">(Jia et al. 2021)</ref>, have shown that vision-language co-training can learn powerful representation for downstream tasks such as zero-shot learning, image captioning, text-image retrieval, etc. However, these methods require a large-scale image-text paired dataset for training. ActionCLIP <ref type="bibr" target="#b28">(Wang, Xing, and Liu 2021)</ref> follows the training scheme of CLIP for video action recognition. A pre-trained CLIP model is used and transformer layers are added for temporal modeling of video data. As for language supervision, label names are directly used as text prompts with prefix and suffix that do not contain much semantic meanings, e.g.,"A video of [action name]", "Human action of [action name]", etc. In contrast, we use a large language model as knowledge engine to generate descriptions of human body movements in actions, which provide fine-grained guidance for representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we present in detail our proposed Language Supervised Training (LST) framework. LST aims to enhance skeleton representation learning with language supervision and it is orthogonal to the backbone networks. Therefore, LST can be coupled with various skeleton and language encoders. In the following sections, we first overview the overall LST framework, then introduce the skeleton encoder, text encoder and the main components of LST in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Supervised Training Framework</head><p>The overall framework of our LST approach is presented in <ref type="figure" target="#fig_0">Figure 2</ref>. It uses part text features to guide the skeleton representation learning with additional multi-part contrastive loss. The part feature is generated by aggregating features of different groups of nodes using average pooling. The skeleton part features are mapped by fully connected layers to keep the same feature dimension as text features. The text part descriptions are encoded by transformer encoders for producing text part features. As described in Eq. 7, the overall training loss is the combination of cross-entropy classification loss and multi-part contrastive loss. It is worth mentioning that part feature pooling and multi-part contrastive loss are only applied at the training stage. At the testing stage, we directly use global features for action probability prediction. Therefore, our LST framework does not bring additional memory or computation cost at inference time.</p><p>The "Pytorch-like" pseudo code of LST can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton Encoder</head><p>Graph Convolution Network (GCN) is prevailing for skeleton action recognition due to its efficiency and strong performance. Therefore, we adopt GCN as the backbone network in our LST framework. Our skeleton encoder consists of multiple GC-MTC blocks, while each block contains a graph convolution (GC) layer and a multiscale temporal convolution (MTC) module. Graph Convolution. The human skeleton can be represented as a graph G = {V, E}, where V is the set of human joints with |V | = N , and E is the set of edges. Let the features of human joints at layer l be H l ? R N ?F with feature dimension F . The graph convolution can be formulated as:</p><formula xml:id="formula_0">H l+1 = ?(D ? 1 2 AD ? 1 2 H l W l ),<label>(1)</label></formula><p>where D ? R N ?N is the degree matrix, A is the adjacency matrix representing joints connections, W l is the learnable parameter of the l-th layer and ? is the activation function.</p><p>Multiscale Temporal Modeling. To model the action at different temporal speed, we utilize the multiscale temporal convolution module ) for temporal modeling. This module contains two convolution branches with various kernel sizes and dilations, a maxpooling branch and a residual connection. The outputs of different branches are aggregated by concatenation.</p><p>Skeleton Classification. The skeleton-based action recognition methods map human skeleton data to one-hot encoding of action labels, which are trained with a crossentropy loss:</p><formula xml:id="formula_1">L cls = ?y log p ? (x),<label>(2)</label></formula><p>where y is the one-hot ground-truth label, x is the input data and p ? (x) is the predicted probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Supervision</head><p>Text Encoder. Considering the recent success of Transformer models in NLP, we follow the work in <ref type="bibr" target="#b26">(Vaswani et al. 2017)</ref> for the design of text encoder. Suppose that the text encoder contains L encoders, each consisting of a Selfattention (SA), a Layer-Norm (LN) and an FFN layer. The transformer encoder could be represented as follows:</p><formula xml:id="formula_2">z l = SA(LN(z l?1 )) + z l?1 , z l = FFN(LN(? l )) +? l ,<label>(3)</label></formula><p>where? l and z l denote the output features of the SA module and the FFN module for block l, respectively. The SA is computed as follows:</p><formula xml:id="formula_3">Attention(Q l , K l , V l ) = softmax( Q l K T l ? d V l ),<label>(4)</label></formula><p>where Q l ,K l ,V l represent the Query, Key and Value matrices for block l and are calculated by the linear mapping of z l?1 with weights W Q l , W K l , W V l , respectively. d is the scaling factor that equals to Query and Key dimension.</p><p>Skeleton-language Contrastive Learning. Comparing to the one-hot label supervision for skeleton classification, skeleton-language contrastive learning employs the supervision from natural language. It has a dual-encoder design with a skeleton encoder E s and a text encoder E t , which encode skeleton data and action descriptions, respectively. The dual-encoders are jointly optimized by contrasting skeletontext pairs in two directions within the batch:</p><formula xml:id="formula_4">p s2t i (s i ) = exp(sim(s i , t i )/? ) N j=1 exp(sim(s i , t j )/? ) , p t2s i (t i ) = exp(sim(t i , s i )/? ) N j=1 exp(sim(t i , s j )/? ) ,<label>(5)</label></formula><p>where s = E s (S), t = E t (T) are encoded features of skeleton and text, sim(s, t) is the cosine similarity of s and t, ? is the temperature parameter and N is the batch size. Unlike image-text pairs in CLIP, which are one-to-one mappings, in our setting, there could be more than one positive matching. Therefore, instead of using cross-entropy loss, we use KL divergence as the skeleton-text contrastive loss:</p><formula xml:id="formula_5">L con = 1 2 E s,t ? D[KL(p s2t (s), y s2t ) + KL(p t2s (t), y t2s )],<label>(6)</label></formula><p>where y s2t and y t2s are ground-truth similarity scores.</p><p>Multi-part Contrastive Learning. Considering the prior of human body parts, skeleton can be divided into multiple groups. We illustrate this framework in <ref type="figure">Figure 1</ref>(c). We apply contrastive loss on different parts features as well as global feature and propose a multi-part contrastive loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label name:</head><p>Prefix: "put on a shoe", a video of action Prefix: "put on a shoe", this is an action Cloze: This is "put on a shoe", a video of action Suffix: Human action of "put on a shoe" Suffix: Playing a kind of action, "put on a shoe"?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAKE:</head><p>Put on a shoe: foot stand on, foot walk to, foot fall down, hand put on, foot tread on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual:</head><p>Put on a shoe: hand reach for, hand put on, hip sit on, leg bend down, foot wear</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-3 (Paragraph):</head><p>Put on a shoe: The man is putting on a shoe. He is bending down and putting his foot into the shoe. He is then tying the shoe. He is doing this quickly and efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-3 (Synonym):</head><p>Put on a shoe: boot, lace up, slip on, step into, strap on, tie, tuck in, zip up, don, fasten</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-3 (Part description):</head><p>Put on a shoe: head tilts slightly forward; hand reaches down and grasps shoe; arm extends down and forward; hip remains stationary; leg bends at the knee, bringing foot closer to the hand; foot inserts into shoe. The part feature could be obtained with part pooling, where joint features within the same group are aggregated to generate part representation. More specifically, we choose the features before the final classification layer for part feature pooling. In <ref type="figure" target="#fig_2">Figure 3</ref>, we show different part partition strategies. For two parts partition, the whole body is divided into upper and lower groups. For four parts partition, the body is divided into four groups: head, hand-arm, hip, leg-foot. For six parts partition, head, hand, arm, hip, leg, foot are grouped separately.</p><p>The overall loss function with multi-part contrastive loss can be represented as follows:</p><formula xml:id="formula_6">L total = L cls + ? 1 K K k=1 L k con ,<label>(7)</label></formula><p>where ? is the trade-off parameter and K is the total part number, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Description Generation</head><p>The action description for text encoder plays a vital role in LST. Here, we explore several different description generation methods. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the text descriptions of action "put on a shoe" by different methods. Label Name. One straight-forward approach is to directly use the label name. Many methods <ref type="bibr" target="#b28">(Wang, Xing, and Liu 2021)</ref> use this kind of text descriptions with prefix and suffix such as "Human action of [action]", "[action], a video of action", etc. Though these prompts could boost the performance for zero-shot and few-shot problems, in our case of supervised learning, this approach does not bring significant Part Description Q: Describing following body parts actions when "put on a shoe": head, hand, arm, hip, leg, foot.</p><p>A: head tilts slightly forward; hand reaches down and grasps shoe; arm extends down and forward; hip remains stationary; leg bends at the knee, bringing foot closer to the hand; foot inserts into shoe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synonym</head><p>Q: Suggest 10 synonyms for "put on a shoe" A: boot, lace up, slip on, step into, strap on, tie, tuck in, zip up, don, fasten Paragraph Q: Describe a person "put on a shoe" in details.</p><p>A: The man is putting on a shoe. He is bending down and putting his foot into the shoe. He is then tying the shoe. He is doing this quickly and efficiently. performance improvement (as shown in our ablation studies) since these prompts do not contain discriminative semantic information about actions.</p><p>HAKE Part State. The HAKE <ref type="bibr" target="#b14">(Li et al. 2022</ref>) dataset contains annotated part states of human-object interactions. For each sample, six body part movements (head, hand, arm, hip, leg, foot) are manually annotated, with 93 part states in total. In order to avoid laborious annotation for each sample, we apply an automatic pipeline which contains two steps: 1) generate text features for both label name and HAKE part states with a pre-trained transformer text encoder; 2) generate text description by finding the K nearest neighbors of action label name in HAKE part state feature space. Those HAKE part states that are closest to the action label name are selected for action description. We then use this generated part description for LST.</p><p>Manual Description. We ask annotators to write down the description of body part movements following the temporal order of the action. The descriptions consist of the predefined atomic movements. The annotators are asked to focus on the most distinguished parts' motions.</p><p>Large-language Model. We use the large-scale language model (e.g., GPT-3) to generate text descriptions. We design text prompts so that it can generate our desired action descriptions. Text descriptions are generated in three ways: a. paragraph: a full paragraph that can describe the action in detail; b. synonym: we collect 10 synonyms of action labels; c. part description: we collect descriptions of different body parts for each action. The body partition strategies follow <ref type="figure" target="#fig_2">Figure 3</ref> in previous section. We take "put on a shoe" as an example to present the prompts used for generating different descriptions in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>To demonstrate the effectiveness of LST, we conduct experiments on large-scale skeleton action recognition benchmarks. We first compare our proposed training framework with strong baseline methods via ablation studies to unveil the effect of different components. We then compare our proposed method with state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>NTU RGB+D <ref type="bibr" target="#b19">(Shahroudy et al. 2016</ref>) is a widely used dataset for skeleton-based human action recognition. It contains 56,880 skeletal action sequences. There are two benchmarks for evaluation, including Cross-Subject (X-Sub) and Cross-View (X-View) settings. For X-Sub, the training and test sets come from two disjoint sets, each having 20 subjects. For X-View, the training set contains 37,920 samples captured by camera views 2 and 3, and the test set includes 18,960 sequences captured by camera view 1. NTU RGB+D 120 <ref type="bibr" target="#b15">(Liu et al. 2019</ref>) is an extension of NTU RGB+D dataset with 57,367 additional skeleton sequences over 60 additional action classes. There are 120 action classes in total. Two benchmark evaluations were suggested by the authors, including Cross-Subject (X-Sub) and Cross-Setup (X-Setup) settings. NW-UCLA <ref type="bibr" target="#b27">(Wang et al. 2014</ref>) dataset is recorded by three Kinect V1 sensors from different viewpoints. The skeleton contains 20 joints and 19 bone connections. It includes 1,494 video sequences of 10 action categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For NTU RGB+D and NTU RGB+D 120, each sample is resized to 64 frames, and we adopt the code of <ref type="bibr" target="#b5">Chi et al. 2022</ref>) for data pre-processing. For NW-UCLA, we follow the data pre-processing in <ref type="bibr" target="#b5">Chi et al. 2022)</ref>. We use CTR-GCN with single-scale temporal convolution for our ablation study, considering its good balance between performance and efficiency. When comparing with other methods, we use CTR-GCN with multiscale temporal convolution as it produces the best results. For text encoder, we use the pretrained transformer model from CLIP or BERT <ref type="bibr" target="#b6">(Devlin et al. 2018)</ref> and finetune its parameters in training. The temperature of contrastive loss is set to 0.1. For NTU RGB+D and NTU RGB+D 120, we train the model for a total number of 110 epochs with batch size 200. We use a warm-up strategy for the first 5 epochs. The initial learning rate is set to 0.1 and reduced by a factor of 10 at 90 and 100 epochs, the weight decay is set to 5e-4 following the strategy in <ref type="bibr" target="#b5">(Chi et al. 2022</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this section, we conduct experiments to evaluate the influences of partition strategy, text prompt, text encoder, skeleton encoder, prompt sources and ? parameter selection. The results are presented in <ref type="table">Table 1</ref>.  <ref type="table">Table 1a</ref>. 'Global' represents using a global description of actions with a single contrastive loss, and it improves over the baseline by 0.6%. Using more parts and multi-part contrastive loss could steadily increase the performance, and it saturates at 85.4% when using 4 parts.</p><p>Influences of Text Prompt. The text prompt design has a large impact on the model performance. We show the influences of different text prompts in <ref type="table">Table 1b</ref>. By directly using label name (with prefix or suffix) as the text prompt in LST, the model only slightly outperforms (0.2%) the baseline model without text encoder, as this does not bring extra information for training. Utilizing a synonym list for label name or a global description paragraph could largely improve the performance (0.6%) over baseline, as it enriches the semantic meanings of each action class. Using part description prompts leads to strong performance with 0.8% improvement. The best performance is achieved by combining synonym of label name and body part description for prompts, resulting in 85.5% accuracy.</p><p>Influences of Text Encoder. In <ref type="table">Table 1c</ref>, we show the influences of text encoders. Note that we use the text encoder only at the training stage so that the computation and memory cost does not increase at inference. We found that both CLIP-L/32 and BERT achieve good performance, indicating that skeleton encoder could benefit from text encoder with different pre-training sources (image-language or pure language). We use CLIP-L/32 as our default text encoder considering its good balance between efficiency and accuracy. Effect of LST on Different Skeleton Encoders. Our proposed LST is decoupled from the network architecture and could be employed to improve different skeleton encoders.</p><p>In <ref type="table">Table 1d</ref>, we show experimental results of applying LST to ST-GCN, CTR-baseline and CTR-GCN. LST brings consistent improvements (0.6-1.2%) over original models without extra computation cost at inference, demonstrating the effectiveness and generalization ability of LST.</p><p>Comparison of Description Methods. We compare several different methods of obtaining text prompts for text encoders in <ref type="table">Table 1e</ref>, including: Manual description; HAKE part state ; Generating text prompts with GPT-3. For manual descriptions and HAKE results, we use them as global description for LST. Among these methods, GPT-3 could provide very detailed description of human parts by using an elaborately designed text prompt, and the generated part text description achieves the best performance. We also implement a part pooling classification baseline for reference, which applies a classification head for every pooled part feature. This baseline does not work well as the part feature may not be sufficient to predict the action classes. We refer readers to our supplementary materials for details of text prompt design and description generation. Influences of ? Selection. In <ref type="table">Table 1f</ref>, we present the influences of trade-off parameter ? in Eq. 7. We search the value of ? in {1.0, 0.8, 0.5, 0.2}. We found that ? = 0.8 achieves the best performance; therefore, we utilize it as our default ? value and employ it for all the experiments. We use the same ? for different benchmarks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the State-of-the-art</head><p>We compare our method with previous state-of-the-art in Tables 2,3 and 4. For fair comparison, we use the 4 ensembles strategy (Joint, Joint-Motion, Bone, Bone-Motion) as it is adopted by most of the previous methods. We present four modalities separately as well as the ensemble results. One can see that our proposed LST method consistently outperforms previous state-of-the-arts. As shown in <ref type="table" target="#tab_5">Table 2</ref>, on NW-UCLA, LST outperforms CTR-GCN by 0.7%. It also outperforms the recent work Info-GCN <ref type="bibr" target="#b5">(Chi et al. 2022)</ref>, which uses self-attention layer and information bottleneck, by 0.6%. We argue that such improvement is significant considering that the model performance on this dataset is already very high. On NTU   RGB+D, LST outperforms CTR-GCN ) by 0.5% on cross-subject and 0.2% on cross-view settings, and it outperforms Info-GCN by 0.2% and 0.1% on the two settings, respectively. On the largest dataset NTU RGB+D 120, as shown in <ref type="table">Table 4</ref>, our method surpasses CTR-GCN by a large margin (1.0%) on cross-subject, and 0.5% on cross-set settings, respectively. Info-GCN also achieves strong performance on this dataset, while LST still outperforms it by 0.5% and 0.4%, respectively. In summary, LST consistently outperforms the state-of-the-arts on NW-UCLA, NTU RGB+D and NTU RGB+D 120 under different settings, validating its effectiveness and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions</head><p>Let us make more discussions on the proposed LST method. We use the model trained with joint modality on NTU RGB+D 120 cross-subject mode. In <ref type="figure">Figure 6</ref>, we show the action classes that have over 4% accuracy differences on NTU120 w./w.o. LST. It can be found that actions such as  <ref type="table">Table 4</ref>: Action classification performance on the NTU RGB+D 120 dataset.</p><p>"writing", "open a box", "eat meal" and "wield knife", etc., benefit most from LST, as the language model generates detailed descriptions of body parts movements for these actions. For example, the "writing" is described as "writing; head is tilted slightly forward and to the left; right hand is holding a pen and left hand is holding the paper; right arm is moving the pen across the paper; hips are stationary; legs are stationary; feet are flat on the floor". More details can be found in the supplementary materials. However, the performance of LST decreases for action classes such as "cutting paper", "taking a selfie", "play magic cube" and "play with phone/tablet". The reason might be that the main description differences of these actions are object related, which are difficult to be recognized from skeleton data.  <ref type="figure">Figure 6</ref>: Action classes with accuracy differences higher than 4% between CTR-GCN and our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We developed a novel language supervised training (LST) framework for skeleton-based action recognition, which is the first work, as far as we know, to use language supervision for skeleton action recognition. We employed largescale language models as knowledge engine to automatically generate detailed descriptions of body parts without labori-ous manual annotation. LST utilized language supervision to guide skeleton encoder and enhance the learned representation with knowledge about relations of actions and human body parts. The extensive experiments demonstrated that LST is a general framework and it can be coupled with various backbone networks to enhance representation learning. LST achieved new state-of-the-arts on NTU RGB+D, NTU RGB+D 120 and NW-UCLA benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More implementation details</head><p>Part partition. The body part partition is slightly different in NW-UCLA as it only contains 20 joints, while NTU RGB-D and NTU RGB-D 120 contain 25 joints. However, the overall groupings remain the same, which contain four parts: head, hands, hip, legs. Implementation. Our implementation is based on CTR-GCN . We also adopt the data preprocessing in InfoGCN <ref type="bibr" target="#b5">(Chi et al. 2022)</ref>, where K = 1 for bone modality and K = 8 or 6 for joint modality in NTU RGB-D 60/120 and NW-UCLA, respectively. For the implementation of text encoder, two different pretraining schemes (image-text and pure text) are considered. For image-text pre-training, we adopt CLIP <ref type="bibr" target="#b18">(Radford et al. 2021)</ref>. For pure text pre-training text encoder, we adopt Roberta 1 . 1 https://github.com/huggingface/transformers We implement our framework with Pytorch 2 . All the models in our experiments are trained with 2 RTX 3090 GPUs and the seed is set to 1. Mixed-precision training is adopted to accelerate training speed and reduce memory footprint. All the codes, models and text descriptions will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text descriptions</head><p>We provide text descriptions generated by GPT-3 (textdavinci-002) for different text prompts on NTU RGB-D 120 in paragraph-GPT3.txt, synonym-GPT3.txt and part-GPT3.txt, respectively. The text prompts used for generating text descriptions are as follows:</p><p>? Describe a person <ref type="bibr">[action]</ref>  The generated descriptions are only edited to correct format error of GPT-3 without changing the semantic contents. We found that paragraph provides rich descriptions but sometimes with unnecessary details. Synonym enriches the label name meanings with similar words. Part description provides the most precise description of actions.</p><p>Several text descriptions are shown below:</p><p>drink water</p><p>? Paragraph: The man is sitting at a table with a plate of food in front of him. He is using a fork and knife to eat the food on his plate. He is chewing each bite of food before swallowing. He is also taking breaks in between bites to drink from a glass of water. ? Synonym: dine, sup, lunch, feast, banquet, picnic, breakfast, brunch, dinner, lunch. ? Part description: head tilts slightly forward; hand brings food to mouth; arm supports hand; hip remains stationary; leg remains stationary; foot remains stationary. eat meal</p><p>? Paragraph: The man is drinking water from a glass. He is holding the glass in his hand and bringing it to his lips. He is tilting the glass back and forth, allowing the water to flow into his mouth. He is swallowing the water and then setting the glass down on the ? Synonym: remove jacket, take jacket off, divest oneself of jacket, unburden oneself of jacket, get rid of jacket, be rid of jacket, dispose of jacket, get jacket off, shed jacket, slip off jacket. ? Part description: head tilts back slightly; grabs the bottom of jacket with both hands, brings hands up the jacket; arms straighten as the jacket falls downs;hip steps out of the jacket;legs straight;feet on the ground. make victory sign ? Paragraph: He is standing with his feet apart and his arms raised in the air, making a V sign with his fingers. He has a triumphant look on his face and is clearly enjoying himself. ? Synonym: give the thumbs up, give a thumbs up, give the okay sign, give the A-okay sign, give the victory sign, give a V sign, give a peace sign, give the finger, give the bird, give the one-finger salute ? Part description: head tilts slightly forward; hand forms a V shape with the index and middle fingers; arm extends fully; hip remains stationary; leg remains stationary; foot remains stationary. make ok sign ? Paragraph: The man is making an OK sign with his hand. His thumb and index finger are touching to form a circle, and his other three fingers are extended. He may be doing this to indicate that everything is okay, or to give someone the okay sign. ? Synonym: A-OK, all right, alright, copacetic, dandy, fine, good, hunky-dory, okay, swell ? Part description: head tilts slightly to the side; hand forms a circle with the thumb and first two fingers; arm extends straight out from the shoulder; hip remains at a neutral position; leg remains at a neutral position; foot remains at a neutral position. move heavy objects ? Paragraph: The man is muscular with broad shoulders.</p><p>He has a deep tan, and his arms are covered in a light sheen of sweat. He grunts with effort as he lifts a heavy object, his biceps straining. His face is set in a determined expression, and his eyes are focused on the task at hand. He moves with purpose, each step sure and confident. He makes the task look easy, despite the obvious strain it is causing him. ? Synonym: drag, haul, lug, tow, carry, convey, transport, shift, relocate, remove ? Part description: head turns to look at the object; hand reaches out and grasps the object; arm pulls the object towards the body; hip moves forward to create leverage; leg pushes against the ground to create power; foot stabilizes the body. support somebody ? Paragraph: He is standing behind the person in front of him, with his hands on their shoulders. He is looking down at them with a concerned expression on his face. He is speaking quietly to them, offering words of encouragement. ? Paragraph: He is a man who is of average height and build. He has dark hair and eyes, and is wearing a pair of jeans and a t-shirt. He is jump up in the air, and his arms and legs are outstretched. He has a look of concentration on his face, and he is landing on his feet. ? Synonym: leap, bound, spring, vault, hop, skip, caper, gambol, frisk, frolic ? Part description: head tilts back and the chin points up;</p><p>hands come up to the chest; arms bend at the elbows and the forearms come up; hips push forward and the legs bend at the knees; legs push off the ground and the feet come up; feet land on the ground and the legs bend at the knees.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overall framework of Language Supervised Training (LST) with multi-part contrastive loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Different part partition strategies. (a) Two parts: upper and lower body. (b) Four parts: head, hand-arm, hip, leg-foot (c) Six parts: head, arm, hand, hip, leg, foot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Text description generated by different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Text description generated from different prompt inputs by GPT-3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TextEncoder Comparison of our proposed language supervised training framework (dual encoder) with other skeleton recognition methods (single encoder). Notice that text encoder is only used at the training stage.</figDesc><table><row><cell>Classification Loss</cell><cell>Classification Loss</cell><cell></cell><cell>Classification Loss</cell><cell></cell></row><row><cell></cell><cell>Contrastive Loss</cell><cell></cell><cell cols="2">Multi-part Contrastive Loss</cell></row><row><cell>Skeleton</cell><cell>Skeleton</cell><cell></cell><cell>Skeleton</cell><cell cols="2">Text</cell></row><row><cell>Encoder</cell><cell>Encoder</cell><cell></cell><cell>Encoder</cell><cell cols="2">Encoder</cell></row><row><cell>skeleton</cell><cell>skeleton</cell><cell>text</cell><cell>skeleton</cell><cell cols="2">text</cell></row><row><cell></cell><cell cols="2">The man is putting on a</cell><cell></cell><cell>Head: tilts forward</cell><cell>Leg: bends at the knee</cell></row><row><cell></cell><cell cols="2">shoe. He is bending down and putting his foot into</cell><cell></cell><cell>Hand: reaches down and grasps shoe</cell><cell>Arm: extends down and forward</cell></row><row><cell></cell><cell>the shoe?</cell><cell></cell><cell></cell><cell cols="2">?</cell></row><row><cell>(a) Skeleton Recognition</cell><cell>(b) Skeleton Recognition</cell><cell></cell><cell cols="3">(c) Skeleton Recognition</cell></row><row><cell>(single encoder)</cell><cell>(dual encoder)</cell><cell></cell><cell cols="3">(dual encoder with multi-part)</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Action classification performance on the NW-UCLA dataset.</figDesc><table><row><cell>Methods</cell><cell>Mode</cell><cell cols="2">NTU-RGB+D X-Sub(%)X-View(%)</cell></row><row><cell>VA-LSTM (Zhang et al. 2017)</cell><cell>2 ensemble</cell><cell>79.4</cell><cell>87.6</cell></row><row><cell>HCN (Li et al. 2018)</cell><cell>2 ensemble</cell><cell>86.5</cell><cell>91.1</cell></row><row><cell>2S-AGCN (Shi et al. 2019)</cell><cell>2 ensemble</cell><cell>88.5</cell><cell>95.1</cell></row><row><cell>SGN (Zhang et al. 2020)</cell><cell>2 ensemble</cell><cell>89.0</cell><cell>94.5</cell></row><row><cell>2S-AGC-LSTM (Si et al. 2019)</cell><cell>2 ensemble</cell><cell>89.2</cell><cell>95.0</cell></row><row><cell>ST-TR (Plizzari et al. 2021)</cell><cell>4 ensemble</cell><cell>89.9</cell><cell>96.1</cell></row><row><cell>TA-CNN (Xu et al. 2021)</cell><cell>4 ensemble</cell><cell>90.4</cell><cell>94.8</cell></row><row><cell cols="2">4S-Shift-GCN (Cheng et al. 2020) 4 ensemble</cell><cell>90.7</cell><cell>96.5</cell></row><row><cell cols="2">DC-GCN+ADG (Cheng et al. 2020) 4 ensemble</cell><cell>90.8</cell><cell>96.6</cell></row><row><cell cols="2">PA-ResGCN-B19 (Song et al. 2020) 4 ensemble</cell><cell>90.9</cell><cell>96.0</cell></row><row><cell>Dynamic GCN (Ye et al. 2020)</cell><cell>4 ensemble</cell><cell>91.5</cell><cell>96.0</cell></row><row><cell>MS-G3D (Liu et al. 2020)</cell><cell>2 ensemble</cell><cell>91.5</cell><cell>96.2</cell></row><row><cell>DSTA (Shi et al. 2020)</cell><cell>4 ensemble</cell><cell>91.5</cell><cell>96.4</cell></row><row><cell>MST-GCN (Chen et al. 2021)</cell><cell>4 ensemble</cell><cell>91.5</cell><cell>96.6</cell></row><row><cell cols="2">EfficientGCN-B4 (Song et al. 2022) 4 ensemble</cell><cell>91.7</cell><cell>95.7</cell></row><row><cell>CTR-GCN (Chen et al. 2021)</cell><cell>4 ensemble</cell><cell>92.4</cell><cell>96.8</cell></row><row><cell>Info-GCN (Chi et al. 2022)</cell><cell>4 ensemble</cell><cell>92.7</cell><cell>96.9</cell></row><row><cell></cell><cell cols="3">Joint/Joint-M 90.2/88.0 95.6/93.7</cell></row><row><cell>Ours</cell><cell cols="3">Bone/Bone-M 91.2/87.8 95.5/93.2</cell></row><row><cell></cell><cell>4 ensemble</cell><cell>92.9</cell><cell>97.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Action classification performance on the NTU RGB+D dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>table . ?</head><label>.</label><figDesc>Synonym: sip, guzzle, gulp, swig, chug, quaff, swill, slug, chug down, toss back. ? Part description: head tilts back slightly; hand grasps cup; arm lifts cup to mouth; hip remains stationary; leg remains stationary; foot remains stationary. take off jacket ? Paragraph: The man is taking off his jacket. He is standing up straight and reaching his arms up above his head. His jacket is coming off easily and he is taking it off quickly. He is not having any trouble taking his jacket off.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>?</head><label></label><figDesc>Synonym: help, assist, back, prop, buttress, shore up,  strengthen, reinforce, hold up, lift  up ? Part description: head tilts slightly forward; hand grasps the other person's arm just above the elbow; arm supports the other person's arm; hip stands upright; leg stands upright; foot stands flat on the ground. open a box ? Paragraph: The man is standing in front of a box. He is reaching for the lid of the box. He is opening the lid of the box. He is looking inside the box. ? Synonym: unpack, unseal, unbox, decant, disentangle, extricate, liberate, release, remove, untie ? Part description: head tilts slightly forward; hand reaches out and grasps the edge of the lid; arm extends forward; hip remains stationary; leg remains stationary; foot remains stationary. open bottle ? Paragraph: He unscrews the cap of the bottle with one hand, while holding the base of the bottle in the other. He twists the cap until it comes off with a small pop. He brings the bottle to his lips and takes a long drink, savoring the flavor of the liquid inside. ? Synonym: jar, can, container, vessel, receptacle, flask, decanter, urn, cruet, carafe ? Part description: head tilts back slightly; hand grasps the neck of the bottle; arm extends the arm holding the bottle; hip remains stationary; leg remains stationary; foot remains stationary. jump up</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">; M</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Larochelle, H.; Ranzato,</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Channel-Wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13359" to="13368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Scale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="536" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">InfoGCN: Representation Learning for Human Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="20186" to="20196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021: The Ninth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Partlevel graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11045" to="11052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Intel RealSense Stereoscopic Depth Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Woodfill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grunnet-Jepsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhowmik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, 786-792. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, 786-792. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06851</idno>
		<title level="m">HAKE: A Knowledge Engine Foundation for Human Activity Understanding</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ntu RGB+D 120: A large-scale benchmark for 3d human activity understanding. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial temporal transformer network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-01-10" />
			<biblScope unit="page" from="694" to="701" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Decoupled spatial-temporal attention network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03263</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1625" to="1633" />
		</imprint>
	</monogr>
	<note>Proceedings of the AAAI conference on artificial intelligence</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ActionCLIP: A New Paradigm for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2109.08472</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13385</idno>
		<title level="m">IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-Scale Mixed Dense Graph Convolution Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="36475" to="36484" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04178</idno>
		<title level="m">Topologyaware Convolutional Neural Network for Efficient Skeleton-based Action Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1963" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
