<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
							<email>lhoyer@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">MPI for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MPI for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<email>ddai@mpi-inf.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Luc Van Gool ETH</orgName>
								<address>
									<settlement>Zurich &amp; KU Leuven</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As acquiring pixel-wise annotations of real-world images for semantic segmentation is a costly process, a model can instead be trained with more accessible synthetic data and adapted to real images without requiring their annotations. This process is studied in unsupervised domain adaptation (UDA). Even though a large number of methods propose new adaptation strategies, they are mostly based on outdated network architectures. As the influence of recent network architectures has not been systematically studied, we first benchmark different network architectures for UDA and newly reveal the potential of Transformers for UDA semantic segmentation. Based on the findings, we propose a novel UDA method, DAFormer. The network architecture of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last few years, neural networks have achieved overwhelming performance on many computer vision tasks. However, they require a large amount of annotated data in order to be trained properly. For semantic segmentation, annotations are particularly costly as every pixel has to be labeled. For instance, it takes 1.5 hours to annotate a single Most previous UDA methods are evaluated using the outdated DeepLabV2 architecture. We rethink the design of the network architecture as well as its training strategies for UDA and propose DAFormer, significantly outperforming previous methods. image of Cityscapes <ref type="bibr" target="#b13">[13]</ref> while, for adverse weather conditions, it is even 3.3 hours <ref type="bibr" target="#b62">[62]</ref>. One idea to circumvent this issue is training with synthetic data <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b61">61]</ref>. However, commonly used CNNs <ref type="bibr" target="#b39">[39]</ref> are sensitive to domain shift and generalize poorly from synthetic to real data. This issue is addressed in unsupervised domain adaptation (UDA) by adapting the network trained with source (synthetic) data to target (real) data without access to target labels.</p><p>Previous UDA methods mostly evaluated their contributions using a DeepLabV2 <ref type="bibr" target="#b6">[6]</ref> or FCN8s <ref type="bibr" target="#b48">[48]</ref> network architecture with ResNet <ref type="bibr" target="#b25">[25]</ref> or VGG <ref type="bibr" target="#b64">[64]</ref> backbone in order to be comparable to previously published works. However, even their strongest architecture (DeepLabV2+ResNet101) is outdated in the area of supervised semantic segmentation. For instance, it only achieves a supervised performance of 65 mIoU <ref type="bibr" target="#b72">[72]</ref> on Cityscapes while recent networks reach up to 85 mIoU <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b92">91]</ref>. Due to the large performance gap, it stands to question whether using outdated network architectures can limit the overall performance of UDA and can also misguide the benchmark progress of UDA. In order to answer this question, this work studies the influence of the network architecture for UDA, compiles a more sophisticated architecture, and successfully applies it to UDA with a few simple, yet crucial training strategies. Naively using a more powerful network architecture for UDA might be suboptimal as it can be more prone to overfitting to the source domain. Based on a study of different semantic segmentation architectures evaluated in a UDA setting, we compile DAFormer, a network architecture tailored for UDA (Sec 3.2). It is based on recent Transformers <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b88">87]</ref>, which have been shown to be more robust than the predominant CNNs <ref type="bibr" target="#b2">[3]</ref>. We combine them with a context-aware multilevel feature fusion, which further enhances the UDA performance. To the best of our knowledge, DAFormer is the first work to reveal the significant potential of Transformers for UDA semantic segmentation.</p><p>Since more complex and capable architectures are more prone to adaptation instability and overfitting to the source domain, in this work, we introduce three training strategies to UDA to address these issues (Sec. <ref type="bibr">3.3)</ref>. First, we propose Rare Class Sampling (RCS) to consider the long-tail distribution of the source domain, which hinders the learning of rare classes, especially in UDA due to the confirmation bias of self-training toward common classes. By frequently sampling images with rare classes, the network can learn them more stably, which improves the quality of pseudo-labels and reduces the confirmation bias. Second, we propose a Thing-Class ImageNet Feature Distance (FD), which distills knowledge from diverse and expressive Im-ageNet features in order to regularize the source training. This is particularly helpful as the source domain is limited to only a few instances of certain classes (low diversity), which have a different appearance than the target domain (domain shift). Without FD this would result in learning less expressive and source-domain-specific features. As Im-ageNet features were trained for thing-classes, we restrict the FD to regions of the image that are labeled as a thingclass. And third, we introduce learning rate warmup <ref type="bibr" target="#b23">[23]</ref> newly to UDA. By linearly increasing the learning rate up to the intended value in the early training, the learning process is stabilized and features from ImageNet pretraining can be better transferred to semantic segmentation. DAFormer outperforms previous methods by a large margin (see <ref type="figure" target="#fig_0">Fig. 1</ref>) supporting our hypothesis that the network architecture and appropriate training strategies play an important role for UDA. On GTA?Cityscapes, we improve the mIoU from 57.5 <ref type="bibr" target="#b95">[94]</ref> to 68.3 and on Synthia?Cityscapes from 55.5 <ref type="bibr" target="#b95">[94]</ref> to <ref type="bibr">60.9</ref>. In particular, DAFormer learns even difficult classes that previous methods struggled with. For instance, we improve the class train from 16 to 65 IoU, truck from 49 to 75 IoU, and bus from 59 to 78 IoU on GTA?Cityscapes. Overall, DAFormer represents a major advance in UDA. Our framework can be trained in one stage on a single consumer RTX 2080 Ti GPU within 16 hours, which simplifies its usage compared to previous methods such as ProDA <ref type="bibr" target="#b95">[94]</ref>, which requires training multiple stages on four V100 GPUs for several days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Image Segmentation Since the introduction of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b39">[39]</ref> for semantic segmentation by Long et al. <ref type="bibr" target="#b48">[48]</ref>, they have been dominating the field. Typically, semantic segmentation networks follow an encoder-decoder design <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b60">60]</ref>. To overcome the problem of the low spatial resolution at the bottleneck, remedies such as skip connections <ref type="bibr" target="#b60">[60]</ref>, dilated convolutions <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b91">90]</ref>, or resolution preserving architectures <ref type="bibr" target="#b66">[66]</ref> were proposed. Further improvements were achieved by harnessing context information, for instance using pyramid pooling <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b97">96]</ref> or attention modules <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b83">82,</ref><ref type="bibr" target="#b92">91</ref>]. Inspired by the success of the attention-based Transformers <ref type="bibr" target="#b75">[75]</ref> in natural language processing, they were adapted to image classification <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b70">70]</ref> and semantic segmentation <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b98">97]</ref> achieving state-of-the-art results. For image classification, CNNs were shown to be sensitive to distribution shifts such as image corruptions <ref type="bibr" target="#b28">[28]</ref>, adversarial noise <ref type="bibr" target="#b67">[67]</ref>, or domain shifts <ref type="bibr" target="#b27">[27]</ref>. Recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57]</ref> show that Transformers are more robust than CNNs with respect to these properties. While CNNs focus on textures <ref type="bibr" target="#b20">[20]</ref>, Transformers put more importance on the object shape <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">55]</ref>, which is more similar to human vision <ref type="bibr" target="#b20">[20]</ref>. For semantic segmentation, ASPP <ref type="bibr" target="#b7">[7]</ref> and skip connections <ref type="bibr" target="#b60">[60]</ref> were reported to increase the robustness <ref type="bibr" target="#b36">[36]</ref>. Further, Xie et al. <ref type="bibr" target="#b88">[87]</ref> showed that their Transformer-based architecture improves the robustness over CNN-based networks. To the best of our knowledge, the influence of recent network architectures on the UDA performance of semantic segmentation has not been systematically studied yet.</p><p>Unsupervised Domain Adaptation (UDA) UDA methods can be grouped into adversarial training and selftraining approaches. Adversarial training methods aim to align the distributions of source and target domain at input <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30]</ref>, feature <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b72">72]</ref>, output <ref type="bibr" target="#b72">[72,</ref><ref type="bibr" target="#b76">76]</ref>, or patch level <ref type="bibr" target="#b73">[73]</ref> in a GAN framework <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b22">22]</ref>. Using multiple scales <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b72">72]</ref> or category information <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b85">84]</ref> for the discriminator can refine the alignment. In self-training, the network is trained with pseudo-labels <ref type="bibr" target="#b40">[40]</ref> for the target domain. Most of the UDA methods pre-compute the pseudolabels offline, train the model, and repeat the process <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b90">89,</ref><ref type="bibr" target="#b101">100,</ref><ref type="bibr" target="#b102">101]</ref>. Alternatively, pseudo-labels can be calculated online during the training. In order to avoid training instabilities, pseudo-label prototypes <ref type="bibr" target="#b95">[94]</ref> or consistency regularization <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b69">69]</ref> based on data augmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b54">54]</ref> or domain-mixup <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b100">99]</ref> are used. Several methods also combine adversarial and self-training <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b78">78]</ref>, train with auxiliary tasks <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b80">79]</ref>, or perform test-time UDA <ref type="bibr" target="#b81">[80]</ref>.</p><p>Datasets are often imbalanced and follow a longtail distribution, which biases models toward common classes <ref type="bibr" target="#b84">[83]</ref>. Strategies to address this problem are re-sampling <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b86">85]</ref>, loss re-weighting <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b63">63]</ref>, and transfer learning <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b45">45]</ref>. Also in UDA, re-weighting <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b101">100]</ref> and class-balanced sampling for image classification <ref type="bibr" target="#b58">[58]</ref> were applied. We extend class-balanced sampling from classification to semantic segmentation and propose Rare Class Sampling, which addresses the co-occurrence of rare and common classes in a single semantic segmentation sample. Further, we demonstrate that re-sampling is particularly effective to train Transformers for UDA.</p><p>Li et al. <ref type="bibr" target="#b43">[43]</ref> have shown that knowledge distillation <ref type="bibr" target="#b29">[29]</ref> from an old task can act as a regularizer for a new task. This concept was successfully deployed with ImageNet features to semi-supervised learning <ref type="bibr" target="#b32">[32]</ref> and adversarial UDA <ref type="bibr" target="#b9">[9]</ref>. We apply this idea to self-training, show that it is particularly beneficial for Transformers, and improve it by restricting the Feature Distance to image regions with thingclasses <ref type="bibr" target="#b3">[4]</ref> as ImageNet mostly labels thing-classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Training (ST) for UDA</head><p>First, we will give an overview over our baseline UDA method for evaluating different network architectures. In UDA, a neural network g ? is trained using source domain images</p><formula xml:id="formula_0">X S = {x (i) S } N S i=1 and one-hot labels Y S = {y (i) S } N S i=1</formula><p>in order to achieve a good performance on target images</p><formula xml:id="formula_1">X T = {x (i) T } N T i=1</formula><p>without having access to the target labels Y T . Naively training the network g ? with a categorical cross-entropy (CE) loss on the source domain</p><formula xml:id="formula_2">L (i) S = ? H?W j=1 C c=1 y (i,j,c) S log g ? (x (i) S ) (j,c)<label>(1)</label></formula><p>usually results in a low performance on target images as the network does not generalize well to the target domain.</p><p>To address the domain gap, several strategies have been proposed that can be grouped into adversarial training <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b78">78]</ref> and self-training (ST) <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b95">94,</ref><ref type="bibr" target="#b101">100]</ref> approaches. In this work, we use ST as adversarial training is known to be less stable and is currently outperformed by ST methods <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b95">94]</ref>. To better transfer the knowledge from the source to the target domain, ST approaches use a teacher network h ? (which we will describe later) to produce pseudo-labels for the target domain data</p><formula xml:id="formula_3">p (i,j,c) T = [c = arg max c h ? (x (i) T ) (j,c ) ] ,<label>(2)</label></formula><p>where [?] denotes the Iverson bracket. Note that no gradients will be backpropagated into the teacher network. Additionally, a quality / confidence estimate is produced for the pseudo-labels. Here, we use the ratio of pixels exceeding a threshold ? of the maximum softmax probability <ref type="bibr" target="#b71">[71]</ref> q (i)</p><formula xml:id="formula_4">T = H?W j=1 [max c h ? (x (i) T ) (j,c ) &gt; ? ] H ? W .<label>(3)</label></formula><p>The pseudo-labels and their quality estimates are used to additionally train the network g ? on the target domain</p><formula xml:id="formula_5">L (i) T = ? H?W j=1 C c=1 q (i) T p (i,j,c) T log g ? (x (i) T ) (j,c) . (4)</formula><p>The pseudo-labels can be generated either online <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b100">99]</ref> or offline <ref type="bibr" target="#b90">[89,</ref><ref type="bibr" target="#b101">100,</ref><ref type="bibr" target="#b102">101]</ref>. We opted for online ST due to its less complex setup with only one training stage. This is important as we compare and ablate various network architectures. In online ST, h ? is updated based on g ? during the training. Commonly, the weights h ? are set as the exponentially moving average of the weights of g ? after each training step t <ref type="bibr" target="#b69">[69]</ref> to increase the stability of the predictions</p><formula xml:id="formula_6">? t+1 ? ?? t + (1 ? ?)? t .<label>(5)</label></formula><p>ST has been shown to be particularly efficient if the student network g ? is trained on augmented target data, while the teacher network h ? generates the pseudo-labels using non-augmented target data for semi-supervised learning <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b69">69]</ref> and unsupervised domain adaptation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b71">71]</ref> In this work, we follow DACS <ref type="bibr" target="#b71">[71]</ref> and use color jitter, Gaussian blur, and ClassMix <ref type="bibr" target="#b56">[56]</ref> as data augmentations to learn more domain-robust features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DAFormer Network Architecture</head><p>Previous UDA methods mostly evaluate their contributions using a (simplified) DeepLabV2 network architecture <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b72">72]</ref>, which is considered to be outdated. For that reason, we compile a network architecture that is tailored for UDA to not just achieve good supervised performance but also provide good domain-adaptation capabilities.</p><p>For the encoder, we aim for a powerful yet robust network architecture. We hypothesize that robustness is an important property in order to achieve good domain adaptation performance as it fosters the learning of domain-invariant features. Based on recent findings <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57]</ref> and an architecture comparison for UDA, which we will present in Sec. 4.2, Transformers <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b70">70]</ref> are a good choice for UDA as they fulfill these criteria. Although the self-attention from Transformers <ref type="bibr" target="#b75">[75]</ref> and the convolution both perform a weighted sum, their weights are computed differently: in CNNs, the weights are learned during training but fixed during testing; in the self-attention mechanism, the weights are dynamically computed based on the similarity or affinity between every pair of tokens. As a consequence, the selfsimilarity operation in the self-attention mechanism provides modeling means that are potentially more adaptive and general than convolution operations.</p><p>In particular, we follow the design of Mix Transformers (MiT) <ref type="bibr" target="#b88">[87]</ref>, which are tailored for semantic segmentation. The image is divided into small patches of a size of 4?4 (instead of 16?16 as in ViT <ref type="bibr" target="#b15">[15]</ref>) in order to preserve  <ref type="figure">Figure 2</ref>. Overview of our UDA framework with Rare Class Sampling, Thing-Class Feature Distance, and DAFormer network. details for semantic segmentation. To cope with the high feature resolution, sequence reduction <ref type="bibr" target="#b82">[81]</ref> is used in the self-attention blocks. The transformer encoder is designed to produce multi-level feature maps</p><formula xml:id="formula_7">F i ? R H 2 i+1 ? W 2 i+1 ?Ci .</formula><p>The downsampling of the feature maps is implemented by overlapping patch merging <ref type="bibr" target="#b88">[87]</ref> to preserve local continuity.</p><p>Previous works on semantic segmentation with Transformer backbones usually exploit only local information for the decoder <ref type="bibr" target="#b82">[81,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b98">97]</ref>. In contrast, we propose to utilize additional context information in the decoder as this has been shown to increase the robustness of semantic segmentation <ref type="bibr" target="#b36">[36]</ref>, a helpful property for UDA. Instead of just considering the context information of the bottleneck features <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref>, DAFormer uses the context across features from different encoder levels as the additional earlier features provide valuable low-level concepts for semantic segmentation at a high resolution, which can also provide important context information. The architecture of the DAFormer decoder is shown in <ref type="figure">Fig. 2 (b)</ref>. Before the feature fusion, we embed each F i to the same number of channels C e by a 1?1 convolution, bilinearly upsample the features to the size of F 1 , and concatenate them. For the context-aware feature fusion, we use multiple parallel 3?3 depthwise separable convolutions <ref type="bibr" target="#b11">[11]</ref> with different dilation rates <ref type="bibr" target="#b91">[90]</ref> and a 1?1 convolution to fuse them, similar to ASPP [7] but without global average pooling. In contrast to the original use of ASPP <ref type="bibr" target="#b7">[7]</ref>, we do not only apply it to the bottleneck features F 4 but use it to fuse all stacked multi-level features. Depthwise separable convolutions have the advantage that they have a lower number of parameters than regular convolutions, which can reduce overfitting to the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Strategies for UDA</head><p>One challenge of training a more capable architecture for UDA is overfitting to the source domain. To circumvent this issue, we introduce three strategies to stabilize and regularize the UDA training: Rare Class Sampling, Thing-Class ImageNet Feature Distance, and learning rate warmup. The overall UDA framework is shown in <ref type="figure">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a).</head><p>Rare Class Sampling (RCS) Even though our more capable DAFormer is able to achieve better performance on difficult classes than other architectures, we observed that the UDA performance for classes that are rare in the source dataset varies significantly over different runs. Depending on the random seed of the data sampling order, these classes are learned at different iterations of the training or sometimes not at all as we will show in Sec. 4.4. The later a certain class is learned during the training, the worse is its performance at the end of the training. We hypothesize that if relevant samples containing rare classes only appear late in the training due to randomness, the network only starts to learn them later, and more importantly, it is highly possible that the network has already learned a strong bias toward common classes making it difficult to 're-learn' new concepts with very few samples. This is further reinforced as the bias is confirmed by ST with the teacher network.</p><p>To address this, we propose Rare Class Sampling (RCS). It samples images with rare classes from the source domain more often in order to learn them better and earlier. The frequency f c of each class c in the source dataset can be calculated based on the number of pixels with class c</p><formula xml:id="formula_8">f c = N S i=1 H?W j=1 [y (i,j,c) S ] N S ? H ? W .<label>(6)</label></formula><p>The sampling probability P (c) of a certain class c is defined as a function of its frequency f c</p><formula xml:id="formula_9">P (c) = e (1?fc)/T C c =1 e (1?f c )/T .<label>(7)</label></formula><p>Therefore, classes with a smaller frequency will have a higher sampling probability. The temperature T controls the smoothness of the distribution. A higher T leads to a more uniform distribution, a lower T to a stronger focus on rare classes with a small f c . For each source sample, a class is sampled from the probability distribution c ? P and an image is sampled from the subset of data containing this class x S ? uniform(X S,c ). Eq. 7 allows to over-sample images containing rare classes (P (c) ? 1/C if f c is small). As a rare class (small f c ) usually co-occurs with multiple common classes (large f c ) in a single image, it is beneficial to sample rare classes more often than common classes (P (c rare ) &gt; P (c common )) to get closer to a balance of the re-sampled classes. For example, the common class road co-occurs with rare classes such as bus, train, or motorcycle and is therefore already covered when sampling images with these rare classes. When decreasing T , more pixels of classes with small f c are sampled but also fewer pixels of classes with medium f c . The temperature T is chosen to reach a balance of the number of re-sampled pixels of classes with small and medium f c by maximizing the number of re-sampled pixels of the class with the least.</p><p>Thing-Class ImageNet Feature Distance (FD) Commonly, the semantic segmentation model g ? is initialized with weights from ImageNet classification to start with meaningful generic features. Given that ImageNet also contains real-world images from some of the relevant high-level semantic classes, which UDA often struggles to distinguish such as train or bus, we hypothesize that the ImageNet features can provide useful guidance beyond the usual pretraining. In particular, we observe that the DAFormer network is able to segment some of the classes at the beginning of the training but forgets them after a few hundred training steps as we will show in Sec. 4.5. Therefore, we assume that the useful features from ImageNet pretraining are corrupted by L S and the model overfits to the synthetic source data. In order to prevent this issue, we regularize the model based on the Feature Distance (FD) of the bottleneck features F ? of the semantic segmentation UDA model g ? and the bottleneck features F ImageNet of the ImageNet model</p><formula xml:id="formula_10">d (i,j) = ||F ImageN et (x (i) S ) (j) ? F ? (x (i) S ) (j) || 2 . (8)</formula><p>However, the ImageNet model is mostly trained on thing-classes (objects with a well-defined shape such as car or zebra) instead of stuff-classes (amorphous background regions such as road or sky) <ref type="bibr" target="#b3">[4]</ref>. Therefore, we calculate the FD loss only for image regions containing thing-classes C things described by the binary mask M things</p><formula xml:id="formula_11">L (i) FD = H F ?W F j=1 d (i,j) ? M (i,j) things j M (i,j) things ,<label>(9)</label></formula><p>This mask is obtained from the downscaled label y S,small</p><formula xml:id="formula_12">M (i,j) things = C c =1 y i,j,c S,small ? [c ? C things ] .<label>(10)</label></formula><p>To downsample the label to the bottleneck feature size, average pooling with a patch size H H F ? W W F is applied to each class channel and a class is kept when it exceeds the ratio r The overall UDA loss L is the weighted sum of the presented loss components L = L S + L T + ? FD L FD .</p><p>Learning Rate Warmup for UDA Linearly warming up the learning rate <ref type="bibr" target="#b23">[23]</ref> at the beginning of the training has successfully been used to train both CNNs <ref type="bibr" target="#b25">[25]</ref> and Transformers <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b75">75]</ref> as it improves network generalization <ref type="bibr" target="#b23">[23]</ref> by avoiding that a large adaptive learning rate variance distorts the gradient distribution at the beginning of the training <ref type="bibr" target="#b46">[46]</ref>. We newly introduce learning rate warmup to UDA. We posit that this is particularly important for UDA as distorting the features from ImageNet pretraining would deprive the network of useful guidance toward the real domain. During the warmup period up to iteration t warm , the learning rate at iteration t is set ? t = ? base ? t/t warm .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Datasets For the target domain, we use the Cityscapes street scene dataset <ref type="bibr" target="#b13">[13]</ref> containing 2975 training and 500 validation images with resolution 2048?1024. For the source domain, we use either the GTA dataset <ref type="bibr" target="#b59">[59]</ref>, which contains 24,966 synthetic images with resolution 1914?1052, or the Synthia dataset <ref type="bibr" target="#b61">[61]</ref>, which consists of 9,400 synthetic images with resolution 1280?760. As a common practice in UDA <ref type="bibr" target="#b72">[72]</ref>, we resize the images to 1024?512 pixels for Cityscapes and to 1280?720 pixels for GTA. Network Architecture Our implementation is based on the mmsegmentation framework <ref type="bibr" target="#b12">[12]</ref>. For the DAFormer architecture, we use the MiT-B5 encoder <ref type="bibr" target="#b88">[87]</ref>, which produces a feature pyramid with C = <ref type="bibr" target="#b64">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">320,</ref><ref type="bibr">512]</ref>. The DAFormer decoder uses C e = 256 and dilation rates of 1, 6, 12, and 18. All encoders are pretrained on ImageNet-1k.</p><p>Training In accordance with <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b88">87]</ref>, we train DAFormer with AdamW <ref type="bibr" target="#b49">[49]</ref>, a learning rate of ? base =6?10 ?5 for the encoder and 6?10 ?4 for the decoder, a weight decay of 0.01, linear learning rate warmup with t warm =1.5k, and linear decay afterwards. It is trained on a batch of two 512?512 random crops for 40k iterations. Following DACS <ref type="bibr" target="#b71">[71]</ref>, we use the same data augmentation parameters and set ?=0.99 and ? =0.968. The RCS temperature is set T =0.01 to maximize the sampled pixels of the class with the least pixels. For FD, r=0.75 and ? FD =0.005 to induce a similar gradient magnitude into the encoder as L S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison of Network Architectures for UDA</head><p>First, we compare several semantic segmentation architectures with respect to their UDA performance (see Sec. 3.1) on GTA?Cityscapes in Tab. 1. Additionally, we also provide the performance of the networks trained only with augmented source data (domain generalization) as well as the oracle performance trained with target labels (supervised learning). In all cases, the model is evaluated on the Cityscapes validation set and the performance is provided as mIoU in %. To compare how well a network is suited for UDA, we further provide the relative performance (Rel.), which normalizes the UDA mIoU by the oracle mIoU. Note that the oracle mIoU is generally lower than reported in the literature on supervised learning as for UDA the images of Cityscapes are downsampled by a factor of two, which is a necessary common practice in UDA to fit images from both domains and additional networks into the GPU memory. The majority of works on UDA use DeepLabV2 <ref type="bibr" target="#b6">[6]</ref> with ResNet-101 <ref type="bibr" target="#b25">[25]</ref> backbone. Interestingly, a higher oracle performance does not necessarily increase the UDA performance as can be seen for DeepLabV3+ <ref type="bibr" target="#b7">[7]</ref> in Tab. 1. Generally, the studied more recent CNN architectures, do not provide a UDA performance gain over DeepLabV2. However, we identified the Transformer-based SegFormer <ref type="bibr" target="#b88">[87]</ref> as a powerful architecture for UDA. It increases the mIoU for source-only / UDA / oracle training significantly from 34.3 / 54.2 / 72.1 to 45.6 / 58.2 / 76.4. We believe that especially the better domain generalization (source-only training) of SegFormer is valuable for the improved UDA performance.</p><p>To get a better insight into why SegFormer works well for UDA, we swap its encoder and decoder with ResNet101 and DeepLabV3+. As the MiT encoder of SegFormer has an output stride of 32 but the DeepLabV3+ decoder is designed for an output stride of 8, we bilinearly upsample the SegFormer bottleneck features by ?4 when combined with the DeepLabv3+ decoder. Tab. 2 shows that the lightweight MLP decoder of SegFormer has a slightly higher relative UDA performance (Rel.) than the heavier DLv3+ decoder (76.2% vs 75.2%). However, the crucial contribution to good UDA performance comes from the Transformer MiT encoder. Replacing it with the ResNet101 encoder leads to a significant performance drop of the UDA performance. Even though the oracle performance drops as well due to  the smaller receptive field of the ResNet encoder <ref type="bibr" target="#b88">[87]</ref>, the drop for UDA is over-proportional as shown by the relative performance decreasing from 76.2% to 71.4%.</p><p>Therefore, we further investigate the influence of the encoder architecture on UDA performance. In Tab. 3, we compare different encoder designs and sizes. It can be seen that deeper models achieve a better source-only and relative performance demonstrating that deeper models generalize/adapt better to the new domain. This observation is in line with findings on the robustness of network architectures <ref type="bibr" target="#b2">[3]</ref>. Compared to CNN encoders, the MiT encoders generalize better from source-only training to the target domain. Overall, the best UDA mIoU is achieved by the MiT-B5 encoder. To gain insights on the improved generalization, <ref type="figure" target="#fig_2">Fig. 3</ref> visualizes the ImageNet features of the target domain. Even though ResNet structures stuff-classes slightly better, MiT shines at separating semantically similar classes (e.g. all vehicle classes), which are usually particularly difficult to adapt. A possible explanation might be the texture-bias of CNNs and the shape-bias of Transform-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning Rate Warmup</head><p>Tab. 4 shows that learning rate warmup significantly improves both UDA and oracle performance. UDA benefits even more than supervised learning from warmup (see column Rel.), showing its particular importance for UDA by stabilizing the beginning of the training, which improves difficult classes (cf. row 1 and 2 in <ref type="figure" target="#fig_6">Fig 6)</ref>. As warmup is essential for a good UDA performance across different architectures, it was already applied in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Rare Class Sampling (RCS)</head><p>When training SegFormer for UDA, we observe that the performance of some classes depends on the random seed for data sampling as can be seen for the blue IoU curves  <ref type="table" target="#tab_3">91 56 87 32 30 40 51 51 89 49 92 62 6 90 60 53 0 20 25  89 50 88 46 44 43 53 55 90 51 93 64 9 91 77 63 0 47 50  95 65 89 50 43 42 54 60 89 47 93 69 30 92 77 70 27 57 63  88 48 88 49 44 41 54 57 90 51 93 66 7 92 73 69 43 53 64  93 62 89 53 44 43 55 61 89 47 93 71 42 92 74 75 62</ref>   in <ref type="figure">Fig. 4</ref>. The affected classes are underrepresented in the source dataset as shown in the supplement. Interestingly, the IoU for the class bicycle starts increasing at different iterations for different seeds. We hypothesize that this is caused by the sampling order, in particular when the relevant rare classes are sampled. Further, the later the IoU starts improving, the worse is the final IoU of this class, probably due to the confirmation bias of self-training that was accumulated over earlier iterations. Therefore, for UDA, it is especially important to learn rare classes early.</p><p>In order to address this issue, the proposed RCS increases the sampling probability of rare classes. <ref type="figure">Fig. 4</ref> (orange) shows that RCS results in an earlier increase of the IoU of rider/bicycle and a higher final IoU independent of the data sampling random seed. This confirms our hypothesis that an (early) sampling of rare classes is important for learning these classes properly. RCS improves the UDA performance by +5.8 mIoU (cf. row 2 and 4 in Tab. 5). The highest IoU increase is observed for the rare classes rider, train, motorcycle, and bicycle (cf. row 2 and 3 in <ref type="figure" target="#fig_6">Fig. 6</ref>). RCS also outperforms its special case T = ?, which corresponds to 'class-balanced sampling' (cf. row 3 and 4 in Tab. 5), as class-balanced sampling does not consider the co-occurrence of multiple classes in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Thing-Class ImageNet Feature Distance (FD)</head><p>While RCS gives a performance boost, the performance for thing-classes (e.g. bus and train) could still be further improved as some of the object classes that are fairly well separated in ImageNet features (see <ref type="figure" target="#fig_2">Fig. 3 right)</ref> are mixed together after the UDA training. When investigating the IoU during the early training (see <ref type="figure">Fig. 5 orange)</ref>, we observe an early performance drop for the class train. We assume that the powerful MiT encoder overfits to the synthetic domain. When regularizing the training with the proposed FD, the performance drop is avoided (see <ref type="figure">Fig. 5 green)</ref>. Also other difficult classes such as bus, motorcycle, and bicycle benefit from the regularization (cf. row 2 and 4 in <ref type="figure" target="#fig_6">Fig. 6</ref>). Overall the UDA performance is improved by +3.5 mIoU (cf. row 2 and 6 in Tab. 5). Note that applying FD only to thingclasses, which the ImageNet features were trained on, is important for its good performance (cf. row 5 and 6).</p><p>When combining RCS and FD, we observe a further im-  provement to 66.2 mIoU showing that they complement each other (see row 7 in Tab. 5). We notice pseudo-label drifts originating from image rectification artifacts and the ego car. As these regions are not part of the street scene segmentation task, we argue that it is not meaningful to produce pseudo-labels for them. Therefore, we ignore the top 15 and bottom 120 pixels of the pseudo-label. As Transformers are more expressive, we further increase ? to 0.999 to introduce a stronger regularization from the teacher. This mitigates the pseudo-label drifts and improves the UDA mIoU (cf. row 7 and 8 in Tab. 5). The overall improvement is +15.2 mIoU for SegFormer (cf. row 1 and 8) and +6.9 mIoU for DeepLabV2 (cf. row 9 and 10). When comparing both architectures, it can be seen that SegFormer benefits noticeably more, supporting our initial hypothesis that the architecture choice can limit the effectiveness of UDA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">DAFormer Decoder</head><p>After regularizing and stabilizing the UDA training for a MiT encoder and a SegFormer decoder, we come back to the network architecture and investigate our DAFormer decoder with the context-aware feature fusion. Tab. 7 shows that it improves the UDA performance over the SegFormer decoder from 67.0 to 68.3 mIoU (cf. row 1 and 8). Further, DAFormer outperforms a variant without depthwise separable convolutions (cf. last two rows) and a variant with ISA <ref type="bibr" target="#b35">[35]</ref> instead of ASPP for feature fusion (cf. row <ref type="bibr">5 and 8)</ref>. This shows that a capable but parameter-effective decoder with an inductive bias of the dilated depthwise separable convolutions is beneficial for good UDA performance. When the context is only considered for bottleneck features, the UDA performance decreases by -1.3 mIoU (cf. row 6 and 8), revealing that the context clues from different encoder stages used in DAFormer are more domain-robust. We further compare DAFormer to UperNet <ref type="bibr" target="#b87">[86]</ref>, which iteratively upsamples and fuses the features and was used together with Transformers in <ref type="bibr" target="#b47">[47]</ref>. Even though UperNet achieves the best oracle performance, it is noticeably outperformed by DAFormer on UDA, which confirms that it is necessary to study and design the decoder architecture, along with the encoder architecture, specifically for UDA.</p><p>Tab. 6 shows that DAFormer outperforms previous methods by a significant margin. On GTA?Cityscapes, it improves the performance from 57.5 to 68.3 mIoU and on Synthia?Cityscapes from 55.5 to 60.9 mIoU. In particular, DAFormer learns even difficult classes well, which previous methods struggled with such as train, bus, and truck.</p><p>Further details are provided in the supplement, including RCS statistics, parameter sensitivity of RCS/FD, ablation of ST, a runtime and GPU memory benchmark, a comprehensive qualitative analysis, and a discussion of limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We presented DAFormer, a network architecture tailored for UDA, which is based on a Transformer encoder and a context-aware fusion decoder, revealing the potential of Transformers for UDA. Additionally, we introduced three training policies to stabilize and regularize UDA, further enabling the capabilities of DAFormer. Overall, DAFormer represents a major advance in UDA and improves the SOTA performance by 10.8 mIoU on GTA?Cityscapes and 5.4 mIoU on Synthia?Cityscapes. We would like to highlight the value of DAFormer by superseding DeepLabV2 to evaluate UDA methods on a much higher performance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>The supplementary material provides further details on DAFormer as well as additional experimental results and analysis. In particular, Sec. B provides further implementation details, Sec. C discusses the class statistics of sampling with and without RCS, Sec. D provides an ablation of the training strategies with DeepLabV2, Sec. E studies the parameter sensitivity of RCS and FD, Sec. F ablates the UDA self-training, Sec. G compares the runtime and memory consumption, Sec. H analyzes example predictions, Sec. I compares DAFormer with additional previous UDA methods, and Sec. J discusses limitations of DAFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Source Code and Further Details</head><p>The source code to reproduce DAFormer and all ablation studies is provided at https://github.com/ lhoyer / DAFormer. Please, refer to the contained README.md for further information such as the environment and dataset setup.</p><p>For the distinction of thing-and stuff-classes, we follow the definition by Caesar et al. <ref type="bibr" target="#b3">[4]</ref>. Applied to Cityscapes, thing-classes are traffic light, traffic sign, person, rider, car, truck, bus, train, motorcycle, and bicycle and stuff-classes are road, sidewalk, building, wall, fence, pole, vegetation, terrain, and sky.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Rare Class Sampling Statistics</head><p>Most (real-world) datasets have an imbalanced class distribution. This is also the case for the used source datasets as can be seen in <ref type="figure" target="#fig_0">Fig. S1</ref> in the blue bars. Please note that the y-axis is scaled logarithmically so that the rare classes are still visible. This problem is addressed by RCS by sampling images with rare classes more often as discussed in Sec. 3.3 of the main paper. Therefore, more pixels of the re-sampled images belong to the rare classes as can be seen in the orange bars of <ref type="figure" target="#fig_0">Fig. S1</ref>, which directly results in a significantly improved IoU for these rare classes as can be seen in <ref type="figure" target="#fig_6">Fig. 6</ref> of the main paper. The RCS temperature T = 0.01 is chosen to (approximately) maximize the number of resampled pixels of the class with the least re-sampled pixels as described in Sec. 3.3 of the main paper. This strategy results in a balance of the number of re-sampled pixels of the classes with the least re-sampled pixels as can be seen in the orange bars of <ref type="figure" target="#fig_0">Fig. S1a</ref> for the classes traffic light and bicycle.</p><p>Further, <ref type="figure">Fig. S2</ref> shows the RCS class sampling probabilities P (c) from Eq. 7 in the main paper for the default RCS temperature T = 0.01. It can be seen that the class sampling probability for rare classes is higher than for common classes as expected. For some common classes such as  <ref type="figure" target="#fig_0">Figure S1</ref>. Class statistics of the corresponding dataset for 10k samples. Note that the y-axis is scaled logarithmically. RCS samples images with rare classes more often than random sampling.  <ref type="figure">Figure S2</ref>. RCS class sampling probability P (c) from Eq. 7 in the main paper with an RCS temperature of T = 0.01.</p><p>road and sky, P (c) is very close to zero. As these common classes are part of almost every image, it is not necessary to specifically sample images containing these particular common classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Strategies for DeepLabV2</head><p>Tab. S1 shows the performance of the improved UDA training strategies for the DeepLabV2 architecture in addition to the SegFormer architecture from the main paper. It demonstrates that learning rate warmup, RCS, FD, and their combination are all beneficial for DeepLabV2 as well. However, the performance improvement for SegFormer is significantly larger than for DeepLabV2, supporting our hypothesis that the network architecture is crucial for UDA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameter Sensitivity of RCS and FD</head><p>To analyze the sensitivity of the parameters of FD and RCS, Tab. S2 shows a study of T for RCS and ? FD for FD. It can be seen that RCS is stable up to a deviation of factor 5 from the default value. For FD, the weighting is stable up to a factor of about 2. Given that both default values are chosen according to an intuitive strategy (maximization of re-sampled pixels for the class with the least re-sampled pixels for T and gradient magnitude balance for ? FD ), the robust range of the hyperparameters is sufficient to select a good value according to the described strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation of Self-Training</head><p>As FD and RCS operate on source data, they can also be used to improve the domain generalization ability of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Runtime and Memory Consumption</head><p>DAFormer can be trained on a single RTX 2080 Ti GPU within 16 hours (0.7 it/s) while requiring about 9.6 GB GPU memory during training. It has a throughput of 8.7 img/s for inference. In Tab. S4, DAFormer is compared with other network architectures and UDA methods with respect to runtime and memory consumption. Even though DAFormer is heavier than DeepLabV2 and SegFormer, it requires only 12% more GPU memory and about 30% more training/inference time than DeepLabV2 when the same UDA configuration is used. When ablating the proposed RCS and FD, the GPU memory consumption is further reduced by 54% and the training time is decreased by 36% mainly due to the additional ImageNet encoder and the feature distance calculation. However, as this is only relevant for training, the inference throughput is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Qualitative Analysis</head><p>Comparison with ProDA The better performance of DAFormer compared to ProDA <ref type="bibr" target="#b95">[94]</ref> is also reflected in example predictions shown in column 4 and 5 of <ref type="figure">Fig. S4-S9</ref>. The major improvements come from a better recognition of the classes train <ref type="figure">(Fig. S4)</ref>, bus <ref type="figure">(Fig. S5)</ref>, truck <ref type="figure" target="#fig_6">(Fig. S6)</ref>,  <ref type="figure" target="#fig_2">Figure S3</ref>. Different annotation policies for bicycle on Cityscapes and GTA. It can be seen that the entire wheel is segmented as bicycle for Cityscapes, while only the tire and spokes of the wheel are segmented as bicycle for GTA. car <ref type="figure">(Fig. S7</ref>), and sidewalk ( <ref type="figure">Fig. S8</ref>) across different perspectives, object sizes, and appearances. <ref type="figure">Fig. S9</ref> also shows a better recognition of rider, bicycle, and fence. Furthermore, in the shown examples, it can generally be seen that DAFormer better segments fine structures, which is especially beneficial for small classes such as pole, traffic sign, and traffic light.</p><p>Domain Generalization Additionally, column 2 and 3 of <ref type="figure">Fig. S4-S9</ref> compare the source-only training of DeepLabV2 and SegFormer. It can be seen that SegFormer better generalizes from the source training to the target domain than DeepLabV2. Still, there is a considerable gap between Seg-Former with source-only training and DAFormer, showing that the adaptation to the target domain is essential.</p><p>Error Cases To give additional insights into the limitations of DAFormer, <ref type="figure" target="#fig_0">Fig. S10</ref>-S14 show some of the typical error cases. This includes confusion of sidewalk and road if the texture is similar or there are cycle path markings ( <ref type="figure" target="#fig_0">Fig. S10)</ref>, confusion of wall and fence <ref type="figure" target="#fig_0">(Fig. S11)</ref>, misclassification of some special vans <ref type="figure" target="#fig_0">(Fig. S12)</ref>, misclassification of partly-occluded busses <ref type="figure" target="#fig_0">(Fig. S13)</ref>, misclassification of persons close to bikes <ref type="figure" target="#fig_0">(Fig. S14 top)</ref>, misclassification of standing riders <ref type="figure" target="#fig_0">(Fig. S14 bottom)</ref>, and segmentation of only the bicycle tires ( <ref type="figure" target="#fig_0">Fig. S14 bottom)</ref>. That only the tires and not the inside of the wheel of a bicycle are segmented is caused by the annotation policy of GTA (see <ref type="figure" target="#fig_2">Fig. S3</ref>). Note that for many of the error cases, also previous methods such as ProDA experience similar issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Comparison with Previous Methods</head><p>In the main paper, we compare DAFormer with a selection of representative UDA methods. However, vari-ous other UDA methods were proposed in the last few years. A comprehensive comparison with these is shown in Tab. S5 for GTA?Cityscapes and in Tab. S6 for Synthia?Cityscapes. On the one side, some of the newly shown methods can achieve a higher IoU for specific classes than the previous methods shown in the main paper, but on the other side, their performance suffers for other classes. Therefore, ProDA <ref type="bibr" target="#b95">[94]</ref> still achieves the best mIoU of the previous state-of-the-art methods. Overall, DAFormer is able to outperform all previous works both in mIoU and classwise IoU for GTA?Cityscapes, often by a considerable margin. On Synthia?Cityscapes, this statement holds except for the stuff-classes road, sidewalk, vegetation, and sky. This might be due to the shape-bias of Transformers <ref type="bibr" target="#b2">[3]</ref>, which causes the network to focus more on shape than texture. The shape bias could improve the generalization ability for thing-classes as their shape is more domainrobust than their texture. However, for stuff-classes, the texture is sometimes crucial to distinguish similar classes such as road and sidewalk and a shape-bias could be hindering.</p><p>The   Context-aware fusion assumes that context correlations are domain-invariant. This is often the case for the typical UDA benchmarks <ref type="bibr" target="#b100">[99]</ref>. However, this assumption can break down for some special cases in other domains, where the context misleads the model (e.g. misclassification of a cow on road as a horse) <ref type="bibr" target="#b34">[34]</ref>.</p><p>RCS is designed to counter a long-tail data distribution of the source dataset. It is unproblematic for RCS if a class is more common in the target dataset than in the source dataset (e.g. bicycle) as it is balanced by RCS on the source domain and regularly sampled on the target domain. If a class is extremely rare in the target dataset, it might happen that this class is not sampled often enough for efficient adaptation. Therefore, the pseudo-labels would not contain this class and, conceptually, it would not be possible to specifically select samples with this class from the target data.</p><p>As shown in the error cases in Sec. H, our method struggles with differences in the annotation policy between source training data and target evaluation data. One example is the bicycle wheel. While the entire wheel is segmented in Cityscapes (see <ref type="figure" target="#fig_2">Fig. S3a</ref>), only the tires and spokes are segmented in GTA (see <ref type="figure" target="#fig_2">Fig. S3b</ref>). Also, there are corner cases, where the annotation policy is not defined by source labels such as cycle paths on road (see the top row in <ref type="figure" target="#fig_0">Fig. S11</ref>) or small busses (see the bottom row in <ref type="figure" target="#fig_0">Fig. S12</ref>). In order to resolve these issues, additional information about the annotations policy and corner cases would be necessary. A potential solution might be the use of a few target training labels as studied in semi-supervised domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2. Potential Negative Impact</head><p>Our work improves the adaptability of semantic segmentation, which can be used to enable many good applications such as autonomous driving. However, UDA might also be utilized in undesired applications such as surveillance or military UAVs. This is a general problem of improving semantic segmentation algorithms. A possible countermeasure could be legal restrictions of the use cases for semantic segmentation algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Progress of UDA over time on GTA?Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y c S,small = [AvgPool(y c S , H/H F , W/W F ) &gt; r] . (11) This ensures that only bottleneck feature pixels containing a dominant thing-class are considered for the feature distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>T-SNE [74] embedding of the bottleneck features after ImageNet pre-training for ResNet101 [25] and MiT-B5 [87] on the Cityscapes val. set, showing a better vehicle separability for MiT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>SegFormer UDA performance for the rare classes rider and bicycle without and with Rare Class Sampling (RCS). SegFormer UDA performance in the beginning of the training with and without ImageNet Feature Distance (FD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>53 63    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of the class-wise IoU of Warmup (W), RCS and FD. The color visualizes the IoU difference to the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Bicycle annotation policy on Cityscapes (b) Bicycle annotation policy on GTA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S10 .Figure S11 .Figure S12 .Figure S13 .Figure S14 .</head><label>S10S11S12S13S14</label><figDesc>Typical error cases on GTA?Cityscapes: Confusion of sidewalk and road if the texture is similar. Typical error cases on GTA?Cityscapes: Confusion of wall and fence. Typical error cases on GTA?Cityscapes: Misclassification of special vans. Typical error cases on GTA?Cityscapes: Misclassification of partly-occluded busses. Typical error cases on GTA?Cityscapes: Misclassification of persons close to bikes (top row), standing riders (bottom row), and missing segmentation of the inside of the bicycle wheel (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison of the mIoU (%) on the Cityscapes val. set of different segmentation architectures for source-only (GTA), UDA (GTA?Cityscapes), and oracle (Cityscapes) training. Additionally, the relative UDA performance (Rel.) wrt. the oracle mIoU is provided. Mean and SD are calculated over 3 random seeds. ?2.2 54.2 ?1.7 72.1 ?0.5 75.2% DA Net [18] 30.9 ?2.1 53.7 ?0.2 72.6 ?0.2 74.0% ISA Net [35] 32.3 ?2.1 53.3 ?0.4 72.0 ?0.5 74.0% DeepLabV3+ [7] 31.0 ?1.4 53.7 ?1.0 75.6 ?0.9 71.0% SegFormer [87] 45.6 ?0.6 58.2 ?0.9 76.4 ?0.2 76.2% Ablation of the SegFormer encoder and decoder.</figDesc><table><row><cell>Architecture</cell><cell>Src-Only</cell><cell>UDA</cell><cell>Oracle</cell><cell>Rel.</cell></row><row><cell cols="2">DeepLabV2 [6] 34.3 Encoder Decoder</cell><cell>UDA</cell><cell>Oracle</cell><cell>Rel.</cell></row><row><cell cols="2">MiT-B5 [87] SegF. [87]</cell><cell>58.2 ?0.9</cell><cell cols="2">76.4 ?0.2 76.2%</cell></row><row><cell cols="3">MiT-B5 [87] DLv3+ [7] 56.8 ?1.8</cell><cell cols="2">75.5 ?0.5 75.2%</cell></row><row><cell>R101 [25]</cell><cell>SegF. [87]</cell><cell>50.9 ?1.1</cell><cell cols="2">71.3 ?1.3 71.4%</cell></row><row><cell>R101 [25]</cell><cell cols="2">DLv3+ [7] 53.7 ?1.0</cell><cell cols="2">75.6 ?0.9 71.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Influence of the encoder on UDA performance. Influence of learning rate warmup on UDA performance. ?0.9 76.4 ?0.2 76.2%</figDesc><table><row><cell>Enc.</cell><cell>Dec.</cell><cell cols="4">Src-Only UDA Oracle Rel.</cell></row><row><cell>R50 [25]</cell><cell>DLv2 [6]</cell><cell>29.3</cell><cell>52.1</cell><cell>70.8</cell><cell>73.6%</cell></row><row><cell>R101 [25]</cell><cell>DLv2 [6]</cell><cell>36.9</cell><cell>53.3</cell><cell>72.5</cell><cell>73.5%</cell></row><row><cell>S50 [93]</cell><cell>DLv2 [6]</cell><cell>27.9</cell><cell>48.0</cell><cell>67.7</cell><cell>70.9%</cell></row><row><cell>S101 [93]</cell><cell>DLv2 [6]</cell><cell>35.5</cell><cell>53.5</cell><cell>72.2</cell><cell>74.1%</cell></row><row><cell>S200 [93]</cell><cell>DLv2 [6]</cell><cell>35.9</cell><cell>56.9</cell><cell>73.5</cell><cell>77.4%</cell></row><row><cell cols="3">MiT-B3 [87] SegF. [87] 42.2</cell><cell>50.8</cell><cell>76.5</cell><cell>66.4%</cell></row><row><cell cols="3">MiT-B4 [87] SegF. [87] 44.7</cell><cell>57.5</cell><cell>77.1</cell><cell>74.6%</cell></row><row><cell cols="3">MiT-B5 [87] SegF. [87] 46.2</cell><cell>58.8</cell><cell>76.2</cell><cell>77.2%</cell></row><row><cell>Architecture</cell><cell cols="2">LR Warmup UDA</cell><cell></cell><cell>Oracle</cell><cell>Rel.</cell></row><row><cell>DeepLabV2 [6]</cell><cell>-</cell><cell cols="4">49.1 ?2.0 67.4 ?1.7 72.8%</cell></row><row><cell>DeepLabV2 [6]</cell><cell></cell><cell cols="4">54.2 ?1.7 72.1 ?0.5 75.2%</cell></row><row><cell cols="2">SegFormer [87] -</cell><cell cols="4">51.8 ?0.8 72.9 ?1.6 71.1%</cell></row><row><cell cols="5">SegFormer [87] 58.2 ResNet101 ImageNet Features MiT-B5 ImageNet Features</cell><cell></cell></row></table><note>road sidew. build. wall fence pole tr.light sign veget. n/a. terrain sky person rider car truck bus train m.bike bike</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation of the components of the UDA framework.</figDesc><table><row><cell cols="2">Network Warmup RCS</cell><cell>FD</cell><cell>Misc.</cell><cell>UDA</cell></row><row><cell>1 SegF. [87] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.8 ?0.8</cell></row><row><cell>2 SegF. [87]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.2 ?0.9</cell></row><row><cell>3 SegF. [87]</cell><cell cols="2">(T =?) -</cell><cell>-</cell><cell>62.0 ?1.5</cell></row><row><cell>4 SegF. [87]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>64.0 ?2.4</cell></row><row><cell>5 SegF. [87]</cell><cell>-</cell><cell cols="2">(all C) -</cell><cell>58.8 ?0.4</cell></row><row><cell>6 SegF. [87]</cell><cell>-</cell><cell></cell><cell>-</cell><cell>61.7 ?2.6</cell></row><row><cell>7 SegF. [87]</cell><cell></cell><cell></cell><cell>-</cell><cell>66.2 ?1.0</cell></row><row><cell>8 SegF. [87]</cell><cell></cell><cell></cell><cell cols="2">Crop PL, ?? 67.0 ?0.4</cell></row><row><cell>9 DLv2 [6] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.1 ?2.0</cell></row><row><cell>10 DLv2 [6]</cell><cell></cell><cell></cell><cell cols="2">Crop PL, ?? 56.0 ?0.5</cell></row></table><note>ers [3]. Before we study the context-aware fusion decoder of DAFormer in Sec. 4.6, we will first discuss how to stabi- lize the training of MiT with the default SegFormer decoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison with state-of-the-art methods for UDA. The results for DAFormer are averaged over 3 random seeds. Road S.walk Build. Wall Fence Pole Tr.Light Sign Veget. Terrain Sky Person Rider Car Truck Bus Train M.bike Bike mIoU GTA5 ? Cityscapes CBST [100] 91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9 DACS [71] 89.9 39.7 87.9 30.7 39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.2 0.0 27.3 34.0 52.1 CorDA [79] 94.7 63.1 87.6 30.7 40.6 40.2 47.8 51.6 87.6 47.0 89.7 66.7 35.9 90.2 48.9 57.5 0.0 39.8 56.0 56.6 ProDA [94] 87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5 DAFormer 95.7 70.2 89.4 53.5 48.1 49.6 55.8 59.4 89.9 47.9 92.5 72.2 44.7 92.3 74.5 78.2 65.1 55.9 61.8 68.3</figDesc><table><row><cell>Synthia ? Cityscapes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison of decoder architectures with MiT encoder and UDA improvements (DSC: depthwise separable convolution). ?0.9 76.3 ?0.4 86.9% Context only at F 4 256 3.2M 67.0 ?0.6 76.6 ?0.2 87.5% DAFormer w/o DSC 256 10.0M 67.0 ?1.5 76.7 ?0.6 87.4% DAFormer 256 3.7M 68.3 ?0.5 77.6 ?0.2 88.0%</figDesc><table><row><cell>Decoder</cell><cell cols="2">Ce #Params</cell><cell>UDA</cell><cell>Oracle</cell><cell>Rel.</cell></row><row><cell>SegF. [87]</cell><cell>768</cell><cell cols="4">3.2M 67.0 ?0.4 76.8 ?0.3 87.2%</cell></row><row><cell>SegF. [87]</cell><cell>256</cell><cell cols="4">0.5M 67.1 ?1.1 76.5 ?0.4 87.7%</cell></row><row><cell>UperNet [86]</cell><cell>512</cell><cell cols="4">29.6M 67.4 ?1.1 78.0 ?0.2 86.4%</cell></row><row><cell>UperNet [86]</cell><cell>256</cell><cell cols="4">8.3M 66.7 ?1.2 77.4 ?0.3 86.2%</cell></row><row><cell>ISA [35] Fusion</cell><cell>256</cell><cell cols="2">1.1M 66.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S1 .</head><label>S1</label><figDesc>Ablation of the components of the UDA framework for SegFormer and DeepLabV2. Mean and standard deviation are calculated over 3 random seeds.</figDesc><table><row><cell>Network</cell><cell cols="3">Warmup RCS FD</cell><cell>mIoU</cell></row><row><cell>SegF. [87]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.8 ?0.8</cell></row><row><cell>SegF. [87]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>58.2 ?0.9</cell></row><row><cell>SegF. [87]</cell><cell></cell><cell>-</cell><cell></cell><cell>61.7 ?2.6</cell></row><row><cell>SegF. [87]</cell><cell></cell><cell></cell><cell>-</cell><cell>64.0 ?2.4</cell></row><row><cell>SegF. [87]</cell><cell></cell><cell></cell><cell></cell><cell>66.2 ?1.0</cell></row><row><cell>DLv2 [6]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.1 ?2.0</cell></row><row><cell>DLv2 [6]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>54.2 ?1.7</cell></row><row><cell>DLv2 [6]</cell><cell></cell><cell>-</cell><cell></cell><cell>55.5 ?1.3</cell></row><row><cell>DLv2 [6]</cell><cell></cell><cell></cell><cell>-</cell><cell>55.7 ?0.5</cell></row><row><cell>DLv2 [6]</cell><cell></cell><cell></cell><cell></cell><cell>56.6 ?1.2</cell></row><row><cell cols="5">Table S2. Hyperparameter sensitivity study for RCS and FD. The</cell></row><row><cell cols="5">default parameters are marked with *. The evaluation setup is</cell></row><row><cell cols="5">equivalent to row 8 in Tab. 5 of the main paper.</cell></row><row><cell>RCS T</cell><cell>mIoU</cell><cell>? FD</cell><cell></cell><cell>mIoU</cell></row><row><cell>0.001</cell><cell>65.6</cell><cell>0.001</cell><cell></cell><cell>65.8</cell></row><row><cell>0.002</cell><cell>66.8</cell><cell>0.002</cell><cell></cell><cell>66.2</cell></row><row><cell>0.01*</cell><cell>66.8</cell><cell cols="2">0.005*</cell><cell>66.8</cell></row><row><cell>0.05</cell><cell>66.8</cell><cell>0.01</cell><cell></cell><cell>66.5</cell></row><row><cell>0.1</cell><cell>65.5</cell><cell>0.02</cell><cell></cell><cell>65.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S3 .</head><label>S3</label><figDesc>Ablation of UDA self-training (ST, see Sec. 3.1 in the main paper) with and without RCS and FD. Warmup is enabled in all configuration. The experiments are conducted with Seg-Former<ref type="bibr" target="#b88">[87]</ref> and complement Tab. 5 in the main paper.</figDesc><table><row><cell></cell><cell cols="4">w/o (RCS+FD) w/ (RCS+FD)</cell></row><row><cell></cell><cell>w/o ST</cell><cell>45.6?0.6</cell><cell cols="2">50.7?0.3</cell></row><row><cell></cell><cell>w/ ST</cell><cell>58.2?0.9</cell><cell cols="2">66.2?1.0</cell></row><row><cell cols="6">Table S4. Runtime and memory consumption of different network</cell></row><row><cell cols="6">architectures and UDA methods on a single RTX 2080 Ti GPU.</cell></row><row><cell cols="2">UDA Method Network</cell><cell>Training Throughput (it/s)</cell><cell>Inference Throughput (img/s)</cell><cell>Training GPU Memory</cell><cell>Num. Params</cell></row><row><cell>ST</cell><cell>DLv2 [6]</cell><cell>1.24</cell><cell cols="3">11.3 5.6 GB 43.2M</cell></row><row><cell>ST</cell><cell>SegF. [87]</cell><cell>0.95</cell><cell cols="3">8.9 7.7 GB 84.6M</cell></row><row><cell cols="2">ST+RCS+FD DLv2 [6]</cell><cell>0.91</cell><cell cols="3">11.3 8.6 GB 43.2M</cell></row><row><cell cols="2">ST+RCS+FD SegF. [87]</cell><cell>0.75</cell><cell cols="3">8.9 8.8 GB 84.6M</cell></row><row><cell cols="2">ST+RCS+FD DAFormer</cell><cell>0.71</cell><cell cols="3">8.7 9.6 GB 85.2M</cell></row><row><cell cols="6">a model trained only on the source domain without self-</cell></row><row><cell cols="6">training (ST) on the target domain. Without ST (row 1 in</cell></row><row><cell cols="6">Tab. S3), RCS and FD increase the network performance</cell></row><row><cell cols="6">by +5.1 mIoU, demonstrating their benefit for domain gen-</cell></row><row><cell cols="6">eralization. Combined with ST (row 2 in Tab. S3), their im-</cell></row><row><cell cols="6">provement even increases to +8.0 mIoU showing that RCS</cell></row><row><cell cols="6">and FD reinforce ST, confirming their particular importance</cell></row><row><cell cols="2">for UDA as well.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>results in Tab. S5 and Tab. S6 are reported with the training configurations used in the original methods, which do not use learning rate warmup. For a fair comparison, we have re-implemented DACS [71] with our training configuration including learning rate warmup, which achieves 54.2 mIoU on GTA?Cityscapes. Still, DAFormer outperforms it by +14.1 mIoU. DAFormer uses the last checkpoint of a training for evaluation. The reported results are averaged over three training runs and have a standard deviation of 0.5 mIoU on both benchmarks, which shows that the training process is stable. Example predictions showing a better recognition of train as opposed to bus by DAFormer on GTA?Cityscapes. Example predictions showing a better recognition of bus by DAFormer on GTA?Cityscapes. Example predictions showing a better recognition of truck by DAFormer on GTA?Cityscapes. Example predictions showing a better recognition of car as opposed to truck by DAFormer on GTA?Cityscapes. Example predictions showing a better recognition of sidewalk as opposed to road by DAFormer on GTA?Cityscapes. Further example predictions on GTA?Cityscapes showing a better recognition of bicycle, rider, and fence.</figDesc><table><row><cell>Image road sidew. build. wall fence pole tr. light tr. sign veget. terrain sky person rider DeepLabV2 Src-Only SegFormer Src-Only ProDA [94] Figure S4. Image DeepLabV2 Src-Only SegFormer Src-Only ProDA [94] Figure S5. Image DeepLabV2 Src-Only SegFormer Src-Only ProDA [94] Figure S6. Image DeepLabV2 Src-Only SegFormer Src-Only ProDA [94] road sidew. build. wall fence pole tr. light tr. sign veget. terrain sky person rider Figure S7. Image DeepLabV2 Src-Only SegFormer Src-Only ProDA [94] Figure S8. Image DeepLabV2 Src-Only SegFormer Src-Only ProDA [94] DeepLabV2 Src-Only SegFormer Src-Only ProDA [94] Figure S9. Image road sidew. build. wall fence pole tr. light tr. sign veget. terrain sky person rider</cell><cell>car car car</cell><cell>DAFormer (Ours) truck bus train m.bike bike Ground Truth n/a. DAFormer (Ours) Ground Truth DAFormer (Ours) Ground Truth DAFormer (Ours) Ground Truth truck bus train m.bike bike n/a. DAFormer (Ours) Ground Truth DAFormer (Ours) Ground Truth DAFormer (Ours) Ground Truth truck bus train m.bike bike n/a.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S5 .</head><label>S5</label><figDesc>Comparison with previous methods on GTA?Cityscapes. Our results (DAFormer) are averaged over 3 random seeds.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We do not use the target validation dataset for checkpoint selection in contrast to some other works <ref type="bibr" target="#b95">[94,</ref><ref type="bibr" target="#b96">95]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1. Limitations</head><p>Due to computational constraints, we only use a selection of network architectures to support our claims. However, there are further interesting architectures that could be explored in the future such as <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b93">92]</ref>. Also, other training aspects such as larger batch sizes could be of relevance.</p><p>In <ref type="figure">Fig. 3</ref> of the main paper and in the discussion of the Synthia results in Sec. I, there have been some indications that a Transformer architecture is not ideal for stuff-classes. This might be due to the shape-bias of Transformers <ref type="bibr" target="#b2">[3]</ref>. The focus on shape instead of texture might be disadvantageous for the distinction of stuff-classes as the texture is an important aspect for their recognition. Even though a further investigation is out of the scope of this work, we believe that this is an interesting aspect for future work, which could potentially lead to a network architecture, even better suited for UDA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding robustness of transformers for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
		<title level="m">Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<idno>2020. 5</idno>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<ptr target="https://www.cityscapes-dataset.com/license/.1,5" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. In Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2865" to="2888" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adasyn: Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Re-distributing biased pseudo labels for semi-supervised semantic segmentation: A baseline investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6930" to="6940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraintbased adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Three ways to improve semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>K?ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11130" to="11140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improving semi-supervised and domainadaptive semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12545</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grid saliency for context explanations of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12273</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Benchmarking the robustness of semantic segmentation models with respect to common corruptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">M2m: Imbalanced classification via major-to-minor translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13896" to="13905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12975" to="12984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn. Worksh</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spigan: Privileged adversarial learning from simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-domain semantic segmentation via domain-invariant interactive relation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Coarse-to-fine domain adaptive semantic segmentation with photometric alignment and category-center regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pixmatch: Unsupervised domain adaptation via pixelwise consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><forename type="middle">K</forename><surname>Manrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12435" to="12445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Intriguing Properties of Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayak</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07581</idno>
		<title level="m">Vision transformers are robust learners</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viraj</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deeksha</forename><surname>Kartik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8558" to="8567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
	<note>Dataset license: CC BY-NC-SA 3.0. 1, 5</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10765" to="10775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DACS: Domain Adaptation via Crossdomain Mixed Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<title level="m">Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8515" to="8525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Continual test-time domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7032" to="7042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Crest: A class-rebalancing self-training framework for imbalanced semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mellina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10857" to="10866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Ocnet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Context-aware mixup for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In Int</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<title level="m">Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Wall Fence Pole Tr.Light Sign Veget. Terrain Sky Person Rider Car Truck Bus Train M.bike Bike mIoU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Road</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Build</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Comparison with previous methods on Synthia?Cityscapes. Our results (DAFormer) are averaged over 3 random seeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S6</forename><surname>Table</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Wall Fence Pole Tr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Road</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Build</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Light Sign Veget. Sky Person Rider Car Bus M.bike Bike mIoU16 mIoU13</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
