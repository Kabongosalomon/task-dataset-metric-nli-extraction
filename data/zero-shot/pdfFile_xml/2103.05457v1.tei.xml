<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rudder: A Cross Lingual Video and Text Retrieval Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayaprakash</forename><forename type="middle">A *</forename><surname>Abhishek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rudder: A Cross Lingual Video and Text Retrieval Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>ACM Reference Format: Jayaprakash A, Abhishek, Rishabh Dabral, Ganesh Ramakrishnan, and Preethi Jyothi. 2021. Rudder: A Cross Lingual Video and Text Retrieval Dataset. In SIGIR &apos;21:ACM SIGIR Conference on Research and Development in In-formation Retrieval, Under Review. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn * Both authors contributed equally to this research. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video retrieval using natural language queries requires learning semantically meaningful joint embeddings between the text and the audio-visual input. Often, such joint embeddings are learnt using pairwise (or triplet) contrastive loss objectives which cannot give enough attention to 'difficult-to-retrieve' samples during training. This problem is especially pronounced in data-scarce settings where the data is relatively small (10% of the large scale MSR-VTT) to cover the rather complex audio-visual embedding space. In this context, we introduce Rudder -a multilingual video-text retrieval dataset that includes audio and textual captions in Marathi, Hindi, Tamil, Kannada, Malayalam and Telugu. Furthermore, we propose to compensate for data scarcity by using domain knowledge to augment supervision. To this end, in addition to the conventional three samples of a triplet (anchor, positive, and negative), we introduce a fourth term -a partial -to define a differential margin based partialorder loss. The partials are heuristically sampled such that they semantically lie in the overlap zone between the positives and the negatives, thereby resulting in broader embedding coverage. Our proposals consistently outperform the conventional max-margin and triplet losses and improve the state-of-the-art on MSR-VTT and DiDeMO datasets. We report benchmark results on Rudder while also observing significant gains using the proposed partial order loss, especially when the language specific retrieval models are jointly trained by availing the cross-lingual alignment across the language-specific datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning low-dimensional, semantically rich representations of audio, visual and textual data (associated with videos) is a fundamental problem in multimodal learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. Such multimodal representations play a critical role in facilitating a multitude of visual analytics tasks such as Video Retrieval <ref type="bibr" target="#b19">[20]</ref>, Visual Question Answering (VQA), video summarization etc.</p><p>Learning such compact representations is already a challenging task owing to the high dimensionality and multi-modality of sensory data. This difficulty is further exacerbated when the available data is limited and biased. With these challenges in focus, this paper introduces a new cross-lingual video dataset, Rudder 1 , with audio and textual descriptions in multiple languages for information retrieval tasks. We also propose a novel approach to learning joint-embeddings between videos (audio-visual) and text while specifically being cognizant of its effectiveness for data-scarce settings.</p><p>Existing multi-lingual video datasets contain only bilingual textual descriptions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> and may contain audio in at most one language <ref type="bibr" target="#b22">[23]</ref>. We present a new resource, Rudder 2 , that is truly multi-lingual with both, captions and narrations, in Marathi, Tamil, Telugu, Kannada, Malayalam and Hindi, thus facilitating training of cross-modal (audio/text/video) and cross-lingual embeddings. To the best of our knowledge, we are the first to propose a cross-linguomodal audio-visual and text dataset. We are also the first to create such a resource catering to Indian languages, which are extremely under-represented when considering multimodal resources.</p><p>We demonstrate the high impact of Rudder by using it to learn cross-linguo-modal embeddings for video-to-text and text-to-video retrieval tasks, across several languages. Specifically, we show that Rudder could be used for cross-linguo-modal tasks in which the audio track and textual captions may come from different languages but they project to the same point in the embedding space. As part of a competitive approach advancing state-of-the-art, we also introduce a novel partial-order contrastive loss that leverages augmented supervision to learn dense embeddings even for data-scarce cases. A typical approach to learning joint video-text embeddings is to use triplet/contrastive losses which prove insufficient in providing dense coverage of the embedding space, particularly in data-scarce settings. We propose an alternative solution in the form of a differential margin based partial order contrastive loss that facilitates augmenting the supervision using domain knowledge. We argue  different roles played by positive, partially overlapping and negative samples in our proposed simple differential margin based partial-order loss. Intuitively, we would like distances between the partially similar pairs to lie in the dark orange zone between the positive and the negative instances, i.e., beyond the 1 margin but within <ref type="bibr" target="#b1">2</ref> . Distances between unrelated pairs should be pushed to the grey region beyond 2 , whereas distances between related pairs should be in the green region within the margin .</p><p>that the triplet formulation, while crucial, is not sufficient. To further ease training, we propose to learn the embeddings using a quadruplet of {anchor, positive, partial, negative} instances. The additional partial samples can be also sampled using meaningful heuristics based on domain knowledge, such that the semantic interpretation of each partial sample falls somewhere in between the positive and negative samples. We demonstrate that these heuristics can be almost as effective as manually identified partial samples that we also release as part of the dataset (Rudder) associated with this paper. We illustrate the intuitiveness of the proposal with the help of an example in <ref type="figure" target="#fig_0">Figure 1</ref>, showing a frame from a video of a man watching television. While the positive sample, 'A person sitting on a sofa watching television' and the negative sample, 'A woman is throwing a dress in the air' are self-explanatory, the partially relevant sample, 'A person is watching videos on a laptop' is neither a strictly positive nor a strictly negative instance. Though the objects are different, the sentences still capture the act of a person watching a (similar-looking) object. We postulate that placing such sentences at an appropriate distance from the anchor can crucially contribute towards coverage within the sparse latent space shared between the videos and the text. When training with partial instances, we differentiate them from the anchors using a partial-margin value that lies in between the margin values of positive and negative samples.</p><p>We evaluate the proposed partial-order loss on the Rudder dataset and demonstrate its effectiveness with respect to strong baselines such as the Max-Margin contrastive loss, the Optimal Transport loss proposed in <ref type="bibr" target="#b35">[36]</ref>, etc. We also validate the similar effectiveness of the partial-order loss on existing benchmark datasets such as MSR-VTT <ref type="bibr" target="#b34">[35]</ref>, Charades <ref type="bibr" target="#b9">[10]</ref>, DiDeMo <ref type="bibr" target="#b12">[13]</ref>, etc.</p><p>In summary, the contributions of this paper are as follows:</p><p>? We release a newly annotated cross-lingual video dataset, Rudder, to test our (and relevant) methods on data-scarce settings. The dataset (described in Section 4) includes audiovisual footages in Marathi, Hindi, English, Tamil, Kannada, Malayalam and Telugu. A significant percentage (75%) of the videos are the same, and for these videos, the audio tracks are largely aligned across the languages. Furthermore, in addition to providing multi-lingual audio for each video, we also provide textual captions in corresponding languages along with the associated start and end timings. ? We release a new challenge based on Rudder that we hope will spur more interest in designing new cross-linguo-modal techniques. ? We propose a new quadruplet-based partial order loss (in Section 5) to improve the coverage of video-text retrieval models. The proposed intuitive loss is a competitive approach that advances state-of-the-art by taking into account augmented supervision along with the already available ground truth relevance. In this loss, we tune margins (such as 1 and 2 in <ref type="figure" target="#fig_0">Figure 1</ref>) that help differentiate positives from the partial and the partial from the negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>MultiLingual Visual Datasets: While vision and language have been used in conjunction in several datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref>, multilingual visual datasets have continued to be under-explored. <ref type="bibr" target="#b8">[9]</ref> proposed a multi-lingual image dataset, FM-IQA, for image question answering in Chinese and English. <ref type="bibr" target="#b22">[23]</ref> propose a Portuguese/English dataset with audio, video and textual captions for multi-modal learning. <ref type="bibr" target="#b30">[31]</ref> introduced a bi-lingual video-text dataset, VaTex for Chinese and English. However, none of the above mentioned datasets goes beyond two languages to be truly multi-lingual. In comparison, our proposed Rudder dataset is a multi-lingual audiovisual and textual dataset with audio narrations and textual captions in six languages.</p><p>Learning Representations for Video-Text Retrieval: Several of the existing approaches for video-text retrieval <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> are based on image-text embedding methods by design, and typically focus on single visual frames.</p><p>[32] focus on learning separate embedding spaces for each POS tag, using triplet losses wherein the positive examples were constructed by finding relevant entities that shared common nouns and verbs. We borrow inspiration from this work while designing our heuristics. Collaborative Experts <ref type="bibr" target="#b18">[19]</ref> is proposed as a network to effectively fuse the action, object, place, text and audio features extracted from state-of-the-art models (experts) with the reasoning that extracting supervision from multiple experts could help compensate for the lack of sufficient data. Our proposed loss function is used within the collaborative experts framework to further help improve coverage, specifically in low-resource settings.</p><p>Loss Functions for Cross-Modal Learning: Several max-margin based ranking loss functions have been proposed for learning joint embeddings. The triplet loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref> and bidirectional max-margin ranking loss <ref type="bibr" target="#b24">[25]</ref> have been popularly used to learn cross-modal embeddings. <ref type="bibr" target="#b35">[36]</ref> propose a new approach to learn distance metrics via optimal transport programming for batches of samples and report faster convergence rates without adversely affecting performance.</p><p>Significant effort has also gone into better sampling methods for training ranking losses. FaceNet <ref type="bibr" target="#b23">[24]</ref> uses semi-hard negatives within the triplet-loss framework. Notably, <ref type="bibr" target="#b32">[33]</ref> propose a distancebased sampling method to choose stable and informative samples that can be trained with a simple max-margin loss. Likewise, <ref type="bibr" target="#b29">[30]</ref> propose to consider samples in the neighborhood of positives. Our partial ordering based loss formulation subsumes <ref type="bibr" target="#b29">[30]</ref> in the sense that we also consider the extended neighborhood between the positives and negatives. The approach closest to our partial-order based loss, however, is the work of <ref type="bibr" target="#b16">[17]</ref>, who use the knowledge of category hierarchy to label samples as positives or semi-positives. In addition to differing from them in our loss formulation, it is worth noting that unlike categorical data, establishing hierarchies in textual queries is not straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>Embedding learning aims to learn semantically-aware and compact fixed length representation vectors from input samples. A kernel function (?; ) : ? takes input ? and generates a feature representation or an embedding. (?; ) is usually defined by a deep neural network, parameterized by . Embedding learning optimizes a discriminative loss objective to minimize intra-class distances while maximizing inter-class distances. For example, the contrastive loss in the seminal work introducing Siamese networks <ref type="bibr" target="#b10">[11]</ref> takes pairs of samples as input and trains two identical networks to learn a deep metric by minimising the following loss functional,</p><formula xml:id="formula_0">L ( , ; ) = + (1 ? ) max{0, ? }<label>(1)</label></formula><p>where the label ? {0, 1} indicates whether a pair of ( , ) is from the same class or not and denotes the distance between and . The margin parameter imposes a threshold of the distances among dissimilar samples. Triplet loss <ref type="bibr" target="#b3">[4]</ref> is conceptually similar to the contrastive loss, and extends pairs of samples to triplets. Given a query with a sample similar to and a dissimilar sample , the triplet loss function is formulated as,</p><formula xml:id="formula_1">L ( , , ; ) = max{0, ? + }.<label>(2)</label></formula><p>Intuitively, it encourages the distance between the dissimilar pair to be larger than the distance between the similar pair by at least a margin .</p><p>Optimal transport distances, also known as Wasserstein distances <ref type="bibr" target="#b26">[27]</ref> or Earth Mover's distances <ref type="bibr" target="#b21">[22]</ref>, define a distance between two probability distributions according to principles from optimal transport theory <ref type="bibr" target="#b28">[29]</ref>. Formally, let and be -dimensional probability measures. The set of transportation plans between probability distributions and is defined as ( , ) := { ? ? + | ? 1 = , ? 1 = }, where 1 is an all ones vector. The set of transportation plans ( , ) contains all non-negative ? elements with rows and columns summing to and , respectively. Give an ? ground distance matrix , the cost of mapping to using a transport matrix is quantified as ? , ?, where ?., .? denotes the Frobenius norm (dot product). The resulting optimisation problem is then,</p><formula xml:id="formula_2">( , ) := min ? ( , ) ? , ?<label>(3)</label></formula><p>which is called an optimal transport problem between and given ground cost . The optimal transport distance ( , ) measures the cheapest way to transport the mass in probability measure to match that in . Optimal transport distances define a more powerful cross-bin metric to measure probabilities compared with some commonly used bin-to-bin metrics, e.g., Euclidean, Hellinger, and Kullback-Leibler divergences. However, the cost of computing is at least ( 3 ( )) when comparing two -dimensional probability distributions in a general metric space. To alleviate it, <ref type="bibr" target="#b4">[5]</ref> formulated a regularized transport problem by adding an entropy regularizer to Eqn. (3). This makes the objective function strictly convex and allows it to be solved efficiently. Particularly, given a transport matrix , the entropy of can be computed as</p><formula xml:id="formula_3">?( ) = ? ?? log .</formula><p>For any &gt; 0, the regularized transport problem can be defined as</p><formula xml:id="formula_4">( , ) := min ? ( , ) ? , ? ? 1 ?( )<label>(4)</label></formula><p>where the larger is, the closer this relaxation ( , ) is to the original ( , ).</p><p>[5] also proposed the Sinkhorn's algorithm to solve Eqn. (4) for the optimal transport * . Specifically, let the matrix = exp(? ) and solve it for the scaling vectors and to a fixedpoint by computing = ./ , = ./ in an alternating way. This yields the optimal transportation plan * = diag( ) diag( ), which can be solved significantly faster ( ( 2 )) than the original transport problem. the captions by translating the Marathi captions into those respective languages using the Google Translate API. While we could have used an ASR system to transcribe the audio of the remaining languages, we found the ASR based transcriptions to be severely lacking in quality due to poor support for these specific languages.  <ref type="table" target="#tab_0">Table 1</ref> lists the amount of human speech available in our dataset to quantify the richness of our audio files. The amount of human speech was calculated using an off-the-shelf voice activity detection model as in <ref type="bibr" target="#b1">[2]</ref>. The average length of a video is 8 secs and each video is associated with a sentence describing it. Of the 3272 videos, 675 videos (roughly 21%) include audio narrations in all languages. 54% videos come with Hindi and Marathi narrations, 27% with Tamil and Marathi, 53% with Kanadda and Marathi, 33% with Malayalam and Marathi, and 23% with Telugu and Marathi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotations for Augmented Supervision</head><p>As motivated in Section 1 and illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, we propose augmented supervision for guiding a loss (such as the partial ordering based loss proposed in Section 5). To enable a sanity check on the process of heuristically generating augmented supervision, we also provide manually generated augmented supervision. For each query caption in the dataset, these manual annotations consist of partial/positive/negative labels assigned to video segments corresponding to that caption that are obtained using the following process. For each query/anchor caption in the dataset, we first find the top = 10 'similar' captions based on sentence similarity scores while ensuring that these 'similar' captions do not belong to the same video as that of the query caption. The annotators are then asked to assign one out of the following three relevance judgements to each pair of similar captions/video segments. We will now illustrate the relevance judgements in <ref type="figure" target="#fig_0">Figure 1</ref> with respect to the example query caption 'A person is sitting on a sofa watching television':</p><p>(i) Positive-the relevance judgement when both the captions/video segments in the pair share an almost similar amount of information. In <ref type="figure" target="#fig_0">Figure 1</ref>, the following caption (and its corresponding video segment) is labeled positive with respect to the query caption: 'A person is sitting on a sofa watching television' (ii) Partial-the relevance judgement when both the captions/video segments in the pair share somewhat similar information. In <ref type="figure" target="#fig_0">Figure 1</ref>, the following caption (and its corresponding video segment) is one of the two labeled partial with respect to the query caption: 'A person is eating in front of a television'. (iii) Negative-the relevance judgement when both the captions/video segments in the pair are irrelevant to each other. In <ref type="figure" target="#fig_0">Figure 1</ref>, the following caption (and its corresponding video segment) is one of the two labeled negative with respect to the query caption: 'A woman is throwing a dress in the air'.</p><p>We provide two sets of relevance judgements with the dataset, one that is manually determined (using guidelines described in detail on our website) and the other based on an automated, domainknowledge driven Noun-Verb heuristic. The annotations were performed on approximately 32700 sentence pairs. While the manual annotation task was divided across three annotators, a separate annotator verified all the annotations. All this amounted to around 200 hours of annotation work cumulatively.</p><p>Noun-Verb Heuristic: For a given query caption, this heuristic labels another caption as partially relevant if either the two captions have exactly the same set of nouns but differ only in their verbs (e.g., 'person eating cake; person sitting next to a cake) or they differ in their nouns but share the same verb(s) (e.g., 'person eating cake'; 'person eating cereals'). A caption is labelled positive with respect to the query if both share the same nouns and verbs, whereas they are labeled negative if they share neither the nouns or verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Rudder Challenge</head><p>We release the Rudder dataset along with an associated crosslingual cross-modal retrieval challenge <ref type="bibr" target="#b4">5</ref> . The challenge requires performing video-to-text and text-to-video retrieval based on queries in multiple languages. The submissions are evaluated on 3 metrics, viz., Recall@K (with K = 1, 5 , 10, and 50) Median Rank, and Mean Rank. The challenge is organized under various tracks, with each track consisting of text from either Hindi or Marathi and audio from either of the four audio languages (Hindi, Marathi, Tamil, Telugu). Currently, we have eight tasks of the form lang1-text+lang2-audio, each of which denotes a video-text retrieval task using lang1 for caption and lang2 for audio. For the submission format, please refer to the challenge website.</p><p>We envisage several other potential uses/impacts of the Rudder challenge. These include training/evaluation for (1) Cross-lingual video-caption retrieval.</p><p>(2) Multi lingual video recommendation: The manually annotated positive and partial tags could serve as video recommendations, in a setting in which the user has only the videos and no textual meta-data. (3) Multilingual and Cross lingual video summarisation, aimed at describing a video in various languages, via captions. (4) Video-guided machine translation, involving translation of a source language description into the target language using the video information as additional spatio-temporal context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A COMPETITIVE APPROACH ADVANCING STATE-OF-THE-ART</head><p>A typical approach to learning joint video-text embeddings involves projecting the video and text inputs onto a common latent space such that semantically similar video-text pairs are in the same neighbourhood whereas dissimilar video-text pairs are pushed far apart. This projection is achieved by training video and text encoders using triplet ranking losses which operate on triplets of &lt;anchor, positive, negative&gt; samples <ref type="bibr" target="#b3">[4]</ref>. Here, the anchor can be a video embedding and the positive and negative parts can correspond to instances with textual embeddings or vice-versa. While this training regime is suitable for resource-rich languages with availability of large audio-visual and textual datasets, the performance quickly deteriorates when applied to data-scarce settings due to two main reasons. Firstly, the scarcity of data weakens the video (and textual) encoders which do not cover the shared embedding space densely enough, thereby restricting their ability to meaningfully project to the shared space. Secondly, data scarcity inevitably prevents the encoders from generalizing to outof-distribution text or video queries. A natural solution to the problem would be to explicitly target the hard positive and hard negative instances in the data. This approach, however, has limited returns as it leads to noisy gradients, thereby leading to a collapsed model <ref type="bibr" target="#b32">[33]</ref>. We propose a quadruplet-based partial-order augmented contrastive loss to learn joint video-text embeddings, which we described next.</p><p>Let V = { 1 , 2 , . . . , | V | } and T = { 1 , 2 , . . . , | T | } denote a set of videos and captions. We define (?) and (?) as video and caption encoders, respectively, both of which embed an ? video, , and a ? caption, , into an -dimensional space. Let , = dist( ( ), ( )) be the distance between the embeddings corresponding to video and caption .</p><p>The standard bidirectional max-margin ranking loss <ref type="bibr" target="#b24">[25]</ref> can be written as:</p><formula xml:id="formula_5">L = ?? , ? [ + , ? , ] + + [ + , ? , ] + .<label>(5)</label></formula><p>Here, is a tunable margin hyperparameter that lower-bounds the distance between negative pairs and [.] + represents the max(., 0) function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Optimal Transport Loss</head><p>In addition to subjecting the pairs of positive, negatives and partials to margin based ranking losses, we also test the efficacy of heuristic-based augmented supervision on Optimal Transport based loss function as proposed by <ref type="bibr" target="#b35">[36]</ref>. As an alternative to the differential margins, one could differentialy weigh the difficult pairs and contrast them against the easier ones. Toward this, we consider a batch-wise Optimal Transport (OT) based loss, building upon the work of <ref type="bibr" target="#b35">[36]</ref>. The originally proposed loss defines ground distances, + and ? for positive and negative pairs and uses them to estimate an optimal transport plan, * , that assigns higher importance to difficult pairs and vice-versa. </p><formula xml:id="formula_6">+ , = ? ( [ , ? , ? ] + +[ , ? , ? ] +) ,<label>(6)</label></formula><formula xml:id="formula_7">L = ?? , * , L ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Partial Order Contrastive Loss</head><p>The bidirectional max-margin loss defined above separates the negative instances from positive instances by a fixed margin, .</p><p>Although this is regarded as one of the standard ways of learning cross-modal embeddings, we argue that in the absence of a large dataset, the loss results in a sparsely covered embedding space, thereby hurting the generalizability of the embeddings. Furthermore, the pretrained embeddings (GLoVE, GPT, etc) for lowresource/underrepresented languages are often either weak or simply may not exist. In order to circumvent the challenges associated with a lowresource language, we propose the following quadruplet based mining setup. Given a batch of videos and their corresponding captions, for every anchor sample, we construct three sets of videocaption pairs, viz., (i) the positive ( + ), (ii) the negative ( ? ) and (iii) the partial-overlap ( ? ). While the positive and negative pairs are chosen as in the bidirectional max-margin loss, we show we can make effective use of dataset-dependent heuristics to sample the partial samples. Intuitively, the partial samples are chosen such that they represent a degree of semantic overlap with the anchor (as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>). We formally define the proposed novel Partial Order (PO) Contrastive Loss as:</p><formula xml:id="formula_8">L = L + + L ? + L ? L + = ?? ( , ) ? + [ , ? , ? ] + + [ , ? , ? ] + L ? = ?? ( , ) ? ? [ + , ? , ] + + [ + , ? , ] + L ? = ?? ( , ) ? ? [ 1 + , ? , ] + + [ 1 + , ? , ] + + [ , ? , ? 2 ] + + [ , ? , ? 2 ] +<label>(</label></formula><p>9) Here, , 1 , 2 and are tunable margin hyperparameters such that &lt; 1 &lt; 2 &lt; . As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, we would like distances between the partially similar pairs to lie in the dark orange zone between the positives and the negatives, i.e., beyond the 1 margin but within 2 . Distances between unrelated pairs should be pushed to the blue region beyond 2 , whereas distances between positive pairs should be in the green region within the margin . Note that the proposed setup is different from the similar-sounding paradigm of semi-hard negative mining <ref type="bibr" target="#b23">[24]</ref>. While semi-hard negative mining is a useful tool to ease the training process, it does not deal with the issue of coverage over the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL SETUP</head><p>While our primary experiments are conducted on the Rudder dataset, we also evaluate our proposed partial-order loss on three existing benchmark datasets:</p><p>(1) Charades-STA <ref type="bibr">[</ref> Evaluation Metrics: We use standard retrieval measures adopted from prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> and show results on both text-to-video (T2V) and video-to-text (V2T) retrieval. R@K (recall at rank K) for K=1,5,10,50 measures the percentage of the top-K retrieved text/video results, for a given video/text query, that match the ground-truth. Median Rank (MdR, lower is better), and Mean Rank (MnR, lower is better) compute the median and mean of the ground truth appearing in the ranking of the predictions, respectively. When computing video to-sentence measures for datasets containing multiple independent sentences per video, such as the Rudder dataset, we follow the evaluation protocol used in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> which corresponds to reporting the minimum rank among all valid text descriptions for a given video query. Implementation Details: We adopt the state-of-the art collaborative experts (CE) framework from <ref type="bibr" target="#b18">[19]</ref> as our base model. We choose four pretrained models to serve as experts and derived features from the scene <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>, audio (VGG <ref type="bibr" target="#b13">[14]</ref>), objects <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref> and action <ref type="bibr" target="#b25">[26]</ref>, that are subsequently fed into the video encoder as inputs. The embeddings from these features are further aggregated within the video encoder using a collaborative gating mechanism, resulting in a fixed-width video embedding (denoted as ( )).</p><p>In order to construct textual embeddings (denoted by ( )), the query sentences are converted into sequences of feature vectors using pretrained word-level embeddings like GloVE and FastText. These word-level embeddings are aggregated using NetVLAD <ref type="bibr" target="#b0">[1]</ref>. We subject the output of the aggregation step to the text encoding architecture proposed in <ref type="bibr" target="#b19">[20]</ref>, which projects text features to different subspaces, one for each expert. Each text feature is further refined by computing attention over aggregated text features and thereafter passed through a feedforward and a softmax layer to produce a scalar for each expert projection. Finally, each expert is scaled and concatenated to form a fixed-width text embedding.</p><p>We use PyTorch for our implementations. The margin values , 1 , 2 and are tuned on the validation sets. More details are provided in the extended version linked on our site. Baselines: We compare the use of the partial order loss against the following baselines (i) CE trained with the standard max-margin loss (MM) (ii) an importance-driven distance metric via batch-wise Optimal Transport (OT) from Xu et al. <ref type="bibr" target="#b35">[36]</ref>. (iii) a Triplet loss (Triplet) baseline, (iv) HN baseline which refers to the hard negative mining strategy, where we pick only the hardest negative to train on and, finally, (v) a distance-weighted sampling based triplet  <ref type="table">Table 2</ref>: Results on human annotated Rudder dataset. PO and PO(M) denote the use of heuristically annotated and manually annotated partial samples for the partial order loss. DW refers to our implementation of Distance-Weighted sampling as in <ref type="bibr" target="#b32">[33]</ref>.</p><p>loss (DW) from <ref type="bibr" target="#b32">[33]</ref>, (vi) S2VT (Sequence to Sequence Video Text retrieval), which is a reimplementation of <ref type="bibr" target="#b27">[28]</ref>, (vii) FSE <ref type="bibr" target="#b36">[37]</ref>, which performs Cross-Modal and Hierarchical Modeling of Video and Text and (viii) specifically on the MSRVTT and DiDeMo datasets, we also present the numbers as reported by <ref type="bibr" target="#b18">[19]</ref> and refer to those as (MM <ref type="bibr" target="#b18">[19]</ref>). To the best of our knowledge, we are the first to evaluate the effectiveness of OT-based loss for retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS AND ANALYSIS</head><p>We next describe experimental results that validate the effect of using the proposed partial ordering based loss and compare them against the max-margin baseline as well as against the optimal transport based loss. Synthetic Experiment. Before presenting our results from experiments on existing datasets, we illustrate the importance of the proposed partial ordering based loss with the help of a synthetic task. This experiment also highlights the relevance of the partial ordering based loss when training data is limited. Consider a simple dataset consisting of 8 classes as shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. Classes with odd-numbered labels correspond to the inner circles and classes with even-numbered labels correspond to the annular rings (i.e., the outer circle minus the respective inner circle). The radii are chosen in such a way that all the classes have the same area. While the model that gets trained by minimizing the max-margin based loss in Eqn (5) uses only the ground truth for training, the partial ordering based loss based on Eqn (9) makes use of the following augmented supervision. For each query point that belongs to class ? {1, 3, 5, 7}, the samples coming from class + 1 should be within the margins 1 and 2 and the samples coming from any other classes other than and + 1 should be more than 2 as described in Eqn <ref type="bibr" target="#b8">(9)</ref> Training points are sampled uniformly from all the 8 classes and the test data is a randomly sampled set of 20 points. <ref type="table">Table 5</ref> shows The numbers are obtained over an average of 5 retrieval random train-test samples. Empirical results on the task of retrieval when performed using a model containing a single linear layer without any activation can be observed in . Note that We observe that partial ordering loss based formulation yields larger gains when the training datasets are smaller. The general observation is that the gains using partial order loss based model over the max-margin loss based model increase as the training data set size is decreased (until an unreasonably small training set size is reached). The explanation for the gain could be explained as follows: Consider a point from class 1. Then, according to partial order loss we can adjust points coming from class 2 to be within 1 and 2 and points coming from other classes to be at least 2 apart. Whereas for the model using max margin loss it could be challenging for it to make points in class 2 to be at least as far away as any other points from other classes due to small amounts of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Results on Synthetic Data</head><p>Training points are sampled uniformly from all the 8 classes and the test data is a randomly sampled set of 20 points. <ref type="table">Table 5</ref> shows the retrieval results when using a model with a single linear layer without any activation function for two different training set sizes (100 and 1000). All the numbers are averaged over five different draws of the train/test samples. We observe larger gains in performance with using our partial order (PO) loss compared to a max-margin (MM) loss, when the train set is smaller; these improvements are diminished with the larger training set. The gains from PO could be intuitively explained using the following example. Consider an anchor point from class 1. With PO, points from class 2 have to be within 1 and 2 away from the anchor point, and points from other classes should be at least 2 apart. With MM, it could be challenging to push points in class 2 to be as far away as points from other classes due to small amounts of training data.</p><p>We now discuss our results and present the underlying analysis. In <ref type="table">Table 2</ref>, we present our main results on the Rudder dataset using manually-derived annotations and heuristically-derived annotations. Further, in <ref type="table">Table 4</ref>, we present the results on the multilingual Rudder setting wherein the textual captions are in Marathi but audio-experts corresponding to other languages are also used. For both these losses, we sample a single negative example from   We observe that adding augmented supervision and invoking our partial order loss yields consistent improvements across all the evaluation metrics, in both Tables 2 and 4. We also observe that multi-lingual training significantly improves the retrieval performance. These numbers clearly validate our claim about the utility of the partial order loss in limited-data conditions. To further confirm the effectiveness of the method, we perform similar experiments with textual queries from languages other than Marathi, such as Tamil, Telugu and Hindi (a mix of unrelated languages). <ref type="table">Table 6</ref> tabulates performances when both the audio and the text corresponds to the same language, whereas <ref type="table" target="#tab_6">Table 7</ref> tabulates results when the text is in Marathi but the audio features are derived from a different language. Additionally, we observe that our results with heuristically annotated Rudder are fairly comparable to those obtained with the manually annotated Rudder, thus proving that one can achieve good performance with reasonable heuristics without having to undertake any additional human evaluations.  Additional Results: We also demonstrate the performance of the proposed loss function on three benchmark datasets: MSR-VTT, DiDeMO and Charades-STA. MSR-VTT is a large-scale dataset with up to 20 captions per video. To conform better to our low-resource setting, we use only 1 caption per video <ref type="bibr" target="#b5">6</ref> . <ref type="table" target="#tab_7">Table 8</ref> shows the results with our loss functions, along with the numbers reported in <ref type="bibr" target="#b18">[19]</ref> for this setting (MM <ref type="bibr" target="#b18">[19]</ref>). We observe that PO outperforms all other loss functions on all metrics (with the exception of MnR), and significantly improves over MM <ref type="bibr" target="#b18">[19]</ref>. <ref type="table" target="#tab_0">Tables 9 and 10</ref> shows results on the DiDeMO and Charades-STA datasets, respectively. PO performs fairly well on the Charades-STA dataset. On DiDeMO, it improves upon the state-of-the-art on MdR and higher values for R@k. Interestingly, we observe a deviation from the trend for R@1 aznd R@5 for the V2T task. We believe this can be attributed to the captions in DiDeMO being of fairly large length on average, which hampers the ability of simple heuristics such as our nounverb heuristic to identify useful partial samples <ref type="bibr" target="#b6">7</ref> .</p><p>Significance-test: The significance of gains of PO over baselines such as Triplet, DW, CE and HN are evident from the Tables 2, 10 and 9. Hence we report Wilcoxon's signed-rank test on the median ranks of PO and the tightly contending MM losses for all the experiments in the paper. We observe statistical significance at a p-value less than 0.0001 in favour of PO against MM. We also observe a p-value less than 0.01 for PO against OT.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we introduced a multi-lingual multi-modal video dataset, Rudder, that provides instructional videos of making toys from trash with both, audio and textual transcriptions, in six Indic languages. The dataset is the first of its kind and has the potential to further spur research in developing cross-linguo-modal embeddings, especially for under-represented languages. The dataset also comes with an evaluation set with multiple tracks for evaluating future works. We also introduced a novel partial-order contrastive loss that helps learn denser embeddings, thereby increasing the coverage across the embedding space. We present strong baselines on the Rudder dataset and demonstrate the effectiveness of the loss on other existing benchmarks for video-text retrieval as well. We hope the introduction of this resource would stimulate future works in the domain of cross-lingual video based Question-Answering, Video Summarization, Video-Text Retrieval, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure illustrating</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?,</head><label></label><figDesc>= ? ( [ ? , + , ] + +[ ? , + , ] + ) (7) * = arg min ?? , P , (1 ? Q , ) , G + , + (1 ? P , )Q , , G ? ,where, P , = 1 if ( , ) ? + , else 0. and Q , = 1 if ( , ) ? ? , else 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Synthetic dataset to illustrate the effectiveness of the partial order loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean and standard deviation of the length of speech (in seconds) across languages in Rudder. The numbers in parenthesis show the percentage of human speech in the entire audio clip Dataset Statistics: The dataset consists of 3272 videos overall. We follow a 70:5:25 percentage split for the train, validation and test sets respectively. Specifically, the training set consists of 2287 videos, 166 videos are identified for validation and 819 videos form part of the test data.</figDesc><table><row><cell></cell><cell>Mean &amp; Std dev</cell><cell></cell><cell>Mean &amp; Std dev</cell></row><row><cell>Language</cell><cell></cell><cell>Language</cell><cell></cell></row><row><cell></cell><cell>Speech Length (secs)</cell><cell></cell><cell>Speech Length (secs)</cell></row><row><cell>Hindi</cell><cell>6.63 ? 3.91 (92.3%)</cell><cell>Kannada</cell><cell>6.05 ? 3.70 (77.2%)</cell></row><row><cell>Malayalam</cell><cell>5.81 ? 3.70 (77.1%)</cell><cell>Marathi</cell><cell>6.40 ? 3.79 (87.5%)</cell></row><row><cell>Tamil</cell><cell>5.95 ? 3.82 (83.4%)</cell><cell>Telugu</cell><cell>6.03 ? 3.80 (80.5%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>10] is a human-centric activity dataset containing one English caption per video. We use a train/validation/test split of 3124/121/1015 clips respectively, while ensuring that each caption is uniquely mapped to one video.(2) MSR-VTT<ref type="bibr" target="#b34">[35]</ref> is a relatively large video dataset consisting of 10K videos with multiple English captions per video. We follow a train-val-test split of 6512, 496, and 2990, respectively. To be consistent with other benchmarks and Rudder, and as in<ref type="bibr" target="#b18">[19]</ref>, we work with 1 caption per video.</figDesc><table><row><cell>(3) DiDeMo: Distinct Describable Moments (DiDeMo) [13] dataset</cell></row><row><cell>consists of over 10K videos in diverse visual settings with</cell></row><row><cell>pairs of localized video segments and referring expressions.</cell></row><row><cell>Each video has around 3-5 descriptions aligned with time-</cell></row><row><cell>stamps, which we concatenate into a single caption per video.</cell></row><row><cell>We follow a train-val-test split of 8392, 1065 and 1004 videos</cell></row><row><cell>respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>37.53 99.33 175.23 PO 3.01 10.70 15.91 36.39 97.67 181.71 37.61 94.00 172.77 PO 3.13 10.50 16.32 38.34 93.33 180.06</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Text to Video Retrieval</cell></row><row><cell cols="4">Loss R@1 R@5 R@10 R@50 MdR</cell><cell>MnR</cell></row><row><cell cols="2">MM 2.16</cell><cell>8.63</cell><cell cols="2">14.29 36.39 99.67 180.15</cell></row><row><cell>OT</cell><cell>2.85</cell><cell cols="2">9.40 14.86 Video to Text Retrieval</cell></row><row><cell cols="4">Loss R@1 R@5 R@10 R@50 MdR</cell><cell>MnR</cell></row><row><cell cols="2">MM 2.12</cell><cell>9.08</cell><cell cols="2">13.84 36.18 96.00 174.87</cell></row><row><cell>OT</cell><cell>2.85</cell><cell>9.48</cell><cell>14.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on heuristically annotated Rudder dataset. .35 18.11 25.31 62.55 32.83 46.65 OT 5.76 16.05 26.75 67.49 29.17 41.95 PO 5.97 16.46 26.95 68.93 27.83 42.09</figDesc><table><row><cell></cell><cell></cell><cell>Text to Video Retrieval</cell><cell></cell></row><row><cell cols="4">Loss R@1 R@5 R@10 R@50 MdR MnR</cell></row><row><cell cols="4">MM 4.73 15.02 24.69 62.96 33.00 47.15</cell></row><row><cell>OT</cell><cell>3.5</cell><cell>16.67 27.98 62.14 31.67</cell><cell>45.8</cell></row><row><cell>PO</cell><cell cols="3">4.12 17.49 29.42 61.93 31.00 45.74</cell></row><row><cell></cell><cell></cell><cell>Video to Text Retrieval</cell><cell></cell></row><row><cell cols="4">Loss R@1 R@5 R@10 R@50 MdR MnR</cell></row><row><cell cols="2">MM 5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :Table 5 :</head><label>45</label><figDesc>Results on the Rudder dataset that is (multilingually) trained using audio from all the 4 languages. Multi-lingual Rudder uses audio experts from four different languages (Hindi, Marathi, Tamil, Telugu). Empirical results on synthetically simulated data for 100 and 1000 training samples. each batch which partly explains the deterioration in performance compared to MM. More details about these implementations are available in the dataset website.</figDesc><table><row><cell cols="3">Loss Training Pts R@1 R@5 R@10 MdR MnR</cell></row><row><cell>MM</cell><cell>100</cell><cell>63.75 96.88 99.38 1.00 1.77</cell></row><row><cell>PO</cell><cell>100</cell><cell>67.50 96.88 99.38 1.00 1.72</cell></row><row><cell>MM</cell><cell>1000</cell><cell>65.00 96.88 99.38 1.00 1.74</cell></row><row><cell>PO</cell><cell>1000</cell><cell>65.63 97.5 99.38 1.00 1.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results on Rudder dataset with audio in different languages and captions in Marathi.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">T2V Retrieval</cell><cell cols="4">V2T Retrieval</cell></row><row><cell>Loss</cell><cell cols="3">R@5 MdR</cell><cell>MnR</cell><cell cols="3">R@5 MdR</cell><cell>MnR</cell></row><row><cell>MM(hin)</cell><cell cols="2">9.39</cell><cell cols="2">68.17 108.98</cell><cell>9.96</cell><cell cols="3">67.67 107.32</cell></row><row><cell cols="9">PO(hin) 10.45 63.5 108.49 10.24 59.67 110.19</cell></row><row><cell cols="4">MM(tam) 17.94 38.83</cell><cell>56.09</cell><cell>17.46</cell><cell>33.5</cell><cell cols="2">50.44</cell></row><row><cell cols="4">PO(tam) 17.62 28.5</cell><cell cols="5">53.74 19.68 32.83 50.57</cell></row><row><cell cols="4">MM(tel) 11.57 56.00</cell><cell>85.25</cell><cell cols="3">11.93 53.17</cell><cell>81.99</cell></row><row><cell>PO(tel)</cell><cell cols="8">11.93 52.83 83.99 12.57 51.67 81.58</cell></row><row><cell cols="9">Table 6: Results on Rudder dataset with audio and captions</cell></row><row><cell cols="4">in the same language.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">T2V Retrieval</cell><cell></cell><cell cols="3">V2T Retrieval</cell></row><row><cell>Loss</cell><cell cols="2">R@5</cell><cell>MdR</cell><cell>MnR</cell><cell>R@5</cell><cell cols="2">MdR</cell><cell>MnR</cell></row><row><cell>MM(hin)</cell><cell>3.87</cell><cell cols="3">125.17 143.19</cell><cell>4.38</cell><cell cols="2">124.</cell><cell>142.99</cell></row><row><cell>PO(hin)</cell><cell cols="8">4.21 119.67 141.49 5.41 122.33 142.93</cell></row><row><cell cols="4">MM(tam) 10.02 54.67</cell><cell>56.41</cell><cell>8.86</cell><cell cols="2">45.5</cell><cell>52.94</cell></row><row><cell>PO(tam)</cell><cell>9.56</cell><cell></cell><cell>46.67</cell><cell>56.38</cell><cell>9.56</cell><cell cols="2">43.67</cell><cell>54.07</cell></row><row><cell cols="3">MM(tel) 11.29</cell><cell>50.5</cell><cell>84.21</cell><cell>12.2</cell><cell cols="2">50.25</cell><cell>83.73</cell></row><row><cell>PO(tel)</cell><cell cols="3">17.85 35.00</cell><cell cols="4">68.09 14.03 40.17</cell><cell>70.81</cell></row><row><cell></cell><cell></cell><cell cols="2">T2V Retrieval</cell><cell></cell><cell cols="3">V2T Retrieval</cell></row><row><cell>Loss</cell><cell cols="8">R@1 R@10 MdR MnR R@1 R@10 MdR MnR</cell></row><row><cell>MM [19]</cell><cell>4.8</cell><cell>25.0</cell><cell>43.3</cell><cell>183.1</cell><cell>8.4</cell><cell>37.1</cell><cell cols="2">20.3 87.2</cell></row><row><cell>MM</cell><cell>5.6</cell><cell>27.6</cell><cell cols="3">35.3 153.7 10.5</cell><cell>41.5</cell><cell cols="2">16.0 69.5</cell></row><row><cell>OT</cell><cell>5.6</cell><cell>27.3</cell><cell>38.0</cell><cell>176.2</cell><cell>9.4</cell><cell>39.6</cell><cell cols="2">17.7 83.9</cell></row><row><cell>PO</cell><cell>6.2</cell><cell cols="4">28.8 33.3 161.29 11.2</cell><cell cols="3">43.4 14.7 74.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Results on MSRVTT dataset using 1 caption.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Results on DiDeMo dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="2">T2V Retrieval</cell><cell></cell><cell cols="2">V2T Retrieval</cell></row><row><cell>Loss</cell><cell cols="6">R@1 R@10 MdR MnR R@1 R@10 MdR MnR</cell></row><row><cell>Triplet</cell><cell>1.6</cell><cell>9.2</cell><cell>153.0 240.5</cell><cell>1.3</cell><cell>8.0</cell><cell>157.2 252.0</cell></row><row><cell>DW</cell><cell>1.7</cell><cell>9.7</cell><cell>147.0 236.1</cell><cell>1.1</cell><cell>7.8</cell><cell>150.3 240.2</cell></row><row><cell>MM</cell><cell>2.4</cell><cell>14.3</cell><cell>89.0 174.6</cell><cell>2.0</cell><cell>13.4</cell><cell>86.8 174.4</cell></row><row><cell>OT</cell><cell>2.7</cell><cell>15.5</cell><cell>80.5 171.4</cell><cell>2.0</cell><cell>14.2</cell><cell>82.3 172.3</cell></row><row><cell>PO</cell><cell>3.6</cell><cell>15.9</cell><cell cols="2">77.0 162.3 3.2</cell><cell>14.9</cell><cell>83.0 164.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Results on Charades-STA dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">THE RUDDER DATASETWe propose a cross-lingual video and text dataset for retrieval tasks on a set of languages that include Marathi, Hindi, English, Telugu and Tamil. The dataset consists of video tutorials for making scientific toys from waste material with audio narrations in multiple languages.<ref type="bibr" target="#b2">3</ref> Each video is associated with an audio track that consists of a descriptive narration of the process of making those toys.<ref type="bibr" target="#b3">4</ref> We also provide transcriptions of the audio tracks, so that they serve as textual captions/descriptions for the videos. Since the videos are instructional in nature, they support a meaningful association of objects with the underlying toy-making process. To the best of our knowledge, we are the first to release a cross-lingual video dataset with multi-lingual audio and textual tracks.Annotation Process. Each video in the dataset originally came with audio narrations in Marathi, Hindi, English, Tamil, Telugu, Kannada and Malayalam. Of these, the audio tracks for Marathi and Hindi were manually transcribed by native speakers to help construct the text captions. For the remaining languages, we construct<ref type="bibr" target="#b2">3</ref> We downloaded these videos from http://www.arvindguptatoys.com/toys-from-trash. php and obtained consent from the content creator to use the videos for research.<ref type="bibr" target="#b3">4</ref> Please refer to https://rudder-2021.github.io/tutorial.html for additional details about the dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://rudder-2021.github.io/challenge.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We also show results on the complete MSR-VTT dataset using all captions in the supplementary material. We do not see benefits from PO over the other losses in this setting given that evaluation using 20 captions has already a somewhat saturated coverage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We note here that we only report baselines presented in prior work for each dataset; all baselines are not listed for each dataset. For Charades-STA, HN baseline results were too poor to even be reported.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wassim Bouaziz, and Marie-Philippe Gill. 2020. pyannote.audio: neural building blocks for speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Manuel</forename><surname>Coria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Lavechin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Fustes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Titeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large Scale Online Learning of Image Similarity Through Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sinkhorn Distances: Lightspeed Computation of Optimal Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting Visual Features From Text for Image and Video Caption Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dual Encoding for Zero-Example Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06181</idno>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeViSE: A Deep Visual-Semantic Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos; Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 29th Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Action2Vec: A Crossmodal Embedding Approach to Action Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meera</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<editor>CVPR-W</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Localizing Moments in Video with Temporal Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quadruplet Selection Methods for Deep Embedding Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gundogdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ko?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Use What You Have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint embeddings with multimodal cues for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How2: A Large-scale Dataset for Multimodal Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Palaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00347</idno>
		<ptr target="http://arxiv.org/abs/1811.00347" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Calculation of the Wasserstein distance between probability distributions on the line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ss Vallender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Translating Videos to Natural Language Using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Two-Branch Neural Networks for Image-Text Matching Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03493</idno>
		<ptr target="http://arxiv.org/abs/1904.03493" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sampling Matters in Deep Embedding Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with batch-wise optimal transport loss for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuai</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-Modal and Hierarchical Modeling of Video and Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
