<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Scene Graph Generation with Data Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Zhang</surname></persName>
							<email>aozhang@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Sea-NExT Joint Lab</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
							<email>yaoyuanthu@163.com</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Sea-NExT Joint Lab</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Sea-NExT Joint Lab</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Scene Graph Generation with Data Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene graph generation</term>
					<term>Plug-and-play</term>
					<term>Large-scale</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene graph generation (SGG) is designed to extract (subject, predicate, object) triplets in images. Recent works have made a steady progress on SGG, and provide useful tools for high-level vision and language understanding. However, due to the data distribution problems including long-tail distribution and semantic ambiguity, the predictions of current SGG models tend to collapse to several frequent but uninformative predicates (e.g., on, at), which limits practical application of these models in downstream tasks. To deal with the problems above, we propose a novel Internal and External Data Transfer (IETrans) method, which can be applied in a plug-and-play fashion and expanded to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the data distribution problem by automatically creating an enhanced dataset that provides more sufficient and coherent annotations for all predicates. By applying our proposed method, a Neural Motif model doubles the macro performance for informative SGG. The code and data are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene graph generation (SGG) aims to detect relational triplets (e.g., (man, riding, bike)) in images. As an essential task for connecting vision and language, it can serve as a fundamental tool for high-level vision and language tasks, such as visual question answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref>, image captioning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7]</ref>, and image retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. However, existing SGG methods can only make correct predictions on a limited number of predicate classes (e.g., 29 out of 50 pre-defined classes <ref type="bibr" target="#b38">[39]</ref>), among which a majority of predicates are trivial and uninformative (e.g., on, and, near). This undermines the application of SGG for downstream tasks. To address the limitation, we first identify two main problems that need to deal with:</p><p>? Long-tail problem: the problem refers to the phenomenon that annotations mainly concentrate on a few head predicate classes, and are much sparse in most tail predicate classes. For example, in Visual Genome <ref type="bibr" target="#b11">[12]</ref>, there are over 100K samples for the top 5 predicate classes, while over 90% of predicate classes have less than 10 samples. As a result, the performance of tail predicate classes is poor due to the lack of effective supervision. ? Semantic ambiguity: many samples can be described as either general predicate class (e.g., on) or an informative one (e.g., riding). However, data annotators prefer some general (and thus uninformative) predicate classes to informative ones for simplicity. This causes conflicts in the widely adopted single-label optimization since different labels are annotated for the same type of instances. Thus, even when the informative predicates have enough training samples, the prediction will easily collapse to the general ones.</p><p>To address the problems mentioned above, recent works propose to use resampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, reweighting <ref type="bibr" target="#b31">[32]</ref>, and post-processing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>. However, we argue that these problems can be better alleviated by enhancing the existing dataset into a reasonable dataset, that contains more abundant training samples for tail classes and also provides coherent annotations for different classes.</p><p>To this end, we propose a novel framework named Internal and External data Transfer (IETrans), which can be equipped to different baseline models in a plug-and-play applied in a fashion. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we automatically transfer data from general predicates to informative ones (Internal Transfer) and relabel relational triplets missed by annotators (External Transfer). <ref type="bibr" target="#b0">(1)</ref> For internal transfer, we first identify the general-informative relational pairs based on the confusion matrix, and then conduct a triplet-level data transfer from general ones to informative ones. The internal transfer will not only alleviate the optimization conflict caused by semantic ambiguity but also provide more data for tail classes; (2) For external transfer, there exist many positive samples missed by annotators <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>, which are usually treated as negative samples by current methods. However, this kind of data can be considered as a potential data source, covering a wide range of predicate categories. Inspired by Visual Distant Supervision <ref type="bibr" target="#b34">[35]</ref> which employs NA samples for pre-training, we also consider the NA samples, which are the union of negative and missed annotated samples. The missed annotated samples can be relabeled to provide more training samples.</p><p>It is worth noting that both internal transfer and external transfer are indispensable for improving SGG performance. Without the internal transfer, the external transfer will suffer from the semantic ambiguity problem. Meanwhile, the external transfer can further provide training samples for tail classes, especially for those that have weak semantic connection with head classes.</p><p>Exhaustive experiments show that our method is both adaptive to different baseline models and expansible to large-scale SGG. We equip our data augmentation method with 4 different baseline models and find that it can significantly boost all models' macro performance and achieve SOTA performance for F@K metric, a metric for overall evaluation. For example, a Neural Motif Model with our proposed method can double the mR@100 performance and achieve the highest F@100 among all model-agnostic methods on predicate classification task of the widely adopted VG-50 <ref type="bibr" target="#b30">[31]</ref> benchmark.</p><p>To validate the scalability of our proposed method, we additionally propose a new benchmark with 1,807 predicate classes (VG-1800), which is more practical and challenging. To provide a reliable and stable evaluation, we manually remove unreasonable predicate classes and make sure there are over 5 samples for each predicate class on the test set. On VG-1800, our method achieves SOTA performance with significant superiority compared with all baselines. The proposed IETrans can make correct predictions on 467 categories, compared with all other baselines that can only correctly predict less than 70 categories. While the baseline model can only predict relations like (cloud, in, sky) and (window, on, building), our method enables to generate informative ones like (cloud, floating through, sky) and (window, on exterior of, building).</p><p>Our main contributions are summarized as follows: (1) To cope with the long-tail problem and semantic ambiguity in SGG, we propose a novel IETrans framework to generate an enhanced training set, which can be applied in a plugand-play fashion. <ref type="bibr" target="#b1">(2)</ref> We propose a new VG-1800 benchmark, which can provide reliable and stable evaluation for large-scale SGG. (3) Comprehensive experiments demonstrate the effectiveness of our IETrans in training SGG models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scene Graph Generation</head><p>As an important tool of connecting vision and language, SGG <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref> has drawn widespread attention from the community. SGG is first proposed as visual relation detection (VRD) <ref type="bibr" target="#b18">[19]</ref>, in which each relation is detected independently. Considering that relations are highly dependent on their context, <ref type="bibr" target="#b30">[31]</ref> further proposes to formulate VRD as a dual-graph generation task, which can incorporate context information. Based on <ref type="bibr" target="#b30">[31]</ref>, different methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> are proposed to refine the object and relation representations in the scene graph. For example, <ref type="bibr" target="#b17">[18]</ref> proposes a novel message passing mechanism that can encode edge directions into node representations. Recently, CPT <ref type="bibr" target="#b35">[36]</ref> and PEVL <ref type="bibr" target="#b33">[34]</ref> propose to employ pre-trained vision-language models for SGG. CPT shows promising few-shot ability and PEVL shows much better performance than models training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Informative Scene Graph Generation</head><p>Although making steady progress on improving recall on SGG task, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref> point out that the predictions of current SGG models are easy to collapse to several general and trivial predicate classes. Instead of only focusing on recall metric, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref> propose a new metric named mean recall, which is the average recall of all predicate classes. <ref type="bibr" target="#b22">[23]</ref> employs a causal inference framework, which can eliminate data bias during the inference process. CogTree <ref type="bibr" target="#b36">[37]</ref> proposes to leverage the semantic relationship between different predicate classes, and design a novel CogTree loss to train models that can make informative predictions. In BGNN <ref type="bibr" target="#b12">[13]</ref>, the authors design a bi-level resampling strategy, which can help to provide a more balanced data distribution during the training process. However, previous works of designing new loss or conducting resampling, only focus on predicate-level adjustment, while the visual relation is triplet-level. For example, given the subject man and object skateboard, the predicate riding is an informative version of standing on, while given the subject man and object horse, riding will not be an informative alternative of standing on. Thus, instead of using less precise predicate-level manipulation, we employ a triplet-level transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Large-scale Scene Graph Generation</head><p>In the last few years, there are some works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35]</ref> focusing on large-scale SGG. Then, how to provide a reliable evaluation is an important problem. <ref type="bibr" target="#b39">[40]</ref> first proposes to study large-scale scene graph generation and makes a new split of Visual Genome dataset named VG80K, which contains 29,086 predicate categories. However, the annotations are too noisy to provide reliable evaluation. To cope with this problem, <ref type="bibr" target="#b0">[1]</ref> further cleans the dataset, and finally reserves 2,000 predicate classes. However, only 1,429 predicate classes are contained in the test set, among which 903 relation categories have no more than 5 samples. To provide enough samples for each predicate class' evaluation, we re-split the Visual Genome to ensure each predicate class on the test set has more than 5 samples, and the total predicate class number is 1,807. For the proposed methods, <ref type="bibr" target="#b39">[40]</ref> employs a triplet loss to regularize the visual representation with the constraint on word embedding space. RelMix <ref type="bibr" target="#b0">[1]</ref> proposes to conduct data augmentation with the format of feature mixup. Visual distant supervision <ref type="bibr" target="#b34">[35]</ref> pre-trains the model on relabeled NA data with the help of a knowledge base and achieve significant improvement on a well-defined VG setting without semantic ambiguity. However, the data extension will be significantly limited by the semantic ambiguity problem. To deal with this problem, we propose an internal transfer method to generate informative triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first introduce the internal data transfer and external data transfer, respectively, and then elaborate how to utilize them collaboratively. <ref type="figure" target="#fig_1">Figure 2</ref> shows the pipeline of our proposed IETrans. The goal of our method is to generate an enhanced dataset automatically, which should provide more training samples for tail classes, and also specify general predicate classes as informative ones. Concretely, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the general relation on between (person, beach) need to be specified as more informative one standing on, and the missed annotations between (person, kite) can be labeled so as to provide more training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Scene Graph Generation. Given an image I, a scene graph corresponding to I has a set of objects</p><formula xml:id="formula_0">O = {(b i , c i )} No i=1 and a set of relational triplets E = {(s i , p (si,oi) , o i )} Ne i=1 . For each object (b i , c i )</formula><p>, it consists of an object bounding box b i ? R 4 and an object class c i which belongs to the pre-defined object class set C. With s i ? O and o i ? O, p i is defined as relation between them and belongs to the pre-defined predicate class set P. Inference. SGG is defined as a joint detection of objects and relations. Generally, an SGG model will first detect the objects in the image I. Based on the detected objects, a typical SGG model will conduct a feature refinement for objects and relation representation, and then classify the objects and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Internal Data Transfer</head><p>The key insight of internal transfer is to transfer samples from general predicate classes to their corresponding informative ones, like the example shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We split the process into 3 sub-steps, including (1) Confusion Pair Discovery: specify confused predicate pairs as potential general-informative pairs for given subject and object classes. Confusion Pair Discovery. To find general predicate classes and corresponding informative ones, a straightforward way is to annotate the possible relation transitions manually. However, relations are highly dependent on the subject and object classes, i.e. relation is triplet-level rather than predicate-level. For example, given the entity pair man and bike, riding is a sub-type of sitting on, while for man and skateboard, riding shares different meaning with sitting on. In this condition, even under 50 predicate classes settings, the possible relation elements will scale up to an infeasible number for human annotation. Another promising alternative is to employ pre-defined knowledge bases, such as WordNet <ref type="bibr" target="#b19">[20]</ref> and VerbNet <ref type="bibr" target="#b10">[11]</ref>. However, existing knowledge bases are not specifically designed to cope with visual relation problems, which result in a gap between visual and textual hierarchies <ref type="bibr" target="#b25">[26]</ref>. Thus, in this work, we try to specify the general-informative pairs by taking advantage of information within the dataset, and leave the exploration of external knowledge sources for future work. A basic observation is that informative predicate classes are easily confused by general ones. Thus, we can first find confusion pairs as candidate generalinformative pairs. By observing the predictions of the pre-trained Motif [39] model, we find that the collapse from informative predicate classes to general ones, appears not only on the test set but also on the training set. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the predicate classes riding and sitting on are significantly confused by a more general predicate class on.</p><p>On the training set, given a relational triplet class (c s , p, c o ), we use a pre-trained baseline model to predict predicate labels of all samples belonging to (c s , p, c o ), and average their score vectors. We denote the aggregated scores for all predicates as S = {s pi |p i ? P}. From S, we select all predicate classes with higher prediction scores than the ground-truth annotation p, which can be formulated as P c = {p i |s pi &gt; s p }. P c can be considered as the most confusing predicate set for (c s , p, c o ), which can serve as candidate transfer sources.</p><p>Transfer Pair Judgement. However, a confused predicate class does not equal to a general one. Sometimes, a general predicate can also be confused by an informative predicate. For example, in <ref type="figure" target="#fig_2">Figure 3</ref>, under the constraint of c s = man and c o = motorcycle, the less informative predicate sitting on is confused by the more informative predicate riding. In this condition, it is not a good choice to transfer from riding to sitting on. Thus, we need to further select the truly general predicates from the candidate set P c .</p><p>To select the most possible general predicate classes from P c , we first introduce an important feature that is useful to recognize general predicate classes. As observed by <ref type="bibr" target="#b36">[37]</ref>, the general predicate classes usually cover more diverse relational triplets, while informative ones are limited. Based on this observation, we can define the attraction factor of a triplet category (c s , p, c o ) as:</p><formula xml:id="formula_1">A(c s , p, c o ) = 1 ci,cj ?C I(c i , p, c j ) ,<label>(1)</label></formula><p>where C is the object categories set and I(t) indicates whether the triplet category t exists in the training set, which can be formulated as:</p><formula xml:id="formula_2">I(t) = 1, if t ? training set 0, otherwise<label>(2)</label></formula><p>The denominator of A(c s , p i , c o ) is the number of relational triplet types containing p i . Thus, A(c s , p i , c o ) with smaller value means p i is more likely to be a general predicate. Concretely, when</p><formula xml:id="formula_3">A(c s , p i , c o ) &lt; A(c s , p, c o ), we transfer data from (c s , p i , c o ) to (c s , p, c o ).</formula><p>However, only considering the number of relational triplet types also has drawbacks: some relational triplets with very limited number of samples (e.g., only 1 or 2 samples) might be annotation noise, while these relational triplets are easily selected as transfer targets. Transferring too much data to such uncertain relational triplets will significantly degenerate models' performance. Thus, we further consider the number of each relational triplet and modify the attraction factor as:</p><formula xml:id="formula_4">A(c s , p, c o ) = N (c s , p, c o ) ci,cj ?C I(c i , p, c j ) ? N (c i , p, c j ) ,<label>(3)</label></formula><p>where N (t) denotes the number of instances with relational type t in the training set. With the attraction factor, we can further filter the candidate confusion set P c to the valid transfer source for (c s , p, c o ):</p><formula xml:id="formula_5">P s = {p i |(p i ? P c ) ? (A(c s , p i , c o ) &lt; A(c s , p, c o ))},<label>(4)</label></formula><p>where ? denotes the logical conjunction operator.</p><p>Triplet Transfer. Given the transfer source P s , we collect all samples in the training set satisfying:</p><formula xml:id="formula_6">T = {(o i , p k , o j )|(c oi = c s ) ? (p k ? P s ) ? (c oj = c o )}.<label>(5)</label></formula><p>Then, we sort T by model's prediction score of p, and transfer the top k I % samples to the target triplet category (c s , p, c o ). Note that, a triplet instance may need to be transferred to more than one relational triplets. To deal with the conflict, we choose the target predicate with the highest attraction factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">External Data Transfer</head><p>The goal of our external transfer is to relabel unannotated samples to excavate missed relational triplets, as the example shown in <ref type="figure" target="#fig_0">Figure 1</ref>. NA Relabeling. NA samples refer to all unannotated samples in the training set, including both truly negative samples and missed positive samples. In external transfer, NA samples are directly considered as the transfer source and are relabeled as existing relational triplet types.</p><p>To get the NA samples, we first traverse all unannotated object pairs in images. However, considering that data transfer from all NA samples to all possible predicate classes will bring heavy computational burden, and inevitably increase the difficulty of conducting precise transfer, so as to sacrifice the quality of transferred data. Thus, we only focus on object pairs whose bounding boxes have overlaps and limit the possible transfer targets to existing relational triplet types. The exploration of borrowing zero-shot relational triplets from NA is left for future work.</p><p>Given a sample (s, NA, o), we can get its candidate target predicate set as:</p><formula xml:id="formula_7">Tar(s, NA, o) = {p|(p ? P) ? (N (c s , p, c o ) &gt; 0) ? (IoU(b s , b o ) &gt; 0)},<label>(6)</label></formula><p>where P denotes pre-defined predicate classes, b s and b o denote bounding boxes of s and o, and IoU denotes the intersection over union. Given a triplet (s, NA, o), the predicate class with the highest prediction score except for NA is chosen. The label assignment can be formulated as:</p><formula xml:id="formula_8">p (s,o) = arg max p?Tar(s,NA,o) (? p (s, o)),<label>(7)</label></formula><p>where ? p (?) denotes the prediction score of predicate p. NA Triplet Transfer. To decide transfer or not, we rank all chosen (s, NA, o) samples according to NA scores in an ascending order. The lower NA score means the sample is more likely to be a missed positive sample. Similar with internal transfer, we simply transfer the top k E % data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Integration</head><p>Internal transfer is conducted on annotated data and external transfer is conducted on unannotated data, which are orthogonal to each other. Thus, we can simply merge the data without conflicts. After obtaining the enhanced dataset, we re-train a new model from scratch and use the new model to make inferences on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first show the generalizability of our method with different baseline models and the expansibility to large-scale SGG. We also make ablation studies to explore the influence of different modules and hyperparameters. Finally, analysis is conducted to show the effectiveness of our method in enhancing the current dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generalizability with Different Baseline Models</head><p>We first validate the generalizability of our method with different baseline models and its effectiveness when compared with current SOTA methods.</p><p>Datasets. Popular VG-50 <ref type="bibr" target="#b30">[31]</ref> benchmark is employed, which consists of 50 predicate classes and 150 object classes.</p><p>Tasks. Following previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>, we evaluate our model on three widely used SGG tasks: (1) Predicate Classification (PREDCLS) provides both localization and object classes, and requires models to recognize predicate classes.</p><p>(2) Scene Graph Classification (SGCLS) provides only correct localization and asks models to recognize both object and predicate classes. <ref type="formula" target="#formula_4">(3)</ref> In Scene Graph Detection (SGDET), models are required to first detect the bounding boxes and then recognize both object and predicate classes.</p><p>Metrics. Following previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b23">24]</ref>, we use Recall@K (R@K) and mean Recall@K (mR@K) as our metrics. However, different trade-offs between R@K and mR@K are made in different methods, which makes it hard to make a direct comparison. Therefore, we further propose an overall metric F@K to jointly evaluate R@K and mR@K, which is the harmonic average of R@K and mR@K.</p><p>Baselines. We categorize several baseline methods into two categories: (1) Model-agnostic baselines. They refers to methods that can be applied in a plug-and-play fashion. For this part, we include Resampling <ref type="bibr" target="#b12">[13]</ref>, TDE <ref type="bibr" target="#b22">[23]</ref>, CogTree <ref type="bibr" target="#b36">[37]</ref>, EBM <ref type="bibr" target="#b21">[22]</ref>, DeC <ref type="bibr" target="#b8">[9]</ref>, and DLFE <ref type="bibr" target="#b4">[5]</ref>. (2) Specific models. We also include some dedicated designed models with strong performance, including KERN <ref type="bibr" target="#b3">[4]</ref>, KERN <ref type="bibr" target="#b3">[4]</ref>, GBNet <ref type="bibr" target="#b37">[38]</ref>, BGNN <ref type="bibr" target="#b12">[13]</ref>, DT2-ACBS <ref type="bibr" target="#b5">[6]</ref>, and PCPL <ref type="bibr" target="#b31">[32]</ref>.</p><p>Implementation Details. Following <ref type="bibr" target="#b22">[23]</ref>, we employ a pre-trained Faster-RCNN <ref type="bibr" target="#b20">[21]</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref> backbone. In the training process, the parameters of the detector are fixed to reduce the computation cost. The batch size is set to 12, and the learning rate is 0.12, except for Transformer. We optimize all models with an SGD optimizer. Specifically, to better balance the data distribution, the external transfer will not be conducted for the top 15 frequent predicate classes. To avoid deviating too much from the original data distribution, the frequency bias item calculated from the original dataset is applied to our IETrans in the inference stage. For internal and external transfer, the k I is set to 70% and k E is set to 100%. Please refer to the Appendix for more details.</p><p>Comparison with SOTAs. We report the results of our IETrans and baselines for VG-50 in <ref type="table" target="#tab_1">Table 1</ref>. Based on the observation of experimental results, we have summarized the following conclusions:</p><p>Our IETrans is adaptive to different baseline models. We equip our method with 4 different models, including Motif <ref type="bibr" target="#b38">[39]</ref>, VCTree <ref type="bibr" target="#b23">[24]</ref>, GPS-Net <ref type="bibr" target="#b17">[18]</ref>, and Transformer <ref type="bibr" target="#b22">[23]</ref>. The module architectures range from conventional CNN to TreeLSTM (VCTree) and self-attention layers (Transformer). The training algorithm contains both supervised training and reinforcement learning (VCTree). Despite the model diversity, our IETrans can boost all models' mR@K metric and also achieve competitive F@K performance. For example, our IETrans can double mR@50/100 and improve the overall metric F@50/100 for over 9 points across all 3 tasks for GPS-Net.</p><p>Compared with other model-agnostic methods, our method outperforms all of them in nearly all metrics. For example, when applying IETrans to Motif on PREDCLS, our model can achieve the highest R@50/100 and mR@50/100 among all model-agnostic baselines except for DeC. After adding the reweighting strategy, our IETrans can outperform DeC on mR@K.</p><p>Compared with strong specific baselines, our method can also achieve competitive performance on mR@50/100, and best overall performance on F@50/100. Considering mR@50/100, our method with reweighting strategy is slightly lower than DT2-ACBS on SGCLS and SGDET tasks, while our method performs much better than them on R@50/100 (e.g., 24.3 points of VCTree on PREDCLS task). For overall comparison considering F@50/100 metrics, our VCTree+IETrans+Rwt can achieve the best F@50/100 on PREDCLS and Motif+IETrans+Rwt achieves the best F@50/100 in SGCLS and SGDET task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expansibility to Large-Scale SGG</head><p>We also validate our IETrans on VG-1800 dataset to show its expansibility to large-scale scenarios.</p><p>Datasets. We re-split the Visual Genome dataset to create a VG-1800 benchmark, which contains 70,098 object categories and 1,807 predicate categories. Different from previous large-scale VG split <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1]</ref>, we clean the misspellings and unreasonable relations manually and make sure all 1,807 predicate categories appear on both training and test set. For each predicate category, there are over 5 samples on the test set to provide a reliable evaluation. Detailed statistics of VG-1800 dataset are provided in Appendix.</p><p>Tasks. In this work, we mainly focus on the predicate-level recognition ability and thus compare models on PREDCLS in the main paper. For SGCLS results, please refer to the Appendix.</p><p>Metrics. Following <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b0">1]</ref>, we use accuracy (Acc) and mean accuracy upon all predicate classes (mAcc). Similar to VG-50, the harmonic average of two metrics is reported as F-Acc. In addition, we also report the number of predicate classes that the model can make at least one correct prediction, denoted as Non-Zero.</p><p>Baselines. We also include model-agnostic baselines including Focal Loss <ref type="bibr" target="#b16">[17]</ref>, TDE <ref type="bibr" target="#b22">[23]</ref>, and RelMix <ref type="bibr" target="#b0">[1]</ref>, and a specific model BGNN <ref type="bibr" target="#b12">[13]</ref>.</p><p>Implementation Details. Please refer to the Appendix for details.  Comparison with SOTAs. Performance of our method and baselines are shown in <ref type="table" target="#tab_2">Table 2</ref>. Based on the observation of experimental results, we have summarized the following conclusions: Our model can successfully work on large-scale settings. On VG-1800 dataset, the long-tail problem is even exacerbated, where hundreds of predicate classes have only less than 10 samples. Simply increasing loss weight (Focal Loss) on tail classes can not work well. Different from these methods, our IETrans can successfully boost the performance on mAcc while keeping competitive results on Acc. For quantitative comparison, our IETrans (k I = 10%) can significantly improve the performance on top-10 mAcc (e.g., <ref type="bibr" target="#b18">19</ref>.12% vs. 4.37%) while maintaining comparable performance on Acc.</p><p>Compared with different baselines, our method can outperform them for overall evaluation. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our IETrans (k I = 90%) can achieve best performance on F-Acc, which is over 3 times of the second highest baseline, RelMix, a method specifically designed for large-scale SGG. To make the visualized comparison, we plot a curve of IETrans with different Acc and mAcc trade-offs by tuning k I , and show the performance of other baselines as points. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, all baselines drawn as points are under our curve, which means our method can achieve better performance than them. Moreover, our IETrans (k I = 90%) can make correct predictions on 467 predicate classes for top-1 results, while the Non-Zero value of all other baselines are less than 70.</p><p>Case Studies. To show the potential of our method for real-world application, we provide some cases in <ref type="figure" target="#fig_3">Figure 5</ref>. We can observe that our IETrans can help to generate more informative predicate classes while keeping faithful to the image content. For example, when the Motif model only predict relational triplets like (foot, of, bear ), (nose, on, bear ) and (cloud, in, sky), our IETrans can generate more informative ones as (foot, belonging to, bear ), (nose, sewn onto, bear ), and (cloud, floating through, sky). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In this part, we analyse the influence of internal transfer, external transfer, and corresponding parameters, k I and k E . Influence of Internal Transfer. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, only using external transfer (yellow cube) is hard to boost the mAcc performance as much as IETrans. The reason is that although introducing samples for tail classes, they will still be suppressed by corresponding general ones. However, by introducing internal transfer (green point) to cope with semantic ambiguity problem, the performance (red cross) can be improved significantly on mAcc, together with minor performance drop on Acc. Influence of External Transfer. Although internal transfer can achieve huge improvement on mAcc compared with Motif, its performance is poor compared with IETrans, which shows the importance of further introducing training data by external transfer. Integration of two methods can maximize the advantages of data transfer. Influence of k I . As shown in <ref type="figure" target="#fig_5">Figure 7</ref> (a), with the increase of k I , the top-10 F-Acc will increase until k I = 80%, and begin to decrease when k I &gt; 80%. The phenomenon indicates that a large number of general predicates can be interpreted as informative ones. Moving these predicates to informative ones will boost the overall performance. However, there also exists some predicates that can not be interpreted as informative ones or be modified suitably by current methods, which is harmful to the performance of models. Influence of k E . As shown in <ref type="figure" target="#fig_5">Figure 7 (b)</ref>, the overall performance increases slowly with the initial 90% transferred data, but improves significantly with the rest 10%. Note that, the data is ranked according to the NA score, which means that the last 10% data is actually  what the model considered as most likely to be truly negative. The phenomenon indicates that the model may easily classify tail classes as negative samples, while this part of data is of vital significance for improving the model's ability of making informative predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Enhanced Dataset</head><p>Enhanced Scene Graph Correctness. We investigate the correctness of enhanced scene graphs from the instance level. An example is shown in <ref type="figure" target="#fig_8">Figure 8(a)</ref>.</p><p>We can see that the IETrans is less accurate on VG-1800, which indicates that it is more challenging to conduct precise data transfer on VG-1800.</p><p>Distribution Change. The distribution change is shown in <ref type="figure" target="#fig_8">Figure 8</ref>(b). We can see that our IETrans can effectively supply samples for non-head classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we design a data transfer method named IETrans to generate an enhanced dataset for the SGG. The proposed IETrans consists of an internal transfer module to relabel general predicate classes as informative ones and an external transfer module to complete missed annotations. Comprehensive experiments are conducted to show the effectiveness of our method. In the future, we hope to extend our method to other large-scale visual recognition problems (e.g., image classification, semantic segmentation) with similar challenges.</p><p>Acknowledgements. This research is funded by Sea-NExT Joint Lab, Singapore. The research is also supported by the National Key Research and Development Program of China (No. 2020AAA0106500).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A VG-1800 Dataset</head><p>The VG-1800 dataset aims to provide reliable evaluation for the large-scale scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Construction</head><p>We construct the dataset based on original Visual Genome dataset <ref type="bibr" target="#b11">[12]</ref> by the following steps: (1) Filtration. Instead of simply auto-filtering <ref type="bibr" target="#b39">[40]</ref> and choosing the top frequent predicate categories <ref type="bibr" target="#b0">[1]</ref>, we manually filter out unreasonable predicate categories, including misspelling predicates (e.g., i frot of), adjectives (e.g., white), nouns (e.g., car), and relative clauses (e.g., who has). To provide enough relation instances for robust evaluation, we retain all object categories and predicate categories with over 5 samples. (2) Split. We split the VG dataset into 70% training and 30% test. Following VG-50 split <ref type="bibr" target="#b30">[31]</ref>, we further split out 5, 000 images from the training set as the validation set, and ensure at least 5 samples on the test set and at least 1 samples on the training set for each predicate category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Statistics</head><p>Finally, the dataset contains 70, 098 object categories,  (e) Influence of kE on mAcc.</p><p>10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Top-10 F-Acc.</p><p>(f) Influence of kE on F-Acc. <ref type="figure">Fig. 9</ref>. Influence of kI and kE in different metrics.</p><p>Comparison with Other VG Splits. We also compare our VG-1800 split with other splits based on Visual Genome <ref type="bibr" target="#b11">[12]</ref> dataset, including a conventional VG- 50 and the other two large-scale SGG splits VG8K and VG8K-LT. Our VG-1800 can provide a more reliable evaluation for large-scale SGG. (1) When compared with VG8K, we provide a much cleaner dataset by manually cleaning the noise. For example, VG8K does not filter out nouns and adjectives, which will lead to an unreliable evaluation. (2) When compared with VG8K-LT, a cleaner version of VG8K, we provide a much stable evaluation for large amount of tail classes. More specifically, as shown in <ref type="table" target="#tab_4">Table 3</ref>, our VG-1800 contains more test images. Meanwhile, VG-1800 also contains more samples of tail classes. As shown in <ref type="figure" target="#fig_0">Figure 10</ref>, our VG-1800 has 1,807 predicate classes with no less than 5 samples, while VG8K-LT has only 526 classes that have no less than 5 samples. remove the frequency bias in the training and inference of base models, which may make the result tend to have higher mR@K, F@K and lower R@K than results in their original papers. For Transformer, some implementation details are different. The batch size can be enlarged to 16 on 2 GPUs. The learning rate is reduced to 0.08 for PREDCLS and SGDET for training stability. For Transformer on SGCLS, where the training is even more unstable, we further lower the learning rate to 0.016. All experiments are done on RTX-2080ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 VG-1800</head><p>Compared with VG-50, the same backbone, parameter fixation, learning rate, optimizer, and learning schedule are used on VG-1800 dataset. Specially, due to the significant increase of (subject, predicate, object) combinations, we equally remove all frequency bias items on VG-1800 to reduce machines' memory usage. For internal and external transfer, the k I is set to 90% and k E is set to 100%.</p><p>For baselines, we find that due to the significant difference between the number of head predicates and tail predicates, the bi-level resampling in BGNN <ref type="bibr" target="#b12">[13]</ref> will make the model pay most of the attention on tail classes while ignoring head classes. The drop out rate of images that do not contain rare predicates are set to almost 100%. This lead to a bad convergence of BGNN. Thus, we remove bi-level resampling for BGNN results. For RelMix <ref type="bibr" target="#b0">[1]</ref>, we equip the proposed VilHub loss and predicate feature mixup to the Neural Motif model. Similar with bi-level sampling, we find that the reweighting strategy also lead to worse results. Thus, we do not include a reweighting version like VG-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Supplementary Experiments</head><p>C.1 Influence of k I abd k E Influence of k I . To provide a more detailed analysis on the influence of k I , we report the performance on Acc and mAcc with different k I . As shown in <ref type="figure">Figure 9</ref>, with the increase of internal transfer percentage k I , Acc decreases linearly, while mAcc first increases when k I ? 80% and then decreases. The phenomenon shows that transferring more in internal transfer does not necessarily mean higher mAcc. For VG-1800, The first 80% internal data transfer is helpful to improve mAcc, while the last confident 20% will harm the overall performance. We guess the last 20% data may contain too noise, which will lower the data quality for model training. Influence of k E . As for k E , external transfer shows almost no influence on Acc and mAcc when k E ? 90%, while significantly boost mAcc when k E = 100%. Contrary to our observations for k I , the last 10% samples which are believed to be unuseful by models, seem to bring the most profitable boost for mAcc. We guess the reason is that model can not distinguish well between tail classes and NA samples, while this part of the data is essential to provide more training samples for tail classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Adaptive Threshold.</head><p>In our IETrans, when determining how much data to transfer, we equally use a fixed percentage number for all relational triplets, which seems to be suboptimal. Thus, we also tried an adaptive threshold by considering the prediction score of concrete relational triplet instances.</p><p>For internal transfer, given a general relational triplet instance (o s , p G , o o ), we are required to decide whether to transfer to its corresponding informative type p I . We denote the model's prediction score of object pair (o s , o o ) on p I as s * p I . We denote the average and standard error of all (c os , p I , c oo ) relational triplet instances' prediction score on p I as ? I and ? I . We conduct the transfer when s * p I satisfies: s * p I &gt; ? I + k? I ,</p><p>where k is a hyperparameter. The intuition is that if a general instance (o s , p G , o o )'s prediction score is over the average of all real (c os , p I , c oo )'s prediction scores on p I , (o s , p G , o o ) can probably be relabeled as an informative one. Meanwhile, the standard error is considered to further control the adaptive threshold. By choosing different k including {?1.0, ?0.5, 0.0, 0.5, 1.0}, we can get a curve with different Acc and mAcc trade-offs. As shown in <ref type="figure" target="#fig_0">Figure 11</ref>, the model with adaptive thresholds is overall worse than our fixed percentage.</p><p>A possible explanation is that the prediction score of an instance is nonlinearly dependent on the number of its own instances and its similarity with different general classes, which results in inconsistency among different relational triplets. Especially when the number of an informative relational triplet is very small, the average of its prediction score is often near to zero, which will easily lead to an over-transfer problem. Thus, we leave the design of a more intelligent adaptive threshold for future work.</p><p>For external transfer, as shown in the paper, the prediction scores on NA of missed annotated samples are unreliable, i.e. the samples with the highest NA score bring maximum benefits for the model's performance. SGCLS Results on VG-1800. We also evaluate our method on SGCLS task on VG-1800 dataset. As shown in table 4, the comparison with other baselines is similar to the results on PREDCLS task. When compared with Motif, our IETrans (k I = 10%) can achieve significant improvement on top-1 mAcc and Non-Zero metrics (over 5 times of Motif) with negligible degeneration (less than 1 point) on top-1 Acc metric. Our IETrans (k I = 90%) can further boost the mAcc and Non-Zero metrics, which shows the ability of our IETrans to generate informative scene graphs. When compared with other baselines, our IEtrans can achieve the best F-Acc metrics across top-1, top-5, and top-10 evaluations. However, there is a large gap between SGCLS results and PREDCLS results (e.g., 2.62% vs. 4.70% for top-1 F-Acc of IETrans (k I = 90%)), which indicates that further effort should be made to explore the joint optimization of both objects and predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Discovered Visual Hierarchy Analysis</head><p>Visual Hierarchy Evaluation. A key element of conducting correct internal transfer is to find reasonable general-informative relation pairs. To evaluate the precision, we randomly choose 50 pairs with over 3 samples being transferred, so as to avoid involving too many noise-to-noise pairs. Then, human evaluation is conducted. The ratio of reasonable general-informative pairs is 76% for VG-50, and 74% for VG-1800. Visualization. In the following, we show 100 discovered general-informative pairs for both VG-50 and VG-1800. The pairs are ranked by the number of samples which are transferred. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Generate an enhanced dataset automatically for better model training with: (a) Internal Transfer: Specify general predicate annotations as informative ones. (b) External Transfer: Relabel missed relations from NA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2 )</head><label>2</label><figDesc>Transfer Pair Judgement: judge whether the candidate pair is valid. (3) Triplet Transfer: transfer data from the selected general predicate class to the corresponding informative one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Confusion matrix of Motif<ref type="bibr" target="#b38">[39]</ref>'s prediction score on all entity pairs in VG training set with the subject man and the object motorcycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of raw Motif model and Motif equipped with our IETrans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The mAcc and Acc curve. (a) is our IETrans method. kI is tuned to generate the blue curve. (b-f) are baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>(a) The influence of kI in the Top-10 F-Acc with only internal transfer. (b) The influence of kE in the Top-10 F-Acc with only external transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The mAcc and Acc curve. (a) Normally trained Motif. (b) ExTrans: external transfer. (c) InTrans: internal transfer. (d) Our proposed IETrans. kI is tuned to generate a curve. The blue circle and arrow mean that combining ExTrans and InTrans can lead to the pointed result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>(a) Enhanced Scene Graph Visualization. Gray line denotes unchanged relation. Blue line denotes changed and reasonable relation. Red line denotes changed but unreasonable relation. (b) Distribution Change. The comparison between distributions of original dataset and enhanced dataset for VG-1800. The x-axis is the relation id intervals from head to tail classes. The y-axis is the corresponding log-frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Comparison of number of predicate classes between different splits on test set. The n ? 5 denotes predicate classes having no less than 5 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Comparison of adaptive thresholds and fixed percentages for Internal Transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>scores External Transfer Internal Transfer Confused Pair Discovery Transfer Pair Judgement</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">scores</cell><cell></cell><cell>*</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell>p1 Transfer</cell><cell>Internal Data</cell></row><row><cell>*</cell><cell>?</cell><cell>predict</cell><cell>* ...</cell><cell cols="2">GT</cell><cell></cell><cell></cell><cell></cell><cell>0.7 0.9</cell><cell>? ?</cell><cell></cell><cell>%</cell></row><row><cell cols="2">All ( , * , ) samples</cell><cell cols="3">predicates</cell><cell></cell><cell></cell><cell>Confused relations</cell><cell></cell><cell cols="3">Attraction factor</cell><cell>*</cell><cell>Dataset Enhanced</cell></row><row><cell></cell><cell cols="2">NA Relabeling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Transfer</cell></row><row><cell></cell><cell>... NA NA</cell><cell>... predict</cell><cell></cell><cell>NA</cell><cell>...</cell><cell>0.4 0.2</cell><cell>?</cell><cell>( (</cell><cell>...</cell><cell>) )</cell><cell>0.2 0.3</cell><cell>%</cell><cell>External Data</cell></row><row><cell>All NA samples</cell><cell>NA</cell><cell>...</cell><cell></cell><cell>...</cell><cell></cell><cell>0.1</cell><cell></cell><cell cols="3">( Relabeled ) NA samples</cell><cell>0.9 NA score</cell><cell>External Data</cell></row><row><cell cols="13">Fig. 2. Illustration of our proposed IETrans to generate an enhanced dataset. Internal</cell></row><row><cell cols="13">transfer is designed to transfer data from general predicate to informative ones. Ex-</cell></row><row><cell cols="13">ternal transfer is designed to relabel NA data. To avoid misunderstanding, (cs, p  *  , co)</cell></row><row><cell cols="13">is a relational triplet class. (si, p (s i ,o i ) , oi) represents a single relational triplet instance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance (%) of our method and other baselines on VG-50 dataset. IETrans denotes different models equipped with our IETrans. Rwt denotes using the reweighting strategy. IETrans+Rwt (ours) 48.6 / 50.5 35.8 / 39.1 41.2 / 44.1 29.4 / 30.2 21.5 / 22.8 24.8 / 26.0 23.5 / 27.2 15.5 / 18.0 18.7 / 21.7 IETrans+Rwt (ours) 48.0 / 49.9 37.0 / 39.7 41.8 / 44.2 30.0 / 30.9 19.9 / 21.8 23.9 / 25.6 23.6 / 27.8 12.0 / 14.9 15.9 / 19.4 IETrans+Rwt (ours) 47.5 / 49.4 34.9 / 38.6 40.2 / 43.3 29.3 / 30.3 19.8 / 21.6 23.6 / 25.2 23.1 / 25.0 16.2 / 18.8 19.0 / 21.5 IETrans+Rwt (ours) 49.0 / 50.8 35.0 / 38.0 40.8 / 43.5 29.6 / 30.5 20.8 / 22.3 24.4 / 25.8 23.1 / 27.1 15.0 / 18.1 18.2 / 21.7</figDesc><table><row><cell></cell><cell>Models</cell><cell cols="3">Predicate Classification</cell><cell cols="3">Scene Graph Classification</cell><cell cols="3">Scene Graph Detection</cell></row><row><cell></cell><cell></cell><cell cols="9">R@50 / 100 mR@50 / 100 F@50 / 100 R@50 / 100 mR@50 / 100 F@50 / 100 R@50 / 100 mR@50 / 100 F@50 / 100</cell></row><row><cell></cell><cell>KERN [4]</cell><cell>65.8 / 67.6</cell><cell>17.7 / 19.2</cell><cell cols="2">27.9 / 29.9 36.7 / 37.4</cell><cell>9.4 / 10.0</cell><cell cols="2">15.0 / 15.8 27.1 / 29.8</cell><cell>6.4 / 7.3</cell><cell>10.4 / 11.7</cell></row><row><cell>Specific</cell><cell>GBNet [38] BGNN [13] DT2-ACBS [6]</cell><cell>66.6 / 68.2 59.2 / 61.3 23.3 / 25.6</cell><cell>22.1 / 24.0 30.4 / 32.9 35.9 / 39.7</cell><cell cols="2">33.2 / 35.5 37.3 / 38.0 40.2 / 42.8 37.4 / 38.5 28.3 / 31.1 16.2 / 17.6</cell><cell>12.7 / 13.4 14.3 / 16.5 24.8 / 27.5</cell><cell cols="2">18.9 / 19.8 26.3 / 29.9 20.7 / 23.1 31.0 / 35.8 19.6 / 21.5 15.0 / 16.3</cell><cell>7.1 / 8.5 10.7 / 12.6 22.0 / 24.0</cell><cell>11.2 / 13.2 15.9 / 18.6 17.8 / 19.4</cell></row><row><cell></cell><cell>PCPL [32]</cell><cell>50.8 / 52.6</cell><cell>35.2 / 37.8</cell><cell cols="2">41.6 / 44.0 27.6 / 28.4</cell><cell>18.6 / 19.6</cell><cell cols="2">22.2 / 23.2 14.6 / 18.6</cell><cell>9.5 / 11.7</cell><cell>11.5 / 14.4</cell></row><row><cell></cell><cell>Motif [39]</cell><cell>64.0 / 66.0</cell><cell>15.2 / 16.2</cell><cell cols="2">24.6 / 26.0 38.0 / 38.9</cell><cell>8.7 / 9.3</cell><cell cols="2">14.2 / 15.0 31.0 / 35.1</cell><cell>6.7 / 7.7</cell><cell>11.0 / 12.6</cell></row><row><cell></cell><cell>-TDE [23]</cell><cell>46.2 / 51.4</cell><cell>25.5 / 29.1</cell><cell cols="2">32.9 / 37.2 27.7 / 29.9</cell><cell>13.1 / 14.9</cell><cell cols="2">17.8 / 19.9 16.9 / 20.3</cell><cell>8.2 / 9.8</cell><cell>11.0 / 13.2</cell></row><row><cell></cell><cell>-CogTree [37]</cell><cell>35.6 / 36.8</cell><cell>26.4 / 29.0</cell><cell cols="2">30.3 / 32.4 21.6 / 22.2</cell><cell>14.9 / 16.1</cell><cell cols="2">17.6 / 18.7 20.0 / 22.1</cell><cell>10.4 / 11.8</cell><cell>13.7 / 15.4</cell></row><row><cell></cell><cell>-EBM [22]</cell><cell>-/ -</cell><cell>18.0 / 19.5</cell><cell>-/ -</cell><cell>-/ -</cell><cell>10.2 / 11.0</cell><cell>-/ -</cell><cell>-/ -</cell><cell>7.7 / 9.3</cell><cell>-/ -</cell></row><row><cell></cell><cell>-DeC [9]</cell><cell>-/ -</cell><cell>35.7 / 38.9</cell><cell>-/ -</cell><cell>-/ -</cell><cell>18.4 / 19.1</cell><cell>-/ -</cell><cell>-/ -</cell><cell>13.2 / 15.6</cell><cell>-/ -</cell></row><row><cell></cell><cell>-DLFE [5]</cell><cell>52.5 / 54.2</cell><cell>26.9 / 28.8</cell><cell cols="2">35.6 / 37.6 32.3 / 33.1</cell><cell>15.2 / 15.9</cell><cell cols="2">20.7 / 21.5 25.4 / 29.4</cell><cell>11.7 / 13.8</cell><cell>16.0 / 18.8</cell></row><row><cell></cell><cell>-IETrans (ours)</cell><cell>54.7 / 56.7</cell><cell>30.9 / 33.6</cell><cell cols="2">39.5 / 42.2 32.5 / 33.4</cell><cell>16.8 / 17.9</cell><cell cols="2">22.2 / 23.3 26.4 / 30.6</cell><cell>12.4 / 14.9</cell><cell>16.9 / 20.0</cell></row><row><cell>Model-Agnostic</cell><cell>-VCTree [24] -TDE [23] -CogTree [37] -EBM [22] -DLFE [5] -IETrans (ours)</cell><cell>64.5 / 66.5 47.2 / 51.6 44.0 / 45.4 -/ -51.8 / 53.5 53.0 / 55.0</cell><cell>16.3 / 17.7 25.4 / 28.7 27.6 / 29.7 18.2 / 19.7 25.3 / 27.1 30.3 / 33.9</cell><cell cols="2">26.0 / 28.0 39.3 / 40.2 33.0 / 36.9 25.4 / 27.9 33.9 / 35.9 30.9 / 31.7 -/ --/ -34.0 / 36.0 33.5 / 34.6 38.6 / 41.9 32.9 / 33.8</cell><cell cols="3">8.9 / 9.5 12.2 / 14.0 18.8 / 19.9 12.5 / 13.5 18.9 / 20.0 24.2 / 25.3 22.7 / 26.3 14.5 / 15.4 30.2 / 34.6 16.5 / 18.6 19.4 / 23.2 23.4 / 24.5 18.2 / 20.4 -/ --/ -16.5 / 18.1 22.0 / 23.6 25.4 / 29.3</cell><cell>6.7 / 8.0 9.3 / 11.1 10.4 / 12.1 7.7 / 9.1 11.8 / 13.8 11.5 / 14.0</cell><cell>11.0 / 13.0 12.6 / 15.0 13.2 / 15.2 -/ -15.5 / 18.1 15.8 / 18.9</cell></row><row><cell></cell><cell>-GPS-Net [18]</cell><cell>65.1 / 66.9</cell><cell>15.0 / 16.0</cell><cell cols="2">24.4 / 25.8 36.9 / 38.0</cell><cell>8.2 / 8.7</cell><cell cols="2">13.4 / 14.2 30.3 / 35.0</cell><cell>5.9 / 7.1</cell><cell>9.9 / 11.8</cell></row><row><cell></cell><cell>-Resampling [13]</cell><cell>64.4 / 66.7</cell><cell>19.2 / 21.4</cell><cell cols="2">29.6 / 32.4 37.5 / 38.6</cell><cell>11.7 / 12.5</cell><cell cols="2">17.8 / 18.9 27.8 / 32.1</cell><cell>7.4 / 9.5</cell><cell>11.7 / 14.7</cell></row><row><cell></cell><cell>-DeC [9]</cell><cell>-/ -</cell><cell>35.9 / 38.4</cell><cell>-/ -</cell><cell>-/ -</cell><cell>17.4 / 18.5</cell><cell>-/ -</cell><cell>-/ -</cell><cell>11.2 / 15.2</cell><cell>-/ -</cell></row><row><cell></cell><cell>-IETrans (ours)</cell><cell>52.3 / 54.3</cell><cell>31.0 / 34.5</cell><cell cols="2">38.9 / 42.2 31.8 / 32.7</cell><cell>17.0 / 18.3</cell><cell cols="2">22.2 / 23.5 25.9 / 28.1</cell><cell>14.6 / 16.5</cell><cell>18.7 / 20.8</cell></row><row><cell></cell><cell>-Transformer [23]</cell><cell>63.6 / 65.7</cell><cell>17.9 / 19.6</cell><cell cols="2">27.9 / 30.2 38.1 / 39.2</cell><cell>9.9 / 10.5</cell><cell cols="2">15.7 / 16.6 30.0 / 34.3</cell><cell>7.4 / 8.8</cell><cell>11.9 / 14.0</cell></row><row><cell></cell><cell>-CogTree [37]</cell><cell>38.4 / 39.7</cell><cell>28.4 / 31.0</cell><cell cols="2">32.7 / 34.8 22.9 / 23.4</cell><cell>15.7 / 16.7</cell><cell cols="2">18.6 / 19.5 19.5 / 21.7</cell><cell>11.1 / 12.7</cell><cell>14.1 / 16.0</cell></row><row><cell></cell><cell>-IETrans (ours)</cell><cell>51.8 / 53.8</cell><cell>30.8 / 34.7</cell><cell cols="2">38.6 / 42.2 32.6 / 33.5</cell><cell>17.4 / 19.1</cell><cell cols="2">22.7 / 24.3 25.5 / 29.6</cell><cell>12.5 / 15.0</cell><cell>16.8 / 19.9</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of our method and baselines on VG-1800 dataset. IETrans denotes the Motif [39] model trained using our IETrans. To better compare with baselines, we show different Acc and mAcc trade-offs by setting different kI .</figDesc><table><row><cell>Models</cell><cell>Top-1</cell><cell></cell><cell></cell><cell>Top-5</cell><cell></cell><cell cols="2">Top-10</cell><cell></cell></row><row><cell></cell><cell cols="8">Acc mAcc F-Acc Non-Zero Acc mAcc F-Acc Non-Zero Acc mAcc F-Acc Non-Zero</cell></row><row><cell>BGNN [13]</cell><cell>61.55 0.59 1.16</cell><cell>37</cell><cell>85.64 2.33</cell><cell>4.5</cell><cell>111</cell><cell>90.07 3.91</cell><cell>7.50</cell><cell>139</cell></row><row><cell>Motif [39]</cell><cell>59.63 0.61 1.21</cell><cell>47</cell><cell>84.82 2.68</cell><cell>5.20</cell><cell>112</cell><cell>89.44 4.37</cell><cell>8.33</cell><cell>139</cell></row><row><cell>-Focal Loss</cell><cell>54.65 0.26 0.52</cell><cell>14</cell><cell>79.69 0.79</cell><cell>1.56</cell><cell>27</cell><cell>85.21 1.36</cell><cell>2.68</cell><cell>41</cell></row><row><cell>-TDE [23]</cell><cell>60.00 0.62 1.23</cell><cell>45</cell><cell>85.29 2.77</cell><cell>5.37</cell><cell>119</cell><cell>89.92 4.65</cell><cell>8.84</cell><cell>152</cell></row><row><cell>-RelMix [1]</cell><cell>60.16 0.81 1.60</cell><cell>65</cell><cell>85.31 3.27</cell><cell>6.30</cell><cell>134</cell><cell>89.91 5.17</cell><cell>9.78</cell><cell>177</cell></row><row><cell cols="2">-IETrans (kI = 10%) (ours) 56.66 1.89 3.66</cell><cell>202</cell><cell cols="2">83.99 8.23 14.99</cell><cell>419</cell><cell cols="2">89.71 13.06 22.80</cell><cell>530</cell></row><row><cell cols="2">-IETrans (kI = 90%) (ours) 27.40 4.70 8.02</cell><cell>467</cell><cell cols="2">72.48 13.34 22.53</cell><cell>741</cell><cell cols="2">83.50 19.12 31.12</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1, 807 predicate categories and 272, 084 distinct relation triplets. It consists of 66, 289, 4, 995, and 32, 893 images for training set, validation set and test set respectively. There are on average 19.5 objects and 16.0 relations for each image.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(a) Influence of kI on Acc.</cell><cell>(b) Influence of kI on mAcc.</cell><cell></cell><cell>(c) Influence of kI on F-Acc.</cell></row><row><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>89</cell><cell></cell><cell>18</cell><cell></cell><cell>30</cell></row><row><cell cols="2">Top-10 Acc.</cell><cell>84 85 86 87 88</cell><cell>Top-10 mAcc.</cell><cell>12 14 16</cell><cell>Top-10 F-Acc.</cell><cell>28 20 22 24 26</cell></row><row><cell></cell><cell></cell><cell>83</cell><cell></cell><cell>10</cell><cell></cell><cell>18</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10% 20% 30% 40% 50% 60% 70% 80% 90% 100% kI</cell><cell>10% 20% 30% 40% 50% 60% 70% 80% 90% 100% kI</cell><cell></cell><cell>10% 20% 30% 40% 50% 60% 70% 80% 90% 100% kI</cell></row><row><cell></cell><cell cols="2">91.00</cell><cell>(d) Influence of kE on Acc.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">90.75</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">90.50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-10 Acc.</cell><cell cols="2">89.75 90.00 90.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">89.50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">89.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">89.00</cell><cell>10% 20% 30% 40% 50% 60% 70% 80% 90% 100% kE</cell><cell>10% 20% 30% 40% 50% 60% 70% 80% 90% 100%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison between different datasets' predicate filtration and split ratio. Filtration denotes the method to remove noisy predicates. The Train, Val, and Test denote number of images in training set, validation set and test set.All normally trained baseline models including Motif, Transformer, VCTree, and GPS-Net are reproduced by us. For fair comparison, we equally remove all resampling and reweighting strategies. Morever, to encourage informative SGG, we</figDesc><table><row><cell>Dataset</cell><cell>Filtration</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>VG-50 [31]</cell><cell>-</cell><cell>57,723</cell><cell>5,000</cell><cell>26,446</cell></row><row><cell>VG8K [40]</cell><cell>Auto</cell><cell>97,961</cell><cell>2,000</cell><cell>4,871</cell></row><row><cell>VG8K-LT [1]</cell><cell>Auto</cell><cell>97,623</cell><cell>1,999</cell><cell>4,860</cell></row><row><cell>VG-1800</cell><cell>Manual</cell><cell>66,289</cell><cell>4,995</cell><cell>32,893</cell></row><row><cell cols="2">B Implementation Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B.1 VG-50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>SGCLS triplet-level evaluation results on VG-1800 dataset.</figDesc><table><row><cell>Models</cell><cell>Top-1</cell><cell></cell><cell>Top-5</cell><cell></cell><cell>Top-10</cell><cell></cell></row><row><cell></cell><cell cols="6">Acc mAcc F-Acc Non-Zero Acc mAcc F-Acc Non-Zero Acc mAcc F-Acc Non-Zero</cell></row><row><cell>BGNN [13]</cell><cell>16.29 0.18 0.36</cell><cell>22</cell><cell>22.99 0.86 1.65</cell><cell>159</cell><cell>24.15 1.48 2.78</cell><cell>221</cell></row><row><cell>Motif [39]</cell><cell>18.93 0.18 0.36</cell><cell>37</cell><cell>26.08 0.74 1.43</cell><cell>90</cell><cell>27.28 1.15 2.21</cell><cell>121</cell></row><row><cell>-Focal Loss</cell><cell>18.55 0.14 0.28</cell><cell>29</cell><cell>25.91 0.51 1.00</cell><cell>52</cell><cell>27.14 0.76 1.48</cell><cell>80</cell></row><row><cell>-TDE [23]</cell><cell>18.02 0.11 0.22</cell><cell>15</cell><cell>24.85 0.38 0.75</cell><cell>38</cell><cell>26.14 0.56 1.10</cell><cell>53</cell></row><row><cell>-RelMix [1]</cell><cell>18.27 0.22 0.43</cell><cell>47</cell><cell>25.57 0.83 1.60</cell><cell>100</cell><cell>26.71 1.26 2.42</cell><cell>130</cell></row><row><cell cols="2">-IETrans (kI = 10%) (ours) 18.24 1.68 3.16</cell><cell>212</cell><cell>25.80 1.68 3.16</cell><cell>212</cell><cell>27.25 2.54 4.65</cell><cell>264</cell></row><row><cell cols="2">-IETrans (kI = 90%) (ours) 4.91 1.78 2.62</cell><cell>298</cell><cell>20.66 4.72 7.68</cell><cell>538</cell><cell>24.85 6.54 10.36</cell><cell>637</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Examples of discovered visual hierarchy in VG-50 (window, on, building) ? (window, part of, building) (man, wearing, arm) ? (man, wears, arm) (boy, wearing, boy) ? (boy, wears, boy) (pillow, on, bed ) ? (pillow, lying on, bed ) (building, has, building) ? (building, made of, building)</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring long tail visual relationship recognition with large vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdelkarim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15921" to="15930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recovering the unbiased scene graphs from the biased ones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1581" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning of visual relations: The devil is in the tails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15404" to="15413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unpaired image captioning via scene graph alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10323" to="10332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From general to specific: Informative scene graph generation via balance adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16383" to="16392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic compositional learning for low-shot scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extending VerbNet with novel verb classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1027" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<title level="m">Visual Genome: Connecting language and vision using crowdsourced dense image annotations. IJCV pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="32" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bipartite graph network with adaptive message passing for unbiased scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11109" to="11119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Invariant grounding for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2928" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interventional video relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4091" to="4099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gps-Net: Graph property sensing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3746" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Energy-based learning for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13936" to="13945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petryk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<title level="m">NBDT: Neural-backed decision trees pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1027" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-modal scene graph matching for relationship-aware image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1508" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the two-stage framework for grounded situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2651" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video as conditional graph hierarchy for multi-granular question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2804" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PCPL: Predicate-correlation perception learning for unbiased scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">PEVL: Position-enhanced pre-training and prompt tuning for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11169</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual distant supervision for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15816" to="15826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">CPT: Colorful prompt tuning for pre-trained vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11797</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">CogTree: Cognition tree loss for unbiased scene graph generation pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1274" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bridging knowledge graphs to generate scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="606" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural Motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale visual relationship understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9185" to="9194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">wearing, coat) ? (man, wears, coat) (building, has, building) ? (building, with, building) (boy, wearing, arm) ? (boy, wears, arm) (man, on, arm) ? (man, riding, arm) (tree, has, branch) ? (tree, with, branch) (woman, wearing, boot) ? (woman, wears, boot) (pillow, on, bed ) ? (pillow, above, bed ) (woman, has, arm) ? (woman, with, arm) (window, on, building) ? (window, to, building) (glass, on, bottle) ? (glass, sitting on, bottle) (sign, on, building) ? (sign, says, building) (man, wearing, bike) ? (man, wears, bike) (tire, on, building) ? (tire, on back of, building) (branch, on, branch) ? (branch, growing on, branch) (book, on, book ) ? (book, laying on, book ) (car, on, car ) ? (car, parked on, car ) (man, wearing, bench) ? (man, wears, bench) (elephant, has, ear ) ? (elephant, using, ear ) (person, on, beach) ? (person, standing on, beach) (wing, on, plane) ? (wing, attached to, plane) (windshield, on, building) ? (windshield, of, building) (arm, on, arm) ? (arm, belonging to, arm) (woman, holding, bag) ? (woman, carrying, bag) (window, on, bike) ? (window, part of, bike) (ear, on, ear ) ? (ear, belonging to, ear ) (man, on, beach) ? (man, walking on, beach) (boy, has, boy) ? (boy, wears, boy) (roof, on, building) ? (roof, covering, building) (leaf, on, branch) ? (leaf, growing on, branch) (head, of, arm) ? (head, belonging to, arm) (wheel, on, building) ? (wheel, on back of, building) (tree, near, building) ? (tree, along, building) (bird, on, bird ) ? (bird, sitting on, bird ) (door, on, door ) ? (door, to, door ) (woman, has, bag) ? (woman, with, bag) (man, wearing, hat) ? (man, wears, hat) (man, on, arm) ? (man, standing on, arm) (sign, on, building) ? (sign, attached to, building) (letter, on, building) ? (letter, painted on, building) (bird, on, bird ) ? (bird, standing on, bird ) (ear, of, cat) ? (ear, belonging to, cat) (window, on, bench) ? (window, part of, bench) (window, near, building) ? (window, part of, building) (wheel, on, bike) ? (wheel, on back of, bike) (building, near, building) ? (building, across, building) (elephant, has, ear ) ? (elephant, between, ear ) (man, has, bag) ? (man, with, bag) (engine, on, plane) ? (engine, mounted on, plane) (man, wearing, chair ) ? (man, wears, chair ) (woman, wearing, building) ? (woman, wears, building) (sign, on, sign) ? (sign, mounted on, sign) (plate, on, bowl ) ? (plate, above, bowl ) (man, wearing, face) ? (man, wears, face) (leg, of, giraffe) ? (leg, part of, giraffe) (pillow, above, bed ) ? (pillow, lying on, bed ) (tire, on, bike) ? (tire, on back of, bike) (leg, of, arm) ? (leg, belonging to, arm) (man, wearing, cap) ? (man, wears, cap) (bird, has, bird ) ? (bird, with, bird ) (trunk, of, ear ) ? (trunk, belonging to, ear ) (roof, of, building) ? (roof, covering, building) (plate, on, bottle) ? (plate, above, bottle) (man, wearing, ear ) ? (man, wears, ear ) (man, has, building) ? (man, wears, building) (window, on, arm) ? (window, part of, arm) Table 6: Examples of discovered visual hierarchy in VG-1800 (window, on, building) ? (window, on exterior of, building) (man, wearing, arm) ? (man, wearing striped, arm) (arm, of, arm) ? (arm, stretched out on, arm) (man, has, arm) ? (man, stretching out, arm) (cloud, in, cloud ) ? (cloud, floating through, cloud ) (pillow, on, bed ) ? (pillow, propped up on, bed ) (tree, in, background ) ? (tree, visible in, background ) (leg, of, arm) ? (leg, belonging to, arm) (boat, in, boat) ? (boat, sailing on, boat) (building, has, building) ? (building, seen outside, building) (cloud, in, building) ? (cloud, floating through, building) (hand, of, arm) ? (hand, hand of, arm) (boat, on, boat) ? (boat, sailing on, boat) (cloud, in, sky) ? (cloud, floating through, sky) (building, in, background ) ? (building, visible in, background ) (man, has, arm) ? (man, sheltering, arm) (man, wearing, arm) ? (man, dressed in, arm) (head, of, arm) ? (head, turning, arm) (man, has, arm) ? (man, losing, arm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">sign, on, building) ? (sign, mounted on, building) (arm, of, arm) ? (arm, belonging to, arm) (sign, on, building) ? (sign, hanging from, building) (man, wearing, bag) ? (man, wears, bag) (window, on, building) ? (window, belonging to, building) (car, on, building) ? (car, parked on, building) (clock, on, building) ? (clock, mounted on, building) (man, wearing, building) ? (man, wears, building) (man, has, arm) ? (man, wears, arm) (man, wearing, boot) ? (man, wears, boot) (bottle, on, bottle) ? (bottle, sitting on, bottle) (window, on, bus) ? (window, belonging to, bus) (book, on, book ) ? (book, above, book ) (man, has, arm) ? (man, with, arm) (ear, of, ear ) ? (ear, belonging to, ear ) (hand, of, arm) ? (hand, belonging to, arm) (bottle, on, bottle) ? (bottle, above, bottle) (window, in, building) ? (window, part of, building) (light, on, building) ? (light, mounted on, building) (door, on, building) ? (door, to, building) (food, on, food ) ? (food, lying on, food ) (bowl, on, bowl ) ? (bowl, above, bowl ) (man, wearing, man) ? (man, wears, man) (flower, in, flower ) ? (flower, painted on, flower ) (woman, wearing, bag) ? (woman, wears, bag) (tree, near, building) ? (tree, in front of, building) (woman, wearing, arm) ? (woman, wears, arm) (window, of, building) ? (window, part of, building) (clock, on, building) ? (clock, part of, building) (bus, on, building) ? (bus, parked on, building) (man</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7631" to="7638" />
		</imprint>
	</monogr>
	<note>man, wearing, background ) ? (man, wearing striped, background ) (boy, wearing, boy) ? (boy, striped, boy) (cloud, in, air ) ? (cloud, floating through, air ) (woman, has, arm) ? (woman, raising, arm) (car, on, building) ? (car, moving down, building) (airplane, in, airplane) ? (airplane, flying under, airplane) (tile, on, bathroom) ? (tile, fixed to, bathroom) (arm, on, arm) ? (arm, stretched out on, arm) (cloud, in, arm) ? (cloud, floating through, arm) (head, of, arm) ? (head, belonging to, arm) (man, wearing, arm) ? (man, kicking up, arm) (airplane, on, airplane) ? (airplane, taking off from, airplane) (sign, on, building) ? (sign, strapped, building) (man, wearing, air ) ? (man, wearing striped, air ) (sign, on, arrow ) ? (sign, strapped, arrow ) (window, on, building) ? (window, adorning, building) (cat, has, cat) ? (cat, possesses, cat) (cloud, in, boat) ? (cloud, floating through, boat) (person, has, arm) ? (person, stretching out, arm) (clock, on, building) ? (clock, attached to side of, building) (train, on, building) ? (train, switching, building) (woman, has, arm) ? (woman, combing, arm) (boy, wearing, arm) ? (boy, striped, arm) (window, on, building) ? (window, on the side of, building) (window, of, building) ? (window, on exterior of, building) (arrow, on, arrow ) ? (arrow, printed, arrow ) (woman, wearing, arm) ? (woman, wearing striped, arm) (head, of, arm) ? (head, turned to, arm) (branch, on, branch) ? (branch, sticking up on, branch) (man, on, arm) ? (man, swimming with, arm) (wing, on, airplane) ? (wing, on left side of, airplane) (mountain, in, background ) ? (mountain, visible in, background ) (bowl, on, bowl ) ? (bowl, placed on, bowl ) (cloud, in, blue sky) ? (cloud, floating through, blue sky) (window, on, arrow ) ? (window, on exterior of, arrow ) (foot, of, arm) ? (foot, belonging to, arm) (toilet, in, bathroom) ? (toilet, installed in, bathroom) (sign, on, building) ? (sign, anchored to, building) (wall, on, building) ? (wall, making up, building) (kite, in, air ) ? (kite, flying through, air ) (leaf, on, building) ? (leaf, growing on, building) (man, wearing, arm) ? (man, adjusting, arm) (person, wearing, arm) ? (person. striped, arm) (tile, on, bathroom) ? (tile, installed on, bathroom) (bag, on, bag) ? (bag, kept in, bag) (sky, in, sky) ? (sky, stretched across, sky) (bear, has, bear ) ? (bear, scratching, bear ) (line, on, building) ? (line, painted in, building) (man, wearing, building) ? (man, wearing striped, building) (book, on, book ) ? (book, arranged on, book ) (wall, near, building) ? (wall, making up, building) (window, on, advertisement) ? (window, lining side of, advertisement) (sink, in, bathroom) ? (sink, mounted in, bathroom) (wave, in, arm) ? (wave, cresting in, arm) (window, in, building) ? (window, on exterior of, building) (blanket, on, bed ) ? (blanket, laying over, bed ) (man, wearing, backpack ) ? (man, carrying, backpack ) (hair, of, arm) ? (hair, on head of, arm) (kite, in, beach) ? (kite, kite in, beach) (man, wearing, arm) ? (man, dressed, arm) (boy, has, arm) ? (boy, outstretched, arm) (picture, on, bed ) ? (picture, framed on, bed ) (window, on, banner ) ? (window, on exterior of, banner ) (arm, of, arm) ? (arm, belonging to, arm) (man, on, board ) ? (man, going off, board ) (shadow, on, arm) ? (shadow, cast over, arm) (arm, of, arm) ? (arm, around neck of, arm</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
