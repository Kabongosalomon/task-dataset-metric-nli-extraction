<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">MARCH 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
						</author>
						<title level="a" type="main">PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published" when="20211">MARCH 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Predictive learning</term>
					<term>spatiotemporal modeling</term>
					<term>recurrent neural networks !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical context, where the visual dynamics are believed to have modular structures that can be learned with compositional subsystems. This paper models these structures by presenting PredRNN, a new recurrent network, in which a pair of memory cells are explicitly decoupled, operate in nearly independent transition manners, and finally form unified representations of the complex environment. Concretely, besides the original memory cell of LSTM, this network is featured by a zigzag memory flow that propagates in both bottom-up and top-down directions across all layers, enabling the learned visual dynamics at different levels of RNNs to communicate. It also leverages a memory decoupling loss to keep the memory cells from learning redundant features. We further propose a new curriculum learning strategy to force PredRNN to learn long-term dynamics from context frames, which can be generalized to most sequence-to-sequence models. We provide detailed ablation studies to verify the effectiveness of each component. Our approach is shown to obtain highly competitive results on five datasets for both action-free and action-conditioned predictive learning scenarios.</p><p>Index Terms-Predictive learning, spatiotemporal modeling, recurrent neural networks ! ? Y. Wang is with MoE Key</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As a key application of predictive learning, generating future frames from historical consecutive frames has received growing interest in machine learning and computer vision communities. It benefits many practical applications and downstream tasks, such as the precipitation forecasting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, traffic flow prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, physical scene understanding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, early activity recognition <ref type="bibr" target="#b8">[9]</ref>, deep reinforcement learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and the vision-based model predictive control <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Many of these existing approaches suggested leveraging RNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> with stacked LSTM units <ref type="bibr" target="#b15">[16]</ref> to capture the temporal dependencies of spatiotemporal data. This architecture is mainly inspired by similar ideas from other well-explored tasks of sequential data, such as neural machine translation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, speech recognition <ref type="bibr" target="#b18">[19]</ref>, video action recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, and video captioning <ref type="bibr" target="#b20">[21]</ref>.</p><p>For stacked LSTMs, a network structure named memory cell plays an important role in alleviating the vanishing gradient problem of RNNs. Strong theoretical and empirical evidence has shown that it can latch the gradients of hidden states inside each LSTM unit in the training process and thereby preserve valuable information of underlying temporal dynamics <ref type="bibr" target="#b15">[16]</ref>. However, the state transition pathway of LSTM memory cells may not be optimal for spatiotemporal predictive learning, as this task requires different focuses on the learned representations in many aspects from other tasks of sequential data. First, most predictive networks for language or speech modeling <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> focus on capturing the long-term, non-Markovian properties of sequential data, rather than spatial deformations of visual appearance. But for future frames prediction, both data structures in space-time are crucial and need to be carefully considered. Second, in other supervised tasks of video data, such as action recognition, high-level semantical features can be informative enough, and the low-level features are less important to final outputs. Due to the absence of complex structures of supervision signals, the stacked LSTMs don't need to preserve finegrained representations from the bottom up. Although the existing recurrent architecture based on inner-layer memory transitions can be sufficient to capture temporal variations at each level of the network, it may not be the best choice for predictive learning, where low-level details and high-level semantics of spatiotemporal data are both significant to generating future frames.</p><p>To jointly model the spatial correlations and temporal dynamics at different levels of RNNs, we propose a new memoryprediction framework named Predictive Recurrent Neural Network (PredRNN), which extends the inner-layer transition function of memory states in LSTMs to spatiotemporal memory flow. The spatiotemporal memory flow traverses all nodes of PredRNN in a zigzag path of bi-directional hierarchies: At each timestep, the low-level information is delivered vertically from the input to the output via a newly-designed memory cell, while at the top layer, the spatiotemporal memory flow brings the high-level memory state to the bottom layer at the next timestep. By this means, the top-down expectations interact with the bottom-up signals to both analyze those inputs and generate predictions of subsequently expected inputs, which is different from stacked LSTMs, where the memory state is latched inside each recurrent unit.</p><p>Accordingly, we define the central building block of PredRNN as the Spatiotemporal LSTM (ST-LSTM), in which the proposed spatiotemporal memory flow interacts with the original, unidirectional memory state of LSTMs. The intuition is that if we expect a vivid imagination of multiple future images, we need a unified memory mechanism to cope with both short-term deformations of spatial details and long-term dynamics: On one hand, the new arXiv:2103.09504v4 <ref type="bibr">[cs.</ref>LG] 9 Apr 2022 1: The evolution of convolutional recurrent units in video prediction models. The "Standard" training scheme is to follow the conventional sequence-to-sequence framework <ref type="bibr" target="#b22">[23]</ref> that feeds the historical real observations to the sequence encoder and feeds the previously generated frames to the decoder during the entire training phase. "SS" is short for scheduled sampling <ref type="bibr" target="#b23">[24]</ref>. <ref type="bibr">Model</ref> Architecture Recurrent unit Training (Encoder + Decoder)</p><p>ConvLSTM <ref type="bibr" target="#b0">[1]</ref> Seq-to-Seq with stacked layers <ref type="bibr" target="#b22">[23]</ref> Convolutional LSTM Standard <ref type="bibr" target="#b22">[23]</ref> + Standard <ref type="bibr" target="#b22">[23]</ref> TrajGRU <ref type="bibr" target="#b24">[25]</ref> Seq-to-Seq with reversed forecasting layers Trajectory GRU Standard + Standard CDNA <ref type="bibr" target="#b25">[26]</ref> Action-conditioned ConvLSTM network ConvLSTM <ref type="bibr" target="#b0">[1]</ref> Standard + SS <ref type="bibr" target="#b23">[24]</ref> PredRNN <ref type="bibr" target="#b1">[2]</ref> (Conf. version) Spatiotemporal memory flow Spatiotemporal LSTM Standard + SS PredRNN++ <ref type="bibr" target="#b26">[27]</ref> Gradient highway Causal LSTM Standard + SS E3D-LSTM <ref type="bibr" target="#b8">[9]</ref> Temporal self-attention on memory cells Eidetic 3D LSTM Standard + SS CrevNet <ref type="bibr" target="#b27">[28]</ref> Two-way autoencoder + Reversible prediction ConvLSTM <ref type="bibr" target="#b0">[1]</ref>; ST-LSTM <ref type="bibr" target="#b1">[2]</ref> Standard + Standard Conv-TT-LSTM <ref type="bibr" target="#b28">[29]</ref> Seq-to-Seq with stacked layers Conv. Tensor-Train LSTM Standard + SS PredRNN-V2 (Ours) Spatiotemporal memory flow ST-LSTM (decoupled memory) Reverse scheduled sampling + SS design of the spatiotemporal memory cell enables the network to learn complex transition functions within short neighborhoods of consecutive frames by increasing the depth of non-linear neurons between time-adjacent RNN states. It thus significantly improves the modeling capability of ST-LSTM for short-term dynamics. On the other hand, ST-LSTM still retains the temporal memory cell of LSTMs and closely combines it with the proposed spatiotemporal memory cell, in pursuit of both long-term coherence of hidden states and their quick response to short-term dynamics. This journal paper extends our previous work <ref type="bibr" target="#b1">[2]</ref> in three technical aspects: First, we propose a decoupling loss to maximize the distance of memory states between the two memory cells in ST-LSTM. It is largely inspired by the theoretical argument that distributed representation has a great potential to match the underlying properties of data distribution <ref type="bibr" target="#b21">[22]</ref>. In other words, we implicitly train the two memory cells to focus on different aspects of spatiotemporal variations. Second, we extend PredRNN to support action-conditioned video prediction, such that it models the effects of various actions taken by an agent in high-dimensional pixel observations. Third, another challenge we observed for video prediction is that the widely used sequence-to-sequence training procedure <ref type="bibr" target="#b16">[17]</ref> prevents the models from learning longterm dynamics in the context frames. To this end, we introduce a new curriculum learning procedure named Reverse Scheduled Sampling, which forces our model to learn more about jumpy frame dependencies by randomly hiding real observations in the input sequence with certain probabilities changing over the training phase. We believe that the proposed training procedure can be easily extended to other video prediction models that also follow the sequence-to-sequence framework <ref type="bibr" target="#b16">[17]</ref>.</p><p>Our approach achieves state-of-the-art performance on five datasets: the Moving MNIST dataset, the KTH action dataset, a radar echo dataset for precipitation forecasting, the Traffic4Cast dataset of high-resolution traffic flows, and the action-conditioned BAIR dataset with robot-object interactions. The last two of them are new in this version of the manuscript. We perform ablation studies to understand the effectiveness of each component of PredRNN, which is complementary to existing methods and further improves them when integrated. We release the code to facilitate future research at https://github.com/thuml/predrnn-pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>For spatiotemporal predictive learning, different inductive biases are encoded into neural networks by using different network architectures <ref type="bibr" target="#b29">[30]</ref>, which, in general, can be roughly divided into three groups: feed-forward models based on CNNs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, recurrent models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, and other models including the combinations of the convolutional and recurrent networks, as well as the Transformer-based and flow-based methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Feed-forward models based on CNNs. The use of convolutional layers has introduced the inductive bias of group invariance over space to spatiotemporal prediction. Oh et al. <ref type="bibr" target="#b30">[31]</ref> defined a convolutional autoencoder for next-frame prediction in Atari games. Xue et al. <ref type="bibr" target="#b37">[38]</ref> presented a probabilistic model that encodes motion information as convolutional kernels and learns to produce a probable set of future frames by learning their conditional distribution. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> used a CNN with residual connections for traffic prediction, which considers the closeness, period, trend, and external factors of the traffic flows. Recent literature proposed to train the predictive CNNs with adversarial learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> to reduce the uncertainty of the learning process and improve the sharpness of the generated frames <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. Compared with the recurrent models, feed-forward models typically show higher computational efficiency on large-scale GPUs. However, these models learn complicated state transitions as compositions of simpler ones by stacking convolutional layers, thus often failing to capture longterm dependency between distant frames.</p><p>Recurrent models. Recent advances in RNNs typically show a greater ability to model the dynamics of historical observations. Ranzato et al. <ref type="bibr" target="#b46">[47]</ref> defined an RNN inspired by language modeling, predicting future frames in a discrete space of patch clusters. Srivastava et al. <ref type="bibr" target="#b33">[34]</ref> proposed a sequence-to-sequence video prediction model borrowed from neural machine translation <ref type="bibr" target="#b16">[17]</ref>. Later on, some work improved the modeling of temporal relations and extended the prediction horizon by organizing 2D recurrent states in hierarchical architectures <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. To model the temporal uncertainty, some work proposed to integrate variational inference with 2D recurrence <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. Another line of work is to factorize the content and motion of videos. Typical approaches include using optical flows <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>, adversarial training schemes <ref type="bibr" target="#b58">[59]</ref>, reasoning about object-centric content and pose vectors <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, differentiable clustering methods <ref type="bibr" target="#b5">[6]</ref>, amortized inference <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref>, and solvers of neural differential equations <ref type="bibr" target="#b63">[64]</ref>. However, the above methods mainly used 2D RNNs to model video dynamics in low-dimensional space, which inevitably causes the loss of visual information in real scenarios. In this paper, we are more focused on improving the spatiotemporal modeling capability of the recurrent predictive backbones.</p><p>Convolutional RNNs. <ref type="table">Table 1</ref> summarizes the evolution of recent video prediction models that combine the advantages of convolutional and recurrent architectures. Shi et al. <ref type="bibr" target="#b0">[1]</ref> proposed the Convolutional LSTM (ConvLSTM), which uses convolutions to replace the matrix multiplication in the recurrent transitions of the original LSTM. Finn et al. <ref type="bibr" target="#b25">[26]</ref> designed an action-conditioned ConvLSTM network for visual planning and control. Shi et al. <ref type="bibr" target="#b24">[25]</ref> combined convolutions with GRUs <ref type="bibr" target="#b17">[18]</ref> and extended the receptive fields of state-to-state transitions with non-local neural connections. Wang et al. <ref type="bibr" target="#b8">[9]</ref> enriched each RNN state by incorporating a time dimension that covers a short snippet, and proposed to model the dynamics using 3D convolutions and temporal selfattention. Su et al. <ref type="bibr" target="#b28">[29]</ref> improved the efficiency of higher-order ConvLSTMs based on low-rank tensor factorization. In summary, the convolutional RNNs jointly model the visual appearances and temporal dynamics, and is also a foundation for the follow-up approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref>. This paper improves upon the existing video prediction models with (i) a zigzag recurrent transition mechanism named spatiotemporal memory flow, (ii) a new convolutional recurrent unit with a pair of decoupled memory cells, and (iii) a new training procedure for sequence-to-sequence predictive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatiotemporal Predictive Learning</head><p>Suppose we are monitoring a dynamical system of J measurements over time, where each measurement is recorded at all locations in a spatial region represented by an M ? N grid. From the spatial view, the observation of these J measurements at any time can be represented by a tensor of X ? R J?M ?N . From the temporal view, the observations over T timesteps form a sequence of X in = {X 1 , . . . , X T }. Given X in , the spatiotemporal predictive learning is to predict the most probable length-K sequence in the future, X out = { X T +1 , . . . , X T +K }. In this paper, we train neural networks parametrized by ?. Concretely, we use stochastic gradient descent to find a set of parameters ? that maximizes the log-likelihood of producing the true target sequence X out given the input data X in for all training pairs {(X n in , X n out )} n : ? = arg max ? (X n in ,X n out ) log P (X n out |X n in ; ?).</p><p>In this paper, we take video prediction as a typical experimental domain, where the observed data at each timestep is an RGB image and the number of measured channels is 3. Another domain is precipitation nowcasting, where the observed data is a sequence of radar echo maps in a certain geographic region 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional LSTM</head><p>The Convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b0">[1]</ref> models the spatial and temporal data structures simultaneously by explicitly encoding the spatial information into tensors, and applying convolution operators to both the state-to-state and input-to-state recurrent transitions. It overcomes the limitation of vector-variate representations in standard LSTM, where the spatial correlation is not explicitly modeled. The input state X t , memory state C t , and hidden state H t at each timestep are 3D tensors in R J?M ?N . The first dimension is either the number of measurements (for input states) or the number of feature maps (otherwise), and the last two dimensions are the numbers of spatial rows M and columns N . Like standard LSTM, the input gate i t , forget gate f t , output gate o t , and inputmodulation gate g t control the information flow across C t , such 1. We usually visualize radar echoes by mapping them to color images. that the gradient will be kept from quickly vanishing by being trapped in the memory cell. ConvLSTM determines the future state of a certain cell in the M ? N grid based on the input frame and past states of its local neighbors:</p><formula xml:id="formula_1">g t = tanh(W xg * X t + W hg * H t?1 + b g ) i t = ?(W xi * X t + W hi * H t?1 + W ci C t?1 + b i ) f t = ?(W xf * X t + W hf * H t?1 + W cf C t?1 + b f ) C t = f t C t?1 + i t g t o t = ?(W xo * X t + W ho * H t?1 + W co C t + b o ) H t = o t tanh(C t ),<label>(2)</label></formula><p>where ? is the Sigmoid activation function, * and denote the convolution operator and the Hadamard product. We observe that the ConvLSTM network can be improved from three aspects.</p><p>Challenge I. For a stacked ConvLSTM network, the input sequence X in is fed into the bottom layer, and the output sequence X out is generated at the top one. With hidden states H t being delivered from the bottom up, spatial representations are encoded layer by layer. However, the memory states C t are merely updated along the arrow of time within each ConvLSTM layer, being less dependent on the hierarchical visual features at other layers. Thus, the first layer at the current timestep may largely ignore what had been memorized by the top layer at the previous timestep.</p><p>Challenge II. In ConvLSTM, the output hidden state H t is dependent on the memory state C t and the output gate o t , which means that the memory cell is forced to cope with long-term and short-term dynamics simultaneously. Therefore, the modeling capability of C t may greatly limit the overall performance of the model for complex spatiotemporal variations.</p><p>Challenge III. The ConvLSTM network follows the training procedure of sequence-to-sequence RNNs <ref type="bibr" target="#b22">[23]</ref>. During training, it always takes as inputs real observations at encoding timesteps but has to rely more on the long-term information from the previous context frames at the forecasting timesteps. Since both the encoding and forecasting parts of ConvLSTM use the same set of model parameters, the one-step training in the encoding phase may prevent the forecaster from learning the jumpy frame dependencies, thus affecting the performance of long-term prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>To tackle the above challenging problems, we propose the Predictive Recurrent Neural Network (PredRNN). For Challenge I, we present a new RNN architecture named the spatiotemporal memory flow, which improves the state transition functions of convolutional RNNs. For Challenge II, we propose the Spatiotemporal LSTM (ST-LSTM) which serves as the building block of PredRNN. It leverages a pair of memory cells that are jointly learned and explicitly decoupled to cover long-and short-term dynamics of spatiotemporal variations. For Challenge III, we improve the training procedure to encourage the predictive network to learn non-Markovian dynamics from longer periods of observation frames. Furthermore, we present an action-conditioned PredRNN, which allows simulating the spatiotemporal variations of the environment in response to the actions of the agent in decision-making scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatiotemporal Memory Flow</head><p>PredRNN employs a stack of convolutional recurrent units to learn unified representations of the spatial correlations and temporal  dynamics from the input sequence, and then transforms these features back to the data space to predict future spatiotemporal frames. We initially adopt the original ConvLSTM layer as the basic building block of PredRNN and apply a novel memory state transition method between the recurrent nodes. In the original ConvLSTM network <ref type="bibr" target="#b0">[1]</ref> shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (right), the memory states C l t are constrained inside individual recurrent layers and updated along the arrow of time. Only the hidden states H l t can be delivered from the observation to the final prediction. This temporal memory flow has been widely used in supervised learning tasks such as video classification, where the hidden representations are more and more abstract and class-specific, starting at the bottom. However, in predictive learning, the input frames and the expected outputs share the same data space, i.e., they may have very close data distributions in the spatial domain and very related underlying dynamics in the temporal domain. Therefore, it becomes important to make the predictions more effectively dependent on the memory representations learned at different levels of recurrent layers. If we want to frame the future vividly, we need both high-level understandings of global motions and more detailed memories of the subtle variations in the input sequence.</p><formula xml:id="formula_2">b X t+2 b X t+1 b X t X t X t 1 X t+1 C 4 t C 2 t C 3 t C 1 t M 1 t M 2 t M 3 t b X t+2 b X t+1 X t X t+1 H 2 t 1 H 4 t 1 M 4 t H 4 t</formula><p>Considering that the memory cell of ConvLSTM can latch the gradients 2 and thereby store valuable information across recurrent nodes, we improve the above horizontal memory flow by updating the memory state in the zigzag direction, so that it can better deliver knowledge from input to output. We show the key equations of this memory flow using ConvLSTM as the building block:</p><formula xml:id="formula_3">g t = tanh(W xg * X t 1 {l=1} + W hg * H l?1 t ) i t = ?(W xi * X t 1 {l=1} + W hi * H l?1 t + W ci * M l?1 t ) f t = ?(W xf * X t 1 {l=1} + W hf * H l?1 t + W cf * M l?1 t ) M l t = f t M l?1 t + i t g t o t = ?(W xo * X t 1 {l=1} + W ho * H l?1 t + W co * M l t ) H l t = o t tanh(M l t ).<label>(3)</label></formula><p>We name this new memory state transition method the spatiotemporal memory flow, and show its transition direction by the orange arrows in <ref type="figure" target="#fig_0">Fig. 1 (left)</ref>. In this way, the memory states in different ConvLSTM layers are no longer independent, and all nodes in the entire recurrent network jointly maintain a memory 2. Like the standard LSTM, the memory cell of ConvLSTM was originally designed to alleviate the gradient vanishing problem during training.</p><p>bank denoted by M. The input gate, input modulation gate, forget gate, and output gate are no longer dependent on the hidden state and the temporal memory state from the previous timestep at the same layer. Instead, they rely on the hidden state H l?1 t and the spatiotemporal memory state M l?1 t (l ? {1, ..., L}) supplied by the previous layer at current timestep (see <ref type="figure" target="#fig_0">Fig. 1</ref> (left)). In particular, the bottom recurrent unit (l = 1) receives state values from the top layer at the previous timestep:</p><formula xml:id="formula_4">H l?1 t = H L t?1 , M l?1 t = M L t?1 .</formula><p>The four layers in this figure have different sets of convolutional parameters regarding the input-to-state and state-to-state transitions. They thereby read and update the values of the memory state based on their individual understandings of the spatiotemporal dynamics as the information flows through the current node. Note that we replace the notation for memory state from C to M to emphasize that it flows in the zigzag direction in PredRNN, instead of the horizontal direction in standard recurrent networks. Different from ConvLSTM which uses Hadamard product for the computation of the gates, we adopt convolution operators * for finer-grained memory transitions. An additional benefit of this change is that the learned PredRNN can be deployed directly on the input sequence of different spatial resolutions.</p><p>The spatiotemporal memory flow provides a recurrent highway for hierarchical visual representations that can reduce the loss of information from the bottom layers to the top of the network. Besides, by introducing more nonlinear gated neurons within temporal neighborhoods, it expands the deep transition path of hidden states and enhances the modeling capability of the network for short-term dynamics. In contrast, the original ConvLSTM network requires larger convolution kernels for input-to-state and state-to-state transitions in order to capture faster motion, resulting in an unnecessary increase in model parameters.</p><p>Alternatively, we can understand the spatiotemporal memory flow from the perspective of memory networks <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74]</ref>. Here, the proposed spatiotemporal memory state M can be viewed as a simple version of the so-called external memory with continuous memory representations, and the stacked recurrent layers can be viewed as multiple computational steps. The layer-wise forget gates, input gates, and input modulation gates respectively determine the read and write functions, as well as the content to be written. One advantage of the classic memory networks is to capture long-term structure (even with multiple temporal hops) within sequences. Our spatiotemporal memory flow is analogous to their mechanism, as it enables our model to consider different levels of video representations before making a prediction. </p><formula xml:id="formula_5">ST-LSTM 1 ST-LSTM 1 ST-LSTM 1 ST-LSTM 2 ST-LSTM 2 ST-LSTM 2 ST-LSTM 3 ST-LSTM 3 ST-LSTM 3 ST-LSTM 4 ST-LSTM 4 ST-LSTM 4 ? ? ? ? Input Gate Output Gate Forget Gate Modulation Gate Temporal Memory g t i t f t o t ? f t Spatiotemporal Memory ? ? i t ? g t b X t+2 b X t+1 b X t X t M l t M l 1 t C l t C l t 1 H l t 1 H l t H 1 t H 2 t H 3 t C 4 t C 2 t C 3 t C 1 t M 1 t M 2 t M 3 t M 4 t X t X t 1 X t+1 L l,t decouple = cos( C l t , M l t ) H 1 t H 2 t H 3 t H 4 t M 4 t 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spatiotemporal LSTM with Memory Decoupling</head><p>As described previously, the spatiotemporal memory state is updated first upwards across layers then forwards to the next timestep. It stretches the state transition path across time and adds extra neurons between horizontally adjacent nodes at the same level of the network. It thus enables the network to learn complex non-linear transition functions of short-term motions. However, this deep-in-time architecture may also bring in the gradient vanishing problem. The roundabout memory transition path may make it difficult to capture long-term dependencies. For both short-term recurrence depth and long-term coherence, we introduce a doubleflow memory transition mechanism that combines the original memory cell C and the new memory cell M, which derives a recurrent unit named Spatiotemporal LSTM (ST-LSTM):</p><formula xml:id="formula_6">g t = tanh(W xg * X t + W hg * H l t?1 ) i t = ?(W xi * X t + W hi * H l t?1 ) f t = ?(W xf * X t + W hf * H l t?1 ) C l t = f t C l t?1 + i t g t g t = tanh(W xg * X t + W mg * M l?1 t ) i t = ?(W xi * X t + W mi * M l?1 t ) f t = ?(W xf * X t + W mf * M l?1 t ) M l t = f t M l?1 t + i t g t o t = ?(W xo * X t + W ho * H l t?1 + W co * C l t + W mo * M l t ) H l M 0 t for the bottom ST-LSTM where l = 1.</formula><p>We construct another set of input gate i t , forget gate f t , and input modulation gate g t for M l t , because the memory transition functions in distinct directions are supposed to be controlled by different signals.</p><p>The final hidden states H l t of each node are dependent on a combination of the horizontal and zigzag memory states: we concatenate the C l t and M l t , and then apply the 1 ? 1 convolutional layer for dimensionality reduction, which makes the hidden state H l t have the same dimensions as the memory states. In addition to simple concatenation, pairs of memory states are finally twisted and unified by output gates with bidirectional control signals (horizontal and vertical), resulting in comprehensive modeling of long-term and short-term dynamics. This dual-memory mechanism benefits from the compositional advantage with distributed representations <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref>. Intuitively, due to different mechanisms of state transitions, the pair of memory cells in ST-LSTM are expected to deal with different aspects of spatiotemporal variations:</p><p>? M introduces a deeper transition path that zigzags across ST-LSTM units. With the forget gate f t and the input-related modules i t g t , it improves the ability to model complex shortterm dynamics from one timestep to the next, and allows H to transit adaptively at different rates. ? C operates on a slower timescale. It provides a shorter gradient path between distant hidden states, thus facilitating the learning process of long-term dependencies.</p><p>However, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we visualized the increments of memory states at each timestep (i.e., ?C l t and ?M l t ) using t-SNE <ref type="bibr" target="#b76">[77]</ref>, and found that they were not automatically separated as expected. In fact, the two memory states are often so intertwined that they are difficult to decouple spontaneously through their respective network architectures. To some extent, it results in the inefficient utilization of network parameters.</p><p>Building upon the first version of ST-LSTM <ref type="bibr" target="#b1">[2]</ref>, we present a new decoupling loss, as shown in <ref type="figure" target="#fig_1">Fig. 2 (right)</ref>, that keeps C and M from learning redundant features. In each ST-LSTM unit, we first add the convolutional layer upon the increments of C l t and M l t at every timestep, and leverage a new decoupling loss to explicitly extend the distance between them in latent space. By this means, different memory states are trained to focus on different aspects of spatiotemporal variations. The overall memory decoupling method can be formulated as follows:</p><formula xml:id="formula_7">?C l t = W decouple * (i t g t ) ?M l t = W decouple * (i t g t ) L decouple = t l c | ?C l t , ?M l t c | ?C l t c ? ?M l t c ,<label>(5)</label></formula><p>where W decouple denotes 1 ? 1 convolutions shared by all ST-LSTM units; ?, ? c and ? c are respectively dot product and 2 norm of flattened feature maps, which are calculated for each channel c. At training time, the increments of memory states, i t g t and i t g t , are derived from Eq. (4). Notably, the new parameters are only used at training time and are removed from the entire model at inference. That is, there is no increase in model size compared to the previous version of ST-LSTM <ref type="bibr" target="#b1">[2]</ref>. By defining the decoupling loss with the cosine similarity, our approach encourages the increments of the two memory states to be orthogonal at any timestep. It unleashes the respective strengths of C and M for long-term and short-term dynamic modeling. As shown by t-SNE visualization in <ref type="figure" target="#fig_2">Fig. 3</ref>, at test time, ?C l t and ?M l t can be easily separated. The decoupling loss is largely inspired by the theoretical evidence that using reasonable inductive bias to construct distributed representations can bring a great performance boost if it matches properties of the underlying data distribution <ref type="bibr" target="#b21">[22]</ref>. There is a similar idea in ensemble learning that generally, to form a good ensemble, the base learners should be as more accurate as possible, and as more diverse as possible <ref type="bibr" target="#b77">[78]</ref>. The diversity of base learners can be enhanced in different ways, such as sub-sampling the training examples, manipulating the input attributes, and employing randomness into training procedures <ref type="bibr" target="#b78">[79]</ref>. It is worth noting that the proposed memory decoupling method has not been used by existing ensemble learning algorithms, though it is inspired by the general idea of enhancing the diversity of base learners. We use it to diversify the pairs of memory states, intuitively in pursuit of a more disentangled representation of long-and short-term dynamics in predictive learning.</p><p>The final PredRNN model is trained end-to-end in a fully unsupervised manner. The overall training objective is:</p><formula xml:id="formula_8">L final = T +K t=2 X t ? X t 2 2 + L decouple ,<label>(6)</label></formula><p>where the first term is the frame reconstruction loss that works on the network output at each timestep, the second term is the memory decoupling regularization from Eq. <ref type="formula" target="#formula_7">(5)</ref>.  <ref type="table" target="#tab_4">1  2  5  10  15  20  25</ref> Input frames Ground truth &amp; predicted future frames PredRNN-V2</p><formula xml:id="formula_9">H 1 H 1 X 1 X 1 a 1 a 1 S 1 S 1 PredRNN SVG t =</formula><formula xml:id="formula_10">SV2P 3 H 2 H 2 X 2 X 2 V 2 V 2 a 2 a 2 S 2 S 2 PredRNN SVG t = 1 2 5 10 15 20</formula><p>Input frames Ground truth &amp; predicted future </p><formula xml:id="formula_11">PredRNN-V2 SV2P X 3 X 3 V 3 V 3 S 3 S 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Action-Conditioned ST-LSTM</head><p>Action-conditioned video prediction models allow for flexibility in learning the policies of agent-based decision making systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81]</ref>. Inspired by the work of Recurrent Environment Simulator <ref type="bibr" target="#b79">[80]</ref>, we extend ST-LSTMs, which can be denoted by ST-LSTM(X t , H l t?1 , C l t?1 , M l?1 t ), to actionconditioned video prediction with Action fusion:</p><formula xml:id="formula_12">V l t = (W hv * H l t?1 ) (W av * A t?1 ) H l t , C l t , M l t = ST-LSTM(X t , V l t , C l t?1 , M l?1 t ),<label>(7)</label></formula><p>where V l t encodes previous hidden state H l t?1 and the action taken at the previous timestep A t?1 . In our experiments, A t has continuous values and has been transformed to the same resolution as the hidden state. By incorporating V l t in each ST-LSTM unit, at both encoding and forecasting timesteps, PredRNN learns to simulate the consequences of future action sequences over long time periods. Different from the original Recurrent Environment Simulator, we validate the effectiveness of action-conditioned ST-LSTM on a dataset collected with a real robot <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training with Reverse Scheduled Sampling</head><p>As shown in <ref type="figure">Fig. 5</ref>, we propose a new curriculum learning strategy for PredRNN, which consists of two components:</p><p>? Reverse scheduled sampling (RSS): It is used at the encoding timesteps and forces the model to learn more about long-term dynamics by randomly hiding real observations with decreasing probabilities as the training phase progresses. ? Scheduled sampling (SS) <ref type="bibr" target="#b23">[24]</ref>: It is used at the forecasting timesteps to alleviate the inconsistency of data flow between the training and inference phases.</p><p>Difficulty in learning long-term dynamics. We follow the common practice <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref> that uses a unified predictive model f ? (?) for sequence encoding and forecasting. We use X in = {X 1 , . . . , X T } to denote the context frames and X out = {X T +1 , . . . , X T +K } for ground-truth output frames. We use X t+1 , t ? {1, . . . , T +K ?1}, to denote the prediction at each timestep. During inference, as well as in the typical training scheme, we have</p><formula xml:id="formula_13">X t+1 = f ? (X t , H t?1 , S t?1 ) if t ? T, f ? ( X t , H t?1 , S t?1 ) otherwise,<label>(8)</label></formula><p>where  <ref type="figure">Fig. 5</ref>: In the training phase, we apply the reverse scheduled sampling at the encoding timesteps to force the model to learn long-term dynamics from context frames. This training scheme can be easily combined with the original scheduled sampling strategy, which is used to close the training-inference gap at forecasting timesteps. Both the encoder and the forecaster are parametrized by ?.</p><formula xml:id="formula_14">S t?1 = {C t?1 , M t?1 }</formula><formula xml:id="formula_15">X T X T +K 1 b X T +K 1 b X T b X T +K b X T +2 ST-LSTMs ? Reverse scheduled sampling X T +1 b X T +1 b X t ST-LSTMs ? Reverse scheduled sampling b X t+1 X t ? ? ? b X T +1</formula><p>between the encoding part (t ? T ) and the forecasting part (t &gt; T ) is whether to use the true frame X t or the previous prediction X t .</p><p>In the encoder, the model is mainly learned to make a one-step prediction, because X t tends to be a more informative signal than H t?1 and S t?1 , keeping PredRNN from digging deeper into the non-Markovian properties of historical observations. But for the forecaster, since X t does not contain new observations, the model has to learn long-term dynamics from the latter. Such a training inconsistency between the encoder and the forecaster may lead to an ineffective optimization of ? and hamper the model from learning long-term dynamics from X in .</p><p>Reverse scheduled sampling. To force the recurrent model to learn long-term dynamics from historical observations, we propose the reverse scheduled sampling, a curriculum learning strategy applied to the input frames at encoding timesteps. At forecasting timesteps, we follow the previous literature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref> to apply the original schedule sampling <ref type="bibr" target="#b23">[24]</ref> to the inputs. Finally, we have</p><formula xml:id="formula_16">X t+1 = f ? ( X t RSS ? ? ? X t , H t?1 , S t?1 ) if t ? T, f ? (X t SS ? ? X t , , H t?1 S t?1 ) otherwise,<label>(9)</label></formula><p>where RSS ? ? ? indicates a gradual change throughout the training phase from taking the previous prediction X t as the input to taking the true frame X t . At encoding timesteps, the changing schedule is in the reverse order of the scheduled sampling strategy denoted by SS ? ? at forecasting timesteps. Concretely, for RSS, there is a probability ( k ? [0, 1]) of sampling the true frame X t ? X in or a corresponding probability (1 ? k ) of sampling X t , such that over the entire encoder, a sequence of sampling outcomes can be seen as a Bernoulli process with T independent trials. k is an increasing function of the number of training iterations k, starting from s and increasing to e , which has the following forms:</p><formula xml:id="formula_17">? Linear increase: k = min{ s + ? l ? k, e }; ? Exponential increase: k = e ? ( e ? s ) ? exp(? k ?e ); ? Sigmoid increase: k = s + ( e ? s ) ? 1 1+exp( ?s?k ?s ) ,</formula><p>where ? l , ? e , ? s &gt; 0 denote the increasing factors and ? s &gt; 0 denotes the starting point of the sigmoid function. These hyperparameters jointly decide the increasing curve of k . Examples of such schemes are shown in <ref type="figure">Fig. 6</ref> in the red curves. The encoder is trained with a progressively simplified curriculum. It gradually changes from generating multi-step future frames, which is challenging due to the absence of some historical observations, to making one-step predictions, just as the encoder does at test time. Such a training scheme encourages the model to extract long-term, non-Markovian dynamics from the input sequence.</p><p>Entire training scheme. <ref type="figure">Fig. 6</ref> shows two feasible strategies to jointly use RSS and the original scheduled sampling method, where ? k denotes the probability of the original scheduled sampling at forecasting timesteps. For simplicity, we make ? k decay linearly, although other scheduled sampling schemes could be employed (such as an exponential decay). The biggest difference between the two strategies lies in whether the variation ranges of the sampling probabilities of encoder and forecaster are close at the early stage of training. Empirically, the second strategy performs slightly better (as shown in <ref type="table" target="#tab_11">Table 6</ref> in Section 5.2). This suggests that in the early training stages, it is helpful to sample true frames with similar probabilities in the encoding and forecasting parts of PredRNN. Like scheduled sampling, the RSS training strategy can be widely used in most sequence-to-sequence models beyond Pre-dRNN to enhance the long-term modeling capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate our approach on five spatiotemporal prediction datasets. We strongly encourage readers to view https: //github.com/thuml/predrnn-pytorch for the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setups</head><p>Datasets. We valuate PredRNN on the following datasets for both synthetic and real-world scenarios: ? Moving MNIST: This dataset contains handwritten digits that are sampled from the static MNIST, placed at random locations, and initialized with a random speed. They bounce off the edges of the image at a certain angle. For a fair comparison, we follow two training setup from ConvLSTM <ref type="bibr" target="#b0">[1]</ref> and CrevNet <ref type="bibr" target="#b27">[28]</ref>. In the first setup, we use a fixed training set of 10,000 samples; while in the second one, we generate training data on the fly, which provides a larger number of training samples. We use a test set of 5,000 sequences where the digits are sampled from a different subset of static MNIST.  <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> or use ST-LSTM as the network backbone <ref type="bibr" target="#b27">[28]</ref>. The differences between them are summarized in <ref type="table">Table 1</ref>.</p><p>Particularly for these approaches, the proposed contributions of memory decoupling and reverse scheduled sampling are shown to be easily combined with them and enable them to approach state-of-the-art performance. To facilitate the discussion of ablation study, we refer to different versions of PredRNN as: It was proposed in our conference paper <ref type="bibr" target="#b1">[2]</ref>. ? PredRNN-V2: This is the final proposed model that improves the training process of the original PredRNN with memory decoupling and reverse scheduled sampling.</p><p>Implementation details. We use the ADAM optimizer <ref type="bibr" target="#b83">[84]</ref> to train the models with a mini-batch of 2 to 16 sequences according to different datasets. Unless otherwise specified, we set the learning rate to 10 ?4 and stop the training process after 80,000 iterations. We typically use four ST-LSTM layers in PredRNN to strike a balance between prediction quality and training efficiency. We set the number of channels of each hidden state to 128 and the size of convolutional kernels inside the ST-LSTM unit to 5 ? 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Moving MNIST Dataset</head><p>In this dataset, future trajectories of moving digits are predictable based on enough historical observations, where part of the challenge is to infer the underlying dynamics with random initial velocities. Moreover, the frequent occlusion of multiple digits leads to complex and short-term variations of local spatial information, which brings more difficulties to spatiotemporal prediction. We train the models to predict the next ten frames given the first ten.  <ref type="table" target="#tab_4">= 1 2  3  4  5  6  7  8  9  10   11 12  13  14  15  16  17  18  19  20   t = 1 2  3  4  5  6  7  8  9  10   11 12  13  14  15  16  17  18  19  20</ref> PredRNN-V2  Quantitative results with a fixed training set. <ref type="table" target="#tab_4">Table 2</ref> shows the results of all compared models averaged per frame. We adopt evaluation metrics that were widely used by previous methods: the Mean Squared Error (MSE), the Structural Similarity Index Measure (SSIM) <ref type="bibr" target="#b84">[85]</ref>, and the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b85">[86]</ref>. The difference between these metrics is that MSE estimates the absolute pixel-wise errors, SSIM measures the similarity of structural information within the spatial neighborhoods, while LPIPS is based on deep features and aligns better to human perceptions. <ref type="figure" target="#fig_6">Fig. 7</ref>   <ref type="table" target="#tab_4">Table 2</ref>, we also show the model size, computational efficiency, and memory usage for the sake of fair comparisons. Note that PredRNN performs better and is more efficient than a large version of the ConvLSTM network (denoted by ConvLSTM*) with a doubled number of channels in the hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VPN Baseline</head><p>Quantitative results with a dynamic training set. To further compare the performance of PredRNN with the state of the art on the Moving MNIST dataset, we follow the experimental setups of recent approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b66">67]</ref> where models are trained on dynamically generated data with two or three flying digits. The training process is stopped after 100,000 iterations. As shown in <ref type="table" target="#tab_5">Table 3</ref>, PredRNN and PredRNN-V2 yield better results than those in <ref type="table" target="#tab_4">Table 2</ref>, as they can have a better understanding of the spatiotemporal dynamics of the dataset with the growth of training steps. Notably, CrevNet also uses ST-LSTM as the recurrent unit. According to the results from its original text <ref type="bibr" target="#b27">[28]</ref>, it is superior to another ConvLSTM-based variant. More importantly, it is shown to be further improved by memory decoupling and reverse scheduled sampling.</p><p>Qualitative comparison. <ref type="figure" target="#fig_9">Fig. 8</ref> shows two examples randomly sampled from the test set, where most of the frames produced by the compared models are severely blurred, especially for long-term predictions. In contrast, PredRNN produces clearer images, which means it can be more certain of future variations due to its stronger long-term modeling capabilities. When we look closely, we can see that the most challenging issues on this dataset are to make accurate predictions of the future trajectories of moving digits and to maintain the correct shape of each digit after occlusions (in the second example, the compared VPN model incorrectly predicts the digit 8 as 3 after the occlusion of 8 and 0). In both cases, the original PredRNN and the newly proposed PredRNN-V2 progressively improve the quality of the prediction results.</p><p>Ablation studies on the memory flow. To understand the benefit of the spatiotemporal memory flow, we compare the performance of (i) truncating the bottom-up state transition path of PredRNN from M 1 t to M L t and (ii) truncating the top-down transition path from M L t to M 1 t+1 . <ref type="figure">Fig. 9(a)</ref> shows the normalized gradient values of ? H 1 t L T +K for 1 ? t &lt; 20, which demonstrates the benefit of the zigzag memory flow in learning long-term data trends due to the ability of alleviating the problem of vanishing gradients. <ref type="table" target="#tab_7">Table 4</ref> gives the quantitative results in MSE, indicating that the top-down transition path contributes more to the final performance. Furthermore, in <ref type="figure" target="#fig_0">Fig. 10</ref>, we study the effect of the memory flow in long-term modeling by analyzing the saturation ratio among all  </p><formula xml:id="formula_18">(b) 1 T ?1 T ? =2 ? H 1 ?</formula><p>Lt , t ? T + 1 <ref type="figure">Fig. 9</ref>: Gradient analyses on the Moving MNIST test set. We use well-trained models and average the results over 100 sequences randomly sampled from the test set. (a) It presents the gradients of previous states concerning the loss at the last forecasting timestep, showing that the proposed memory flow benefits long-term modeling by alleviating the vanishing gradients. (b) It presents the accumulated gradients of the encoding states concerning the losses over the forecasting timesteps, showing the importance of RSS in encoding long-term dynamics of the inputs. We here use the RSS training strategy with an exponentially increasing k (from <ref type="figure">Fig. 6(b)</ref>). Both PredRNN and PredRNN+RSS take as input the historical real observations at encoding timesteps during testing.  elements in the forget gates (f t &lt; 0.1) for the temporal memory cell C l t . A high saturation ratio indicates that the model tends to block the information flow of long-term trends. Compared with ConvLSTM, the spatiotemporal memory flow releases the modeling capability of C l t for long-term dynamics. Ablation study on the memory decoupling. To show that memory decoupling facilitates both long-term and short-term dependencies, as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>, we make the pixel intensity of the images change regularly or irregularly over time. Thanks to the decoupled memory cells of ST-LSTMs, our approach can respond to sudden changes more rapidly and adapt to video dynamics at different timescales. As shown by the purple curves, we also include the results of a larger ConvLSTM network with a model size significantly larger than that of PredRNN+Memory Decoupling (65.96 MB vs. 23.86 MB). Nonetheless, our model responds more quickly to the rapidly changing pixel intensity. In <ref type="table" target="#tab_10">Table 5</ref>, we use the MIM model <ref type="bibr" target="#b3">[4]</ref>, which is also based on the ST-LSTM unit, to specifically show the generality of the memory decoupling loss to different network backbones.</p><p>Ablation study on the reverse scheduled sampling. We compare different combinations of the original and the reverse scheduled sampling techniques. From <ref type="table" target="#tab_11">Table 6</ref>, the second strategy in <ref type="figure">Fig. 6</ref> with an exponentially increased k performs best. It is because  the encoder-forecaster discrepancy can be effectively reduced by keeping their probabilities of sampling the true context frames close to each other in the early stage of training. To further demonstrate that the reverse scheduled sampling can contribute to learning longterm dynamics, we perform empirical analyses on the long-term gradients in <ref type="figure">Fig. 9(b)</ref> and the saturation ratio of forget gates in <ref type="figure" target="#fig_0">Fig. 10</ref>. For the gradient analysis, we evaluate the gradients of the encoder's hidden states with respect to the loss functions at different output timesteps, and average the results over the entire   input sequence: 1</p><formula xml:id="formula_19">T ?1 T ? =2 ? H 1 ? L t , t ? [T + 1, T + K].</formula><p>The normalized gradient curves show that the context information can be encoded more effectively by using reverse scheduled sampling. In <ref type="figure" target="#fig_0">Fig. 10</ref>, we find that with RSS, both the ConvLSTM network and PredRNN maintain lower saturation ratios for the forget gates in the temporal memory cell C l t , which indicates that RSS improves the learning process of long-term dynamics.</p><p>Generality of the proposed techniques. <ref type="table" target="#tab_10">Table 5</ref> shows the results of applying the proposed techniques to some existing models that are also based on convolutional RNNs. It is worth noting that, first, our original ST-LSTM has been the foundation of some recent architectures, thus allowing further improvements introduced by memory decoupling. Besides, the proposed reverse scheduled sampling method is shown to be a general training approach for recurrent models, enabling them to approach the state of the art. In particular, although PredRNN++ is also based on ST-LSTMs, it only achieves a 3% performance gain. The reason is that, on one hand, like RSS, the gradient highway unit of PredRNN++ is also to improve the long-term modeling capability; On the other hand, it models the state transitions from C l t to M l t , which implicitly allows M to learn new information beyond what has been leaned by C. However, the two components mentioned above in PredRNN++ introduce additional parameters and increase the computational burden. In contrast, memory decoupling and RSS increase the number of model parameters by a negligible amount but have greater benefits for the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">KTH Action Dataset</head><p>In this dataset, all compared models are trained across the 6 action categories by predicting 10 future frames from 10 observations. At test time, the prediction horizon is expanded to 20 timesteps. We adopt the Peak Signal to Noise Ratio (PSNR) from the previous literature as the third evaluation metric, in addition to SSIM and LPIPS. Like MSE, PSNR also estimates the pixel-level similarity of two images (higher is better). The evaluation results of different methods are shown in <ref type="table" target="#tab_13">Table 7</ref>, and the corresponding frame-wise comparisons are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, from which we have two observations: First, our models show significant improvements in both short-term and long-term predictions over the ConvLSTM network. Second, with the newly proposed memory decoupling and reverse scheduled sampling, PredRNN-V2 improves the conference version by a large margin in LPIPS (from 0.204 to 0.139). As mentioned above, LPIPS is more sensitive to perceptual human judgments, indicating that PredRNN-V2 has a stronger ability to generate high-fidelity images. In accordance with these results, we can see from the visual examples in <ref type="figure" target="#fig_0">Fig. 13</ref> that our approaches (especially PredRNN-V2) obtain more accurate predictions of future movement and body details. By decoupling memory states, PredRNN-V2 learns to capture the complex spatiotemporal variations from different timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Precipitation Forecasting from Radar Echoes</head><p>The accurate prediction of the movement of radar echoes in the next 0-2 hours is the foundation of precipitation forecasting. In this dataset, each sequence contains 10 input frames and 10 output frames, covering the historical data for the past hour and that for the future hour. It is challenging because the echoes tend to have non-rigid shapes and may move, accumulate or dissipate rapidly due to complex atmospheric physics, which makes it important to learn the dynamics in a unified spatiotemporal feature space.  In <ref type="table" target="#tab_15">Table 8</ref>, we compare PredRNN with three existing methods that have been shown effective for precipitation forecasting. Follow- ing a common practice, we evaluate the predicted radar maps with the Critical Success Index (CSI). Concretely, we first transform the pixel values back to echo intensities in dBZ, and then take 30, 40 and 50 dBZ as thresholds to compute: CSI = hits hits+misses+false alarms , where hits indicates the true positive, misses indicates the false positive, and false alarms is the false negative. From <ref type="table" target="#tab_15">Table 8</ref>, PredRNN consistently achieves the best performance over all CSI thresholds. In <ref type="figure" target="#fig_0">Fig. 14,</ref> we visualize the predicted radar frames by mapping them into RGB space, where areas with echo values over 40 dBZ tend to have severe weather.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Traffic4Cast Dataset</head><p>We evaluate the proposed methods on Traffic4Cast <ref type="bibr" target="#b82">[83]</ref>, a realworld dataset with complex and drastically changing information. We follow the experimental setups from CrevNet <ref type="bibr" target="#b27">[28]</ref>, which is a competitive model on this dataset and use the data of Berlin to construct a training set of 82,080 frames and a test set of 2,160 frames. The models are trained to predict 3 future frames from 9 context frames. All pixel values are normalized to [0, 1].</p><p>To cope with high-dimensional input frames, we apply the autoencoder architecture of U-Net <ref type="bibr" target="#b87">[88]</ref> to the network backbone of PredRNN. Specifically, the decoder of U-Net contains four ST-LSTM layers, and the CNN encoder takes both traffic flow maps and spatiotemporal memory states as inputs. We also apply the proposed methods to CrevNet, which uses either ConvLSTM or ST-LSTM as the alternatives of recurrent units in the autoencoder architecture. Therefore, it can be further improved by memory decoupling and reverse scheduled sampling.</p><p>In <ref type="table" target="#tab_16">Table 9</ref>, we follow the standard evaluation metric of MSE from the competition and have the following observations. Initially, the use of ST-LSTM improves the performance of the predictive models based on U-Net and CrevNet. Besides, the proposed  <ref type="figure" target="#fig_0">Fig. 16</ref>: Frame-wise SSIM and PSNR on the action-conditioned BAIR dataset. Note that the prediction time horizon is 10 during the training phase and extended to 28 during testing phase. memory decoupling and reverse scheduled sampling achieve further improvements over the models based on vanilla ST-LSTMs. <ref type="figure" target="#fig_0">Fig. 15</ref> provides the qualitative comparisons of different variants of our methods based on the architecture of CrevNet. As shown in the red box at t = 11, the final model with memory-decoupled ST-LSTM and reverse scheduled sampling predicts the rapid changes of traffic flows most accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Action-Conditioned BAIR Robot Pushing Dataset</head><p>We evaluate the action-conditioned PredRNN on the BAIR dataset that is widely used by previous methods, e.g., SVG <ref type="bibr" target="#b50">[51]</ref> and SV2P <ref type="bibr" target="#b34">[35]</ref>. We follow the experimental setups in SV2P to train the models to predict 10 frames into the future based on the first 2 visual observations, as well as the entire action sequence covering both encoding and forecasting timesteps. During the testing phase, instead, we extend the prediction time horizon to 28 timesteps for long-term evaluation. We mainly compare the performance of our approach with that of SVG and SV2P, which are competitive probabilistic models for stochastic video prediction. Specifically for these models, we draw 100 prediction samples from the prior distribution given a single testing sequence and report the results of the sample with the best SSIM score. From <ref type="table" target="#tab_18">Table 10</ref>, by comparing the proposed models with the action-free PredRNN, we find that the action information plays a significant role in future image generation. The performance of action-free PredRNN is mainly affected by the lack of observable dynamics due to limited input sequence, as well as the unpredictable spatiotemporal changes when future actions are unknown. It can be concluded that Eq. (7) provides an effective method to fuse the action information into the predictive network.</p><p>More importantly, in <ref type="figure" target="#fig_0">Fig. 16</ref>, we have the following observations. First, the action-conditioned PredRNN and PredRNN-V2 outperform SVG and SV2P significantly, especially for long-term  <ref type="figure" target="#fig_0">Fig. 17</ref>: Prediction examples on the BAIR dataset, in which 28 frames are predicted given the first 2 frames and the entire action sequence at both encoding and forecasting timesteps. We zoom in the areas bounded by red boxes for clear comparison. future prediction. Although probabilistic video prediction is useful for many downstream tasks such as epistemic POMDPs with implicit partial observability. The results indicate that in our cases in the BAIR dataset, the visual observations and action sequences provide rich information for future prediction. Second, the proposed models significantly outperform the action-conditioned ConvLSTM network, where we also apply the action fusion operation in Eq. <ref type="bibr" target="#b6">(7)</ref> to each ConvLSTM unit. Again, the results validate the effectiveness of ST-LSTM over ConvLSTM. Third, with memory decoupling and RSS, PredRNN-V2 presents a consistent improvement over PredRNN and achieves the state of the art. <ref type="figure" target="#fig_0">Fig. 17</ref> shows the qualitative results, in which our model predicts the future position of commanded gripper more precisely and largely enriches the details of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Model Limitations</head><p>To explore on what conditions the proposed model contributes the most, we visualize the examples that have comparable quantitative results in SSIM and MSE with the second-best models on the KTH, radar, and BAIR datasets. As shown in <ref type="figure" target="#fig_0">Fig. 18</ref>, on KTH and BAIR, i.e., datasets with clear visual structures, PredRNN-V2 significantly outperforms the other models in the visual quality of the generated future images. It makes more accurate predictions about future positions of the character (for KTH) and the robot arm (for BAIR). However, as for the radar echo dataset, <ref type="figure" target="#fig_0">Fig. 18</ref> reveals the limitation of our approach in dealing with the special cases of the long-tail distribution of the dataset. In these cases, the radar observations  <ref type="figure" target="#fig_0">Fig. 18</ref>: Examples that PredRNN-V2 yields comparable quantitative results with the previous art.</p><p>show very sparse but still significant areas of high intensity echoes. Although PredRNN-V2 still achieves a better performance than the compared model in the forecast of future positions of the highintensity areas, it cannot perfectly model the dynamics of the sparse echoes. Precipitation nowcasting is an extremely difficult research problem for existing spatiotemporal predictive learning approaches. A future research direction for mitigating the long-tail effect is to explicitly consider the complex physical properties and combine them with deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a recurrent network named PredRNN for spatiotemporal predictive learning. With a new Spatiotemporal LSTM unit, PredRNN models the short-term deformations in spatial appearance and the long-term dynamics over multiple frames simultaneously. The core of the Spatiotemporal LSTM is a zigzag memory flow that propagates across stacked recurrent layers vertically and through all temporal states horizontally, which enables the interactions of the hierarchical memory representations at different levels of PredRNN. Building upon the conference version of this paper, we introduce a new method to decouple the twisted memory states along the horizontal and the zigzag pathway of recurrent state transitions. It enables the model to benefit from learning distributed representations that could cover different aspects of spatiotemporal variations. Furthermore, we also propose a new curriculum learning strategy named reverse scheduled sampling, which forces the encoding part of PredRNN to learn temporal dynamics from longer periods of the context frames. Our approach achieves state-of-the-art performance on synthetic and natural spatiotemporal datasets, including both action-free and action-conditioned predictive learning scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Left: the spatiotemporal memory flow architecture that uses ConvLSTM as the building block. The orange arrows show the deep-in-time path of memory state transitions. Right: the original ConvLSTM network proposed by Shi et al. [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Left: the main architecture of PredRNN, in which the orange arrows denote the state transition paths of M l t , namely the spatiotemporal memory flow. Right: the ST-LSTM unit with twisted memory states that serves as the building block of the proposed PredRNN, where the orange circles denote the unique structures compared with ConvLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of ?C l t (red points) and ?M l t (black points) using t-SNE<ref type="bibr" target="#b76">[77]</ref> on the KTH action dataset. Models are respectively trained without or with memory decoupling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Graphical model of the action-conditioned PredRNN. For simplicity, we display only one ST-LSTM layer and use S t as the combination of the memory states C t and M t in Eq.<ref type="bibr" target="#b6">(7)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 Fig. 6 :</head><label>26</label><figDesc>Two feasible strategies to combine the reverse scheduled sampling (in red) and the original scheduled sampling (in blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Frame-wise results on the Moving MNIST test set produced by models trained on the fixed training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>?</head><label></label><figDesc>PredRNN-M-Only: This model improves the ConvLSTM network with the spatiotemporal memory flow (M). The architecture is shown in Fig. 1 (left). ? PredRNN: This model uses ST-LSTMs as the building blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Prediction examples on the Moving MNIST test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>5 (a) ? H 1 tL</head><label>51</label><figDesc>T +K , t ? 1, T = 10,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>An analysis of the ratios of the saturated forget gates (f t &lt; 0.1) in all transitions of temporal memory states C l t (l ? {1, 2, 3, 4}). We obtain the results by evaluating 100 samples from the Moving MNIST test set. The results demonstrate the effect of the RSS and the zigzag memory flow in long-term modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>(b) Responses to long-term and short-term dynamics Model performance under different types of temporal dynamics, where the models take as inputs the real observations at all timesteps, and the Y axis represents the mean value of predictions averaged over 100 test samples. We show two ground-truth sequences as examples at the top of the figure for a better intuitive understanding of the two types of pixel intensity changes. For each type, we apply the same rate of changes to all test samples. With memory-decoupled ST-LSTMs, our model is shown to (a) respond more rapidly to unexpected, sudden variations, and (b) simultaneously capture temporal dynamics at different timescales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 :</head><label>12</label><figDesc>Frame-wise results on KTH. The prediction horizon is 10 timesteps at training time and 20 timesteps at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 :</head><label>15</label><figDesc>Showcases at the last forecasting timestep on Traffic4cast. Please note the differences in areas in the orange and red boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Use the true previous frame with an increasing probability ?k over training iterations</figDesc><table><row><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>Scheduled</cell><cell>Scheduled</cell></row><row><cell></cell><cell>sampling</cell><cell>sampling</cell></row></table><note>represents the temporal dynamics as a combination of previous memory states. The main difference ST-LSTMs ? ST-LSTMs? Sequence Forecaster: Use the true previous frame with a decreasing probability over training iterations Sequence Encoder:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>This dataset contains the action-conditioned videos collected by a Sawyer robotic arm pushing a variety of objects. At each timestep, we have a 64 ? 64 RGB image and the action vector of the commanded gripper pose. We have a training set of 822,016 sequences and a test set of 4,864 sequences.</figDesc><table><row><cell>Compared models. We use the ConvLSTM network [1] as the</cell></row><row><cell>primary baseline model, and compare PredRNN with the current</cell></row><row><cell>states of the art for each dataset, including:</cell></row></table><note>? KTH [82]: This dataset contains 6 types of human actions, i.e., walking, jogging, running, boxing, hand-waving, and hand- clapping, performed by 25 persons in 4 different scenes. The videos last 4 seconds on average and have a frame rate of 25 FPS. We resize the frames to a resolution of 128 ? 128. We adopt the protocol from [57], i.e., persons 1-16 for training; persons 17-25 for testing, and obtain a training set of 108,717 sequences and a test set of 4,086 sequences.? Radar echo dataset: This dataset contains 10,000 consecutive radar maps recorded every 6 minutes at Guangzhou, China. We transform the radar maps to pixel values and represent them as 128 ? 128 gray-scale images. We divide the dataset into 7,800 training sequences and 1,800 test sequences.? Traffic4Cast [83]: This dataset records the GPS trajectories of consecutive traffic flows in Berlin, Moscow, and Istanbul in the form of video frames in 2019. The size of each frame is 495 ? 436 ? 3. The value of each pixel corresponds to the traffic information in an area of 100m?100m in 5 minutes, including mean speed, volume, and major traffic direction.? BAIR [13]:? CrevNet [28] for Moving MNIST and Traffic4Cast.? Conv-TT-LSTM [29] for the KTH action dataset.? TrajGRU [25] for precipitation forecasting.? SV2P [35] for action-conditioned video prediction. Notably, some existing approaches have extended PredRNN in different aspects</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Performance on the Moving MNIST test set, averaged over 10 prediction timesteps. All models are trained with a fixed training set of 10,000 sequences. For a fair comparison, we also report the model size and computation efficiency. ConvLSTM* denotes a 4-layer network with a 256-channel hidden state each.</figDesc><table><row><cell>Model</cell><cell cols="5">MSE SSIM LPIPS #Params. FLOPS Mem. (?) (?) (?) (MB) (G) (GB)</cell></row><row><cell>ConvLSTM [1]</cell><cell cols="2">103.3 0.707 0.156</cell><cell>16.60</cell><cell>80.7</cell><cell>2.58</cell></row><row><cell>ConvLSTM* [1]</cell><cell cols="2">62.8 0.846 0.126</cell><cell>65.96</cell><cell>320.8</cell><cell>5.62</cell></row><row><cell>CDNA [26]</cell><cell>97.4 0.721</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VPN Baseline [67]</cell><cell>64.1 0.870</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MIM [4]</cell><cell cols="2">52.0 0.874 0.079</cell><cell>37.37</cell><cell>181.7</cell><cell>4.22</cell></row><row><cell cols="3">Conv-TT-LSTM [29] 64.3 0.846 0.133</cell><cell>23.91</cell><cell>116.3</cell><cell>6.17</cell></row><row><cell>PredRNN-M-Only</cell><cell cols="2">74.0 0.851 0.109</cell><cell>18.30</cell><cell>89.02</cell><cell>2.37</cell></row><row><cell>PredRNN (Conf.)</cell><cell cols="2">56.8 0.867 0.107</cell><cell>23.85</cell><cell>115.9</cell><cell>3.52</cell></row><row><cell>PredRNN-V2</cell><cell cols="2">48.4 0.891 0.071</cell><cell>23.86</cell><cell>116.6</cell><cell>3.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>We follow the setup of CrevNet to train the predictive models on dynamically generated Moving MNIST. The results of CrevNet are taken directly from the paper<ref type="bibr" target="#b27">[28]</ref>. Models marked by * have three 64-channel ST-LSTM layers with fewer parameters.</figDesc><table><row><cell>Model</cell><cell cols="3">MSE (?) 2-digit 3-digit 2-digit 3-digit SSIM (?)</cell><cell>#Params. (MB)</cell></row><row><cell cols="2">CrevNet w/ ConvLSTM 38.5</cell><cell>57.2</cell><cell>0.928 0.886</cell><cell>2.77</cell></row><row><cell>CrevNet w/ ST-LSTM</cell><cell>22.3</cell><cell>40.6</cell><cell>0.949 0.916</cell><cell>5.00</cell></row><row><cell>PredRNN*</cell><cell>21.5</cell><cell>39.6</cell><cell>0.931 0.884</cell><cell>4.40</cell></row><row><cell>PredRNN-V2*</cell><cell>19.9</cell><cell>34.8</cell><cell>0.939 0.900</cell><cell>4.41</cell></row><row><cell>PredRNN</cell><cell>15.7</cell><cell>26.7</cell><cell>0.951 0.918</cell><cell>23.85</cell></row><row><cell>PredRNN-V2</cell><cell>13.4</cell><cell>24.7</cell><cell>0.958 0.928</cell><cell>23.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>provides the corresponding frame-wise comparisons. The final PredRNN model significantly outperforms all previous approaches, and all the proposed techniques have their contributions. First, with the proposed spatiotemporal memory flow, the PredRNN-M-only model reduces the per-frame MSE of the ConvLSTM baseline from 103.3 down to 74.0. Second, by using the ST-LSTM in place of the ConvLSTM unit, our model further reduces the MSE down to 56.8. Finally, the employment of the memory decoupling and the reverse scheduled sampling techniques brings another 14.8% improvement in MSE (from 56.8 to 48.4). In</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>An ablation study on the zigzag memory flow. We compare the baseline PredRNN with two alternatives that truncate the memory flow at specific positions in the transition path of M l t .</figDesc><table><row><cell>Model</cell><cell>MSE (?)</cell></row><row><cell>PredRNN (w/o RSS or memory decoupling)</cell><cell>56.8</cell></row><row><cell>-Truncated at M 1 t ? . . . ? M L t (bottom-up) -Truncated at M L t ? M 1 t+1 (top-down)</cell><cell>57.6 59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 :</head><label>5</label><figDesc>Effect of memory decoupling and reverse scheduled sampling on different network backbones (sorted by publish date).</figDesc><table><row><cell>Backbone</cell><cell cols="4">Decoupling RSS MSE (?) Perf. gain</cell></row><row><cell>ConvLSTM [1]</cell><cell>? ?</cell><cell>?</cell><cell>103.3 64.2</cell><cell>37.8%</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>56.8</cell><cell></cell></row><row><cell>PredRNN</cell><cell></cell><cell>?</cell><cell>51.1</cell><cell>14.8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>48.4</cell><cell></cell></row><row><cell>PredRNN++ [27]</cell><cell>?</cell><cell>?</cell><cell>46.5 44.8</cell><cell>3.7%</cell></row><row><cell>E3D-LSTM [9]</cell><cell>?</cell><cell>?</cell><cell>44.2 42.1</cell><cell>4.8%</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>52.0</cell><cell></cell></row><row><cell>MIM [4]</cell><cell></cell><cell>?</cell><cell>47.9</cell><cell>11.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>45.8</cell><cell></cell></row><row><cell>Conv-TT-LSTM [29]</cell><cell>? ?</cell><cell>?</cell><cell>64.3 59.0</cell><cell>8.2%</cell></row><row><cell>MotionRNN [71]</cell><cell>?</cell><cell>?</cell><cell>52.4 49.4</cell><cell>5.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6 :</head><label>6</label><figDesc>Ablation study on the training schemes. For RSS, k changes from s to e . Models are trained w/o the decoupling loss.</figDesc><table><row><cell>Method</cell><cell>s</cell><cell>e</cell><cell>RSS mode</cell><cell>MSE (?)</cell></row><row><cell>PredRNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.3</cell></row><row><cell>+ Scheduled sampling [24]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell>51.8</cell></row><row><cell>+ 1st strategy in Fig. 6(a)</cell><cell cols="2">0.0 1.0</cell><cell>Sigmoid</cell><cell>53.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Exponential</cell><cell>51.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell>51.9</cell></row><row><cell>+ 2nd strategy in Fig. 6(b)</cell><cell cols="2">0.5 1.0</cell><cell>Sigmoid</cell><cell>50.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Exponential</cell><cell>50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 7 :</head><label>7</label><figDesc>Results on KTH averaged over 20 future timesteps.</figDesc><table><row><cell>Model</cell><cell cols="3">PSNR (?) SSIM (?) LPIPS (?)</cell></row><row><cell>ConvLSTM [1]</cell><cell>23.58</cell><cell>0.712</cell><cell>0.231</cell></row><row><cell>MCnet + Residual [57]</cell><cell>26.29</cell><cell>0.806</cell><cell>-</cell></row><row><cell>TrajGRU [25]</cell><cell>26.97</cell><cell>0.790</cell><cell>-</cell></row><row><cell>DFN [87]</cell><cell>27.26</cell><cell>0.794</cell><cell>-</cell></row><row><cell>Conv-TT-LSTM [29]</cell><cell>27.62</cell><cell>0.815</cell><cell>0.196</cell></row><row><cell>PredRNN</cell><cell>27.55</cell><cell>0.839</cell><cell>0.204</cell></row><row><cell>PredRNN-V2</cell><cell>28.37</cell><cell>0.838</cell><cell>0.139</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Prediction examples on the KTH test set, where we predict 20 frames into the future based on the past 10 frames. Prediction examples on the radar echo test set, in which 10 future frames are generated from the past 10 observations.</figDesc><table><row><cell cols="2">Input frames</cell><cell cols="6">Ground truth &amp; predicted future frames</cell><cell></cell><cell cols="2">Input frames</cell><cell></cell><cell cols="5">Ground truth &amp; predicted future frames</cell><cell></cell></row><row><cell>t = 6</cell><cell>9</cell><cell>12</cell><cell>15</cell><cell>18</cell><cell>21</cell><cell>24</cell><cell>27</cell><cell>30</cell><cell>t = 6</cell><cell>9</cell><cell>12</cell><cell>15</cell><cell>18</cell><cell>21</cell><cell>24</cell><cell>27</cell><cell>30</cell></row><row><cell cols="2">PredRNN-V2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PredRNN-V2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PredRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PredRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Conv-TT-LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv-TT-LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MCnet +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MCnet +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Residual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Residual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Fig. 13: Ground truth &amp; predicted future frames Input frames</cell><cell></cell><cell cols="2">Input frames</cell><cell cols="5">Ground truth &amp; predicted future frames</cell><cell></cell></row><row><cell>t = 1</cell><cell></cell><cell>10</cell><cell>12</cell><cell></cell><cell>14</cell><cell>16</cell><cell>18</cell><cell>20</cell><cell>t = 1</cell><cell></cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>18</cell><cell>20</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50 dBZ</cell><cell cols="2">PredRNN-V2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PredRNN-V2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>45 dBZ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">PredRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PredRNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40 dBZ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>35 dBZ</cell><cell></cell><cell>MIM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MIM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30 dBZ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25 dBZ</cell><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20 dBZ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">TrajGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TrajGRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15 dBZ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 14:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 8 :</head><label>8</label><figDesc>Quantitative results on the radar echo dataset.</figDesc><table><row><cell>Model</cell><cell cols="4">MSE (?) CSI-30 (?) CSI-40 (?) CSI-50 (?)</cell></row><row><cell>TrajGRU [25]</cell><cell>68.3</cell><cell>0.309</cell><cell>0.266</cell><cell>0.211</cell></row><row><cell>ConvLSTM [1]</cell><cell>63.7</cell><cell>0.381</cell><cell>0.340</cell><cell>0.286</cell></row><row><cell>MIM [4]</cell><cell>39.3</cell><cell>0.451</cell><cell>0.418</cell><cell>0.372</cell></row><row><cell>PredRNN</cell><cell>39.1</cell><cell>0.455</cell><cell>0.417</cell><cell>0.358</cell></row><row><cell>PredRNN-V2</cell><cell>36.4</cell><cell>0.462</cell><cell>0.425</cell><cell>0.378</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 9 :</head><label>9</label><figDesc>Quantitative results on the Traffic4cast dataset.</figDesc><table><row><cell>Backbone</cell><cell cols="4">Recurrent unit Decoupl. RSS MSE (10 ?3 )</cell></row><row><cell>U-Net [88]</cell><cell>None</cell><cell>?</cell><cell>?</cell><cell>6.992</cell></row><row><cell>U-Net + PredRNN</cell><cell>ST-LSTM ST-LSTM</cell><cell>?</cell><cell>?</cell><cell>7.035 5.135</cell></row><row><cell></cell><cell>ConvLSTM</cell><cell>?</cell><cell>?</cell><cell>6.789</cell></row><row><cell>CrevNet [28]</cell><cell>ST-LSTM</cell><cell>?</cell><cell>?</cell><cell>6.613</cell></row><row><cell></cell><cell>ST-LSTM</cell><cell></cell><cell></cell><cell>6.506</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 10 :</head><label>10</label><figDesc>Quantitative results on the BAIR robot pushing dataset averaged over 28 future timesteps in both action-free and actionconditioned experimental settings.</figDesc><table><row><cell>Inputs</cell><cell>Model</cell><cell cols="2">SSIM (?) PSNR (?)</cell></row><row><cell>Action-Free</cell><cell>PredRNN PredRNN-V2</cell><cell>0.536 0.577</cell><cell>16.21 16.80</cell></row><row><cell></cell><cell>ConvLSTM [4]</cell><cell>0.807</cell><cell>20.52</cell></row><row><cell></cell><cell>SVG [51]</cell><cell>0.821</cell><cell>19.97</cell></row><row><cell>Action-Conditioned</cell><cell>SV2P [35]</cell><cell>0.837</cell><cell>21.32</cell></row><row><cell></cell><cell>PredRNN</cell><cell>0.843</cell><cell>21.93</cell></row><row><cell></cell><cell>PredRNN-V2</cell><cell>0.861</cell><cell>22.74</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t H 3 t H 1 t H 4 t H 1 t H 2 t H 3 t H 1 t H 2 t H 3 t M 4</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = o t tanh(W 1?1 * [C l t , M l t ]).(4)InFig. 2, we present the final PredRNN model by taking ST-LSTM as the building block in place of ConvLSTM. There are two memory states: First, C l t is the temporal memory that transits within each ST-LSTM unit from the previous node at t ? 1 to the current timestep. We adopt the original gates for C l t from the standard LSTM, but remove the Hadamard terms W ?? C l t?1 from the computation of i t and f t , due to empirical performance and model efficiency. Second, M l t is the spatiotemporal memory, which transits vertically to the current node from the lower l ?1 ST-LSTM unit at the same timestep. In particular, we assign M L t?1 to</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Natural Science Foundation of China (62022050, 62106144 and 62021002). M. Long was supported by Beijing Nova Program (Z201100006820041) and BNRist Innovation Fund (BNR2021RC01002). Y. Wang was supported by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and Shanghai Sailing Program (21Z510202133). The work was in part done when Y. Wang was a student at Tsinghua University. Y. Wang and H. Wu contributed equally to this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yunbo Wang received the BE degree from Xi'an Jiaotong University in 2012, and the ME and PhD degrees from Tsinghua University in 2015 and 2020. He received the CCF Outstanding Doctoral Dissertation Award in 2020, advised by Philip S. Yu and Mingsheng Long. He is now an assistant professor at the AI Institute and the Department of Computer Science at Shanghai Jiao Tong University. He does research in deep learning, especially predictive learning, spatiotemporal modeling, and model-based decision making.</p><p>Haixu Wu received the BE degree in computer software from Tsinghua University in 2020. He is working towards the ME degree in computer software at Tsinghua University. His research interests include machine learning and computer vision.</p><p>Jianjin Zhang received the ME degree in computer software from Tsinghua University in 2019. His research interests lie on the intersection of machine learning, time series analysis, and natural language processing. Zhifeng Gao received the ME degree in computer software from Tsinghua University in 2019. His research interests include machine learning and big data systems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="802" to="810" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PredRNN: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PredCNN: Predictive learning with cascade convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2940" to="2947" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Memory in memory: A predictive neural network for learning higher-order non-stationarity from spatiotemporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="9154" to="9162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to see physics via visual de-animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational neural expectation maximization: Unsupervised discovery of objects and their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="2688" to="2697" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised discovery of parts, structure, and dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Eidetic 3D LSTM: A model for video prediction and beyond</title>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2450" to="2462" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2555" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the number of response regions of deep feed forward networks with piecewise linear activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6098</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3104" to="3112" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1171" to="1179" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5617" to="5627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pre-dRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="5123" to="5132" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient and information-preserving future frame prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Easterbrook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Convolutional tensor-train LSTM for spatio-temporal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review on deep learning techniques for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Castro-Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Actionconditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2863" to="2871" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Videoflow: A flow-based generative model for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1655" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4271" to="4280" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dual motion GAN for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1744" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Future video synthesis with object motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5539" to="5548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hierarchical patch vae-gan: Generating diverse videos from a single sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning in latent space for video prediction and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="6038" to="6046" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational temporal abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11" to="570" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="1174" to="1183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">High fidelity video prediction with large stochastic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improved conditional vrnns for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7608" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stochastic latent residual video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delasalles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3233" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Greedy hierarchical variational autoencoders for large-scale video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2318" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning semantic-aware dynamics for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="902" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4417" to="4426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="517" to="526" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchical video prediction using relational layouts for human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Unsupervised video decomposition using spatio-temporal iterative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zablotskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Dominici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14727</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Guen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Contextvp: Fully context-aware video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="753" to="769" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="716" to="731" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Structure preserving video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1460" to="1469" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Motionrnn: A flexible model for video prediction with spacetime-varying motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2440" to="2448" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwi?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Taking on the curse of dimensionality in joint distributions using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="557" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural network ensembles, cross validation, and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vedelsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="231" to="238" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of biometrics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Recurrent environment simulators,&quot; in ICLR</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Traffic4cast 2019: Traffic map movie forecasting</title>
		<ptr target="https://www.iarai.ac.at/traffic4cast/2019-competition/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="586" to="595" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="667" to="675" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
