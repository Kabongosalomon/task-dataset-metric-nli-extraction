<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiface: A Dataset for Neural Face Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hsin</forename><surname>Wuu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyuan</forename><surname>Zheng</surname></persName>
							<email>zhengningyuan@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ardisson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Bali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Belko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brockmeyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Evans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyowon</forename><surname>Ha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hypes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Koska</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Krenn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevyn</forename><surname>Mcphail</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Millerschoen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pitts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junko</forename><surname>Saragih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Stewart</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Autumn</forename><surname>Trimble</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Whitewolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multiface: A Dataset for Neural Face Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Photorealistic avatars of human faces have come a long way in recent years, yet research along this area is limited by a lack of publicly available, high-quality datasets covering both, dense multi-view camera captures, and rich facial expressions of the captured subjects. In this work, we present Multiface, a new multi-view, high-resolution human face dataset collected from 13 identities at Reality Labs Research for neural face rendering. We introduce Mugsy, a large scale multi-camera apparatus to capture high-resolution synchronized videos of a facial performance. The goal of Multiface is to close the gap in accessibility to high quality data in the academic community and to enable research in VR telepresence. Along with the release of the dataset, we conduct ablation studies on the influence of different model architectures toward the model's interpolation capacity of novel viewpoint and expressions. With a conditional VAE model [8] serving as our baseline, we found that adding spatial bias, texture warp field, and residual connections improves performance on novel view synthesis. Our code and data is available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Photo-realistic human face rendering and reconstruction is essential to real-time telepresence technology that drives modern Virtual Reality applications. Since humans are social animals that have evolved to express and read emotions from subtle changes in facial expressions, tiny artifacts give rise to the uncanny valley that could hurt user experience. Nowadays, many modern 3D telepresence methods leverage deep learning models and neural rendering for highfidelity reconstruction, and to tackle difficult problems such as novel view synthesis and view-dependent effects modeling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. These approaches are usually datahungry, and the design of a capture system and data collection pipeline directly determines the performance of those models. Pushing the boundaries in such photo-realistic human face models therefore requires a large dataset of highresolution, multi-view facial images spanning a wide variety of expressions. We introduce such a dataset, collected by a high-end multi-view capturing system (Mugsy) that we built at Reality Labs Research in Pittsburgh. Compared to existing face modeling datasets such as HUMBI <ref type="bibr" target="#b22">[23]</ref> and FaceWarehouse <ref type="bibr" target="#b0">[1]</ref>, our Codec-Avatar dataset contains facial data of unprecedented quality, variation in facial expressions, and number of camera views. We capture 13 subjects with a great variety of high-fidelity facial expressions along with the geometry mesh tracked over the capturing time. For each subject, we have over a hundred facial expressions captured by multiple machine vision cameras synchronously at 4096?2668 resolution (about 11 megapixels). The capture system is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We release the images of all 13 captured participants as well as tracked meshes and unwrapped textures, along with camera calibrations and audio data. Moreover, we provide code to train a Codec Avatar from scratch as well as pretrained Codec Avatars for all identities.</p><p>The creation of data-driven avatars is challenging along three major axes: (1) novel view synthesis, as we could not have cameras placed everywhere (2) novel expression synthesis, as we could not ask the participants to enact all possible facial expressions during the capture, and (3) relighting, as it is impossible to capture every possible lighting configuration. In this report, we focus on the first two axes, while relighting is beyond the scope of this work. We use a conditional VAE model <ref type="bibr" target="#b7">[8]</ref> as a baseline, and evaluate the model's reconstruction quality with respect to different net- work architectures, which includes spatial biases, a texture warp field, and residual blocks. Empirically, we found that the baseline model benefits from these architectural modifications in interpolating novel views.</p><p>After a brief discussion of related work, we provide details on our capture system Mugsy in Section 3 and describe the capture script and process used to collect the dataset. In Section 4, we describe the model architectures and training pipeline for building the photo-realistic Codec Avatar. In Section 5, we present an ablation study of how different model architectures respond to synthesizing viewpoints and expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>We briefly review two prior efforts in human face data collection, HUMBI <ref type="bibr" target="#b22">[23]</ref> and FaceWarehouse <ref type="bibr" target="#b0">[1]</ref>, and compare these datasets with our dataset, see <ref type="table" target="#tab_0">Table 1</ref>.</p><p>HUMBI <ref type="bibr" target="#b22">[23]</ref> is a large-scale multi-view dataset designed to facilitate high resolution pose-and view-specific appearance of human body expressions (gaze, face, hand, body, and garment). The database uses a dense camera array composed of 107 synchronized cameras to capture 772 distinctive subjects doing diverse activities. The presence of HUMBI shows a new opportunity to build a versatile model that generates data-driven photo-realistic rendering for full body avatars. While HUMBI focuses on capturing multiview images for the entire body, it does not provide high enough resolution for the images. This restriction poses a challenge on reconstructing the subtle changes of human facial expression, which we aim to overcome with our dataset. FaceWarehouse <ref type="bibr" target="#b0">[1]</ref> is a database of 3D facial expressions for visual computing applications. The database uses Kinect, an off-the-shelf RGBD camera, to capture and estimate facial geometry and texture of 150 subjects, each with 20 expressions. Compared with previous 3D facial databases, FaceWarehouse has richer collections of expressions for each person that enables depiction of most human facial actions. This dataset has potential on applications such as facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image. However, the data in FaceWarehouse does not contain detailed facial geometries such as wrinkles due to the low precision in depth information provided from the capture apparatus. This insufficiency makes applications such as high-fidelity 3D facial reconstruction very challenging. Our dataset, in contrast, provides the richness in facial expressions together with high-resolution images that enables us to model nuanced yet important subtleties in human faces up to the level of skin pores.</p><p>Existing works on Codec Avatars. In the past, a line of works emerged from our lab that is built on the data released in this dataset. Deep appearance models <ref type="bibr" target="#b7">[8]</ref> are the first successful approach to building photorealistic, high quality avatars but suffer from limitations in their expressiveness and controllability. To overcome these issues, in <ref type="bibr" target="#b1">[2]</ref>, the original deep appearance model has been replaced by a fully convolutional architecture that aims at increased modularity among different facial regions and thereby achieves higher expressiveness. Focusing on the importance of eye contact in human communication, <ref type="bibr" target="#b17">[18]</ref> extends Codec Avatars with an explicit eye model. A limitation of all above approaches is their reliance on traditional mesh-based rendering, which falls short in its ability to accurately render thin structures, translucency, and biological motion. With the uprise of neural rendering, this reliance on mesh-based rendering has been largely overcome, and avatars have been shown to expose outstanding details and realistic modeling of difficult regions such as hair <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Audio-visual data has been leveraged in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, both in a mesh-based setup as well as in fully-textured avatar animation from audio and gaze. Finally, Pixel Codec Avatars <ref type="bibr" target="#b10">[11]</ref> provide a lightweight model that can render photorealistic avatars on commodity hardware such as a Quest 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Creation</head><p>In this section, we detail how the dataset was created, starting with an overview of the dataset characteristics, followed by a capture system description, a description of the capture script participants went through, and the tracking pipeline used to process the captured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Overview</head><p>Our dataset consists of high quality recordings of the faces of 13 identities, each captured in a multi-view capture stage performing various facial expressions. An average of 12,200 (capture version 1) to 23,000 (capture version 2) frames per subject were captured at 30 fps. Each frame has roughly 40 (v1) or 150 (v2) different camera views under uniform illumination.</p><p>We provide the captured images from each camera view at a resolution of 2048 ? 1334 pixels, tracked meshes including headposes, unwrapped textures at 1024 ? 1024 pixels, metadata including intrinsic and extrinsic camera calibrations, and audio. Additionally, we release code to download the dataset and build a Codec Avatar using a deep appearance model <ref type="bibr" target="#b7">[8]</ref>. All required code and dataset documentation is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Capture Studio</head><p>In order to capture synchronized multi-view videos of a facial performance, we built a multi-video-camera capture dome called Mugsy (short from Mugshooter), see <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The cameras are placed on the surface of a sphere with radius 1.2 meters. The cameras all point inward to the mid-dle of the sphere, which is where the head of the participant is located. An overview of all the views is shown in <ref type="figure">Figure 2</ref>. The sensors used are IMX253 with pixel size 3.45 ?m. We capture at resolution 4096x2668, which is roughly 11 megapixels. Shutter speed is at 2.222 ms. In order for the cameras to capture synchronously, they all listen to the rising edge of a single trigger. While the system is able to capture at 90fps, we only release data at 30fps and downsample the images to 2048x1334 to limit the total dataset size.</p><p>For lights, we use point light sources that are pointing towards the center of the sphere to illuminate the face of the participant. The lights have diffusers installed to reduce specular highlights on the person's face and better approximate uniform lighting.</p><p>All cameras are jointly calibrated using a 3D calibration target <ref type="bibr" target="#b3">[4]</ref> mounted on a robot arm. The calibration process is based on corner detection, intrinsics calibration, and then a final bundle adjustment. We provide the intrinsics and extrinsics of each camera and for each participant as part of the dataset.</p><p>We release ten captures from the original Mugsy (v1) and three captures obtained with an extended version featuring a significantly larger amount of cameras (Mugsy v2), see <ref type="table" target="#tab_1">Table 2</ref> for details. Note that the number of cameras in each capture may be less than the number of cameras shown in the table as individual cameras might have failed during a capture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Capture Script</head><p>The goal of the face captures is to cover the full range of facial expressions such that a neural avatar like a deep appearance model <ref type="bibr" target="#b7">[8]</ref> can learn to interpolate from a range of captured expressions to all possible facial expressions. To this end, we design a capture script that captures a range of expressions, gaze directions, and 50 phonetically balanced sentences, see <ref type="table" target="#tab_2">Table 3</ref>. The ten captures from Mugsy v1 follow a script where the expression portion is focused on peak expressions as in <ref type="figure" target="#fig_1">Figure 3</ref>, which aim to capture motion in different regions of the face independently. The three captures from Mugsy v2 follow a slightly modified script, where the expression portion is focused on full-face range of motion tasks, and where the gaze portion has been simplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Data Processing: The Tracking Pipeline</head><p>In order to obtain meshes and unwrapped textures from the raw images, we run the captured data through a sophisticated tracking pipeline. Note that we release the resulting meshes and textures, so rebuilding the tracking pipeline is not required for users of this dataset. The pipeline follows several steps as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. First, we run a modified version of parallel Patchmatch <ref type="bibr" target="#b2">[3]</ref> on each frame to get  Participants are asked to make their best effort for expressions they may not be able to do, e.g., raise only left or right eyebrow.</p><p>a dense 3D mesh reconstruction. Note that dense meshes in different frames do not share the same topology. Next, we detect 2D keypoints on the face. Then, we run sequential tracking with model-free mesh tracking <ref type="bibr" target="#b19">[20]</ref> with images, keypoints, and the dense 3D mesh reconstructions as input to get corresponding tracked meshes. Due to the computational cost of these steps, we run this sequential tracking pipeline only on a subset of the captured data, i.e., the expressions and the gaze portion, but not the sentences. Given these tracked results, we generate training data for personalized keypoint detection. The personalized keypoints are not just constrained to landmarks where a human annotator can consistently annotate, but also locations on the cheek and forehead, which is hard for a human to annotate consistently, but could be annotated accurately by model-free mesh tracking. The training data is used to train a personalized keypoint detector, which is then used as an initialization of a PCA model-based mesh tracking method. The advantage of using the personalized keypoints and PCA model-based tracking is that sequential tracking is no longer required, and all frames can be tracked in parallel, thus reducing the computational cost significantly and allowing us to process the complete capture efficiently. Finally, once we have the tracked meshes and an image from each view, we unwrap the texture for that specific view to obtain all necessary data for codec avatar generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Dataset Summary</head><p>In summary, for each of the 13 captured subjects, we provide the following data. Raw Images. Raw images are directly captured from 40 (v1) to 160 (v2) multi-view cameras at the rate of 30 fps. We release raw images at the resolution of 2048 ? 1334. Raw images can be used as ground truth to compute the screen loss with predicted rendered images from the model during model training. Unwrapped Textures. Unwrapped textures are provided at a resolution of 1024 ? 1024, and are generated by unwrapping the raw images from the geometry. We wrap each mesh triangle to the corresponding UV texture triangle using barycentric interpolation. Each camera and each frame has its unique view-dependent unwrapped texture.  <ref type="figure" target="#fig_1">Fig. 3</ref> Participants go from neutral expression to peak to neutral. Only data from peak to neutral is processed and released.</p><p>1 single range-of-motion segment: ROM07</p><p>Participants look at 25 fixed markers without turning their head.</p><p>Participants will look at the leds with normal eyes, wide eyes, squinty eyes, and small head rotations.</p><p>50 phonetically balanced sentences v2 2 peak expressions: neutral and eyes closed mouth lightly open.</p><p>18 range-of-motion segments covering 118 expressions over the entire face.</p><p>Participants look at 9 cameras without turning their head. When looking at each camera, look at it with normal eyes, wide eyes, squinty eyes, blink, and wink.</p><p>Additionally, look at a camera and do 5 head rotations. Tracked Meshes. Meshes are tracked per frame and stored in .obj format. Each mesh consists of 7,306 vertices, with no vertices inside eyes or mouth. The meshes share the same topology. By projecting with the provided camera calibrations and headposes, meshes can be aligned with raw images. Headposes. A headpose is a 3 ? 4 matrix consisting of rotation and translation that represents the rigid body transformation of the head mesh at each frame. Audio. While most captured expressions are silent, each participant was asked to read 50 phonetically balanced sentences. We provide audio data for these 50 sentences of each participant.</p><p>Metadata. The following metadata is provided as well:</p><p>? camera calibrations: we provide each camera's intrinsic and extrinsic matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Technical Details</head><p>In this section, we demonstrate how to use the dataset and the provided scripts for training. For detailed instructions how to train a codec avatar using our codebase, see the Github repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model</head><p>Our model greatly resembles the deep appearance model <ref type="bibr" target="#b7">[8]</ref>, which, at its core, is a variational autoencoder (VAE) that takes meshes and average texture as input and decodes the view-dependent textures for rendering, see Figure 5. We follow the original algorithm of <ref type="bibr" target="#b7">[8]</ref> with some minor exceptions that will be outlined in the following.</p><p>As input, the model consumes the average texture from all camera views for a given training frame as well as the tracked mesh. Using a neural encoder, these inputs are mapped to a 256-dimensional KL-regularized latent space. Note that the latent space is view-independent as the inputs  to the encoder are view-agnostic. A neural decoder consumes such a view-independent latent representation and a view vector and generates the texture for this specific view as well as the mesh. The texture encoder consists of 8 convolutional layers, each with kernel size 4 and stride 2, that downsamples the input texture from a resolution of 1024x1024 to a 4x4 feature map. The mesh is encoded with a multi-layer perceptron (MLP) and encoding from texture encoder and mesh encoder are combined into a single 256-dimensional latent vector using a fully connected layer. On the decoder side, the view information is fed into an MLP and the viewfeature is concatenated with the latent code. Thus, the texture decoder can be conditioned on this view information and models view-dependent effects in texture space. We explore several different architectures to investigate their generalizing capacity on novel expressions and camera views. Color Correction. Since different cameras could have different color space, we optimize color correction parameters for each camera. Color correction is performed on the output texture by scaling and adding a bias to each RGB chan-nel. The scaling factors and biases are initialized to 1 and 0, respectively. We fix the color correction parameters of one camera as an anchor and train the other parameters as a part of the model. Applying color correction is necessary, otherwise the reconstruction error will be dominated by the global color difference instead of exact colors of the pixels. Spatial Bias. For convolutional layers in the decoder for upsampling, instead of adding the same bias value per channel in the feature map, we add a bias tensor that has the same shape as the feature map, meaning that each spatial location has its own bias value. In this way, the model is able to capture more position-specific details in the texture, such as wrinkles and lips. Warp Field. We can also decode a warp field from the latent space and bilinearly sample the output texture with the warp field. Conceptually, texture generation can be decomposed into two steps: a synthesized texture on a deformation-free template followed by a deformation field that introduces shape variability. Denote T (p) as the value of the synthesized texture at coordinate p = (x, y). Denote W (p) as the estimated deformation field at location p. Then, the observed image, I(p), can be reconstructed as follows: I(p) = T (W (p)), namely the image appearance at position p is obtained by looking up the synthesized appearance at position W (p). We obtain a warp field in the same way as deformable autoencoders <ref type="bibr" target="#b18">[19]</ref>: by integrating both vertically and horizontally on the generated warping grid to avoid flipping of relative pixel positions. Residual Connection. We insert residual layers <ref type="bibr" target="#b4">[5]</ref> into our network to make it deeper. We investigate whether this increase in model capacity would make it generalize better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Pipeline</head><p>In <ref type="bibr" target="#b7">[8]</ref>, the model was trained with an ? 2 -loss on predicted mesh vertices and textures. Here, we present a set of experiments that diverge from this training strategy. We optimize the screen space loss of the predicted avatars di-rectly against the ground truth images. To propagate gradients from screen space to the predicted textures and geometries, we use Nvdiffrast <ref type="bibr" target="#b6">[7]</ref> as the differentiable rendering engine.</p><p>More formally, given a ground truth image I(v) captured from a camera at viewpoint v, the loss can be computed by rendering the predicted textureT and the predicted geom-etry? from viewpoint v using the differentiable rendering function R, and comparing it to the respective ground truth image I(v),</p><formula xml:id="formula_0">L = ?R(v,T ,?) ? I(v)? 2 .<label>(1)</label></formula><p>However, naively computing an ? 2 -loss from the 3D rendered avatar in screen space and the ground truth images is not reasonable: ground truth images include background that must not be learned by the model. Moreover, humans are more sensitive to changes in around the eyes and mouth.</p><p>To mitigate these two issues, we apply a foreground mask F calculated from non-background pixels in image space and assign a higher weight to eyes and mouth regions during training than to the remaining face regions using a manually created texture weight mask M . Note that this mask is defined in texture space but we aim to compute the loss in screen space. We therefore render not only the predicted geometry? and the predicted textureT into screen space, but also the weight mask. The resulting loss then is defined as</p><formula xml:id="formula_1">L = ?R(v, M,?) ? R(v,T ,?) ? F ? I(v) ? F ? 2 ,<label>(2)</label></formula><p>where ? denotes element-wise multiplication. We add an explicit loss between predicted and (tracked) ground truth geometry to enhance learning the right geometric shape of the face during training as well as the KL loss for the VAE. The combined loss term is</p><formula xml:id="formula_2">L =? 1 ? ?R(v, M,?) ? R(v,T ,?) ? F ? I(v) ? F ? 2 +? 2 ? ?? ? G? 2 +? 3 ? KL(N (? z , ? z )||N (0, I)),<label>(3)</label></formula><p>with ? z and ? z being the predicted mean and variance of the latent distribution. Here the images and textures are normalized by per-pixel mean and variance during training. For faster convergence and unequal learning rate, we multiply the output mean of the encoder by 0.1 and the log standard deviation by 0.01. We use Adam <ref type="bibr" target="#b5">[6]</ref> as optimizer and perform 200K iterations for all the experiments. We set ? 1 = ? 2 = 1 and ? 3 = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the following, we evaluate the changes to the original model from <ref type="bibr" target="#b7">[8]</ref> that have been suggested in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setting</head><p>The ablation study is performed on a single identity, m-20180227-0000-6795937-GHS, and with varying training and test camera views. To evaluate the effect of the number of camera views available during training, we run each experiment with four different settings: <ref type="formula" target="#formula_0">(1)</ref>  Evaluation. We conduct ablation studies for four different model architectures: (1) the model from <ref type="bibr" target="#b7">[8]</ref> without the spatial bias, (2) the original model from <ref type="bibr" target="#b7">[8]</ref> including spatial bias, (3) the model plus spatial bias and warp field, and (4) the model with spatial bias and residual connections, as described in Section 4.1.</p><p>Building photorealistic 3D facial avatars requires models to accurately produce novel views on the avatar (novel view synthesis), and to generalize well to facial expressions that are unseen in the training set (novel expression synthesis). For novel view synthesis, we evaluate the reconstruction error on held out cameras (red dots in <ref type="figure">Figure 6</ref>) and expressions seen during training, and for novel expression synthesis we evaluate the reconstruction error on camera views used during training (blue dots in <ref type="figure">Figure 6</ref>) but with held out facial expressions. Additionally, we examine both properties jointly by evaluating on held out cameras and held out expressions.</p><p>To achieve a fair comparison, we train color correction parameters for unseen testing cameras for two epochs on the validation data before evaluation. We also fine-tune the encoder on the testing expression as we would like to test the capacity of the conditional decoder independent of the encoder. Hence, the encoder needs to be able to generate the latent code that is specialized to the given dataset. We therefore train the encoder on the validation data while freezing the decoder weights on given cameras for 10 epochs before evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Analysis</head><p>Novel View Synthesis. We first evaluate the model with respect to its ability to synthesize novel views. In <ref type="figure">Figure 7a</ref>, we plot the screen error measured on held out cameras over the number of available training cameras. The expressions we evaluate on are seen during training but the testing cameras are at unseen positions. Not surprisingly, the higher the number of available training cameras, the lower the reconstruction error. Moreover, spatial biases in the network are crucial for high accuracy in novel view synthesis as they en- <ref type="figure">Figure 6</ref>: Sets of training and testing camera splits used in ablation study. Red dots represents the position of testing cameras and blue dots represents the position of training cameras. Note that for better visibility of the camera sets, we rendered the plots such that the z-axis is the one pointing away from the identity's face. <ref type="bibr" target="#b16">17</ref> 23  <ref type="figure">Figure 7</ref>: Reconstruction errors for four architectures trained on 17, 23, 27, and 37 camera views. We compare system performance for novel view synthesis, novel expression synthesis, and joint synthesis of novel expressions in novel views. The reconstruction errors refer to pixel intensity difference.</p><p>code important view-independent texture information, compare the baseline without spatial bias (red bar) to all other architectures that have spatial biases. We find an increased number of layers achieved through the use of residual connections particularly important for a lower number of training cameras (green bar). Novel Expression Synthesis. In a next step, we evaluate the model's ability to generate unseen facial expressions. We therefore evaluate on seen views (i.e., training cameras) only, see <ref type="figure">Figure 7b</ref>. As before, a lack of spatial biases leads to a significant degradation of reconstruction quality compared to other architectures that use spatial biases. While the baseline model plus spatial bias performs best when dense training views are available, it quickly dete-riorates for fewer available training cameras. The deeper model with residual connections shows the most consistent performance, being at a rather low reconstruction error independent of the number of available training cameras. Joint View-and Expression Synthesis. Last, we combine the two previous settings and evaluate the performance of our four architectures on unseen facial expressions and novel camera views simultaneously, see <ref type="figure">Figure 7c</ref>. As before, and again of no surprise, the larger the number of available training camera views, the smaller the reconstruction error. Note, however, that each individual task alone is easier to solve than the joint task: the reconstruction errors for novel view synthesis and novel expression synthesis alone are significantly lower than the reconstruction errors for the Qualitative Results. <ref type="figure" target="#fig_6">Figure 8</ref> shows predicted frames after training of a warp-field model. The top row shows groundtruth and the bottom shows rendered image using predicted mesh and texture. Although the reconstruction generally looks good, the model struggles to predict high frequency details such as teeth and eye-lashes, and those regions tend to be blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We release a large-scale multi-view codec-avatar dataset for neural face rendering, along with training, evaluation, and visualization code and pretrained models for 13 codec avatars. Besides the dataset, to understand how different model architectures respond to interpolating on unseen viewpoint and expression, we conduct an ablation study with a VAE as our baseline and identify that the baseline model benefits from adding spatial bias, texture warp field and residual connections. We hope that this dataset will serve to push the limit of facial reconstruction further and facilitate the community in future research of VR telepresence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Mugsy v1 (left) and Mugsy v2 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The 65 peak expressions used in the v1 script.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>50 phonetically balanced sentencesFigure 4 :</head><label>4</label><figDesc>Snapshot of intermediate results from different parts of the pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>? frame list: a list of all frames captured by Mugsy, each line consists of segment name and frame index. ? texture mean: the mean of the textures across all frames and all cameras. ? texture variance: the variance of the textures across all frames and all cameras. ? vertex mean: the mean of the vertices of the meshes across all frames and all cameras. ? vertex variance: the variance of the vertices of the meshes across all frames and all cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The model consumes, as input, a tracked mesh and an average texture over all camera views and maps these inputs to a latent code, from which a view-dependent texture and the tracked mesh can be decoded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>training on 37 cameras, (2) training on 27 cameras, (3) training on 23 cameras, and (4) training on 17 cameras. The locations of the train cameras for these four settings are illustrated in Figure 6. In total, training takes around one day for each avatar using a P3.16x instance with eight Nvidia V100 GPUs on AWS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Reconstruction result of the warp-field model using 37 camera views during training. Top row is groundtruth, bottom row is the 3D reconstruction generated by the model. joint task of novel view-and expression synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of multi-view datasets.</figDesc><table><row><cell>Dataset</cell><cell>Camera Resolution</cell><cell cols="2"># Expressions # View</cell></row><row><cell>FaceWarehouse</cell><cell>640x480</cell><cell>20</cell><cell>1</cell></row><row><cell>HUMBI</cell><cell>1920x1080</cell><cell>20</cell><cell>32</cell></row><row><cell>Mugsy v1</cell><cell>2048x1334</cell><cell>65</cell><cell>40</cell></row><row><cell>Mugsy v2</cell><cell>2048x1334</cell><cell>118</cell><cell>150</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Specifications for each generation of Mugsy.</figDesc><table><row><cell>Mugsy</cell><cell cols="2"># Cameras Camera resolution</cell><cell>Camera lens</cell><cell cols="2">FPS # Lights</cell></row><row><cell>v1</cell><cell>40 color + 50 monochrome</cell><cell cols="3">4096x2668 35mm * 2, 85mm * 12, Remaining are 50 mm 30/90</cell><cell>350</cell></row><row><cell>v2</cell><cell>160 color</cell><cell>4096x2668</cell><cell cols="2">All 35 mm 30/90</cell><cell>450</cell></row></table><note>Figure 2: Left: camera views for Mugsy v1. Right: camera views for Mugsy v2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between v1 and v2 capture scripts.</figDesc><table><row><cell>Script</cell><cell>Expressions</cell><cell>Gaze</cell><cell>Sentences</cell></row><row><cell>v1</cell><cell>65 peak expressions as shown in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Mugsy and script version, # cameras, and # frames for each released capture.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/multiface</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Expressive telepresence via modular codec avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="330" to="345" />
		</imprint>
	</monogr>
	<note>Sanja Fidler, and Yaser Sheikh</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deltille grids for geometric camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyowon</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
	<note>Hatem Alismail, In So Kweon, and Yaser Sheikh</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modular primitives for high-performance differentiable rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongho</forename><surname>Seol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>2020. 7</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep appearance models for face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Andreas Lehrmann, and Yaser Sheikh. Neural volumes. ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixture of volumetric primitives for efficient neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pixel codec avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuecheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno>abs/2003.08934, 2020. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Strand-accurate multi-view hair capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giljoo</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<idno>abs/2011.12948, 2020. 1</idno>
		<title level="m">Deformable neural radiance fields. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enric</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer. D-Nerf</surname></persName>
		</author>
		<title level="m">Neural radiance fields for dynamic scenes. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10313" to="10322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audio-and gaze-driven facial animation of codec avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter conference on applications of computer vision</title>
		<meeting>the IEEE/CVF winter conference on applications of computer vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meshtalk: 3d face animation from speech using cross-modality disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The eyes have it: An integrated eye and face model for photorealistic facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Te-Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="91" to="92" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deforming autoencoders: Unsupervised disentangling of shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Sahasrabudhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Guler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dimitris Samaras, Nikos Paragios, and Iasonas Kokkinos</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constraining dense hand surface tracking with elasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breannan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Peluse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Human hair inverse rendering using multi-view photometric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giljoo</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Aliaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Hery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning compositional radiance fields of dynamic human heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5704" to="5713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Humbi: A large multiview dataset of human body expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Kyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
