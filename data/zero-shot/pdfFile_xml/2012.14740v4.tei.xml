<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinei</forename><surname>Florencio</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
							<email>chazhang@microsoft.com4minzhang@suda.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
							<email>lidongz@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-ofthe-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 ? 0.8420), CORD (0.9493 ? 0.9601), SROIE (0.9524 ? 0.9781), Kleister-NDA (0.8340 ? 0.8520), RVL-CDIP (0.9443 ? 0.9564), and DocVQA (0.7295 ? 0.8672). We made our model and code publicly available at https://aka.ms /layoutlmv2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visually-rich Document Understanding (VrDU) aims to analyze scanned/digital-born business documents (images of invoices, forms in PDF format, etc.) where structured information can be automatically extracted and organized for many business * Equal contributions during internship at MSRA applications. Distinct from conventional information extraction tasks, the VrDU task relies on not only textual information but also visual and layout information that is vital for visually-rich documents. Different types of documents indicate that the text fields of interest located at different positions within the document, which is often determined by the style and format of each type as well as the document content. Therefore, to accurately recognize the text fields of interest, it is inevitable to take advantage of the cross-modality nature of visually-rich documents, where the textual, visual, and layout information should be jointly modeled and learned end-to-end in a single framework.</p><p>The recent progress of VrDU lies primarily in two directions. The first direction is usually built on the shallow fusion between textual and visual/layout/style information <ref type="bibr" target="#b17">(Yang et al., 2017;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr">Sarkhel and Nandi, 2019;</ref><ref type="bibr">Majumder et al., 2020;</ref><ref type="bibr" target="#b19">Zhang et al., 2020)</ref>. These approaches leverage the pre-trained NLP and CV models individually and combine the information from multiple modalities for supervised learning. Although good performance has been achieved, the domain knowledge of one document type cannot be easily transferred into another, so that these models often need to be re-trained once the document type is changed. Thereby the local invariance in general document layout (key-value pairs in a left-right layout, tables in a grid layout, etc.) cannot be fully exploited. To this end, the second direction relies on the deep fusion among textual, visual, and layout information from a great number of unlabeled documents in different domains, where pre-training techniques play an important role in learning the cross-modality interaction in an end-to-end fashion <ref type="bibr">(Lockard et al., 2020;</ref>. In this way, the pre-trained models absorb cross-modal knowledge from different document types, where the local invariance among these layouts and styles is preserved. Furthermore, when the model needs to be transferred into another domain with different document formats, only a few labeled samples would be sufficient to fine-tune the generic model in order to achieve state-of-the-art accuracy. Therefore, the proposed model in this paper follows the second direction, and we explore how to further improve the pre-training strategies for the VrDU tasks.</p><p>In this paper, we present an improved version of LayoutLM , aka LayoutLMv2. Different from the vanilla LayoutLM model where visual embeddings are combined in the fine-tuning stage, we integrate the visual information in the pre-training stage in LayoutLMv2 by taking advantage of the Transformer architecture to learn the cross-modality interaction between visual and textual information. In addition, inspired by the 1-D relative position representations <ref type="bibr">(Shaw et al., 2018;</ref><ref type="bibr">Raffel et al., 2020;</ref><ref type="bibr" target="#b1">Bao et al., 2020)</ref>, we propose the spatial-aware self-attention mechanism for LayoutLMv2, which involves a 2-D relative position representation for token pairs. Different from the absolute 2-D position embeddings that Lay-outLM uses to model the page layout, the relative position embeddings explicitly provide a broader view for the contextual spatial modeling. For the pre-training strategies, we use two new training objectives for LayoutLMv2 in addition to the masked visual-language modeling. The first is the proposed text-image alignment strategy, which aligns the text lines and the corresponding image regions. The second is the text-image matching strategy popular in previous vision-language pre-training models <ref type="bibr">(Tan and Bansal, 2019;</ref><ref type="bibr" target="#b9">Lu et al., 2019;</ref><ref type="bibr">Su et al., 2020;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr">Sun et al., 2019)</ref>, where the model learns whether the document image and textual content are correlated.</p><p>We select six publicly available benchmark datasets as the downstream tasks to evaluate the performance of the pre-trained LayoutLMv2 model, which are the FUNSD dataset <ref type="bibr" target="#b11">(Jaume et al., 2019)</ref> for form understanding, the CORD dataset <ref type="bibr">(Park et al., 2019)</ref> and the SROIE dataset <ref type="bibr" target="#b9">(Huang et al., 2019)</ref> for receipt understanding, the Kleister-NDA dataset <ref type="bibr">(Grali?ski et al., 2020)</ref> for long document understanding with a complex layout, the RVL-CDIP dataset <ref type="bibr" target="#b6">(Harley et al., 2015)</ref> for document image classification, and the DocVQA dataset <ref type="bibr">(Mathew et al., 2021)</ref> for visual question answering on document images. Experiment results show that the LayoutLMv2 model significantly outperforms strong baselines, including the vanilla LayoutLM, and achieves new state-of-the-art results in all of these tasks.</p><p>The contributions of this paper are summarized as follows:</p><p>? We propose a multi-modal Transformer model to integrate the document text, layout, and visual information in the pre-training stage, which learns the cross-modal interaction endto-end in a single framework. Meanwhile, a spatial-aware self-attention mechanism is integrated into the Transformer architecture.</p><p>? In addition to the masked visual-language model, we add text-image alignment and textimage matching as the new pre-training strategies to enforce the alignment among different modalities.</p><p>? LayoutLMv2 significantly outperforms and achieves new SOTA results not only on the conventional VrDU tasks but also on the VQA task for document images, which demonstrates the great potential for the multi-modal pre-training for VrDU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we will introduce the model architecture and the multi-modal pre-training tasks of LayoutLMv2, which is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>We build a multi-modal Transformer architecture as the backbone of LayoutLMv2, which takes text, visual, and layout information as input to establish deep cross-modal interactions. We also introduce a spatial-aware self-attention mechanism to the model architecture for better modeling the document layout. Detailed descriptions of the model are as follows.</p><p>Text Embedding Following the common practice, we use WordPiece <ref type="bibr" target="#b14">(Wu et al., 2016)</ref>   <ref type="figure">Figure 1</ref>: An illustration of the model architecture and pre-training strategies for LayoutLMv2 final sequence's length is exactly the maximum sequence length L. The final text embedding is the sum of three embeddings. Token embedding represents the token itself, 1D positional embedding represents the token index, and segment embedding is used to distinguish different text segments. Formally, we have the i-th (0 ? i &lt; L) text embedding</p><formula xml:id="formula_0">A C C C C A A A A A A A A</formula><formula xml:id="formula_1">t i = TokEmb(w i )+PosEmb1D(i)+SegEmb(s i )</formula><p>Visual Embedding Although all information we need is contained in the page image, the model has difficulty capturing detailed features in a single information-rich representation of the entire page. Therefore, we leverage the output feature map of a CNN-based visual encoder, which converts the page image to a fixed-length sequence. We use ResNeXt-FPN <ref type="bibr" target="#b15">(Xie et al., 2017;</ref><ref type="bibr">Lin et al., 2017)</ref> architecture as the backbone of the visual encoder, whose parameters can be updated through backpropagation.</p><p>Given a document page image I, it is resized to 224 ? 224 then fed into the visual backbone. After that, the output feature map is average-pooled to a fixed size with the width being W and height being H. Next, it is flattened into a visual embedding sequence of length W ?H. The sequence is named VisTokEmb(I). A linear projection layer is then applied to each visual token embedding to unify the dimensionality with the text embeddings. Since the CNN-based visual backbone cannot capture the positional information, we also add a 1D positional embedding to these visual token embeddings. The 1D positional embedding is shared with the text embedding layer. For the segment embedding, we attach all visual tokens to the visual segment <ref type="bibr">[C]</ref>.</p><formula xml:id="formula_2">The i-th (0 ? i &lt; W H) visual embedding can be represented as v i = Proj VisTokEmb(I) i +PosEmb1D(i)+SegEmb([C])</formula><p>Layout Embedding The layout embedding layer is for embedding the spatial layout information represented by axis-aligned token bounding boxes from the OCR results, in which box width and height together with corner coordinates are identified. Following the vanilla LayoutLM, we normalize and discretize all coordinates to integers in the range [0, 1000], and use two embedding layers to embed x-axis features and y-axis features separately. Given the normalized bounding box of the i-th (0 ? i &lt; W H + L) text/visual token box i = (x min , x max , y min , y max , width, height), the layout embedding layer concatenates six bounding box features to construct a token-level 2D positional embedding, aka the layout embedding</p><formula xml:id="formula_3">l i = Concat PosEmb2D x (x min , x max , width),</formula><p>PosEmb2D y (y min , y max , height)</p><p>Note that CNNs perform local transformation, thus the visual token embeddings can be mapped back to image regions one by one with neither overlap nor omission. When calculating bounding boxes, the visual tokens can be treated as evenly divided grids. An empty bounding box box PAD = (0, 0, 0, 0, 0, 0) is attached to special tokens [CLS], [SEP] and [PAD].</p><p>Multi-modal Encoder with Spatial-Aware Self-Attention Mechanism The encoder concatenates visual embeddings {v 0 , ..., v W H?1 } and text embeddings {t 0 , ..., t L?1 } to a unified sequence and fuses spatial information by adding the layout embeddings to get the i-th (0 ? i &lt; W H + L) first layer input</p><formula xml:id="formula_4">x (0) i = X i + l i , where X = {v 0 , ..., v W H?1 , t 0 , ..., t L?1 }</formula><p>Following the architecture of Transformer, we build our multi-modal encoder with a stack of multi-head self-attention layers followed by a feedforward network. However, the original selfattention mechanism can only implicitly capture the relationship between the input tokens with the absolute position hints. In order to efficiently model local invariance in the document layout, it is necessary to insert relative position information explicitly. Therefore, we introduce the spatial-aware self-attention mechanism into the self-attention layers. For simplicity, the following description is for a single head in a single self-attention layer with hidden size of d head and projection matrics W Q , W K , W V . The original self-attention mechanism captures the correlation between query x i and key x j by projecting the two vectors and calculating the attention score</p><formula xml:id="formula_5">? ij = 1 ? d head x i W Q x j W K T</formula><p>Considering the large range of positions, we model the semantic relative position and spatial relative position as bias terms to prevent adding too many parameters. Similar practice has been shown effective on text-only Transformer architectures <ref type="bibr">(Raffel et al., 2020;</ref><ref type="bibr" target="#b1">Bao et al., 2020)</ref>. Let b (1D) , b (2Dx) and b (2Dy) denote the learnable 1D and 2D relative position biases respectively. The biases are different among attention heads but shared in all encoder layers. Assuming (x i , y i ) anchors the top left corner coordinates of the i-th bounding box, we obtain the spatial-aware attention score</p><formula xml:id="formula_6">? ij = ? ij + b (1D) j?i + b (2Dx) x j ?x i + b (2Dy) y j ?y i</formula><p>Finally, the output vectors are represented as the weighted average of all the projected value vectors with respect to normalized spatial-aware attention scores</p><formula xml:id="formula_7">h i = j exp ? ij k exp ? ik x j W V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-training Tasks</head><p>Masked Visual-Language Modeling Similar to the vanilla LayoutLM, we use the Masked Visual-Language Modeling (MVLM) to make the model learn better in the language side with the crossmodality clues. We randomly mask some text tokens and ask the model to recover the masked tokens. Meanwhile, the layout information remains unchanged, which means the model knows each masked token's location on the page. The output representations of masked tokens from the encoder are fed into a classifier over the whole vocabulary, driven by a cross-entropy loss. To avoid visual clue leakage, we mask image regions corresponding to masked tokens on the raw page image input before feeding it into the visual encoder.</p><p>Text-Image Alignment To help the model learn the spatial location correspondence between image and coordinates of bounding boxes, we propose the Text-Image Alignment (TIA) as a fine-grained cross-modality alignment task. In the TIA task, some tokens lines are randomly selected, and their image regions are covered on the document image. We call this operation covering to avoid confusion with the masking operation in MVLM. During pretraining, a classification layer is built above the encoder outputs. This layer predicts a label for each text token depending on whether it is covered, i.e., <ref type="bibr">[Covered]</ref> or [Not Covered], and computes the binary cross-entropy loss. Considering the input image's resolution is limited, and some document elements like signs and bars in a figure may look like covered text regions, the task of finding a word-sized covered image region can be noisy. Thus, the covering operation is performed at the line-level. When MVLM and TIA are performed simultaneously, TIA losses of the tokens masked in MVLM are not taken into account. This prevents the model from learning the useless but straightforward correspondence from [MASK] to <ref type="bibr">[Covered]</ref>.</p><p>Text-Image Matching Furthermore, a coarsegrained cross-modality alignment task, Text-Image Matching (TIM) is applied to help the model learn the correspondence between document image and textual content. We feed the output representation at [CLS] into a classifier to predict whether the image and text are from the same document page. Regular inputs are positive samples. To construct a negative sample, an image is either replaced by a page image from another document or dropped. To prevent the model from cheating by finding task features, we perform the same masking and covering operations to images in negative samples. The TIA target labels are all set to <ref type="bibr">[Covered]</ref> in negative samples. We apply the binary cross-entropy loss in the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>In order to pre-train and evaluate LayoutLMv2 models, we select datasets in a wide range from the visually-rich document understanding area. Following LayoutLM, we use IIT-CDIP Test Collection <ref type="bibr">(Lewis et al., 2006)</ref> as the pre-training dataset. Six datasets are used as down-stream tasks. The FUNSD <ref type="bibr" target="#b11">(Jaume et al., 2019)</ref>, <ref type="bibr">CORD (Park et al., 2019)</ref>, SROIE <ref type="bibr" target="#b9">(Huang et al., 2019)</ref> and <ref type="bibr">Kleister-NDA (Grali?ski et al., 2020)</ref> datasets define entity extraction tasks that aim to extract the value of a set of pre-defined keys, which we formalize as a sequential labeling task. <ref type="bibr">RVL-CDIP (Harley et al., 2015)</ref> is for document image classification. DocVQA <ref type="bibr">(Mathew et al., 2021)</ref>, as the name suggests, is a dataset for visual question answering on document images. Statistics of datasets are shown in <ref type="table" target="#tab_2">Table 1</ref>. Refer to the Appendix for details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Settings</head><p>Following the typical pre-training and fine-tuning strategy, we update all parameters including the visual encoder layers, and train whole models endto-end for all the settings. Training details can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training LayoutLMv2</head><p>We train Lay-outLMv2 models with two different parameter sizes.</p><p>We use a 12-layer 12-head Transformer encoder and set hidden size d = 768 in LayoutLMv2 BASE . While in the LayoutLMv2 LARGE , the encoder has 24 Transformer layers with 16 heads and d = 1024. Visual backbones in the two models are based on the same ResNeXt101-FPN architecture. The numbers of parameters are 200M and 426M approximately for LayoutLMv2 BASE and LayoutLMv2 LARGE , respectively.</p><p>For the encoder along with the text embedding layer, LayoutLMv2 uses the same architecture as UniLMv2 <ref type="bibr" target="#b1">(Bao et al., 2020)</ref>, thus it is initialized from UniLMv2. For the ResNeXt-FPN part in the visual embedding layer, the backbone of a Mask-RCNN  model trained on Pub-LayNet <ref type="bibr" target="#b20">(Zhong et al., 2019)</ref> is leveraged. <ref type="bibr">1</ref> The rest of the parameters in the model are randomly initialized.</p><p>During pre-training, we sample pages from the IIT-CDIP dataset and select a random sliding window of the text sequence if the sample is too long. We set the maximum sequence length L = 512 and assign all text tokens to the segment [A]. The output shape of the average pooling layer is set to W = H = 7, so that it transforms the feature map into 49 visual tokens. In MVLM, 15% text tokens are masked among which 80% are replaced by a special token [MASK], 10% are replaced by a random token sampled from the whole vocabulary, and  10% remains the same. In TIA, 15% of the lines are covered. In TIM, 15% images are replaced, and 5% are dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning LayoutLMv2</head><p>We use the [CLS] output along with pooled visual token representations as global features in the document-level classification task RVL-CDIP. For the extractive question answering task DocVQA and the other four entity extraction tasks, we follow common practice like <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> and build task specified head layers over the text part of LayoutLMv2 outputs.</p><p>In the DocVQA paper, experiment results show that the BERT model fine-tuned on the SQuAD dataset <ref type="bibr">(Rajpurkar et al., 2016)</ref> outperforms the original BERT model. Inspired by this fact, we add an extra setting, which is that we first fine-tune Lay-outLMv2 on a question generation (QG) dataset followed by the DocVQA dataset. The QG dataset contains almost one million question-answer pairs generated by a generation model trained on the SQuAD dataset.</p><p>Baselines We select three baseline models in the experiments to compare LayoutLMv2 with the text-only pre-trained models as well as the vanilla LayoutLM model. Specifically, we compare LayoutLMv2 with BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, UniLMv2 <ref type="bibr" target="#b1">(Bao et al., 2020)</ref>, and LayoutLM  for all the experiment settings. We use the publicly available PyTorch models for BERT <ref type="bibr">(Wolf et al., 2020)</ref> and LayoutLM, and use our in-house implementation for the UniLMv2 models. For each baseline approach, experiments are conducted using both the BASE and LARGE parameter settings. <ref type="table" target="#tab_4">Table 2</ref> shows the model accuracy on the four datasets FUNSD, CORD, SROIE, and Kleister-NDA, which we regard as sequential labeling tasks evaluated using entity-level F1 score. We report the evaluation results of Kleister-NDA on the validation set because the ground-truth labels and the submission website for the test set are not available right now. For text-only models, the UniLMv2 models outperform the BERT models by a large margin in terms of the BASE and LARGE settings. For text+layout models, the LayoutLM family, especially the LayoutLMv2 models, brings significant performance improvement over the text-only baselines. Compared to the baselines, the LayoutLMv2 models are superior to the SPADE <ref type="bibr">(Hwang et al., 2020)</ref> decoder method, as well as the text+layout pre-training approach BROS <ref type="bibr" target="#b8">(Hong et al., 2021)</ref> that is built on the SPADE decoder, which demonstrates the effectiveness of our modeling approach. Moreover, with the same modal information, our LayoutLMv2 models also outperform existing multi-modal approaches PICK , TRIE <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> and the previous top-1 method on the leaderboard, 2 confirming the effectiveness of our pre-training for text, layout, and visual information. The best performance on all the four datasets is achieved by  <ref type="bibr" target="#b3">(Das et al., 2018)</ref> 91.11% Ensemble <ref type="bibr" target="#b3">(Das et al., 2018)</ref> 92.21% InceptionResNetV2 <ref type="bibr">(Szegedy et al., 2017)</ref> 92.63% LadderNet <ref type="bibr">(Sarkhel and Nandi, 2019)</ref> 92.77% Single model <ref type="bibr" target="#b4">(Dauphinee et al., 2019)</ref> 93.03% Ensemble <ref type="bibr" target="#b4">(Dauphinee et al., 2019)</ref> 93.07% <ref type="table">Table 3</ref>: Classification accuracy on the RVL-CDIP dataset the LayoutLMv2 LARGE , which illustrates that the multi-modal pre-training in LayoutLMv2 learns better from the interactions from different modalities, thereby leading to the new SOTA on various document understanding tasks. <ref type="table">Table 3</ref> shows the classification accuracy on the RVL-CDIP dataset, including textonly pre-trained models, the LayoutLM family as well as several image-based baseline models. As shown in the table, both the text and visual information are important to the document image classification task because document images are text-intensive and represented by a variety of layouts and formats. Therefore, we observed that the LayoutLM family outperforms those text-only or image-only models as it leverages the multi-modal information within the documents. Specifically, the LayoutLMv2 LARGE model significantly improves the classification accuracy by more than 1.2% point over the previous SOTA results, which achieves an accuracy of 95.64%. This also verifies that the pre-trained LayoutLMv2 model benefits not only the information extraction tasks in document understanding but also the document image classification task through effective multi-model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Extraction Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RVL-CDIP</head><p>DocVQA  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies</head><p>To fully understand the underlying impact of different components, we conduct an ablation study to explore the effect of visual information, the pretraining tasks, spatial-aware self-attention mechanism, as well as different text-side initialization models. <ref type="table">Table 5</ref> shows model performance on the DocVQA validation set. Under all the settings, we pre-train the models using all IIT-CDIP data for one epoch. The hyper-parameters are the same as those used to pre-train LayoutLMv2 BASE in Section 3.2. "LayoutLM" denotes the vanilla LayoutLM architecture in , which can be regarded as a LayoutLMv2 architecture without visual module and spatial-aware self-attention mechanism. "X101-FPN" denotes the ResNeXt101-FPN visual backbone described in Section 3.2. We first evaluate the effect of introducing visual information. From #1 to #2a, we add the visual module without changing the pre-training strategy, where results show that LayoutLMv2 pre-  <ref type="table">Table 5</ref>: Ablation study on the DocVQA dataset, where ANLS scores on the validation set are reported. "SASAM" means the spatial-aware self-attention mechanism. "MVLM", "TIA" and "TIM" are the three pre-training tasks. All the models are trained using the whole pre-training dataset for one epoch with the BASE model size.</p><p>trained with only MVLM can leverage visual information effectively. Then, we compare the two cross-modality alignment pre-training tasks TIA and TIM. According to the four results in #2, both tasks improve the model performance substantially, and the proposed TIA benefits the model more than the commonly used TIM. Using both tasks together is more effective than using either one alone. According to this observation, we keep all the three pre-training tasks and introduce the spatial-aware self-attention mechanism (SASAM) to the model architecture. Compare the results #2d and #3, the proposed SASAM can further improve the model accuracy. Finally, in settings #3 and #4, we change the text-side initialization checkpoint from BERT to UniLMv2, and confirm that LayoutLMv2 benefits from the better initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In recent years, pre-training techniques have become popular in both NLP and CV areas, and have also been leveraged in the VrDU tasks. <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> introduced a new language representation model called BERT, which is designed to pre-train deep bidirectional representations from the unlabeled text by jointly conditioning on both left and right context in all layers. <ref type="bibr" target="#b1">Bao et al. (2020)</ref> propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model. Our multi-modal Transformer architecture and the MVLM pre-training strategy extend Transformer and MLM used in these work to leverage visual information. <ref type="bibr" target="#b9">Lu et al. (2019)</ref> proposed ViLBERT for learning task-agnostic joint representations of image content and natural language by extending the popular BERT architecture to a multi-modal two-stream model. <ref type="bibr">Su et al. (2020)</ref> proposed VL-BERT that adopts the Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. Different from these vision-language pre-training approaches, the visual part of LayoutLMv2 directly uses the feature map instead of pooled ROI features, and benefits from the new TIA pre-training task.  proposed LayoutLM to jointly model interactions between text and layout information across scanned document images, benefiting a great number of real-world document image understanding tasks such as information extraction from scanned documents. This work is a natural extension of the vanilla LayoutLM, which takes advantage of textual, layout, and visual information in a single multi-modal pre-training framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a multi-modal pre-training approach for visually-rich document understanding tasks, aka LayoutLMv2. Distinct from existing methods for VrDU, the LayoutLMv2 model not only considers the text and layout information but also integrates the image information in the pretraining stage with a single multi-modal framework. Meanwhile, the spatial-aware self-attention mechanism is integrated into the Transformer architecture to capture the relative relationship among different bounding boxes. Furthermore, new pre-training objectives are also leveraged to enforce the learning of cross-modal interaction among different modalities. Experiment results on 6 different VrDU tasks have illustrated that the pre-trained LayoutLMv2 model has substantially outperformed the SOTA baselines in the document intelligence area, which greatly benefits a number of real-world document understanding tasks.</p><p>For future research, we will further explore the network architecture as well as the pre-training strategies for the LayoutLM family. Meanwhile, we will also investigate the language expansion to make the multi-lingual LayoutLMv2 model available for different languages, especially the non-English areas around the world. evaluation tools. 5 Words and bounding boxes are extracted from the raw PDF file. We use heuristics to locate entity spans because the normalized standard answers may not appear in the utterance. As the labeled answers are normalized into a canonical form, we apply post-processing heuristics to convert the extracted date information into the "YYYY-MM-DD" format, and company names into the abbreviations such as "LLC" and "Inc.". Fine-tuning for Document Image Classification This task depends on high-level visual information, thereby we leverage the image features explicitly in the fine-tuning stage. We pool the visual embeddings into a global pre-encoder feature, and pool the visual part of LayoutLMv2 output representations into a global post-encoder feature. The pre and post-encoder features along with the [CLS] output feature are concatenated and fed into the final classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RVL</head><p>Fine-tuning for Sequential Labeling We formalize FUNSD, SROIE, CORD, and Kleister-NDA as the sequential labeling tasks. To fine-tune Lay-outLMv2 models on these tasks, we build a tokenlevel classification layer above the text part of the output representations to predict the BIO tags for each entity field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Experiment Results</head><p>Tables list per-task detailed results for the four entity extraction tasks, with <ref type="table" target="#tab_12">Table 6</ref> for FUNSD, <ref type="table" target="#tab_13">Table 7</ref> for CORD, <ref type="table" target="#tab_14">Table 8</ref> for SROIE, and <ref type="table">Table 9</ref> for Kleister-NDA.     <ref type="table">Table 9</ref>: Model accuracy (entity-level F1) on the validation set of the Kleister-NDA dataset using the official evaluation toolkit</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Entity-level F1 scores of the four entity extraction tasks: FUNSD, CORD, SROIE and Kleister-NDA. Detailed per-task results are in the Appendix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>lists the Average Normalized</cell></row><row><cell>Levenshtein Similarity (ANLS) scores on the</cell></row><row><cell>DocVQA dataset of text-only baselines, LayoutLM</cell></row><row><cell>family models, and the previous top-1 on the</cell></row><row><cell>leaderboard. With multi-modal pre-training, Lay-</cell></row><row><cell>outLMv2 models outperform LayoutLM models</cell></row><row><cell>and text-only baselines by a large margin when fine-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: ANLS score on the DocVQA dataset, "QG"</cell></row><row><cell>denotes the data augmentation with the question gener-</cell></row><row><cell>ation dataset.</cell></row><row><cell>tuned on the train set. By using all data (train + dev)</cell></row><row><cell>as the fine-tuning dataset, the LayoutLMv2 LARGE</cell></row><row><cell>single model outperforms the previous top-1 on the</cell></row><row><cell>leaderboard which ensembles 30 models. 3 Under</cell></row><row><cell>the setting of fine-tuning LayoutLMv2 LARGE on a</cell></row><row><cell>question generation dataset (QG) and the DocVQA</cell></row><row><cell>dataset successively, the single model performance</cell></row><row><cell>increases by more than 1.6% ANLS and achieves</cell></row><row><cell>the new SOTA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>? 10 ?5 , weight decay of 1 ? 10 ?2 ,and (? 1 , ? 2 ) = (0.9, 0.999). The learning rate is linearly warmed up over the first 10% steps then linearly decayed. LayoutLMv2 BASE is trained with a batch size of 64 for 5 epochs, and LayoutLMv2 LARGE is trained with a batch size of 2048 for 20 epochs on the IIT-CDIP dataset. https://gitlab.com/filipg/geval Fine-tuning for Visual Question Answering We treat the DocVQA as an extractive QA task and build a token-level classifier on top of the text part of LayoutLMv2 output representations. Question tokens, context tokens and visual tokens are assigned to segment [A],[B]  and [C], respectively. The maximum sequence length is set to L = 384.</figDesc><table><row><cell>-CDIP RVL-CDIP (Harley et al., 2015) con-</cell></row><row><cell>sists of 400,000 grayscale images, with 8:1:1 for</cell></row><row><cell>the training set, validation set, and test set. A multi-</cell></row><row><cell>class single-label classification task is defined on</cell></row><row><cell>RVL-CDIP. The images are categorized into 16</cell></row><row><cell>classes, with 25,000 images per class. The evalu-</cell></row><row><cell>ation metric is the overall classification accuracy.</cell></row><row><cell>Text and layout information is extracted by Mi-</cell></row><row><cell>crosoft OCR.</cell></row><row><cell>DocVQA As a VQA dataset on the document un-</cell></row><row><cell>derstanding field, DocVQA (Mathew et al., 2021)</cell></row><row><cell>consists of 50,000 questions defined on over 12,000</cell></row><row><cell>pages from a variety of documents. Pages are split</cell></row><row><cell>into the training set, validation set, and test set with</cell></row><row><cell>a ratio of about 8:1:1. The dataset is organized as</cell></row><row><cell>a set of triples page image, questions, answers .</cell></row><row><cell>Thus, we use Microsoft Read API to extract text</cell></row><row><cell>and bounding boxes from images. Heuristics are</cell></row><row><cell>used to find given answers in the extracted text.</cell></row><row><cell>The task is evaluated using an edit distance based</cell></row><row><cell>metric ANLS (aka average normalized Levenshtein</cell></row><row><cell>similarity). Given that human performance is about</cell></row><row><cell>98% ANLS on the test set, it is reasonable to as-</cell></row><row><cell>sume that the found ground truth which reaches</cell></row><row><cell>over 97% ANLS on training and validation sets is</cell></row><row><cell>good enough to train a model. Results on the test</cell></row><row><cell>set are provided by the official evaluation site.</cell></row><row><cell>B Model Training Details</cell></row><row><cell>Pre-training We pre-train LayoutLMv2 models</cell></row><row><cell>using Adam optimizer (Kingma and Ba, 2015;</cell></row><row><cell>Loshchilov and Hutter, 2019), with the learn-</cell></row><row><cell>ing rate of 2</cell></row></table><note>5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Model accuracy (entity-level Precision, Recall, F1) on the FUNSD dataset</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>BERTBASE</cell><cell>0.8833</cell><cell cols="2">0.9107 0.8968</cell></row><row><cell>UniLMv2BASE</cell><cell>0.8987</cell><cell cols="2">0.9198 0.9092</cell></row><row><cell>BERTLARGE</cell><cell>0.8886</cell><cell cols="2">0.9168 0.9025</cell></row><row><cell>UniLMv2LARGE</cell><cell>0.9123</cell><cell cols="2">0.9289 0.9205</cell></row><row><cell>LayoutLM BASE</cell><cell>0.9437</cell><cell cols="2">0.9508 0.9472</cell></row><row><cell>LayoutLM LARGE</cell><cell>0.9432</cell><cell cols="2">0.9554 0.9493</cell></row><row><cell>LayoutLMv2 BASE</cell><cell>0.9453</cell><cell cols="2">0.9539 0.9495</cell></row><row><cell>LayoutLMv2 LARGE</cell><cell>0.9565</cell><cell cols="2">0.9637 0.9601</cell></row><row><cell>SPADE (Hwang et al., 2020)</cell><cell>-</cell><cell>-</cell><cell>0.9150</cell></row><row><cell>BROS (Hong et al., 2021)</cell><cell>0.9558</cell><cell cols="2">0.9514 0.9536</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Model accuracy (entity-level Precision, Recall, F1) on the CORD dataset</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>BERTBASE</cell><cell>0.9099</cell><cell cols="2">0.9099 0.9099</cell></row><row><cell>UniLMv2BASE</cell><cell>0.9459</cell><cell cols="2">0.9459 0.9459</cell></row><row><cell>BERTLARGE</cell><cell>0.9200</cell><cell cols="2">0.9200 0.9200</cell></row><row><cell>UniLMv2LARGE</cell><cell>0.9488</cell><cell cols="2">0.9488 0.9488</cell></row><row><cell>LayoutLM BASE</cell><cell>0.9438</cell><cell cols="2">0.9438 0.9438</cell></row><row><cell>LayoutLM LARGE</cell><cell>0.9524</cell><cell cols="2">0.9524 0.9524</cell></row><row><cell>LayoutLMv2 BASE</cell><cell>0.9625</cell><cell cols="2">0.9625 0.9625</cell></row><row><cell>LayoutLMv2 LARGE</cell><cell>0.9661</cell><cell cols="2">0.9661 0.9661</cell></row><row><cell>LayoutLMv2 LARGE (Excluding OCR mismatch)</cell><cell>0.9904</cell><cell cols="2">0.9661 0.9781</cell></row><row><cell>BROS (Hong et al., 2021)</cell><cell>0.9493</cell><cell cols="2">0.9603 0.9548</cell></row><row><cell>PICK (Yu et al., 2020)</cell><cell>0.9679</cell><cell cols="2">0.9546 0.9612</cell></row><row><cell>TRIE (Zhang et al., 2020)</cell><cell>-</cell><cell>-</cell><cell>0.9618</cell></row><row><cell>Top-1 on SROIE Leaderboard (Excluding OCR mismatch)</cell><cell>0.9889</cell><cell cols="2">0.9647 0.9767</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Model accuracy (entity-level Precision, Recall, F1) on the SROIE dataset(until 2020-12-24)    </figDesc><table><row><cell>Model</cell><cell>F1</cell></row><row><cell>BERTBASE</cell><cell>0.779</cell></row><row><cell>UniLMv2BASE</cell><cell>0.795</cell></row><row><cell>BERTLARGE</cell><cell>0.791</cell></row><row><cell>UniLMv2LARGE</cell><cell>0.818</cell></row><row><cell>LayoutLM BASE</cell><cell>0.827</cell></row><row><cell>LayoutLM LARGE</cell><cell>0.834</cell></row><row><cell>LayoutLMv2 BASE</cell><cell>0.833</cell></row><row><cell>LayoutLMv2 LARGE</cell><cell>0.852</cell></row><row><cell cols="2">RoBERTaBASE in (Grali?ski et al., 2020) 0.793</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">"MaskRCNN ResNeXt101 32x8d FPN 3X" setting in https://github.com/hpanwar08/detectron2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Unpublished results, the leaderboard is available at https://rrc.cvc.uab.es/?ch=13&amp;com=eval uation&amp;task=3</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Unpublished results, the leaderboard is available at https://rrc.cvc.uab.es/?ch=17&amp;com=eval uation&amp;task=1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://docs.microsoft.com/en-us/azu re/cognitive-services/computer-vision/co ncept-recognizing-text</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key R&amp;D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072 and 61772153.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Details of Datasets</head><p>Introduction to the dataset and task definitions along with the description of required data processing are presented as follows.</p><p>Pre-training Dataset Following LayoutLM, we pre-train LayoutLMv2 on the IIT-CDIP Test <ref type="bibr">Collection (Lewis et al., 2006)</ref>, which contains over 11 million scanned document pages. We extract text and corresponding word-level bounding boxes from document page images with the Microsoft Read API. 4 FUNSD FUNSD <ref type="bibr" target="#b11">(Jaume et al., 2019</ref>) is a dataset for form understanding in noisy scanned documents. It contains 199 real, fully annotated, scanned forms where 9,707 semantic entities are annotated above 31,485 words. The 199 samples are split into 149 for training and 50 for testing. The official OCR annotation is directly used with the layout information. The FUNSD dataset is suitable for a variety of tasks, where we focus on semantic entity labeling in this paper. Specifically, the task is assigning to each word a semantic entity label from a set of four predefined categories: question, answer, header, or other. The entity-level F1 score is used as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CORD</head><p>We also evaluate our model on the receipt key information extraction dataset, i.e. the public available subset of <ref type="bibr">CORD (Park et al., 2019)</ref>. The dataset includes 800 receipts for the training set, 100 for the validation set, and 100 for the test set. A photo and a list of OCR annotations are equipped for each receipt. An ROI that encompasses the area of receipt region is provided along with each photo because there can be irrelevant things in the background. We only use the ROI as input instead of the raw photo. The dataset defines 30 fields under 4 categories and the task aims to label each word to the right field. The evaluation metric is entity-level F1. We use the official OCR annotations.</p><p>SROIE The SROIE dataset (Task 3) <ref type="bibr" target="#b9">(Huang et al., 2019)</ref> aims to extract information from scanned receipts. There are 626 samples for training and 347 samples for testing in the dataset. The task is to extract values from each receipt of up to four predefined keys: company, date, address, or total. The evaluation metric is entity-level F1. We use the official OCR annotations and results on the test set are provided by the official evaluation site.</p><p>Kleister-NDA Kleister-NDA (Grali?ski et al., 2020) contains non-disclosure agreements collected from the EDGAR database, including 254 documents for training, 83 documents for validation, and 203 documents for testing. This task is defined to extract the values of four fixed keys. We get the entity-level F1 score from the official</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cutting the error by half: Investigation of very deep cnn and advanced training strategies for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Muhammad Zeshan Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheraz</forename><surname>K?lsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="883" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Document image classification with intradomain transfer learning and stacked generalization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3180" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modular multimodal architecture for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Dauphinee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Mehdi</forename><surname>Rashidi</surname></persName>
		</author>
		<idno>abs/1912.04376</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Filip Grali?ski, Tomasz Stanis?awek, Anna Wr?blewska, Dawid Lipi?ski, Agnieszka Kaliska, Paulina Rosalska</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Bartosz Topolski, and Przemys?aw Biecek. 2020. Kleister: A novel task for information extraction involving long documents with complex layout</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional nets for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Ufkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.322</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">{BROS}: A pre-trained language model for understanding texts in document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teakgyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Icdar2019 competition on scanned receipt ocr and information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00244</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Seunghyun Park, Sohee Yang, and Minjoon Seo. 2020. Spatial dependency parsing for semi-structured document information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyeong</forename><surname>Yim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Funsd: A dataset for form understanding in noisy scanned documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Jaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Hazim Kemal Ekenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thiran</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdarw.2019.10029</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition Workshops</title>
		<imprint>
			<publisher>ICDARW</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Online. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.634</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Layoutlm: Pretraining of text and layout for document image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3394486.3403172</idno>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Lee</forename><surname>Giles</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.462</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pick: Processing key information extraction from documents using improved graph learning-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Ping Gong, and Rong Xiao</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Trie: End-to-end text reading and information extraction for document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Publaynet: largest dataset ever for document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00166</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
