<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Squeezed Very Deep Convolutional Neural Networks for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?a</forename><forename type="middle">B</forename><surname>Duque</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mac?do</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?</forename><surname>L?zaro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleber</forename><surname>Zanchettin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Centro de Inform?tica</orgName>
								<orgName type="institution">Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Centro de Inform?tica Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Centro de Inform?tica Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Centro de Inform?tica Universidade Federal de Pernambuco</orgName>
								<address>
									<addrLine>50.740-560</addrLine>
									<settlement>Recife</settlement>
									<region>PE</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Squeezed Very Deep Convolutional Neural Networks for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the research in convolutional neural networks has focused on increasing network depth to improve accuracy, resulting in a massive number of parameters which restricts the trained network to platforms with memory and processing constraints. We propose to modify the structure of the Very Deep Convolutional Neural Networks (VDCNN) model to fit mobile platforms constraints and keep performance. In this paper, we evaluate the impact of Temporal Depthwise Separable Convolutions and Global Average Pooling in the network parameters, storage size, and latency. The squeezed model (SVDCNN) is between 10x and 20x smaller, depending on the network depth, maintaining a maximum size of 6MB. Regarding accuracy, the network experiences a loss between 0.4% and 1.3% and obtains lower latencies compared to the baseline model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The general trend in deep learning approaches has been developing models with increasing layers. Deeper neural networks have achieved high-quality results in different tasks such as image classification, detection, and segmentation. Deep models can also learn hierarchical feature representations from images <ref type="bibr" target="#b0">[1]</ref>. In the Natural Language Processing (NLP) field, the belief that compositional models can also be used to textrelated tasks is more recent.</p><p>The increasing availability of text data motivates research for models able to improve accuracy in different language tasks. Following the image classification Convolutional Neural Network (CNN) tendency, the research in text classification has placed effort into developing deeper networks. The first CNN based approach for text was a shallow network with one layer <ref type="bibr" target="#b1">[2]</ref>. Following this work, deeper architectures were proposed <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Conneau et al. <ref type="bibr" target="#b2">[3]</ref> were the first to propose Very Deep Convolutional Neural Networks (VDCNN) applied to text classification. VDCNN accuracy increases with depth. The approach with 29 layers is the state-of-the-art accuracy of CNNs for text classification. *Authors contributed equally and are both first writers. However, regardless of making networks deeper to improve accuracy, little effort has been made to build text classification models to constrained resources. It is a very different scenario compared to image approaches, where size and speed constrained models have been proposed <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In real-world applications, size and speed are constraints to an efficient mobile and embedded deployment of deep models <ref type="bibr" target="#b5">[6]</ref>.</p><p>Several relevant real-world applications depend on text classification tasks such as sentiment analysis, recommendation and opinion mining. The appeal for these applications combined with the boost in mobile devices usage motivates the need for research in restrained text classification models. Concerning mobile development, there are numerous benefits to developing smaller models. Some of the most relevant are requiring fewer data transferring while updating the client model <ref type="bibr" target="#b4">[5]</ref> and increasing usability by diminishing the inference time. Such advantages would boost the usage of deep neural models in text-based applications for embedded platforms.</p><p>In this paper, we investigate modifications on the network proposed by Conneau et al. <ref type="bibr" target="#b2">[3]</ref> with the aim of reducing its number of parameters, storage size and latency with minimal performance degradation. To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques. Therefore, our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks (SVDCNN), a text classification model which requires significantly fewer parameters compared to the stateof-the-art CNNs.</p><p>Section II provides an overview of the state-of-the-art in CNNs for text classification. Section III presents the VDCNN model. Section IV explains the proposed model SVDCNN and the subsequent impact in the total number of parameters of the network. Section V details how we perform experiments. Section VI analyses the results and lastly, Section VII, presents conclusions and direction for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>CNNs were originally designed for Computer Vision with the aim of considering feature extraction and classification as one task <ref type="bibr" target="#b6">[7]</ref>. Although CNNs are very successful in image classification tasks, its use in text classification is relatively new and has some peculiarities. Contrasting with traditional image bi-dimensional representations, texts are onedimensionally represented. Due to this property, the convolutions are designed as temporal convolutions. Furthermore, it is necessary to generate a numerical representation from the text so the network can be trained using this representation. This representation, namely embeddings, is usually obtained through the application of a lookup table, generated from a given dictionary.</p><p>An early approach for text classification tasks consisted of a shallow neural network working on the word level and using only one convolutional layer <ref type="bibr" target="#b1">[2]</ref>. The author reported results in smaller datasets. Later, Zhang et al. <ref type="bibr" target="#b3">[4]</ref> proposed the first CNN performing on a character level (Char-CNN), which allowed them to train up to 6 convolutional layers, followed by three fully connected classification layers. Char-CNN uses convolutional kernels of size 3 and 7, as well as simple maxpooling layers. <ref type="bibr" target="#b2">Conneau et al. (2016)</ref> proposed the Very Deep CNN (VD-CNN) <ref type="bibr" target="#b2">[3]</ref> also on a character level, presenting improvements compared to Char-CNN. <ref type="bibr" target="#b2">Conneau et al. (2016)</ref> have shown that text classification accuracy increases when the proposed model becomes deeper. VDCNN uses only small kernel convolutions and pooling operations. The proposed architecture relies on the VGG and ResNet philosophy <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>: The number of feature maps and the temporal resolution is modeled so that their product is constant. This approach makes it easier to control the memory footprint of the network. Both Zhang and Conneau et al. CNNs utilized standard convolutional blocks and fully connected layers to combine convolution information <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. This architecture choice increases the number of parameters and storage size of the models. However, size and speed was not the focus of those works.</p><p>The idea of developing smaller and more efficient CNNs without losing representative accuracy is a less explored research direction in NLP, but it has already been a trend for computer vision applications <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Most approaches consist in compressing pre-trained networks or training small networks directly <ref type="bibr" target="#b5">[6]</ref>. A recent tendency in deep models is replacing standard convolutional blocks with Depthwise Separable Convolutions (DSCs). The purpose is to reduce the number of parameters and consequently the model size. DSCs were initially introduced in <ref type="bibr" target="#b10">[11]</ref> and since then have been successfully applied to image classification and <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> machine translation <ref type="bibr" target="#b12">[13]</ref> to reduce the computation in convolutional blocks. Another approach is the use of a Global Average Pooling (GAP) layer at the output of the network to replace fully connected layers. This approach has become a standard architectural decision for newer CNNs <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VDCNN MODEL FOR TEXT CLASSIFICATION</head><p>The VDCNN is a modular architecture for text classification tasks developed to offer different depth levels (9, 17, 29 and 49). <ref type="figure" target="#fig_0">Fig. 1</ref> presents the architecture for depth 9. The network begins with a lookup table, which generates the embeddings for the input text and stores them in a 2D tensor of size (f 0, s). The number of input characters (s) is fixed to 1,024 while the embedding dimension (f 0) is 16. The embedding dimension can be seen as the number of RGB channels of an image.</p><p>The following layer (3, Temp Convolution, 64) applies 64 temporal convolutions of kernel size 3, so the output tensor has size 64 * s. Its primary function is to fit the lookup table output with the modular network segment input composed by convolutional blocks. Each aforenamed block is a sequence of two temporal convolutional layers, each one accompanied by a temporal batch normalization layer <ref type="bibr" target="#b14">[15]</ref> and a ReLU activation. Besides, the different network depths are obtained varying the number of convolutional blocks. As a convention, the depth of a network is given as its total number of convolutions. For instance, the architecture of depth 17 has two convolutional blocks of each level of feature maps, which results in 4 convolutional layers for each level (see <ref type="table" target="#tab_0">Table I</ref>). Considering the first convolutional layer of the network, we obtain the depth 2 * (2+2+2+2)+1 = 17. The different depth architectures provided by VDCNN model are summarized in <ref type="table" target="#tab_0">Table I</ref>. The following rule is employed to minimize the network's memory footprint: Before each convolutional block doubling the number of feature maps, a pooling layer halves the temporal dimension. This strategy is inspired by the VGG and ResNets philosophy and results in three levels of feature maps: 128, 256 and 512 (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Additionally, the VDCNN network also contains shortcut connections <ref type="bibr" target="#b7">[8]</ref> for each convolutional blocks implemented through the usage of 1 ? 1 convolutions. Lastly, for the classification task, the k most valuable features (k = 8) are extracted using k-max pooling, generating a one-dimensional vector which supplies three fully connected layers with ReLU hidden units and softmax outputs. The number of hidden units is 2,048, and they do not use dropout but rather batch normalization after convolutional layers perform the network regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SVDCNN MODEL FOR TEXT CLASSIFICATION</head><p>The primary objective is reducing the number of parameters so that the resulting network has a significative lower storage size. We first propose to modify the convolutional blocks of VDCNN model by the usage of Temporal Depthwise Separable Convolutions (TDSCs). Next, we reduce the number of fully connected layers using the Global Average Pooling (GAP) technique. The resulting proposed architecture is called Squeezed Very Deep Convolutional Neural Networks (SVD-CNN). a) Temporal Depthwise Separable Convolutions (TD-SCs): The use of TDSCs over standard convolutions allowed reducing the number of parameters without relevant accuracy loss <ref type="bibr" target="#b5">[6]</ref>. TDSCs work decompounding the standard convolution into two parts: Depthwise and Pointwise. The first one is responsible for applying a convolutional filter to each channel of the input at a time. For an image input, one possibility of channels are the RGB components, whereas in a text input the dimensions of the embedding can be used instead. For both cases mentioned above, the result is one feature map by channel. The second convolution unifies the generated feature maps successively applying 1x1 convolutions so that the target amount of feature maps can be achieved. TDSCs are DSCs which work with one-dimensional convolutions. Although DSCs hold verified results in image classification networks, the usage of its temporal version for text related tasks is less explored. <ref type="figure" target="#fig_1">Fig. 2a</ref> presents the architecture of a temporal standard convolution while <ref type="figure" target="#fig_1">Fig. 2b</ref> presents the TDSC.</p><p>For a more formal definition, let P tsc be the number of parameters of a temporal standard convolution, where In and Out are the numbers of Input and Output channels respectively, and D k is the kernel size:</p><formula xml:id="formula_0">P tsc = In * Out * D k<label>(1)</label></formula><p>Alternatively, a TDSC achieves fewer parameters (P tdsc ):</p><formula xml:id="formula_1">P tdsc = In * Dk + In * Out<label>(2)</label></formula><p>In the VDCNN model, one convolutional block is composed of two temporal standard convolutional layers. The first one doubles the number of feature maps while the second keeps the same value received as input. Besides, each convolutional layer is followed by a Batch Normalization and a ReLU layers. In our model, we proposed changing the temporal standard convolutions by TDSCs. <ref type="figure" target="#fig_2">Fig. 3</ref> presents the standard convolutional block on the left and the proposed convolutional block using TDSC on the right. The pattern used in the figure for the convolutional layers is the following: "Kernel Size, Conv type, Output Feature Maps"; as a brief example consider "3x1, Temporal Conv, 256", which means a Temporal Convolution with kernel size 3 and 256 feature maps as output. <ref type="figure" target="#fig_0">From Equation 1</ref>, we have the number of parameters of the original convolutional block (P convblock ) as follows:</p><formula xml:id="formula_2">P convblock = In * Out * 3 + Out * Out * 3<label>(3)</label></formula><p>Moreover, from equation 2, the number of parameters of the proposed convolutional block (P convblock?tdsc ) that uses TDSC being: P convblock?tdsc = In * 3+In * Out+Out * 3+Out * Out (4) For illustration, following the same characteristics of <ref type="figure" target="#fig_2">Fig. 3</ref>, consider that the number of input channels In is equal to 128 and the number of output channels Out is equal to 256. Our proposed approach accumulates a total of 99,456 parameters. In contrast, there are 294,912 parameters in the original convolutional block. The use of TDSC yields a reduction of 66.28% in the network size.</p><p>Lastly, since each standard temporal convolution turns into two (Depthwise and Pointwise), the number of convolutions per convolutional block has doubled. Nevertheless, these two convolutions work as one because it is not possible to use them separately keeping the same propose. In this way, we count them as one layer in the network depth. This decision holds the provided depth architectures the same as the VDCNN model summarized in <ref type="table" target="#tab_0">Table I</ref>, contributing to a proper comparison between the models. b) Global Average Pooling (GAP): The VDCNN model uses a k-max pooling layer (k = 8) followed by three fully connected (FC) layers to perform the classification task <ref type="figure">(Fig.  4a</ref>). Although this approach is the traditional architecture choice for text classification CNNs, it introduces a significant number of parameter in the network. The resulting number of the FC layers parameters (P f c ) aforementioned is presented below, for a problem with four target classes: Instead of maintaining these fully connected layers, we directly aggregate the output of the last convolutional block through the usage of an average pooling layer. This method, known as Global Average Pooling, contributes substantially to the parameters reduction without degrading the network accuracy significantly <ref type="bibr" target="#b15">[16]</ref>. The number of resulting feature maps given by the average pooling layer was the same as the original k-max pooling layer (k = h = 8). <ref type="figure">Fig. 4b</ref> presents this proposed modification. The number of parameters obtained by the usage of GAP (P gap ) is revealed as follows:</p><formula xml:id="formula_3">P gap = 4, 096 * 4 P gap = 16, 384<label>(6)</label></formula><p>Our proposed approach accumulates a total of 16,384 parameters. In contrast, there are 12,591,104 parameters in the original classification method. The use of GAP yields a reduction of 99.86%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>The experiment goal is to investigate the impact of modifying the convolutional block of VDCNN to TDSCs and using GAP instead of the original fully connected layers. We evaluate Char-CNN, VDCNN, and SVDCNN according to the number of parameters, storage size, inference time and accuracy. The source code of the proposed model is available in the GitHub repository SVDCNN <ref type="bibr" target="#b0">1</ref> The original VDCNN paper reported the number of parameters of the convolutional layers, in which we reproduce in this article. For SVDCNN and Char-CNN, we calculated the abovementioned number from the network architecture implemented in PyTorch. As for the FC layer's parameters, the number is obtained as the summation of the product of the input and output size of each FC layer for each CNN.</p><p>Considering the network parameters P and assuming that one float number on Cuda environment takes 4 bytes, we can calculate the network storage in megabytes, for all the models, as follows:</p><formula xml:id="formula_4">S = P * 4 ? 1, 024 2<label>(7)</label></formula><p>Regarding the inference time, its average and standard deviation were calculated as the time to predict one instance of the AG's News dataset throughout 1,000 repetitions. The SVDCNN experimental settings are similar to the original VDCNN paper, using the same dictionary and the same embedding size of 16 <ref type="bibr" target="#b2">[3]</ref>. The training is also performed with SGD, utilizing size batch of 64, with a maximum of 100 epochs. We use an initial learning rate of 0.01, a momentum of 0.9 and a weight decay of 0.001. All the experiments were performed on an NVIDIA GTX 1060 GPU + Intel Core i7 4770s CPU.</p><p>The model's performance is evaluated on three large-scale public datasets also used by Zhang et al. <ref type="bibr" target="#b3">[4]</ref> in the introduction of Char-CNN and VDCNN models. <ref type="table" target="#tab_0">Table II</ref> presents the details of the utilized datasets: AG's News, Yelp Polarity and Yelp Full. <ref type="table" target="#tab_0">Table IV</ref> presents the number of parameters, storage size, and accuracy for the SVDCNN, VDCNN, and Char-CNN in all datasets. The use of TDSCs promoted a significant reduction in convolutional parameters compared to VDCNN. For the most in-depth network evaluated, which contains 29 convolutional layers (depth 29), the number of parameters of these convolutional layers had a reduction of 66.08%, from 4.6 to 1.56 million parameters. This quantity is slightly larger than the one obtained from the Char-CNN, 1.40 million parameters, but this network has only six convolutional layers (depth 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>The network reduction obtained by the GAP is even more representative since both compared models use three FC layers for their classification tasks. Considering a dataset with four target classes, and comparing SVDCNN with VDCNN, the number of parameters of the FC layers has passed from 12.59 to 0.02 million parameters, representing a reduction of 99.84%. Following with the same comparison, but to Char-CNN, the proposed model is 99.82% smaller, 0.02 against 11.36 million of FC parameters.</p><p>The reduction of the total parameters impacts directly on the storage size of the networks. While our most in-depth model (29) occupies only 6MB, VDCNN with the same depth occupies 64.16MB of storage. Likewise, Char-CNN (which has depth 6) occupies 43.25MB. This reduction is a significant result because many embedded platforms have several memory constraints. For example, FPGAs often have less than 10MB of on-chip memory and no off-chip memory or storage <ref type="bibr" target="#b5">[6]</ref>.</p><p>Regarding accuracy results, usually, a model with such parameter reduction should present some loss of accuracy in comparison to the original model. Nevertheless, the performance difference between VDCNN and SVDCNN models varies between 0.4 and 1.3%, which is pretty modest considering the parameters and storage size reduction aforementioned. In <ref type="table" target="#tab_0">Table IV</ref>, it is possible to see the accuracy scores obtained by the compared models. Another two fundamental results obtained are a) The base property of VDCNN model is preserved on its squeezed model: the performance still increasing up with the depth and b) The performance evaluated for the most extensive dataset, i.e., Yelp Review (62.30%), still overcomes the accuracy of the Char-CNN model (62.05%).</p><p>Deep learning processing architecture has the property of being high parallelizable; it is expected smaller latencies when performing inferences in hardware with high parallelization power. Despite this property, the model ability to use all hardware parallel potential available also depends on the network architecture. The more parameters per layers, the more parallelizable a model tends to be, while the increase of the depth gets the opposite result. Another natural comprehension fact is if a model has few parameters, there exists less content to be processed, and then we have a faster inference time.</p><p>Concerning mobile devices, the presence of dedicated hardware for deep learning is not entirely feasible. This hardware usually requires more energy and dissipates more heat, two undesirable features for a mobile platform. Therefore, obtaining fewer inference times, even out of environments with high parallelization capabilities, is a pretty desirable characteristic for a model designed to work on mobile platforms. The latency ratio between CPU and GPU inference times indicates how undependable of dedicated hardware a model is, with higher values meaning more independence.</p><p>The inference times obtained for the three models compared are available in <ref type="table" target="#tab_0">Table III</ref>. As explained in Section IV a), each convolutional layer of the convolutional blocks was substituted by two convolutions. This change could impact the inference time negatively, but the significant parameter reduction allows the SVDCNN to obtain better results than the VDCNN model. The CPU inference time obtained by the proposed model was smaller than the base model for the depth 9 (25.88ms against 29,13ms) and depth 17 (47.80ms against 48.05ms),  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we presented a squeezed version of the VDCNN model considering the number of parameters and size. The new model proprieties became it feasible for mobile platforms. To achieve this goal, we analyzed the impact of including Temporal Depthwise Separable Convolutions and a Global Average Pooling layer in a very deep convolutional neural network for text classification. The SVDCNN model reduces about 92.45% the number of parameters and storage size while presents an inference time ratio (CPU/GPU), 31.94% higher.</p><p>For future works, we plan to evaluate other techniques able to reduce storage size, such as model compression. Moreover, the model accuracy over even more massive datasets will be evaluated as well as the efficiency of its depth 49 configuration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Depth 9 VDCNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>a) Temporal Standard Convolution; b) Temporal Depthwise Separable Convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>a) Standard convolutional block of the VDCNN; b) Modified convolutional block of the SVDCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>P 5 )Fig. 4 :</head><label>54</label><figDesc>f c = 512 * k * 2, 048 + 2, 048 * 2, 048 + 2, 048 * 4 P f c = 12, 591, 104 (a) VDCNN classification layers; b) SVDCNN classification layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Number of convolutional layers for each different VDCNN depth architecture</figDesc><table><row><cell>Depth</cell><cell cols="4">9 17 29 49</cell></row><row><cell>Convolutional Block 512</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>6</cell></row><row><cell>Convolutional Block 256</cell><cell>2</cell><cell>4</cell><cell cols="2">4 10</cell></row><row><cell>Convolutional Block 128</cell><cell>2</cell><cell cols="3">4 10 16</cell></row><row><cell>Convolutional Block 64</cell><cell>2</cell><cell cols="3">4 10 16</cell></row><row><cell cols="2">First Convolutional Layer 1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Datasets used in experiments</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Train #Test #Classes Classification Task</cell></row><row><cell>AG's News</cell><cell>120k</cell><cell>7.6k</cell><cell>4</cell><cell>News categorization</cell></row><row><cell cols="2">Yelp Polarity 560k</cell><cell>38k</cell><cell>2</cell><cell>Sentiment analysis</cell></row><row><cell>Yelp Full</cell><cell>650k</cell><cell>50k</cell><cell>5</cell><cell>Sentiment analysis</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Time results for AG's News dataset</figDesc><table><row><cell></cell><cell cols="2">Inference Time</cell><cell></cell></row><row><cell></cell><cell>GPU</cell><cell cols="2">CPU Ratio</cell></row><row><cell>SVDCNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>5.53ms ? 0.16</cell><cell>25.88ms ? 0.52</cell><cell>0.21</cell></row><row><cell>17</cell><cell>9.84ms ? 0.28</cell><cell>47.80ms ? 1.01</cell><cell>0.21</cell></row><row><cell>29</cell><cell>15.14ms ? 0.44</cell><cell>74.03ms ? 1.15</cell><cell>0.20</cell></row><row><cell>VDCNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>4.48ms ? 0.19</cell><cell>29.13ms ? 0.87</cell><cell>0.15</cell></row><row><cell>17</cell><cell>7.08ms ? 0.20</cell><cell>48.05ms ? 1.26</cell><cell>0.15</cell></row><row><cell>29</cell><cell>10.26ms ? 0.26</cell><cell>65.80ms ? 1.51</cell><cell>0.16</cell></row><row><cell>Char-CNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">6 10.32ms ? 0.43 313.53ms ? 4.97</cell><cell>0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Number of parameters, storage and accuracy results for all evaluated CNNs Ratio was higher for all depths (0.20 against 0.15 in average). These results, as explained above, are pretty significant for mobile platforms. Looking to Char-CNN, this model got notably inferior results compared to the proposed method, with 313.53ms of CPU inference time and Ratio of 0.03.</figDesc><table><row><cell></cell><cell></cell><cell>SVDCNN</cell><cell></cell><cell></cell><cell>VDCNN</cell><cell></cell><cell>Char-CNN</cell></row><row><cell></cell><cell>9</cell><cell>17</cell><cell>29</cell><cell>9</cell><cell>17</cell><cell>29</cell><cell>6</cell></row><row><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>#Conv Params [M]</cell><cell>0.71</cell><cell>1.43</cell><cell>1.56</cell><cell>2.20</cell><cell>4.40</cell><cell>4.60</cell><cell>1.37</cell></row><row><cell>#FC Params [M]</cell><cell>0.02</cell><cell>0.02</cell><cell>0.02</cell><cell cols="3">12.59 12.59 12.59</cell><cell>11.34</cell></row><row><cell>#Total Params [M]</cell><cell>0.73</cell><cell>1.45</cell><cell>1.58</cell><cell cols="3">14.79 16.99 17.19</cell><cell>12.71</cell></row><row><cell>Storage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Storage Size [MB]</cell><cell>2.80</cell><cell>5.52</cell><cell>6.03</cell><cell cols="3">54.75 62.74 64.16</cell><cell>43.25</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ag News</cell><cell cols="3">90.13 90.43 90.55</cell><cell cols="3">90.83 91.12 91.27</cell><cell>92.36</cell></row><row><cell>Yelp Polarity</cell><cell cols="3">94.99 95.04 95.26</cell><cell cols="3">95.12 95.50 95.72</cell><cell>95.64</cell></row><row><cell>Yelp Full</cell><cell cols="3">61.97 63.00 63.20</cell><cell cols="3">63.27 63.93 64.26</cell><cell>62.05</cell></row><row><cell>while the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Link: https://github.com/lazarotm/SVDCNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank FACEPE and CNPq (Brazilian research agencies) for financial support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing squeezenet storage size with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zanchettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ludermir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">357</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Depthwise separable convolutions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03059</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
