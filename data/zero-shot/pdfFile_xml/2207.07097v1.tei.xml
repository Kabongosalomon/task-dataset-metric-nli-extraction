<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReAct: Temporal Action Detection with Relational Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfeng</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Meituan Inc 3 JD Explore Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Meituan Inc 3 JD Explore Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ReAct: Temporal Action Detection with Relational Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal action detection (TAD) has been actively studied because of the deep learning era. Inspired by the advance of one-stage object detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>, many recent works focus on one-stage action detectors <ref type="bibr" target="#b17">[18]</ref>, which show excellent performance while having a relatively simple structure. On the other hand, DETR <ref type="bibr" target="#b3">[4]</ref>, which tackles object detection in a Transformer encoder-decoder framework, attracted considerable attention. In this work, we propose a novel one-stage action detector ReAct that is based on such a learning paradigm. Inspired by DETR, ReAct models action instances as a set of learnable action queries. These action queries are fed into the decoder as inputs, and they iteratively attend to the output features of the encoder as well as update their <ref type="figure" target="#fig_6">Fig. 1</ref>. The relation of queries. We choose the green one as the reference query , and the queries in a different relation to it are labeled with different colors. Only the Distinctsimilar pair (Blue ones) will be kept for attention computation.</p><p>predictions. The action classification and localization are then predicted by two simple feedforward neural nets.</p><p>However, the DETR-like methods suffer from several problems when applied to TAD task. First, the inter-query relations are not fully explored by the selfattention in the decoder, which is performed densely over all the queries. Second, DETR-like methods may suffer from the inadequate training of action classification since the number of positive training samples for the classifier is relatively small compared to anchor-based/free methods. Moreover, when multiple queries fire for the same action instance at inference, queries with higher classification scores may not necessarily have better temporal localization. In the following, we elaborate on these problems and introduce the proposed methods to alleviate them in three aspects: attention mechanism, training losses, and inference.</p><p>The decoder in DETR-like methods applies the self-attention over the action queries to capture their relations, which can not fully explore the complex relations among queries. In this work, we denote the action queries that are responsible for localizing different action instances of similar or same action classes as distinct-similar queries, and those detecting different action classes as distinct-dissimilar queries. For the queries that fire for the same action instance, we regard them as duplicate queries. In this work, we propose a novel attention mechanism, named Relational Attention with IoU Decay (RAID), to explicitly handle these three types of query relations in the decoder. As <ref type="figure" target="#fig_6">Fig. 1</ref> shows, RAID focuses on the communication among distinct-similar queries (since they are expected to provide more informative signals) and blocks the attention between distinct-dissimilar and duplicate queries. Furthermore, the proposed IoU decay encourages the duplicate queries to be slightly different from each other to enable a more diverse prediction.</p><p>Another problem is that a DETR-like approach may have a relatively low classification accuracy due to inadequate classification training. This is because the positive training samples for the classification of DETR-like methods are much fewer than those of the anchor-free methods. Namely, for DETR-like methods, the number of positives per input clip is only the same as the ground truth actions because of the bipartite-matching-based label assignment. To address this problem, we propose two training losses, codenamed Action Classification Enhancement (ACE) losses, to facilitate the classification learning. The first loss ACE-enc is applied to the input features of the encoder and is designed to reduce the intra-class variance and inter-class similarity of action instances. This loss explicitly improves the discriminability of video features regarding acting classes, thus benefiting the classification. Meanwhile, a ACE-dec loss is proposed as the classification loss in the decoder, which considers both the predicted segments and the ground-truth segments for action classification. It increases the training samples and generates a stable learning signal for the classifier.</p><p>Lastly, the action queries are redundant by design compared to the actual action instances. At inference, it is a common situation where multiple actions queries fire for the same action instance. Hence, it is important to focus on precise action localization queries. Nonetheless, the classification score is deficient in measuring the temporal localization quality. As a result, we propose a Segment Quality to predict the localization quality of each action query at inference, such that the more high-quality queries can be distinguished.</p><p>To summarize, we make the following contributions in this work:</p><p>-We approach temporal action detection using a DETR-like framework and identify three limitations of such method when directly applied to TAD. -We propose the relational attention with IoU decay, the action classification enhancement losses, and the segment quality prediction, which alleviate the identified problems from the perspectives of attention mechanism, training losses, and network inference, respectively. -Experiments on two action detection benchmarks demonstrate the superiority of ReAct: it achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Extensive ablation studies are conducted to verify the effectiveness of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Temporal action detection. Temporal action detection (TAD) aims to detect all the start and end timestamps and the corresponding action types based on the video stream information. The existing methods can be roughly divided into two categories: two-stage methods and one-stage methods. Two-stage methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref> split the detection task into two subtasks: proposal generation and proposal classification. Concretely, some methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> generate the proposals by predicting the probability of the start point and endpoint of the action and then selecting the proposal segments according to prediction score. In addition, PGCN <ref type="bibr" target="#b42">[43]</ref> considers the relationship between proposals, then refines and classifies the proposals by Graph Convolutional Network. These twostage methods can perform better by combining proposal generation networks and proposal classification networks. However, they can not be trained in an endto-end manner and are computationally inefficient. To solve the above problems, some one-stage methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref> are proposed. Some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40]</ref> try to adapt to the high variance of the action duration by constructing a temporal feature pyramid, while Liu et al. <ref type="bibr" target="#b22">[23]</ref> propose to dynamically sample temporal features by learnable parameters. These one-stage methods reduce the complexity of the models, which are more computationally friendly. In this work, we mainly follow the one-stage fashion and the deformable convolution design <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23]</ref> to build a efficient action detector, which will be detailed in the Section 3.</p><p>Attention-based model. Attention-based models <ref type="bibr" target="#b32">[33]</ref> have achieved great success in machine translation and been extended to the field of computer vision <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8]</ref> in recent years. The attention module computes a soft weight dynamically for a set of points at runtime. Concretely, DETR <ref type="bibr" target="#b3">[4]</ref> proposes a Transformer-based image detection paradigm. It learns decoder input features shared by all input videos and detects a fixed number of outputs. Deformable DETR <ref type="bibr" target="#b46">[47]</ref> improves DETR by reducing the number of pairs to be computed in the attention module with learnable spatial offsets. Liu et al. <ref type="bibr" target="#b22">[23]</ref> propose an end-to-end framework for TAD based on Deformable DETR. This type of training paradigm is highly efficient and fast in prediction. However, there is still a performance gap between these methods and the latest methods in TAD <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>. Our work is built on DETR-like workflows. In contrast to the above work, our approach suppresses the flow of invalid information by constricting a computational subset for the attention module, which improves performance effectively.</p><p>Contrastive learning. Contrastive learning <ref type="bibr" target="#b6">[7]</ref> is a method that has been widely used in unsupervised learning. NCE <ref type="bibr" target="#b12">[13]</ref> mines data features by distinguishing between data and noise. Info-NCE <ref type="bibr" target="#b26">[27]</ref> is proposed to extract representations from high-dimensional data with a probabilistic contrastive loss. Lin et al. <ref type="bibr" target="#b17">[18]</ref> leverage contrastive learning to help network identify action boundaries. Inspired by these works, we use contrastive learning to extract a global common representation of action categories and enlarge the feature distance between action segments and noise segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Problem definition. This work focuses on the problem of temporal action detection (TAD). Specifically, given a set of untrimmed videos</p><formula xml:id="formula_0">D = {V i } n i=1 . A set of {X i , Y i } can be extracted from each video V i , where X i = {x t } T t=1</formula><p>corresponds to the image (and optical flow) features of T snippets and Y i = {m k , d k , c k } Ki k=1 is K i segment labels for the video V i with the action segment midpoint time m k , the action duration d k and the corresponding action category c k . Temporal action detection aims at predicting all segments Y i based on the input feature X i . Method overview. Motivated by DETR <ref type="bibr" target="#b3">[4]</ref>, we approach the problem of TAD by an encoder-decoder framework based on the transformer network. As <ref type="figure" target="#fig_1">Fig. 3</ref> shows, the overall architecture of ReAct contains three parts: a video feature extractor, an action encoder, and an action decoder. First, video clip features are extracted from each RGB frame by using the widely-used 3D-CNN (e.g., TSN <ref type="bibr" target="#b34">[35]</ref> or I3D <ref type="bibr" target="#b4">[5]</ref>). The optical flow features are also extracted using TVL1 optical flow algorithm <ref type="bibr" target="#b41">[42]</ref>. Following that, a 1-D conv layer is used to modify the feature dimension of the clip features. The output features are then passed to the action encoder, which is a L E -layer transformer network. The encoded clip features serve as one of the inputs to the action decoder. The decoder is a L D -layer transformer, and it differs from the encoder in two aspects. It has action queries (which are learnable embeddings) as inputs, and the queries attend the encoder outputs in each layer of the decoder, known as Cross-attention. Essentially, ReAct maps action instances as a set of action queries. The action queries are transformed by the decoder into output embeddings which are used for both action classification and temporal localization by separate feed-forward neural nets. The details of the encoder structure are provided in the appendix. At training, following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23]</ref>, the Hungarian Algorithm <ref type="bibr" target="#b15">[16]</ref> is applied to assign labels to the action queries. The edge weight is defined by the summation of the segment IoU, the probability of classification, and the L1 norm between two coordinates. Based on the matching, ReAct applies several losses to the action queries, including the action classification loss and temporal segment regression loss.</p><p>Limitations of DETR-like methods for TAD. DETR-like methods may suffer from several problems when applied to TAD task. First, the decoder performs the self-attention densely over all the queries, which causes the inter-query relations not to be sufficiently explored. Second, compared with anchor-based/free methods, DETR-like methods may have issues in deficit training of action classification attributed to relatively smaller number of positive training samples for the classifier. Third, queries with higher classification scores may not be reliable due to multiple queries firing for the same action instance at inference.</p><p>In this work, we mitigate these problems in three aspects: (1) We propose the Relational Attention with IoU Decay which allows each action query to attend to others in the decoder based on their relations; (2) We design two Action Classification Enhancement losses to enhance the action classification learning at the encoder and decoder, respectively; (3) We introduce a Segment Quality to predict the localization quality of each action query at inference to compensate the deficiency of classification score at inference. We elaborate on these three aspects in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relational Attention with IoU Decay</head><p>To better explore the inter-query relation in the decoder, we present the Relational Attention with IoU Decay (RAID) which replaces the self-attention in the transformer decoder. Below, we describe the proposed method in detail.</p><p>Relational attention. As a recap, we define three types of queries with respect to an action query q i , which are differentiated by their relations to q i . Distinctsimilar queries are the queries that try to detect different action instances but of similar (or same) action class to q i . Distinct-dissimilar queries are those which try to detect different action instances and of dissimilar action class to q i . Duplicate queries are the queries that try to detect the same action instance as q i . Intuitively, we anticipate that attending to distinct-dissimilar queries does not provide informative signals to q i , since they focus on different action classes, and the relation between action classes may not be a reliable cue for detecting actions. On the contrary, attending to distinct-similar queries can benefit the query q by gathering some background information and cues around q i . For example, some actions may occur multiple times in a clip, and attending to each other can increase the confidence of the detection. Moreover, duplicate queries only repeat the prediction as q i , so they bring no extra information and should be ignored in the attention for q i .</p><p>To find the distinct-similar queries for a query q i , we consider two properties, namely, high context similarity and low temporal overlap. To measure context similarity, we compute a similarity matrix A ? R Lq?Lq (L q is the number of queries) based on the query features, where each element represents the cosine similarity of two queries. Then the query-pair set E sim is constructed by</p><formula xml:id="formula_1">E sim = {(i, j)|A[i, j] ? ? &gt; 0},<label>(1)</label></formula><p>where ? ? [?1, 1] is a preset similarity threshold. To identify the queries having low temporal overlap with q, a natural strategy is using the Interaction of Union (IoU) in the time domain, which measures the overlap between two temporal segments. Therefore, we compute a pair-wise IoU matrix B ? R Lq?Lq for the reference segments and construct a query-pair set E IoU as follows:</p><formula xml:id="formula_2">E IoU = {(i, j)|B[i, j] ? ? &lt; 0},<label>(2)</label></formula><p>where i and j denote the i-th and j-th queries respectively, and ? ? [0, 1] is a preset IoU threshold. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, this simple strategy removes the segments which have large temporal overlap. We can then define the distinctsimilar query-pair set E by combining E sim , E IoU and the query itself E s . The definition is given as follow:</p><formula xml:id="formula_3">E = (E IoU \ E sim ) ? E s .<label>(3)</label></formula><p>For a query q i and its distinct query-pair set E i , the key and value features can be written as</p><formula xml:id="formula_4">K i = concatenate({k j |(i, j) ? E i }) and V i = concatenate({v j |(i, j) ? E i })</formula><p>. Then, the query features q i are updated by</p><formula xml:id="formula_5">q ? i = a i V T i ,<label>(4)</label></formula><p>where the attention weight a i is</p><formula xml:id="formula_6">a i = Sof tmax K (q i K T i ).<label>(5)</label></formula><p>Note that by considering both the context similarity and temporal overlap, the proposed relational attention successfully preserves the communication between q i and useful queries while blocking that between uninformative ones.</p><p>IoU decay. Apart from relational attention, we introduce a further improvement by handling duplicate queries. Namely, we propose a regularization, termed as IoU decay, which is added to the network optimization. It is given as</p><formula xml:id="formula_7">? d = 1 2 Lq i=1 Lq j=1</formula><p>IoU (s i , s j ).</p><p>During the detector training, it penalizes the IoU between queries, such that duplicate queries can be diversified and different from each other, which can increase the probability of obtaining a more precise localization for the target action instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Action Classification Enhancement</head><p>To combat the issue of the inadequate learning of classification when applying the DETR-like methods to TAD, we propose two Action Classification Enhancement (ACE) losses to boost the classification performance. ACE-enc loss. We aim to enhance the features with respect to action classification in the phase of encoder by enlarging the similarity of inter-class action instances and reducing the variance between intra-class action instances. We posit that explicitly increasing the discriminability of the features on the action detection dataset in an early stage can also benefit the final action classification. Specifically, we optimize the input features of the encoder using contrastive loss.</p><p>The positive and negative action instance pairs are constructed as follows. For a given ground-truth action segment s g and its category c g in a video v i , we choose its positive instances by sampling the action segments of the same category c g from either the same or different videos. As for its negative instances, we choose them from two different sources: (1) segments of action categories different from c g and (2) segments that are completely inside the ground-truth segment, but their IoU is less than a specific threshold ?.</p><p>For a given segment s, we denote x ? R T ?D ? and x ? R T ?D as the pretrained video feature and feature further projected by a fully-connected layer l (i.e. x = l(x)), respectively. Then, the segment feature after temporal RoI pooling <ref type="bibr" target="#b37">[38]</ref> can be denoted as f = RoI( x, s) ? R D . With the above definitions, the loss L ACE?enc is given by</p><formula xml:id="formula_9">L ACE?enc = ? log exp(f T f p ) j?D exp(f T f j ) ,<label>(7)</label></formula><p>where f p is a positive segment of f and D is the index of k random negative instances as well as a positive instance. ACE-dec loss. Anchor-based/free methods treat all (or multiple) the temporal locations within the ground truth action segment as positives (i.e., belonging to an action class rather than backgrounds) for training the action classifiers, whereas DETR-like methods have much fewer positives due to the bipartite matching at label assignment. We, therefore, propose the ACE-dec loss to train the action classifiers.</p><p>As <ref type="figure" target="#fig_1">Fig. 3 (right)</ref> shows, in the training phase, an additional positive training sample is fed to the action classifiers for each query segment (i.e., the green one) matched with a ground-truth action instance. The additional positive is obtained by feeding the ground-truth segment (i.e., the yellow one) as a normal query segment to the cross-attention layer. The details of the cross-attention layer are described in the supplementary material.</p><p>Concretely, every decoder layer is attached a ACE-dec loss which is given by</p><formula xml:id="formula_10">L ACE?dec = L q f oc + 1 y? =? [L gt f oc ],<label>(8)</label></formula><p>where L q f oc and L gt f oc is the sigmoid Focal Loss <ref type="bibr" target="#b21">[22]</ref> for the query and ground-truth classification loss respectively. Note that, only the queries that are matched to ground-truth segments will contribute the ground-truth classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Segment Quality Prediction</head><p>To remedy the problem that classification score is unreliable for selecting the best query among a set of duplicate queries, we propose a Segment Quality to predict the localization quality of each action query at inference for distinguishing highquality queries. The proposed segment quality prediction considers both the midpoint of the segment as well as its temporal coverage on the action instance.</p><p>Concretely, given a predicted segment s q and its query feature f q , we define (? 1 , ? 2 ) = ?(f q ), where ? is a single fully-connected layer and ? 1 , ? 2 ? [0, 1]. Then, a final quality value ? is defined by ? = ? 1 ? ? 2 . Segment Quality is supervised by a two-dimensional vector composing of the offset of the predicted midpoint and its ground truth for localizing the midpoint precisely, and the IoU between the predicted segment and its closest ground-truth segment for accurate temporal localization and coverage. The overall loss is given by</p><formula xml:id="formula_11">L ? = ?(f q ) ? (exp(? 1 l gt |m q ? m gt |), IoU (s q , s gt )) 1 ,<label>(9)</label></formula><p>where m q is the midpoint of the predicted segment, and m gt , l gt are the midpoint and length of the ground-truth segment, respectively. At inference, ? is multiplied with the classification score of the segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Losses</head><p>At training, based on the label assignment by the Hungarian Algorithm, ReAct is trained by the total loss as follow:</p><formula xml:id="formula_12">L = L ACE?enc + L ACE?dec + L ? + L reg .<label>(10)</label></formula><p>Here, L reg is the commonly used regression loss for TAD which regresses the midpoint and the duration of the detected segments using the summation of L1 distance and the generalized IoU distance <ref type="bibr" target="#b28">[29]</ref> for the matched pair. We define each objective as follows:</p><formula xml:id="formula_13">L reg = 1 N cgt? =? j?Lq 1 c (j) gt ? =? [? 1 L (j) L1 + ? 2 L (j) gIoU ], L (j) L1 = |m (j) gt ? m (j) | + |d (j) gt ? d (j) |, L (j) gIoU = 1 ? gIoU (s (j) gt , s (j) ),<label>(11)</label></formula><p>where s (j) = (m (j) , d (j) ) is j-th detected segment represented by midpoint and the duration. c gt . In addition, we follow the segment refinement fashion <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b22">23]</ref> to predict detections in each decoder layer, each of which will be updated by summing with the upper layer segment and re-normalizing it. In this way, each layer provides auxiliary classification loss L ? cls and regression loss L ? reg , which further helps the network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We conduct experiments on two challenging datasets: THUMOS14 <ref type="bibr" target="#b13">[14]</ref> and ActivityNet-1.3 <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Architecture details. Optimization parameters and inference. We train the ReAct with AdamW optimizer with a batch size of 16. The learning rate is set to 2?10 ?4 and 1?10 ?4 for THUMOS14 and ActivityNet-1.3 respectively. ReAct is trained for 15 epochs on THUMOS14 and 35 epochs on ActivityNet-1.3. At inference, the classification head output is activated by sigmoid. Then all the predictions will be processed with Soft-NMS <ref type="bibr" target="#b1">[2]</ref> to remove the redundant and low-quality segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>On THUMOS14 (see Tab. 1), our ReAct achieves superior performance and suppresses the state-of-the-art one-stage and two-stage methods in mAP at different thresholds. In particular, ReAct achieves 55.0% in the average mAP, which outperforms TadTR by a large margin, namely about the 9.4% absolute improvement. Besides, we compare the computational performance during testing. We adopt Floating-point operations per second (FLOPs) per clip following the previous works. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48]</ref>. We can see that our model has FLOPS of 0.68G, which is 0.06G lower than TadTr and much lower than all the other methods. Note that the FLOPS we report in the table does not include the computation of video feature extraction with backbone. For methods like AFSD, which fine-tunes the backbone and does feature extraction during testing, we ignore the computation of feature extraction and only report the FLOPs afterward. On ActivityNet-1.3, our method achieves comparable results to the stateof-the-art (See Tab. 2). The ReAct outperforms the other DETR-based methods while enjoying a low computational cost (e.g., 0.38G). The Actioness and Anchor-based methods tend to have higher performance compared with the DETR-based methods. One possible reason is that the DETR-based methods take learnable query embedding as input, which is video-agnostic and only keeps statistical information. For a dataset with a large variance in action time, a query feature has to take both long and short action into account (See appendix for more details) and is prone to conflicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we conduct the ablation studies on the THUMOS14 dataset.</p><p>Main components. We demonstrate the effectiveness of three proposed components in ReAct: RAID, ACE, and Segment Quality. From Tab. 3 (row 2 and row 5), we can see that compared with the plain deformable decoder layer, our RAID brings about a 3.7% absolute improvement in the average mAP, proving the effectiveness of the module by introducing the relational attention based on the defined distinct-similar, distinct-dissimilar and duplicated queries. Besides, from rows 4 and 5 of the <ref type="table">Table,</ref> we see our ACE improves the average mAP performance by 2.9%, which shows its effectiveness by designing new losses to enhance classification learning. Finally, from rows 3 and 5, the proposed Segment Quality achieves 2.8% improvements in average mAP, which effectively estimates the predicted segments' quality at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of RAID.</head><p>We study the effect of two hyperparameters ? and ? in Section 3.1 for thresholding the similarity scores and IoU values when constructing the distinct similar and dissimilar query sets. First, we set ? = 1 and plot the average mAP when varying ?. From <ref type="figure" target="#fig_5">Fig. 5</ref>(a) we see that as ? increases, the mAP exhibits an increase followed by a decrease, with a peak at ? = 0.2. Besides, we observe that the detection performance shows greater volatility as ? decreases further (i.e., ? &lt; ?0.1). Intuitively, smaller ? leads to more irrelevant query pairs communicating, thus introducing greater uncertainty. Next, we study the effect of the choice of ? by fixing ? = 0.2. From <ref type="figure" target="#fig_5">Fig. 5(b)</ref> we observe a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Time</head><p>Reference Query Distinct-similar Distinct-dissimilar Duplicated Ground-Truth <ref type="figure">Fig. 4</ref>. Visualization of the queries for a test video in THUMOS14. Some example frames are shown for the queries, and we can see that many distinct-dissimilar queries correspond to noises (i.e. not actions).  similar trend with the figure for similarity as ? changes, and the optimal value is obtained at 0.5. Notice that the smaller ? is, the more queries will be excluded, and when ? = 0, only those that do not overlap will be retained. Intuitively, partially overlapped queries tend to be in the vicinity of the target query, which helps to perceive the information near the boundary. A visualized example of the queries is presented in <ref type="figure" target="#fig_1">Fig. 4.3</ref> to illustrate the work of RAID.</p><p>Analysis of ACE. We analyze the effect of ACE-enc loss in the following aspects: the construction of contrastive pairs, where to apply ACE-enc loss and training losses. First, we study how contrastive pairs affect performance. In particular, to form the positive segment pairs, we randomly choose segments of the same category from either the same video or different videos, denoted by S1 and S2, respectively. As for negative pairs, there are two ways: segment pairs belonging to different action classes (denoted by N1), and segment pairs that one completely includes the other, but their IoU is less than a threshold (denoted by N2), as described in 3.2. Tab. 4 presents the results using different combinations of positive and negative pairs. In Tab. 4, we see that N2 play a more important role in training than N1 (e.g., average mAP 53.9 versus 53.4), and merging them can gain further promotion (i.e., 54.3). Secondly, we study the effect of where to apply ACE-enc loss. We mainly consider two positions: before the transformer encoder and after it. We train a single fully connected layer for the former to enhance the video features. For the latter, we use the encoder output. The experimental results show that a single fully connected layer is much better than a complex transformer encoder. Intuitively, after encoder processing, the features on each frame already contain local temporal information, therefore, the pooled segment features can not represent the action precisely, leading to inaccurate convergence.</p><p>Finally, to go deeper into the ACE-dec loss, we conducted three experiments: query classification loss only, ground-truth classification loss only, and the complete ACE-dec loss. For the case of the ground-truth classification loss only, we still predict and match the ground-truth segment with the input query feature, which provides the matched query position and reference ground-truth segment. However, we only update the network with ground-truth classification loss L gt f oc . From the Tab. 4, neither L q f oc nor L gt f oc can perform well, but when we combine them together, the result are significantly better (e.g., 53.4 versus 51.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we consider the task of temporal action detection and propose a novel one-stage action detector ReAct based on a DETR-like learning framework. Three limitations of such a method when directly applied to TAD are identified. We propose the relational attention with IoU decay, the action classification enhancement losses, and the segment quality prediction and handle those issues from three aspects: attention mechanism, training losses, and network inference, respectively. ReAct achieves the state-of-the-art performance with much lower computational costs than previous methods on THUMOS14. Extensive ablation studies are also conducted to demonstrate the effectiveness of each proposed component. In the future, we plan to include the video feature extractor in the action detection training to improve the performance further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Encoder in Detail</head><p>To be self-contained, we provide the detailed structure of the encoder. As <ref type="figure" target="#fig_6">Fig.  1</ref> shows, for the input video feature F ? R T ?D , a local offset position and attention weight will be predicted with two fully-connected layers, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Decoder in Detail</head><p>To help understand our method better, we introduce the decoder in detail. There are two attention modules in the decoder: the proposed relational attention module and a cross-attention module.</p><p>In the following, we elaborate on the deformable cross-attention module. As <ref type="figure" target="#fig_0">Fig. 2</ref> showed, reference segment, offset position, and attention weights are predicted by three fully-connect layers, based on which the network samples sparse features to update the query feature at each decoder layer. There are two main differences in the deformable attention module between the encoder and decoder. First, the inputs and outputs are different. The input of the crossattention in the decoder is the queries, while the input of the encoder is video features. The second difference is the reference segment. In the encoder, temporal offsets for each frame are sampled only around that frame. Whereas for the crossattention module, an additional reference segment length is predicted for each query feature, and the offsets are normalizes such that the sampled frames are always in the segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Architecture and Training Detail</head><p>For THUMOS14, following <ref type="bibr" target="#b37">[38]</ref>, we use the TSN network <ref type="bibr" target="#b34">[35]</ref> pre-trained on Kinetics <ref type="bibr" target="#b14">[15]</ref> to extract features, which are then down-sampled every five frames. Each video feature is cropped in sequence with a window size 256, and two adjacent windows will have 192 overlapped features with a stride rate of 0.25. In the training phase, ground-truth cut by windows over 75% duration will be kept, and all empty windows without any ground-truth are removed. Finally, all ground-truth coordinates are re-normalized to the window coordinate system. we set L q = 40, L E = 2, L D = 4 for the number of queries, encoder layer and decoder layer, respectively. Each deformable attention module will sample 4 temporal offsets for computing the attention. The hidden layer dimension of the feedforward network is set to 1024, and the other hidden feature dimension in the intermediate of the network is all set to 256. The pair-wise IoU threshold ? and feature similarity threshold ? in ACE module are set to 0.5 and 0.2, respectively. For ActivityNet, the pre-trained TSN network by Xiong et al. <ref type="bibr" target="#b35">[36]</ref> is adopted to extract features. Then each video feature downsamples every 16 frames, and the resultant feature will be rescaled to 100 snippets using linear interpolation. We only do video-level detection instead of window-level. We set the L q = 60, L E = 3, L D = 4. We sample 4 temporal offsets for the deformable module. The dimension of hidden features is set to 256, and we set the pair-wise IoU threshold ? and feature similarity threshold ? to 0.9 and -0.2, respectively. Following previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b39">40]</ref>, we combined the Untrimmed-Net videolevel classification results <ref type="bibr" target="#b33">[34]</ref> with our classification score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualization of the Classification Loss</head><p>To further demonstrate the effect of ACE-dec loss, we compute the classification loss for the Activitynet-1.3 test set. As <ref type="figure" target="#fig_1">Fig. 3</ref> shows, compared to the Focal Loss, the ACE-dec loss improves not only the convergence speed but also the accuracy.  <ref type="figure" target="#fig_1">Fig. 3</ref>. Visualization of the test classification loss. We record the testing loss with or without ACE-dec loss during training</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the proposed framework. The video feature is extracted by a pretrained backbone, followed by a fully-connected layer to project the feature, and is additionally supervised by the AEC-Enc loss. After enhancement by the Transformer encoder, the features are fed into the decoder and attended by Lq action queries in the decoder. The classification head is trained with the proposed ACE-Dec loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of our decoder. Left: plain deformable decoder. Each query performs the attention operation with all other query features and sample segment features from the encoder output. Right: decoder of ReAct. Each query only attends to specific queries based on the inter-query relation. Besides, the ground-truth segment provides an additional loss to further supervise the classification head. Note that for clarity, the LayerNorm, FFN, and residual connection are not shown in the figure (see appendix for detailed network structure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>gt is a set of the ground-truth segments that s j is matched and N cgt? =? is the number of segments in c gt ) is the matched ground-truth segment of s j and s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For THUMOS14, we set L q = 40, L E = 2, L D = 4 for the number of queries, encoder layer and decoder layer, respectively. Each deformable attention module samples 4 temporal offsets for computing the attention. The hidden layer dimension of the feedforward network is set to 1024, and the other hidden feature dimension in the intermediate of the network is all set to 256. The pair-wise IoU threshold ? and feature similarity threshold ? in ACE module are set to 0.2 and 0.2, respectively. For ActivityNet-1.3, we set L q = 60, L E = 3, L D = 4, ? = 0.9, ? = ?0.2. We sample 4 temporal offsets for the deformable module. For more implementation details including feature extraction and training details, please refer to the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>(a) is visualization of the choice of the hyperparameter ?, with ? = 1; (b) is visualization of the choice of the hyperparameter ? , with ? = 0.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .</head><label>1</label><figDesc>For each time step, feature are then sampled according to the K offsets with linear interpolation. The sampled features are weighted by the attention weights and summed up to produce the updated frame feature for the corresponding time step. Illustration of the encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the Deformable Cross Attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the state-of-the-art methods on THUMOS14 dataset. We report the mean Average Precision (mAP) in different thresholds and the floating-point operations (FLOPs, G).</figDesc><table><row><cell>Type</cell><cell>Method</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell cols="2">0.7 Avg. FLOPs</cell></row><row><cell></cell><cell>BSN[21]</cell><cell cols="6">53.5 45.0 36.9 28.4 20.0 36.8</cell><cell>3.4</cell></row><row><cell></cell><cell>BMN[19]</cell><cell cols="6">56.0 47.4 38.8 29.7 20.5 38.5 171.0</cell></row><row><cell></cell><cell>G-TAD[38]</cell><cell cols="6">54.5 47.6 40.3 30.8 23.4 39.3 639.8</cell></row><row><cell></cell><cell>TAL[6]</cell><cell cols="6">53.2 48.5 42.8 33.8 20.8 39.8</cell><cell>-</cell></row><row><cell>Two-stage</cell><cell>TCANet[28]</cell><cell cols="6">60.6 53.2 44.6 36.8 26.7 44.3</cell><cell>-</cell></row><row><cell></cell><cell cols="7">CSA+BMN[30] 64.4 58.0 49.2 38.2 27.8 47.5</cell><cell>-</cell></row><row><cell></cell><cell>P-GCN[43]</cell><cell cols="3">63.6 57.8 49.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.4</cell></row><row><cell></cell><cell cols="7">RTD-Net[31] 68.3 62.3 51.9 38.8 23.7 49.0</cell><cell>-</cell></row><row><cell></cell><cell>VSGN[45]</cell><cell cols="6">66.7 60.4 52.4 41.0 30.4 50.2</cell><cell>-</cell></row><row><cell></cell><cell cols="7">ContextLoc[48] 68.3 63.8 54.3 41.8 26.2 50.9</cell><cell>3.1</cell></row><row><cell></cell><cell>SSAD[20]</cell><cell cols="3">43.0 35.0 24.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SSN[41]</cell><cell cols="3">51.9 41.0 29.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>One-stage</cell><cell>A2Net[40] AFSD[18]</cell><cell cols="6">58.6 54.1 45.5 32.5 17.2 41.6 67.3 62.4 55.5 43.7 31.1 52.0</cell><cell>30.4 5.1</cell></row><row><cell></cell><cell>TadTr[23]</cell><cell cols="6">62.4 57.4 49.2 37.8 26.3 46.6</cell><cell>0.75</cell></row><row><cell></cell><cell>ReAct</cell><cell cols="6">69.2 65.0 57.1 47.8 35.6 55.0 0.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison with the state-of-the-art methods on ActivityNet-1.3 dataset. Ablation study on three main components.</figDesc><table><row><cell>Type</cell><cell></cell><cell cols="2">Method</cell><cell cols="2">0.5 0.75 0.95 Avg. FLOPs(G)</cell></row><row><cell></cell><cell></cell><cell cols="2">BSN[21]</cell><cell cols="2">46.5 30.0 8.0 28.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">SSN[41]</cell><cell cols="2">43.2 28.7 5.6 28.3</cell><cell>-</cell></row><row><cell>Actioness</cell><cell></cell><cell cols="4">BMN[19] G-TAD[38] 50.4 34.6 9.0 34.1 50.1 34.8 8.3 33.9</cell><cell>45.6 45.7</cell></row><row><cell></cell><cell></cell><cell cols="4">BU-TAL[46] 43.5 33.9 9.2 34.3</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="4">VSGN[45] 52.3 35.2 8.3 34.7</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">TAL[6]</cell><cell cols="2">38.2 18.3 1.3 20.2</cell><cell>-</cell></row><row><cell cols="2">Anchor-based</cell><cell cols="4">PGCN[43] TCANet[28] 52.3 36.7 6.9 35.5 48.3 33.2 3.3 31.1</cell><cell>5.0 -</cell></row><row><cell></cell><cell></cell><cell cols="2">AFSD[18]</cell><cell cols="2">52.4 35.2 6.5 34.3</cell><cell>15.3</cell></row><row><cell></cell><cell></cell><cell cols="4">RTD-Net[31] 47.2 30.7 8.6 30.8</cell><cell>-</cell></row><row><cell cols="2">DETR-based</cell><cell cols="2">TadTr[23]</cell><cell cols="2">49.1 32.6 8.5 32.3</cell><cell>0.38</cell></row><row><cell></cell><cell></cell><cell cols="2">ReAct</cell><cell cols="2">49.6 33.0 8.6 32.6</cell><cell>0.38</cell></row><row><cell cols="5">Method RAID ACE SQ 0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7 Avg.</cell></row><row><cell>Our Base</cell><cell>? ? ?</cell><cell>? ? ?</cell><cell>? ? ?</cell><cell cols="2">66.6 59.2 49.7 38.0 25.0 47.7 66.6 61.5 53.7 43.4 31.2 51.3 67.0 62.6 54.4 44.0 32.2 52.1 69.1 63.3 54.2 43.5 31.0 52.2 69.2 65.0 57.1 47.8 35.6 55.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison of different settings of ACE module.</figDesc><table><row><cell>Module</cell><cell>Setting</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7 Avg.</cell></row><row><cell></cell><cell>No Contrastive</cell><cell cols="5">68.1 63.4 55.0 46.0 32.8 53.1</cell></row><row><cell></cell><cell>{S1,S2} + {N1}</cell><cell cols="5">68.3 63.4 55.4 46.2 33.9 53.4</cell></row><row><cell></cell><cell>{S1,S2} + {N2}</cell><cell cols="5">69.7 64.6 55.7 45.6 33.8 53.9</cell></row><row><cell>ACE-enc</cell><cell>{S1,S2} + {N1,N2}</cell><cell cols="5">69.7 64.5 56.6 45.9 34.7 54.3</cell></row><row><cell></cell><cell>{S1} + {N1,N2}</cell><cell cols="5">69.1 64.4 56.3 46.2 34.6 54.1</cell></row><row><cell></cell><cell cols="6">Before Transformer Enc. 69.7 64.3 56.1 46.4 34.2 54.1</cell></row><row><cell></cell><cell cols="6">After Transformer Enc. 66.4 61.2 53.3 43.4 32.0 51.2</cell></row><row><cell></cell><cell>L q f oc Only</cell><cell cols="5">67.5 62.6 53.9 43.3 33.2 52.1</cell></row><row><cell>ACE-dec</cell><cell>L gt f oc Only</cell><cell cols="5">66.1 61.1 53.6 44.2 30.9 51.2</cell></row><row><cell></cell><cell>L q f oc + L gt f oc</cell><cell cols="5">68.3 63.4 55.4 46.2 33.9 53.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dearkd: Dataefficient early knowledge distillation for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12052" to="12062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07755</idno>
		<title level="m">Tood: Task-aligned onestage object detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning salient boundary feature for anchor-free temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3320" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">End-to-end temporal action detection with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10271</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding. arXiv e-prints pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1807</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal context aggregation network for temporal action proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="485" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Class semanticsbased attention for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13739" to="13748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13526" to="13535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8535" to="8548" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5522" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10108</idno>
		<title level="m">Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video self-stitching graph network for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13658" to="13667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enriching local and global contexts for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13516" to="13525" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
