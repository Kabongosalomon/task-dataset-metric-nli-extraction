<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning for Physical Interaction through Video Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
							<email>cbfinn@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">Goodfellow</forename><surname>Openai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>slevine@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Learning for Physical Interaction through Video Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection, tracking, and motion prediction are fundamental problems in computer vision, and predicting the effect of physical interactions is a critical challenge for learning agents acting in the world, such as robots, autonomous cars, and drones. Most existing techniques for learning to predict physics rely on large manually labeled datasets (e.g. <ref type="bibr" target="#b17">[18]</ref>). However, if interactive agents can use unlabeled raw video data to learn about physical interaction, they can autonomously collect virtually unlimited experience through their own exploration. Learning a representation which can predict future video without labels has applications in action recognition and prediction and, when conditioned on the action of the agent, amounts to learning a predictive model that can then be used for planning and decision making.</p><p>However, learning to predict physical phenomena poses many challenges, since real-world physical interactions tend to be complex and stochastic, and learning from raw video requires handling the high dimensionality of image pixels and the partial observability of object motion from videos. Prior video prediction methods have typically considered short-range prediction <ref type="bibr" target="#b16">[17]</ref>, small image patches <ref type="bibr" target="#b21">[22]</ref>, or synthetic images <ref type="bibr" target="#b19">[20]</ref>. Such models follow a paradigm of reconstructing future frames from the internal state of the model. In our approach, we propose a method which does not require the model to store the object and background appearance. Such appearance information is directly available in the previous frame. We develop a predictive model which merges appearance information from previous frames with motion predicted by the model. As a result, the model is better able to predict future video sequences for multiple steps, even involving objects not seen at training time.</p><p>To merge appearance and predicted motion, we output the motion of pixels relative to the previous image. Applying this motion to the previous image forms the next frame. We present and evaluate three motion prediction modules. The first, which we refer to as dynamic neural advection (DNA), outputs a distribution over locations in the previous frame for each pixel in the new frame. The predicted pixel value is then computed as an expectation under this distribution. A variant on this approach, which we call convolutional dynamic neural advection (CDNA), outputs the parameters of multiple normalized convolution kernels to apply to the previous image to compute new pixel values. The last approach, which we call spatial transformer predictors (STP), outputs the parameters of multiple affine transformations to apply to the previous image, akin to the spatial transformer network previously proposed for supervised learning <ref type="bibr" target="#b10">[11]</ref>. In the case of the latter two methods, each predicted transformation is meant to handle separate objects. To combine the predictions into a single image, the model also predicts a compositing mask over each of the transformations. DNA and CDNA are simpler and easier to implement than STP, and while all models achieve comparable performance, the object-centric CDNA and STP models also provide interpretable internal representations.</p><p>Our main contribution is a method for making long-range predictions in real-world videos by predicting pixel motion. When conditioned on the actions taken by an agent, the model can learn to imagine different futures from different actions. To learn about physical interaction from videos, we need a large dataset with complex object interactions. We collected a dataset of 59,000 robot pushing motions, consisting of 1.5 million frames and the corresponding actions at each time step. Our experiments using this new robotic pushing dataset, and using a human motion video dataset <ref type="bibr" target="#b9">[10]</ref>, show that models that explicitly transform pixels from previous frames better capture object motion and produce more accurate video predictions compared to prior state-of-the-art methods. The dataset, video results, and code are all available online: sites.google.com/site/robotprediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video prediction: Prior work on video prediction has tackled synthetic videos and short-term prediction in real videos. Yuan et al. <ref type="bibr" target="#b29">[30]</ref> used a nearest neighbor approach to construct predictions from similar videos in a dataset. Ranzato et al. proposed a baseline for video prediction inspired by language models <ref type="bibr" target="#b20">[21]</ref>. LSTM models have been adapted for video prediction on patches <ref type="bibr" target="#b21">[22]</ref>, actionconditioned Atari frame predictions <ref type="bibr" target="#b19">[20]</ref>, and precipitation nowcasting <ref type="bibr" target="#b27">[28]</ref>. Mathieu et al. proposed new loss functions for sharper frame predictions <ref type="bibr" target="#b16">[17]</ref>. Prior methods generally reconstruct frames from the internal state of the model, and some predict the internal state directly, without producing images <ref type="bibr" target="#b22">[23]</ref>. Our method instead transforms pixels from previous frames, explicitly modeling motion and, in the case of the CDNA and STP models, decomposing it over image segments. We found in our experiments that all three of our models produce substantially better predictions by advecting pixels from the previous frame and compositing them onto the new image, rather than constructing images from scratch. This approach differs from recent work on optic flow prediction <ref type="bibr" target="#b24">[25]</ref>, which predicts where pixels will move to using direct optical flow supervision. Boots et al. predict future images of a robot arm using nonparametric kernel-based methods <ref type="bibr" target="#b3">[4]</ref>. In contrast to this work, our approach uses flexible parametric models, and effectively predicts interactions with objects, including objects not seen during training. To our knowledge, no previous video prediction method has been applied to predict real images with novel object interactions beyond two time steps into the future.</p><p>There have been a number of promising methods for frame prediction developed concurrently to this work <ref type="bibr" target="#b15">[16]</ref>. Vondrick et al. <ref type="bibr" target="#b23">[24]</ref> combine an adversarial objective with a multiscale, feedforward architecture, and use a foreground/background mask similar to the masking scheme proposed here. De Brabandere et al. <ref type="bibr" target="#b5">[6]</ref> propose a method similar to our DNA model, but use a softmax for sharper flow distributions. The probabilistic model proposed by Xue et al. <ref type="bibr" target="#b28">[29]</ref> predicts transformations applied to latent feature maps, rather than the image itself, but only demonstrates single frame prediction.</p><p>Learning physics: Several works have explicitly addressed prediction of physical interactions, including predicting ball motion <ref type="bibr" target="#b4">[5]</ref>, block falling <ref type="bibr" target="#b1">[2]</ref>, the effects of forces <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>, future human interactions <ref type="bibr" target="#b8">[9]</ref>, and future car trajectories <ref type="bibr" target="#b25">[26]</ref>. These methods require ground truth object pose information, segmentation masks, camera viewpoint, or image patch trackers. In the domain of reinforcement learning, model-based methods have been proposed that learn prediction on images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>, but they have either used synthetic images or instance-level models, and have not demonstrated generalization to novel objects nor accurate prediction on real-world videos. As shown by our  <ref type="figure">Figure 1</ref>: Architecture of the CDNA model, one of the three proposed pixel advection models. We use convolutional LSTMs to process the image, outputting 10 normalized transformation kernels from the smallest middle layer of the network and an 11-channel compositing mask from the last layer (including 1 channel for static background). The kernels are applied to transform the previous image into 10 different transformed images, which are then composited according to the masks. The masks sum to 1 at each pixel due to a channel-wise softmax. Yellow arrows denote skip connections.</p><p>comparison to LSTM-based prediction designed for Atari frames <ref type="bibr" target="#b19">[20]</ref>, models that work well on synthetic domains do not necessarily succeed on real images.</p><p>Video datasets: Existing video datasets capture YouTube clips <ref type="bibr" target="#b11">[12]</ref>, human motion <ref type="bibr" target="#b9">[10]</ref>, synthetic video game frames <ref type="bibr" target="#b19">[20]</ref>, and driving <ref type="bibr" target="#b7">[8]</ref>. However, to investigate learning visual physics prediction, we need data that exhibits rich object motion, collisions, and interaction information. We propose a large new dataset consisting of real-world videos of robot-object interactions, including complex physical phenomena, realistic occlusions, and a clear use-case for interactive robot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motion-Focused Predictive Models</head><p>In order to learn about object motion while remaining invariant to appearance, we introduce a class of video prediction models that directly use appearance information from previous frames to construct pixel predictions. Our model computes the next frame by first predicting the motions of image segments, then merges these predictions via masking. In this section, we discuss our novel pixel transformation models, and propose how to effectively merge predicted motion of multiple segments into a single next image prediction. The architecture of the CDNA model is shown in <ref type="figure">Figure 1</ref>. Diagrams of the DNA and STP models are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pixel Transformations for Future Video Prediction</head><p>The core of our models is a motion prediction module that predicts objects' motion without attempting to reconstruct their appearance. This module is therefore partially invariant to appearance and can generalize effectively to previously unseen objects. We propose three motion prediction modules:</p><p>Dynamic Neural Advection (DNA): In this approach, we predict a distribution over locations in the previous frame for each pixel in the new frame. The predicted pixel value is computed as an expectation under this distribution. We constrain the pixel movement to a local region, under the regularizing assumption that pixels will not move large distances. This keeps the dimensionality of the prediction low. This approach is the most flexible of the proposed approaches.</p><p>Formally, we apply the predicted motion transformationm to the previous image prediction? t?1 for every pixel (x, y) to form the next image prediction? t as follows:</p><formula xml:id="formula_0">I t (x, y) = k?(??,?) l?(??,?)m xy (k, l)? t?1 (x ? k, y ? l)</formula><p>where ? is the spatial extent of the predicted distribution. This can be implemented as a convolution with untied weights. The architecture of this model matches the CDNA model in <ref type="figure">Figure 1</ref>, except that the higher-dimensional transformation parametersm are outputted by the last (conv 2) layer instead of the LSTM 5 layer used for the CDNA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Dynamic Neural Advection (CDNA):</head><p>Under the assumption that the same mechanisms can be used to predict the motions of different objects in different regions of the image, we consider a more object-centric approach to predicting motion. Instead of predicting a different distribution for each pixel, this model predicts multiple discrete distributions that are each applied to the entire image via a convolution (with tied weights), which computes the expected value of the motion distribution for every pixel. The idea is that pixels on the same rigid object will move together, and therefore can share the same transformation. More formally, one predicted object transformation m applied to the previous image I t?1 produces image? t for each pixel (x, y) as follows:</p><formula xml:id="formula_1">J t (x, y) = k?(??,?) l?(??,?)m (k, l)? t?1 (x ? k, y ? l)</formula><p>where ? is the spatial size of the normalized predicted convolution kernelm. Multiple transformations {m (i) } are applied to the previous image? t?1 to form multiple images {? (i) t }. These output images are combined into a single prediction? t as described in the next section and show in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Transformer Predictors (STP):</head><p>In this approach, the model produces multiple sets of parameters for 2D affine image transformations, and applies the transformations using a bilinear sampling kernel <ref type="bibr" target="#b10">[11]</ref>. More formally, a set of affine parametersM produces a warping grid between previous image pixels (x t?1 , y t?1 ) and generated image pixels (x t , y t ).</p><formula xml:id="formula_2">x t?1 y t?1 =M x t y t 1</formula><p>This grid can be applied with a bilinear kernel to form an image? t :</p><formula xml:id="formula_3">J t (x t , y t ) = W k H l? t?1 (k, l) max(0, 1 ? |x t?1 ? k|) max(0, 1 ? |y t?1 ? l|)</formula><p>where W and H are the image width and height. While this type of operator has been applied previously only to supervised learning tasks, it is well-suited for video prediction. Multiple transformations {M (i) } are applied to the previous image? t?1 to form multiple images {? (i) t }, which are then composited based on the masks. The architecture matches the diagram in <ref type="figure">Figure 1</ref>, but instead of outputting CDNA kernels at the LSTM 5 layer, the model outputs the STP parameters {M (i) }.</p><p>All of these models can focus on learning physics rather than object appearance. Our experiments show that these models are better able to generalize to unseen objects compared to models that reconstruct the pixels directly or predict the difference from the previous frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composing Object Motion Predictions</head><p>CDNA and STP produce multiple object motion predictions, which need to be combined into a single image. The composition of the predicted images? (i) t is modulated by a mask ?, which defines a weight on each prediction, for each pixel. Thus,</p><formula xml:id="formula_4">? t = c? (c) t</formula><p>? ? c , where c denotes the channel of the mask and the element-wise multiplication is over pixels. To obtain the mask, we apply a channel-wise softmax to the final convolutional layer in the model (conv 2 in <ref type="figure">Figure 1</ref>), which ensures that the channels of the mask sum to 1 for each pixel position.</p><p>In practice, our experiments show that the CDNA and STP models learn to mask out objects that are moving in consistent directions. The benefit of this approach is two-fold: first, predicted motion transformations are reused for multiple pixels in the image, and second, the model naturally extracts a more object centric representation in an unsupervised fashion, a desirable property for an agent learning to interact with objects. The DNA model lacks these two benefits, but instead is more flexible as it can produce independent motions for every pixel in the image.</p><p>For each model, including DNA, we also include a "background mask" where we allow the models to copy pixels directly from the previous frame. Besides improving performance, this also produces interpretable background masks that we visualize in Section 5. Additionally, to fill in previously occluded regions, which may not be well represented by nearby pixels, we allowed the models to generate pixels from an image, and included it in the final masking step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Action-conditioned Convolutional LSTMs</head><p>Most existing physics and video prediction models use feedforward architectures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b14">15]</ref> or feedforward encodings of the image <ref type="bibr" target="#b19">[20]</ref>. To generate the motion predictions discussed above, we employ stacked convolutional LSTMs <ref type="bibr" target="#b27">[28]</ref>. Recurrence through convolutions is a natural fit for multi-step video prediction because it takes advantage of the spatial invariance of image representations, as the laws of physics are mostly consistent across space. As a result, models with convolutional recurrence require significantly fewer parameters and use those parameters more efficiently.</p><p>The model architecture is displayed in <ref type="figure">Figure 1</ref> and detailed in Appendix B. In an interactive setting, the agent's actions and internal state (such as the pose of the robot gripper) influence the next image. We integrate both into our model by spatially tiling the concatenated state and action vector across a feature map, and concatenating the result to the channels of the lowest-dimensional activation map. Note, though, that the agent's internal state (i.e. the robot gripper pose) is only input into the network at the beginning, and must be predicted from the actions in future timesteps. We trained the networks using an l 2 reconstruction loss. Alternative losses, such as those presented in <ref type="bibr" target="#b16">[17]</ref> could complement this method. One key application of action-conditioned video prediction is to use the learned model for decision making in visionbased robotic control tasks. Unsupervised learning from video can enable agents to learn about the world on their own, without human involvement, a critical requirement for scaling up interactive learning. In order to investigate action-conditioned video prediction for robotic tasks, we need a dataset with real-world physical object interactions. We collected a new dataset using 10 robotic arms, shown in <ref type="figure" target="#fig_0">Figure 2</ref>, pushing hundreds of objects in bins, amounting to 57,000 interaction sequences with 1.5 million video frames. Two test sets, each with 1,250 recorded motions, were also collected. The first test set used two different subsets of the objects pushed during training. The second test set involved two subsets of objects, none of which were used during training. In addition to RGB images, we also record the corresponding gripper poses, which we refer to as the internal state, and actions, which corresponded to the commanded gripper pose. The dataset is publically available 2 . Further details on the data collection procedure are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Robotic Pushing Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our method using the dataset in Section 4, as well as on videos of human motion in the Human3.6M dataset <ref type="bibr" target="#b9">[10]</ref>. In both settings, we evaluate our three models described in Section 3, as well as prior models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. For CDNA and STP, we used 10 transformers. While we show stills from the predicted videos in the figures, the qualitative results are easiest to compare when the predicted videos can be viewed side-by-side. For this reason, we encourage the reader to examine the video results on the supplemental website 2 . Code for training the model is also available on the website.</p><p>Training details: We trained all models using the TensorFlow library <ref type="bibr" target="#b0">[1]</ref>, optimizing to convergence using ADAM <ref type="bibr" target="#b12">[13]</ref> with the suggested hyperparameters. We trained all recurrent models with and without scheduled sampling <ref type="bibr" target="#b2">[3]</ref> and report the performance of the model with the best validation error. We found that scheduled sampling improved performance of our models, but did not substantially affect the performance of ablation and baseline models that did not model pixel motion.  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17]</ref>. All models were trained for 8-step prediction, except <ref type="bibr" target="#b16">[17]</ref>, trained for 1-step prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Action-conditioned prediction for robotic pushing</head><p>Our primary evaluation is on video prediction using our robotic interaction dataset, conditioned on the future actions taken by the robot. In this setting, we pass in two initial images, as well as the initial robot arm state and actions, and then sequentially roll out the model, passing in the future actions and the model's image and state prediction from the previous time step. We trained for 8 future time steps for all recurrent models, and test for up to 18 time steps. We held out 5% of the training set for validation. To quantitatively evaluate the predictions, we measure average PSNR and SSIM, as proposed in <ref type="bibr" target="#b16">[17]</ref>. Unlike <ref type="bibr" target="#b16">[17]</ref>, we measure these metrics on the entire image. We evaluate on two test sets described in Section 4, one with objects seen at training time, and one with previously unseen objects. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the performance of our models compared to prior methods. We report the performance of the feedforward multiscale model of <ref type="bibr" target="#b16">[17]</ref> using an l 1 +GDL loss, which was the best performing model in our experiments -full results of the multi-scale models are in Appendix C. Our methods significantly outperform prior video prediction methods on all metrics. The FC LSTM model <ref type="bibr" target="#b19">[20]</ref> reconstructs the background and lacks the representational power to reconstruct the objects in the bin. The feedforward multiscale model performs well on 1-step prediction, but performance quickly drops over time, as it is only trained for 1-step prediction. It is worth noting that our models are significantly more parameter efficient: despite being recurrent, they contain 12.5 million parameters, which is slightly less than the feedforward model with 12.6 million parameters and significantly less than the FC LSTM model which has 78 million parameters. We found that none of the models suffered from significant overfitting on this dataset. We also report the baseline performance of simply copying the last observed ground truth frame.</p><p>In <ref type="figure">Figure 4</ref>, we compare to models with the same stacked convolutional LSTM architecture, but that predict raw pixel values or the difference between previous and current frames. By explicitly modeling pixel motion, our method outperforms these ablations. Note that the model without skip connections is most representative of the model by Xingjian et al. <ref type="bibr" target="#b27">[28]</ref>. We show a second ablation in <ref type="figure" target="#fig_2">Figure 5</ref>, illustrating the benefit of training for longer horizons and from conditioning on the action of the robot. Lastly, we show qualitative results in <ref type="figure">Figure 6</ref> of changing the action of the arm to examine the model's predictions about possible futures.</p><p>For all of the models, the prediction quality degrades over time, as uncertainty increases further into the future. We use a mean-squared error objective, which optimizes for the mean pixel values. The <ref type="figure">Figure 4</ref>: Quantitative comparison to models which reconstruct rather than predict motion. Notice that on the novel objects test set, there is a larger gap between models which predict motion and those which reconstruct appearance. model thus encodes uncertainty as blur. Modeling this uncertainty directly through, for example, stochastic neural networks is an interesting direction for future work. Note that prior video prediction methods have largely focused on single-frame prediction, and most have not demonstrated prediction of multiple real-world RGB video frames in sequence. Action-conditioned multi-frame prediction is a crucial ingredient in model-based planning, where the robot could mentally test the outcomes of various actions before picking the best one for a given task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human motion prediction</head><p>In addition to the action-conditioned prediction, we also evaluate our model on predicting future video without actions. We chose the Human3.6M dataset, which consists of human actors performing various actions in a room. We trained all models on 5 of the human subjects, held out one subject for validation, and held out a different subject for the evaluations presented here. Thus, the models have never seen this particular human subject or any subject wearing the same clothes. We subsampled the video down to 10 fps such that there was noticeable motion in the videos within reasonable time frames. Since the model is no longer conditioned on actions, we fed in 10 video frames and trained the network to produce the next 10 frames, corresponding to 1 second each. Our evaluation measures performance up to 20 timesteps into the future.</p><p>The results in <ref type="figure" target="#fig_3">Figure 7</ref> show that our motion-predictive models quantitatively outperform prior methods, and qualitatively produce plausible motions for at least 10 timesteps, and start to degrade thereafter. We also show the masks predicted internally by the model for masking out the previous 0 action 1x action 1.5x action t = 1 3 5 7 9 1 3 5 7 9 <ref type="figure">Figure 6</ref>: CDNA predictions from the same starting image, but different future actions, with objects not seen in the training set. By row, the images show predicted future with zero action (stationary), the original action, and an action 150% larger than the original. Note how the prediction shows no motion with zero action, and with a larger action, predicts more motion, including object motion. frame, which we refer to as the background mask. These masks illustrate that the model learns to segment the human subject in the image without any explicit supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future Directions</head><p>In this work, we develop an action-conditioned video prediction model for interaction that incorporates appearance information in previous frames with motion predicted by the model. To study unsupervised learning for interaction, we also present a new video dataset with 59,000 real robot interactions and 1.5 million video frames. Our experiments show that, by learning to transform pixels in the initial frame, our model can produce plausible video sequences more than 10 time steps into the future, which corresponds to about one second. In comparisons to prior methods, our method achieves the best results on a number of previous proposed metrics.</p><p>Predicting future object motion in the context of a physical interaction is a key building block of an intelligent interactive system. The kind of action-conditioned prediction of future video frames that we demonstrate can allow an interactive agent, such as a robot, to imagine different futures based on the available actions. Such a mechanism can be used to plan for actions to accomplish a particular goal, anticipate possible future problems (e.g. in the context of an autonomous vehicle), and recognize interesting new phenomena in the context of exploration. While our model directly predicts the motion of image pixels and naturally groups together pixels that belong to the same object and move together, it does not explicitly extract an internal object-centric representation (e.g. as in <ref type="bibr" target="#b6">[7]</ref>). Learning such a representation would be a promising future direction, particularly for applying efficient reinforcement learning algorithms that might benefit from concise state representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Data Collection Details</head><p>In this section, we discuss additional details of the data collection procedure.</p><p>The data was collected using 10 7-degree-of-freedom robot arms, and included the robot's joint angles, gripper pose, commanded gripper pose, measured torques, and 640 ? 512 RGB images captured from the robot's camera. The images were center-cropped and downsampled to 64 ? 64 for our experiments. We used smaller images for faster training. In principle, the proposed methods should be able to handle larger images if desired. The images and robot sensor readings were recorded at 10 Hz, and the robot was commanded via impedance control in task space. In our experiments, the robot's internal state was the gripper pose, and the action was the commanded gripper pose. Two test sets, each with 1,500 recorded motions, were also collected. Both used the same robot control method described above. The first test set used two different subsets of the objects pushed during training.</p><p>The second test set involved two subsets of objects, none of which were used during training.</p><p>During data collection, between ten and twenty random objects were placed in bins in front of each robot, and swapped out for new objects after approximately 4,000 randomized interactions. The robots were programmed to repeatedly perform one of two different types of pushing motions: a random push or a randomized sweep to the middle. The sweep to the middle starts from a random position on the outside border of the bin, and meandered randomly towards the middle. The sweep motion was designed to prevent objects from piling up on the edges of the bin. Each type of motion lasted for approximately 3-5 seconds. Between each motion, the arm was programmed to move out of the camera scene, and an image was recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Details</head><p>In this section, we present additional details for each model. A diagram of the CDNA model is shown in the main paper in <ref type="figure">Figure 1</ref>, and additional diagrams for the DNA and STP models are shown in <ref type="figure">Figures 8 and 9</ref>, respectively. Each model consists of a core trunk made up of one stride-2 5 ? 5 convolution, followed by convolutional LSTMs. Each of these LSTM layers has the weights arranged into 5 ? 5 convolutions, and the output of the preceding LSTM is fed directly into the next one. LSTM layers 3 and 5 are preceded by stride 2 downsampling to reduce resolution, and LSTM layers 5, 6, and 7 are preceded by 2? upsampling. The end of the LSTM stack is followed by a 2? upsampling stage and a final convolutional layer, which then outputs a full-resolution mask for compositing the various transformed predictions (in the case of the CDNA and STP) and compositing against the static background (in the case of all models, including the DNA). To preserve high-resolution  <ref type="figure">Figure 9</ref>: Architecture of the STP model. This model is identical to the CDNA, with the only difference being that instead of outputting 10 transformation kernels, the model outputs 10 affine transformation matrices (with an additional fully connected layer). As with the CDNA, the transformations are each applied to the previous image, and the 10 resulting transformed images are composited by using a mask.</p><p>information, skip connections are included from LSTM 1 to conv 2 and from LSTM 3 to LSTM 7. The skip connections simply concatenate the skip layer activations and those of the preceding layer before sending them to the following layer (e.g. the input to LSTM 7 consists of the concatenation of LSTM 6 and LSTM 3).</p><p>In the case of the action-conditioned robot manipulation task, all three models also include as input the current state and action of the robot (corresponding to gripper pose and gripper motion command). This 10-dimensional vector is first tiled into a 8 ? 8 response map with 10 channels, and then concatenated, channel-wise, to the input of LSTM 5. The next state is predicted linearly from the current state and action, though more sophisticated prediction models could be used for more complex systems.</p><p>The three models differ in the form of the transformation that is applied to the previous image. The object-centric CDNA and STP models output the transformation parameters after LSTM 5. In both cases, the output of LSTM 5 is flattened and linearly transformed, either directly into filter parameters in the case of the CDNA, or through one 100-unit hidden layer in the case of the STP. There are 10 CDNA filters, which are 5 ? 5 in size and normalized to sum to 1 via a spatial softmax, so that each filter represents a distribution over positions in the previous image from which a new pixel value can be obtained. The STP parameters correspond to 10 3 ? 2 affine transformation matrices. The transformations are applied to the preceding image to create 10 separate transformed images. The CDNA transformation corresponds to a convolution (though with the kernel being an output of the network), while the STP transformation is an affine transformation.</p><p>The DNA model differs from the other two in that the transformation parameters are outputted at the last layer, in the same place as the mask. This is because the DNA model outputs a transformation map as large as the entire image. For each image pixel, the model outputs a 5 ? 5 convolutional kernel that can be applied to the previous image to obtain a new pixel value, similarly to the CDNA model. However, because the kernel is spatially-varying, this model is not equivalent to the CDNA. This transformation only produces one transformed image.</p><p>After transformation, the transformed image(s) and the previous image are composited together based on the mask. The previous image is included as a static "background" image and, as shown in Section 5, the mask on the background image indeed tends to pick out static parts of the scene. The final image is formed by multiplying each transformed image and the background image by their mask values, and adding all of the masked images together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experimental Results</head><p>Here we show additional experimental results. <ref type="figure">Figure 10</ref>: Comparison of various losses using the feedforward multi-scale architecture from <ref type="bibr" target="#b16">[17]</ref>. We were unable to get the adversarial objective to train desirably.</p><p>To evaluate the method of <ref type="bibr" target="#b16">[17]</ref>, we trained the feedforward multiscale architecture using four of the proposed objectives. For the robot motion dataset, we added the actions to the architecture at each scale via tiling. We were unable to successfully train the model with an adversarial loss, and, as shown in <ref type="figure">Figure 10</ref>, the model trained GDL+l 1 performed the best on the robot dataset. Thus, we presented the results of this model in the main evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Robot data collection setup (top) and example images captured from the robot's camera (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative and quantitative reconstruction performance of our models, compared with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Ablation of DNA involving not including the action, and different prediction horizons during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Quantitative and qualitative results on human motion video predictions with a held-out human subject. All recurrent models were trained for 10 future timesteps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Architecture of the DNA model. In contrast to the other models, the DNA model outputs the spatially-varying transformation kernels from the last layer, rather than the middle convolutional LSTM layer. The kernels are applied to transform the previous image, and the transformed image is composited with a 2-channel foreground-background mask.</figDesc><table><row><cell>RGB input</cell><cell>5x5 conv 1</cell><cell>5x5 conv LSTM 1</cell><cell>5x5 conv LSTM 2</cell><cell>5x5 conv LSTM 3</cell><cell>5x5 conv LSTM 4</cell><cell cols="2">5x5 conv LSTM 5</cell><cell cols="2">5x5 conv LSTM 6</cell><cell cols="2">5x5 conv LSTM 7</cell><cell>1x1 conv 2</cell><cell>composi!ng masks</cell></row><row><cell></cell><cell>32 c</cell><cell>32 c</cell><cell>32 c</cell><cell>64 c</cell><cell>64 c</cell><cell>128 c</cell><cell></cell><cell>64 c</cell><cell></cell><cell>32 c</cell><cell>11 c</cell><cell>channel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>soGmax</cell></row><row><cell>stride 2</cell><cell></cell><cell></cell><cell cols="2">stride 2</cell><cell cols="2">stride 2</cell><cell cols="2">deconv 2</cell><cell cols="2">deconv 2</cell><cell>deconv 2</cell></row><row><cell cols="5">!le 5x5 conv stride 2 5x5 conv LSTM 2 32x32 32 c LSTM 3 16x16 64 c robot state 5 ac!on 5 robot state 5 Figure 8: !le stride 2 5x5 conv 1 RGB input 64x64x3 32x32 32 c 5x5 conv LSTM 1 32x32 32 c 64x64x3 32x32 32x32 32x32 16x16 ac!on 5</cell><cell cols="7">5x5 conv LSTM 4 16x16 64 c 8x8 10 c stride 5x5 conv LSTM 5 8x8 138 c concatenate fully connected, 5x5 conv LSTM 6 16x16 64 c reshape &amp; normalize CDNA kernels 5x5 conv LSTM 7 32x32 32 c 10 5x5 convolve 2 deconv 2 deconv 2 deconv 2 8x8 fully connected 10 3x2 transforma!on matrices 100 16x16 8x8 10 c 16x16 transform concatenate 32x32 FC 1</cell><cell>1x1 conv 2 64x64 11 c 10 64x64x3 channel soGmax transformed images masked composi!ng masks 64x64 composi!ng 64x64 RGB predic!on ? 10 64x64x3 transformed images 64x64 RGB predic!on 64x64 64x64 masked composi!ng</cell></row></table><note>FC ?</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See http://sites.google.com/site/robotprediction</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Vincent Vanhoucke, Mrinal Kalakrishnan, Jon Barron, Deirdre Quillen, and our anonymous reviewers for helpful feedback and discussions. We would also like to thank Peter Pastor for technical support with the robots.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning predictive models of a depth camera &amp; manipulator from raw execution traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating contact dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action-reaction: Forecasting the dynamics of human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compuer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Autonomous reinforcement learning on raw visual input data in a real world application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voigtlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning physical intuition of block towers by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Newtonian image understanding: Unfolding the dynamics of objects in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">learning to predict the effect of forces in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>What happens if</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Unsupervised learning of video representations using lstms. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Anticipating the future by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1504.08023</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A data-driven approach for event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
