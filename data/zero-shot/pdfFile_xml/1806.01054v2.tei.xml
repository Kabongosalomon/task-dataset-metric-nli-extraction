<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RedNet: Residual Encoder-Decoder Network for indoor RGB-D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Automation Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunan</forename><surname>Zheng</surname></persName>
							<email>aulnzheng@sina.com</email>
							<affiliation key="aff0">
								<orgName type="department">The School of Automation Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Luo</surname></persName>
							<email>aufeiluo@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">The School of Automation Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Zhang</surname></persName>
							<email>auzjzhang@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">The School of Automation Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RedNet: Residual Encoder-Decoder Network for indoor RGB-D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Indoor semantic segmentation has always been a difficult task in computer vision. In this paper, we propose an RGB-D residual encoder-decoder architecture, named RedNet, for indoor RGB-D semantic segmentation. In RedNet, the residual module is applied to both the encoder and decoder as the basic building block, and the skip-connection is used to bypass the spatial feature between the encoder and decoder. In order to incorporate the depth information of the scene, a fusion structure is constructed, which makes inference on RGB image and depth image separately, and fuses their features over several layers. In order to efficiently optimize the network's parameters, we propose a 'pyramid supervision' training scheme, which applies supervised learning over different layers in the decoder, to cope with the problem of gradients vanishing. Experiment results show that the proposed RedNet(ResNet-50) achieves a state-of-the-art mIoU accuracy of 47.8% on the SUN RGB-D benchmark dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Indoor space is likely to be the main workplace for service robots in the near future. In order to work well in an indoor space, the robots should possesses the ability of visual scene understanding. To do so, the semantic segmentation in indoor scene is becoming one of the most popular tasks in computer vision.</p><p>Over the pass few years, fully convolutional networks (FCNs) type architectures have shown great potential on semantic segmentation task <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>, and have dominated the semantic segmentation task of many datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34]</ref>. Some of this FCNs-type architectures focus on indoor environment, and usually utilize the depth information as the complementary information for RGB to improve the segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. In general, the FCNs architectures can be generally divide into two categories, i.e., the encoder-decoder type architectures and dilated convolution architectures. The encoder-decoder architectures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15]</ref> have a downsample path to extract the semantic information from images and a upsample path to recover a full-resolution semantic segmentation map. By contrast, the dilated convolution architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>   convolution such that the convolutional network expands receptive field exponentially without downsampling. With less or even zero downsampling operation, dilated architectures keep the spatial information in the image through out the whole networks, so the architectures serve as a discriminative model that classify every pixel on the image. Encoder-decoder architectures, on the other hand, lost spatial information during the discriminative encoder, and thus some of the networks apply skip-architecture to recover the spatial information during the generative decoder path.</p><p>Even though the dilated convolution architectures have the advantage of keeping the spatial information, they generally have higher memory consumption on the training step. Because the spatial resolution of the activation map is not downsampled as the network proceed and it needs to be stored for gradient computation. Therefore, the high memory consumption stops the network from having a deeper structure. This could cause disadvantages on this method, since convolutional networks learn richer features as the structure gets deeper, which would benefit the inference of the semantic information.</p><p>In this paper, we propose a novel structure named RedNet that employ the encoder-decoder network structure for indoor RGB-D semantic segmentation. In RedNet, the residual block is used as the building module to avoid the model degradation problem <ref type="bibr" target="#b15">[16]</ref>. This allows the performance of networks to improve as the structure goes deeper. Moreover, we apply fusion structure to incorporate depth information into the network, and use skip-architecture to bypass the spatial information from encoder to decoder. Further, inspired by the training scheme in <ref type="bibr" target="#b34">[35]</ref>, we propose the pyramid supervision that apply supervised learning over different layers on the decoder for better optimization. The overall structure of RedNet is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The remainder of this paper is organized in four sections. In section 2, the literature on residual networks and indoor RGB-D semantic segmentation is previewed. The architecture of RedNet and the idea of pyramid supervision are stated in detail in section 3. In section 4, the comparative experiments are conducted to evaluate the efficiency of the model. Finally, we draw a conclusion of this paper in section 5.</p><p>Before ending this section, the main contributions of this paper are listed as the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>A novel residual encoder-decoder architecture (termed RedNet) is proposed for indoor RGB-D semantic segmentation, which applies residual module as the basic building block in both the encoder path and decoder path. 2. A pyramid supervision training scheme is proposed to optimize the network, which applies supervised learning over different layers on the upsample path of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Two comparison experiments are conducted on SUN RGB-D benchmark to</head><p>verify the effectiveness of the proposed RedNet architecture and the pyramid supervision training scheme.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Residual Networks</head><p>Residual network was first proposed by He et al. in <ref type="bibr" target="#b15">[16]</ref>. In their work, they analyzed the problem of model degradation, which present as saturation and then degradation of accuracy as the network depth increasing. They argued that the degradation problem is an optimization problem, and as the depth of the network increase, the network gets harder to train. It was assumed that the desired mapping of a convnet is comprised of an identity mapping and a residual mapping. Therefore, a deep residual learning framework is proposed. Instead of letting a convnet learn the desired mapping, it fits the residual mapping and uses shortcut connection to merge it with the identity input. With this configuration, the residual network become easy to optimize and can enjoy accuracy gains from greatly increased depth. Veit et al. <ref type="bibr" target="#b35">[36]</ref> presented a complementary explanation of the increased performance of residual networks, i.e., the residual networks avoid the vanishing gradient problem by introducing the short paths between input and output. Later, He et al. <ref type="bibr" target="#b16">[17]</ref> analyzed the propagation formulations behind the connection mechanisms of residual networks and proposed a new structure of residual unit. In their work, they extended the depth of a deep residual networks to 1001 layers. Zagoruyko et al. <ref type="bibr" target="#b39">[40]</ref> investigated the memory consumption of residual networks and propose a novel residual unit that aims to decrease depth and increase width of a deep residual network. The idea of residual learning was later adopted to architectures for semantic segmentation task. Pohlen et al. <ref type="bibr" target="#b29">[30]</ref> proposed a fully convolutional network with residual learning for semantic segmentation in street scenes. The network has an encoder-decoder architecture and applies residual module on the skipconnection structure with the full-resolution residual units (FRRUs). Quan et al. <ref type="bibr" target="#b30">[31]</ref> presented a FCN architecture, named FusionNet, for connectomics image segmentation. Instead of using residual block on skip-connection structure, Fu-sionNet applies them on each layer in the encoder and decoder path along with standard convolution, max-pooling, and transpose of convolution <ref type="bibr" target="#b27">[28]</ref>. Similarly, Drozdzal et al. <ref type="bibr" target="#b7">[8]</ref> studied the importance of skip-connection in biomedical image segmentation, showing that the "short skip connections" in residual module is more effective than the "long skip connections" between encoder and decoder on biomedical image analyzing. Yu et al. <ref type="bibr" target="#b38">[39]</ref> combined the idea of residual networks and dilated convolution to build a dilated residual networks for semantic segmentation. In their paper, they also studied the gridding artifact introduced by dilation convolution and developed a 'degridding' method to removing these artifacts. Dai et al. <ref type="bibr" target="#b6">[7]</ref> used ResNet-101 as the basic network and apply the Multi-task Network Cascades for instance segmentation. Lin et al. <ref type="bibr" target="#b24">[25]</ref> and Lin et al. <ref type="bibr" target="#b23">[24]</ref> also used ResNet structure as a feature extractor and employed a multi-path refinement network to exploits information along the down-sampling process for full resolution semantic segmentation.</p><p>In 2017, Chaurasia et al. <ref type="bibr" target="#b2">[3]</ref> proposed a encoder-decoder architecture (named LinkNet) for efficient semantic segmentation. The LinkNet architecture uses ResNet18 as the encoder and applies the bottleneck unit in the decoder for feature upsample. Under this efficient configuration, the network achieve stateof-the-art accuracy on several uban street dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>. Inspired by this work, we propose a straightforward encoder-decoder structure that apply residual unit on both the downsample path and upsample path, and employs the pyramid supervision to optimize it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indoor RGB-D Semantic Segmentation</head><p>Currently, the accurate indoor semantic segmentation is still a challenging problem due to the high similarity of color and structure between objects, and the non-uniform illumination in indoor environment. Therefore, some work started utilizing the depth information as the complementary information to solve the problem. For instance, Koppula et al. <ref type="bibr" target="#b21">[22]</ref> and Huang et al. <ref type="bibr" target="#b17">[18]</ref> used depth information to build 3D point clouds of full indoor scenes, and applied graphical model to capture features and contextual relations of objects in RGB-D data for semantic labeling. Gupta et al. <ref type="bibr" target="#b12">[13]</ref> proposed a superpixel-based architecture for RGB-D semantic segmentation in indoor scene. Their method applied superpixel regions extraction on RGB image and feature extraction of each superpixel on RGB-D data, then employ Random Forest (RF) and Support Vector Machine (SVM) to classify each superpixel and build a full-resolution semantic map. Later, Gupta et al. <ref type="bibr" target="#b13">[14]</ref> improved this segmentation model by introducing a HHA encoding for depth information and use a Convolutional Neural Network (CNN) for feature extraction. In HHA encoding, depth information is encoded into three channel, i.e., horizontal disparity, height above ground, and angle between gravity &amp; surface normal. These implied that the HHA encoding emphasize the geocentric discontinuities in the image.</p><p>After the release of several indoor RGB-D datasets <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, many researches started employing deep learning architectures for indoor semantic segmentation. Couprie et al. <ref type="bibr" target="#b5">[6]</ref> presented a multi-scale convolutional network for indoor semantic segmentation. The study showed that the recognition of object classes with similar depth appearance and location is improved when incorporating the depth information. Long et al. <ref type="bibr" target="#b26">[27]</ref> applied FCNs structure on indoor semantic segmentation and compare different inputs to the network, including three channel RGB, stacked four channel RGB-D, and stacked six channel RGB-HHA. The research further showed that the RGB-HHA input outperform all other input form, while the RGB-D have similar accuracy with RGB input. Hazirbas et al. <ref type="bibr" target="#b14">[15]</ref> presented a fusion-based encoder-decoder FCNs for indoor RGB-D semantic segmentation. Their work shows that the HHA encoding does not hold more information than the depth itself. In order to fully utilize the depth information, they apply two branches of convolutional network to compute RGB and depth image respectively and apply features fusion on different layers. Based on the same depth fusion structure, our previous work <ref type="bibr" target="#b20">[21]</ref> proposed a DeepLabtype architecture <ref type="bibr" target="#b3">[4]</ref> that applies depth incorporation on a dilated FCNs and build a RGB-D conditional random field (CRF) as the post-process.</p><p>In this work, we will also apply depth fusion structure on the downsample part of the network, and apply skip-connection to bypass the fused information to the decoder for full-resolution semantic prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RedNet Architecture</head><p>The architecture of RedNet is presented in <ref type="figure" target="#fig_1">Fig. 2</ref>. For clear illustration, we use blocks with different color to indicate different kinds of layers. Notice that each convolution operation in RedNet is followed by a batch normalization layer <ref type="bibr" target="#b18">[19]</ref> before relu function, and it is omitted in the figure for simplification.</p><p>The upper half of the figure up to Layer4/Layer4 d is the encoder of the network, it has two convolutional branches, i.e., the RGB branch and the Depth branch. Structures of both encoder branches can be adopted from one of the five ResNet architectures proposed in <ref type="bibr" target="#b15">[16]</ref>, in which we remove the last two layers of ResNet, i.e., the global average pooling layer and fully-connected layer. The RGB branch and the Depth branch in the model have the same network configuration, except that the convolution kernel of Conv1 d on Depth branch has only one feature channel, since the Depth input presented as an one channel gray image. The encoder starts with two downsample operation, which is the 7 ? 7 convolution layer with stride two and a 3 ? 3 max-pooling layer with stride two. This max-pooling is the only pooling layer in the whole architecture, all other downsample and upsample operations in the network are implemented with two-stride convolution and transpose of convolution. The following layers in encoder are residual layers with different numbers of residual unit. It is worth pointing out that only Layer1 in the encoder does not have downsample unit, and all other ResLayer have one residual unit that downsample the feature map and increase the feature channel by a factor of 2. The Depth branch ending at Layer4 d, and its features are fused into RGB branch on five layers. Here, element-wise summation is performed as the feature fusion method. The lower half of <ref type="figure" target="#fig_1">Fig. 2</ref>, starting with Trans1 layer, is the decoder of the network. Here, except the Final Conv layer, which is a single 2 ? 2 transpose of convolution layer, all other layers in the decoder are residual layers. The first four layers, i.e., the Trans1, Trans2, Trans3, and Trans4, have one upsample residual unit to upsample the feature map by a factor of 2. Different from the bottleneck building block in the encoder, we employ the standard residual building block <ref type="bibr" target="#b15">[16]</ref> in the decoder that have two consecutive 3 ? 3 convolution layers for residual computation. With regard to the upsample operation, we present a upsample residual unit that is shown in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we compare the downsample unit in ResNet-50 and ResNet-34, as well as the upsample unit we propose in the decoder. Here, for Conv[(k, k), s, * /c], (k, k) means the spatial size of the convolution kernel. Parameter s is the stride of the convolution, and c is the increase or decrease factor of the output feature channel. Red block denotes the convolution that changes the spatial size of the input feature map, i.e., downsample or upsample. For example, a Conv[(2, 2), 0.5, /2] in red means a 2 ? 2 kernel size transpose of convolution that upsample the width and height of the feature map by a factor of 2 and decrease the feature channel by a factor of 2.  <ref type="table" target="#tab_1">Table 1</ref> shows the network configuration when using ResNet-50 as the encoder, here m denotes the number of input feature channel, n denote the number of output feature channel, and l unit denote the number of residual unit in that layer. The upsample ResLayer has different residual unit order compared with the downsample ResLayer. The downsample layer starts with a downsample residual unit and followed by several residual units, by contrast, the upsample layer starts with several residual unit and ends with one upsample residual unit. As shown in the table, the output of residual layer in ResNet-50 encoder has large channel size since it use channel expansion. Therefore, we employ the Agent layers shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which are single 1 ? 1 convolutional layer with strides one. It is designed to project the feature map for lower channel size, allowing the decoder to have a lower memory consumption. Notice that the agent layers only exist when ResNet-50 is employed, they will be removed when the encoder employ ResNet-34 structure. This is because it does not have channel expansion on residual unit. In addition, we also remove skip-connection between output of Conv1 and output of Trans4 on ResNet-34 encoder setting for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pyramid Supervision</head><p>The pyramid supervision training scheme alleviate the gradient vanishing problem by introducing supervised learning over five different layers. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the algorithm compute four intermediate outputs from feature maps of four upsample ResLayer in addition to the final output, these intermediate outputs are called side outputs. Each side output score map is computed using a convolution layer with 1 ? 1 kernel size and stride one. Therefore, all outputs have different spatial resolutions. The final Output of RedNet is a full resolution score map, while the side outputs Out4, Out3, Out2, and Out1 are downsampled. For instance, the Out1 has 1/16 the height and width of the Output. The four side outputs and the final output are then feed into a softmax layer and cross entropy function to build the loss function.</p><formula xml:id="formula_0">Loss(s, g) = 1 N i ? log exp(s i [g i ]) k exp(s i [k])<label>(1)</label></formula><p>More concretely, the loss function of each output has the same form shown in Eq. 1. Here, g i ? R denote the class index on the groundtruth semantic map on location i. s i ? R Nc denote the score vector of the network output on location i with N c being the number of classes in the dataset. N denotes the spatial resolution of the specific output. When dealing with the loss function of Out1 to Out4, the groundtruth map g is downsampled using nearest-neighbor interpolation. The overall cross entropy loss is thus the summation of all five cross entropy losses over five outputs. Notice that instead of assigning equallyweighted loss on pixels in different outputs, these overall loss configuration assign more weight on pixels of downsampled output, e.g., Out1. In practice, we find that this configuration provide better performance than the equally-weighted loss configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we evaluate the RedNet architectures with ResNet-34 and ResNet-50 as the encoder using the SUN RGB-D indoor scene understanding benchmark suit <ref type="bibr" target="#b33">[34]</ref>. The SUN RGB-D dataset is currently the largest RGB-D indoor scene semantic segmentation dataset It has 10,335 densely annotated RGB-D images taken from 20 different scenes, at a similar scale as the PASCAL VOC RGB dataset <ref type="bibr" target="#b9">[10]</ref>. It also include all images data from NYU Depth v2 dataset <ref type="bibr" target="#b32">[33]</ref>, and selected images data from Berkeley B3DO <ref type="bibr" target="#b19">[20]</ref> and SUN3D <ref type="bibr" target="#b36">[37]</ref> dataset.</p><p>To improve the quality of the depth map, the paper proposes a algorithm that estimates the 3D structure of the scene from multiple frames to conduct depth denoising and fill in the missing values. Each pixel in the RGB-D images is assigned a semantic label in one of the 37 classes or the 'unknown' class. In the experiment evaluation, we use the default trainval-test split of the dataset that has 5285 training/validation instances and 5050 testing instances to evaluation our proposed RedNet architecture.</p><p>Training Images in SUN RGB-D dataset were captured by four different kinds of sensors with different resolutions and fields of view. In the training step, we resize all RGB images, Depth images, and the Groundtruth semantic maps into a 480 ? 640 height and width spatial resolution, additionally, the Groundtruth maps are further resized into four downsampled maps with resolution from 240 ? 320 to 30 ? 40 for pyramid supervision of the side output. Here, the RGB images are applied bilinear interpolation while the Depth images and Groundtruth maps are applied nearest-neighbor interpolation. During training, the inputs and Groundtruths data are augmented by applying random scale and crop and the input RGB images are further augmented by applying random hue, brightness, and saturation adjustment. In addition, we calculate the mean and standard deviation of the RGB and Depth images in the whole dataset to normalize each input value.</p><p>The two networks in the experiment, i.e., the RedNet (ResNet-34) and Red-Net (ResNet-50), share the same training strategy and have the identical values of all hyperparameters. We use the PyTorch deep learning framework <ref type="bibr" target="#b28">[29]</ref> for implementation and training of the architecture 1 . The encoder of the network is pretrained on the ImageNet object classification dataset <ref type="bibr" target="#b22">[23]</ref>, while the parameters on other layers are initialized by the Xavier initializer <ref type="bibr" target="#b11">[12]</ref>. Since the imbalance of pixels of each class presented in the dataset, we reweight the training loss of each class in the cross-entropy function using the median frequency setting proposed in <ref type="bibr" target="#b8">[9]</ref>. That is, we weight each pixel by a factor of ? c = median prob/prob(c), where c is the groundtruth class of the pixel, prob(c) is the pixel probability of that class, median prob is the median of all the probabilities of these classes. The network is training with momentum SGD as the optimization algorithm. The initial learning rate of all layers are set to 0.002 and will decay by a factor of 0.8 in every 100 epochs. The momentum of the optimizer is set to 0.9, and a weight decay of 0.0004 is applied for regularization. The network is trained on a NVIDIA GeForce GTX 1080 GPU with a batch size of 5, and we stop the training when the loss no longer decrease. Evaluation The network is evaluated on the default testing set of SUN RGB-D dataset. Three criterias for segmentation tasks are used to measure the performance of the network under 5050 testing instances, i.e., the pixel accuracy, the mean accuracy and the intersection-over-union (IoU) score. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison result of RedNet and other state-of-the-art methods on SUN RGB-D testing set. As we can see in the table, the proposed RedNet(ResNet-34) and RedNet(ResNet-50) architecture outperform most of the exist methods. Here, the FuseNet-SF5 <ref type="bibr" target="#b14">[15]</ref> and DFCN-DCRF <ref type="bibr" target="#b20">[21]</ref> networks use the same depth fusion technique in RedNet for depth incorporation. The RefineNet-152 <ref type="bibr" target="#b24">[25]</ref> and CFN (RefineNet-152) <ref type="bibr" target="#b23">[24]</ref> architecture use the same residual network in RedNet for feature extraction. Notice that, these two architectures are both using ResNet-152 structure for feature extraction, while RedNet performs a 47.8% accuracy using the ResNet-50 as the encoder. It also worth notice that the RedNet(ResNet-34) network and the RedNet(ResNet-50) network share the same decoder structure, and the comparison result shows that the deeper structure of encoder in RedNet(ResNet-50) provides a better performance.</p><p>In addition, to show that the pyramid supervision training scheme is able to effectively improve the performance of the network, a experiment is conducted to compare the performance of the proposed RedNet architectures trained with and without pyramid supervision. The result is shown in <ref type="table">Table 3</ref>. It shows that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a RGB-D encoder-decoder residual network named Red-Net for indoor RGB-D semantic segmentation. The RedNet combines the short skip-connection in residual unit and the long skip-connection between encoder and decoder for an accurate semantic inference. It also applies fusion structure in the encoder to incorporate the depth information. Moreover, we present the pyramid supervision training scheme that apply supervised learning over several layers on the decoder to improve the performance of the encoder-decoder network. The comparative experiment shows that the proposed RedNet architecture with pyramid supervision achieves state-of-the-art result on SUN RGB-D dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overall structure of the proposed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Layer configuration of the proposed RedNet (ResNet-50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Downsample and upsample residual unit. (a): a downsample residual unit in (ResNet-50) encoder. (b): a downsample residual unit in (ResNet-34) encoder. (c): a upsample residual unit we propose in decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Prediction of side outputs and final output the pyramid supervision improve the performance of the network on all three criterias. Notice that the ResNet-34 encoder RedNet with pyramid supervision training scheme outperform the ResNet-50 encoder RedNet without pyramid supervision, this fully demonstrate the effectiveness of pyramid supervision. The testing prediction of side outputs and final output can be obtained in Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>employ dilated arXiv:1806.01054v2 [cs.CV] 6 Aug 2018</figDesc><table><row><cell>RGB</cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell>Skip Connection</cell><cell></cell></row><row><cell>Depth</cell><cell>Depth Fusion</cell><cell>Pyramid Output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Encoder (ResNet-50) and Decoder configuration</figDesc><table><row><cell>Block</cell><cell cols="2">Encoder</cell><cell>Block</cell><cell cols="2">Decoder</cell></row><row><cell>m</cell><cell>n</cell><cell>lunit</cell><cell>m</cell><cell>n</cell><cell>lunit</cell></row><row><cell cols="3">Layer4 1024 2048 3</cell><cell cols="3">Trans1 512 256 6</cell></row><row><cell cols="3">Layer3 512 1024 6</cell><cell cols="3">Trans2 256 128 4</cell></row><row><cell cols="3">Layer2 256 512 4</cell><cell cols="2">Trans3 128 64</cell><cell>3</cell></row><row><cell>Layer1 64</cell><cell cols="2">256 3</cell><cell>Trans4 64</cell><cell>64</cell><cell>3</cell></row><row><cell>Conv1 3</cell><cell>64</cell><cell>-</cell><cell>Trans5 64</cell><cell>64</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of SUN RGB-D testing results</figDesc><table><row><cell>Model</cell><cell cols="3">Pixel Mean mIoU</cell></row><row><cell>FCN-32s [27]</cell><cell>68.4</cell><cell>41.1</cell><cell>29.0</cell></row><row><cell>SegNet [1]</cell><cell>71.2</cell><cell>45.9</cell><cell>30.7</cell></row><row><cell>Context-CRF [26]</cell><cell>78.4</cell><cell>53.4</cell><cell>42.3</cell></row><row><cell>RefineNet-152 [25]</cell><cell>80.6</cell><cell>58.5</cell><cell>45.9</cell></row><row><cell>CFN (RefineNet-152) [24]</cell><cell>-</cell><cell>-</cell><cell>48.1</cell></row><row><cell>FuseNet-SF5 [15]</cell><cell>76.3</cell><cell>48.3</cell><cell>37.3</cell></row><row><cell>DFCN-DCRF [21]</cell><cell>76.6</cell><cell>50.6</cell><cell>39.3</cell></row><row><cell>RedNet(ResNet-34)</cell><cell>80.8</cell><cell>58.3</cell><cell>46.8</cell></row><row><cell>RedNet(ResNet-50)</cell><cell cols="3">81.3 60.3 47.8</cell></row><row><cell cols="5">Table 3. SUN RGB-D testing results on pyramid supervision</cell></row><row><cell>Model</cell><cell></cell><cell cols="3">Pixel Mean mIoU</cell></row><row><cell cols="3">RedNet(ResNet-34) without pyramid 80.3</cell><cell>55.5</cell><cell>45.0</cell></row><row><cell>RedNet(ResNet-34)</cell><cell></cell><cell>80.8</cell><cell>58.3</cell><cell>46.8</cell></row><row><cell cols="3">RedNet(ResNet-50) without pyramid 80.5</cell><cell>57.4</cell><cell>46.0</cell></row><row><cell>RedNet(ResNet-50)</cell><cell></cell><cell>81.3</cell><cell>60.3</cell><cell>47.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our source code will be avaliable at https://github.com/JindongJiang/RedNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence PP(99)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03718</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3572</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object-level segmentation of rgbd data. ISPRS Annals of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops on Consumer Depth Cameras for Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops on Consumer Depth Cameras for Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1168" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Incorporating depth into both cnn and crf for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07383</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic labeling of 3d point clouds for indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1311" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring context with deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03183</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08323</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fusionnet: A deep fully residual convolutional neural network for image segmentation in connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Hilderbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision Workshops</title>
		<meeting>IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
