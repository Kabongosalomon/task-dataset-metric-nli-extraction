<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-modal Representation Learning for Zero-shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
							<email>chungching.lin@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
							<email>lindsey.li@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu Microsoft</surname></persName>
						</author>
						<title level="a" type="main">Cross-modal Representation Learning for Zero-shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a cross-modal Transformer-based framework, which jointly encodes video data and text labels for zero-shot action recognition (ZSAR). Our model employs a conceptually new pipeline by which visual representations are learned in conjunction with visual-semantic associations in an end-to-end manner. The model design provides a natural mechanism for visual and semantic representations to be learned in a shared knowledge space, whereby it encourages the learned visual embedding to be discriminative and more semantically consistent. In zero-shot inference, we devise a simple semantic transfer scheme that embeds semantic relatedness information between seen and unseen classes to composite unseen visual prototypes. Accordingly, the discriminative features in the visual structure could be preserved and exploited to alleviate the typical zero-shot issues of information loss, semantic gap, and the hubness problem. Under a rigorous zero-shot setting of not pretraining on additional datasets, the experiment results show our model considerably improves upon the state of the arts in ZSAR, reaching encouraging top-1 accuracy on UCF101, HMDB51, and ActivityNet benchmark datasets. Code will be made available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition with supervised training is highly successful [9, <ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b62">60,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b71">68]</ref>, e.g., AssemblyNet++ <ref type="bibr" target="#b55">[53]</ref> and X3D <ref type="bibr" target="#b16">[17]</ref>. In comparison, zero-shot action recognition (ZSAR) generally lags behind because it requires a model to make meaningful inferences about unseen concepts, given only the data provided from seen training concepts and additional high-level semantic label information <ref type="bibr" target="#b59">[57]</ref>. Addressing the general zero-shot challenges, e.g., distribution shift and semantic gap, recent ZSAR methods mainly exploit visual features extracted from off-the-shelf pretrained action recognition models and focus on studying a more robust visual-to-semantic mapping or learning a joint embed- <ref type="bibr">Figure 1</ref>. ResT is a cross-modal transformer network, which learns visual representations along with visual-semantic associations for ZSAR. In ResT, visual tokens attend to visual tokens (modality-specific attention); Word tokens attend to visual and text tokens on the left (cross-modal attention).</p><p>ding space on which to project visual and semantic features. However, there are limitations in this typical framework.</p><p>First, visual features are usually acquired by pretrained action recognition models and remain unchanged during training (e.g., C3D <ref type="bibr" target="#b62">[60]</ref> is employed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b68">65]</ref>, I3D [9] is adopted in <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b54">52]</ref>). Hence, they may not contain enough information for learning a fair representation <ref type="bibr" target="#b37">[35]</ref>. Besides, when directly using pretrained action recognition models to extract visual features, the intrinsic zero-shot setting may not be preserved. The pretrained model acquires the knowledge of classes that should not be seen during training <ref type="bibr" target="#b54">[52]</ref>. For example, I3D was trained on the ImageNet <ref type="bibr">[13]</ref> and Kinetics <ref type="bibr" target="#b28">[28]</ref> datasets. C3D was trained on the I380K and Sports-1M datasets <ref type="bibr" target="#b27">[27]</ref>. Similar to Kinetics, between Sports-1M and UCF101 <ref type="bibr" target="#b57">[55]</ref>, they share 23 identical classes <ref type="bibr" target="#b15">[16]</ref>.</p><p>Second, action classes are usually manually labeled with limited descriptions of many complex actions, causing the semantic information is often imprecise or incomplete. For example, many action classes are only labeled by a noun (e.g., "uneven bars," "hula hoop"), or a single verb (e.g., "pour," "dive"). In contrast, videos, upon which the words are placed, are true visual reflections of various classes of actions. Visual observations are unstructured and complex. It follows, there is richer and more discriminative information in the visual feature space. Despite the visual and semantic spaces being connected, the visual discrimination after mapping or projection of the two spaces often shrinks to a certain extent <ref type="bibr" target="#b49">[47,</ref><ref type="bibr" target="#b76">73]</ref>. This affects the knowledge transferability from seen classes to unseen classes. Besides, as the projection is performed in high-dimensional embedding spaces, there inevitably exists the hubness problem <ref type="bibr" target="#b38">[36,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b75">72]</ref>, whereby some class prototypes appear to be the nearest neighbors of many irrelevant test instances.</p><p>The question is, how can the visual and semantic spaces be bridged, while at the same time maintaining the visual discrimination for an effective knowledge transfer?</p><p>In this paper, we study an end-to-end trainable crossmodal framework, named ResT (ResNet-Transformer).</p><p>ResT is able to associate both visual and semantic spaces, while preserving the descriptive and discriminative information implied in the visual embedding space. Considering the essence of ZSAR, we frame the problem in two ways: (1) Instead of using pretrained feature extractors, we attach a vanilla non-pretrained ResNet module to the Transformer to extract visual features, which is to ensure that no prior knowledge of unseen classes is acquired during training. <ref type="bibr" target="#b1">(2)</ref> Contrary to normal practice, we integrate the learning of visual representations and visual-semantic associations into the same unified architecture. This model design provides a natural mechanism for visual and semantic representations to be learned in a shared knowledge space, which can bridge the semantic gap and encourage the learned visual embedding to be discriminative and more semantically consistent.</p><p>Considering the disconnection among source and target domains, which makes great difficulty in inferring their relationships via a coarse one-on-one nearest neighbor matching, we develop a simple semantic transfer scheme for zeroshot inference. The idea is to leverage the visual-semantic associations in ResT and to composite a visual prototype for each unseen class by embedding a combination of relevant information. Specifically, in light of the observations that human activities (e.g., play basketball) are composed of a series of simple actions (e.g., run, pass, jump and shoot) or are related to a set of partial elements of other complex activities, we posit that actions are implicitly compositional <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr">39]</ref>. The semantic transfer scheme is thus formulated as a subgraph selection, based on the semantic relatedness distances of seen and unseen labels, to composite the visual prototype of each unseen class in visual space for the ZSAR task. With the transfer scheme, because the visual representations are not projected onto other spaces, the hubness problem is alleviated, and the visual distinction is preserved. Accordingly, the framework achieves good expandability in various unseen domains.</p><p>The present work follows a stricter, but more realistic zero-shot setting proposed by <ref type="bibr" target="#b4">[5]</ref>, where the set of training classes that overlap with test data are removed by a similarity threshold. The model is trained from scratch with random initial weights on one dataset and tested on three disjointed target datasets. No pre-training on auxiliary datasets is performed to ensure no prior knowledge of unseen classes acquired during training. Our approach employs the inductive configuration <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b75">72]</ref>, where the test data is entirely unknown during training.</p><p>The main contributions of this paper are:</p><p>? Our approach, based on the described cross-modal framework, bridges the visual and semantic spaces while still maintaining the visual discrimination for an effective knowledge transfer. With a single trained model on the Kinetics dataset, our framework establishes new state-of-the-art ZSAR results on UCF101 <ref type="bibr" target="#b57">[55]</ref>, HMDB51 <ref type="bibr" target="#b34">[33]</ref>, and ActivityNet <ref type="bibr" target="#b6">[7]</ref> benchmarks.</p><p>? We develop a simple yet effective semantic transfer scheme to composite unseen visual prototypes, with which ZSAR could be realized in the visual space to alleviate information loss and the hubness problem.</p><p>? Our approach has three nice properties: first, it is end-to-end trainable; second, it achieves a good accuracy-complexity trade-off; third, it offers the flexibility of utilizing different feature encoder backbones and is capable of cooperating with concurrent pretrained models for generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual and Semantic Association. In ZSAR, the visualsemantic mapping can be generally summarized in three main approaches. First, a wide range of methods <ref type="bibr">[19, 21, 42, 43, 48, 64-67, 71, 76]</ref> employ an indirect semantic-visual mapping. Both visual and semantic embeddings are projected onto a common intermediate space, and ZSAR is performed in the space. Wang and Chen <ref type="bibr" target="#b68">[65]</ref> propose a two-stage framework to learn a latent embedding space and embed the semantic representations of unseen-class labels onto it via the guidance of the landmarks. Recently, Chen and Huang <ref type="bibr" target="#b9">[10]</ref> recommend a method with human involvements to construct Elaborative Descriptions sentences from action class names using Wikipedia and Dictionary, and generate Elaborative Concepts of the objects in videos using WordNet. The method uses the BiT model <ref type="bibr" target="#b32">[31]</ref> pretrained on Ima-geNet21k [13] to predict object probabilities, and leverages objects involved in seen and unseen classes for ZSAR.</p><p>Second, instead of indirect mapping, many methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b72">69,</ref><ref type="bibr" target="#b73">70]</ref> have been proposed to project visual features into semantic embedding space and perform classifications directly in that space. Brattoli et al., <ref type="bibr" target="#b4">[5]</ref> propose an end-to-end method that jointly optimizes the visual embeddings acquired by C3D or R(2+1)D <ref type="bibr" target="#b63">[61]</ref> architectures and the semantic embeddings computed by the Word2Vec function. Third, the reverse mapping from semantic to visual embeddings is another option <ref type="bibr" target="#b40">[38,</ref><ref type="bibr" target="#b75">72]</ref> to alleviate the hubness problem. Zhang and Peng <ref type="bibr" target="#b75">[72]</ref> propose to model the joint distribution over high-level video features and semantic knowledge via the Generative Adversarial Network (GAN), in which seen-to-unseen correlation is embedded to synthesize video features of unseen categories. Mandal et al. <ref type="bibr" target="#b40">[38]</ref> adapt the conditional Wasserstein GAN with additional loss terms to synthesize unseen features for training an out-of-distribution detector. Unlike prior work using class labels to synthesize visual features via GANs, our model leverages the learned visual representations of seen classes to construct the unseen visual prototypes.</p><p>Transformer Architecture. Transformer architectures have shown exemplary performance on a wide range of applications, e.g., image recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">59]</ref>, object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b78">75]</ref>, and visual-language tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b53">51,</ref><ref type="bibr" target="#b58">56]</ref>. Among the models, the most popular ones include ViT <ref type="bibr" target="#b14">[15]</ref>, VideoBERT <ref type="bibr" target="#b58">[56]</ref>, VIVO <ref type="bibr" target="#b26">[26]</ref>, and CLIP <ref type="bibr" target="#b51">[49]</ref>. Based on a contrastive approach to learning image representations from texts, CLIP achieves amazing zero-shot transfer capabilities on many downstream tasks. CLIP is trained on a large corpus of 400 million image-text pairs. It takes 18 days to train a CLIP model with ResNet backbone on 592 units of V100 GPUs. In contrast, our model is trained from scratch on the Kinetics dataset, which consists of around 500 thousand clips. This underlines the differences in the scale of the CLIP and the proposed method. We aim to showcase the extensibility of a model trained on a dataset with limited data points to multiple disjointed test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>The conceptual diagram of our framework is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. ResT consists of a primary task and an auxiliary task. To learn discriminative visual representations, the primary task is trained under action classification without semantic cues. This task is modality-specific, without cross-talk through attention between two modalities (described in Section 3.2.2). The goal is to learn richer and structure-preserving visual embeddings for effective knowledge transfer. The auxiliary task performs masked language modeling with visual cues. For a given pair of video data and text labels in the training dataset, this task aims to take both modalities into account to predict a masked class label. This task drives the network to align the visual and semantic content (described in Section 3.2.3). Together, the joint objective enables the model to produce more semantically consistent visual representations.</p><p>After the training, with a simple semantic transfer scheme, we use the prototypes of the seen classes to composite visual prototypes of the unseen classes in the learned visual space by reflecting their semantic relatedness in the semantic label embedding space. During testing, as the visual representation learning task is modality-specific, the model could take a new observation with a single modality input (video data) to produce its visual representation. The representation is then classified with the nearest neighbor search by evaluating its distance to the unseen visual prototypes that are composited by the seen visual prototypes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><formula xml:id="formula_0">Let ( = {E B 8 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ResT Model Formulation</head><p>Our model consists of a frame-level feature encoder F and a cross-modal transformer T . We use a vanilla 2D ResNet <ref type="bibr" target="#b25">[25]</ref> as a base network to extract visual features because 2D models are typically more efficient than 3D models in terms of memory and runtime. The outputs of frame features are fed into the transformer network. To keep ResT computationally efficient, we take the flattened global features after the last average pooling layer in F as the visual features, which has been shown to work reasonably well on video understanding tasks <ref type="bibr" target="#b65">[63]</ref>. To ensure the restriction of zero-shot setting is preserved, both feature encoder F and transformer T are initialized with random weights. During the training, both F and T are optimized together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Model inputs</head><p>ResT takes a video-text pair ( O, Y) as an input, where O is a sequence of ) sampled frames { 1 , 2 , ..., ) } in a video and Y is the corresponding action label text. For a frame </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Visual representation learning</head><p>The primary task in ResT is to perform video classification without semantic clues. We incorporate a feature aggregator and a classifier into the final layer of the transformer to predict which action class the visual representation belongs to. Given a training dataset, this task is to train a model % to learn the visual representation G = %(z 0 ) with only visual cue r. Our transformer consists of ! layers. We denote z l as the output of ; th layers and )) as the attention mask.</p><p>To exploit all possible temporal relationships, all visual feature tokens are used with bidirectional attention to jointly encode all visual features together ( )) (A ? , A : ) = 1, 8A ? , A : ). As every visual feature token is able to attend to all other visual feature tokens, the model could leverage both long-term and short-term temporal relationships to learn a better representation. In addition, we set all visual features not to attend to any of the word tokens ( )) (A ? , F : ) = 0, 8A ? , F : ). This is to encourage the model to dedicate itself to learning the visual representations and prevent the model from behaving unexpectedly when label text input is not available during testing.</p><p>Because a transformer encoder creates an embedding for each of its feature inputs, we add a 1-hidden-layer MLP, 5 (?), as a classification head to the output of the special token [CLS], I 0 ! , to obtain a unique visual representation. The classification head, 5 (?), is used to predict the final video classes with Softmax cross-entropy loss:</p><formula xml:id="formula_1">L 2;B = ? (A)?( log ?(H B | 5 (G)). Only I 0</formula><p>! is used for classification during training, which forces the embedding of the [CLS] token to aggregate and contextualize all frame-level feature embeddings. I 0 ! serves as a video representation G. G = I 0 ! .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Visual-semantic association</head><p>The auxiliary task in ResT performs masked language modeling with visual cues. Given a pair of input (r, w), this task attends over both modalities to predict the masked tokens, reconstructing the corresponding class label. As illustrated in <ref type="figure">Figure 1</ref>, unlike visual tokens that attend to visual tokens only, word tokens attend to all visual and word tokens on their left side. The task is designed to align visual-semantic concepts and serve as a bridge to connect the visual and semantic spaces. We apply the Masked Token Loss (MTL) to the discrete token sequence w for training. This is similar to the standard task of Masked Language Modeling (MLM) in BERT <ref type="bibr" target="#b13">[14]</ref>. Each input token in F is masked at random with a probability of 15%, but the minimum number of masked tokens is set to one to ensure MLM takes part in each iteration. The masked one, F U , is replaced with a special token <ref type="bibr">[MASK]</ref>. The objective of the training is to predict these masked tokens based on their surrounding word tokens w \" and all visual feature tokens r by minimizing the negative log-likelihood:</p><formula xml:id="formula_2">L &lt;C; = ? (A,F)?( log ?(F U |w \" , r).</formula><p>This task leads the model to grasp the dependencies between visual and semantic contents, thus driving the network to align the visual and semantic concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training</head><p>The loss of L 2;B is used to guide the model in learning discriminative visual representations, and the loss of L &lt;C; is used to associate visual and semantic representations. By optimizing both loss functions in the same network, the model is trained to learn a more semantically consistent visual representation. The final training objective is the sum of both losses: L = L 2;B + l &lt;C; ? L &lt;C; , where l &lt;C; is the weighting of L &lt;C; .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Zero-shot Action Recognition</head><p>After the training, we generate a visual prototype i B 8 for each seen class ;</p><formula xml:id="formula_3">B 8 : i B 8 = 1 # B 8 ?# B 8 @=1 G B,8 @ , 8 = 1, ...,^, where G B,8</formula><p>@ is @ th visual representation of seen class ; B 8 , and # B 8 is the total number of videos in the seen class ; B 8 . As ResT is designed to learn more semantically consistent visual representations, our supposition is that, the underlying embeddings share a co-occurrence relationship to some extent when the video-text pairs describe the same thing (e.g., the visual of "jump" &amp; the text semantic of "jump") <ref type="bibr" target="#b77">[74]</ref>. The idea of our transfer scheme is to use the outputs of the learned video-text pairs as anchors to build each unseen visual prototype, i D 9 . We first obtain the most representative and distinctive bidirectional relationships between the seen and unseen class labels based on their semantic relatedness, and then embed the combination of this relevant information into the learned visual space to composite i D 9 . A weighted sum is commonly used as a combination operator <ref type="bibr" target="#b5">[6]</ref>. The proposed scheme is only applied in the testing phase, without employing any video instance from unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Semantic relatedness transfer</head><p>Given a set of W unseen class labels</p><formula xml:id="formula_4">! D = {; D 1 , ..., ; D W } and^seen class labels ! B = {; B 1 , ..., ; B } with their cor- responding semantic embeddings ? D = {b D 1 , ..., b D W } and ? B = {b B 1 , ..., b B } where ! D \ ! B = ?,</formula><p>our task is to link the best sets of the seen labels ! B to the unseen labels ! D subject to a set of constraints, such that we could obtain representative and distinctive composite visual prototypes for the unseen classes. This task can be formulated as a subgraph selection problem. Each class label represents a node. In this problem, for each of the unseen class nodes, we aim to construct a subgraph with possibly different sets of edges to the seen class nodes.</p><p>We first construct a semantic relatedness matrix S 2 R W?^w here each element &lt; 9,8 in S is defined as the semantic relatedness between b D 9 and b B 8 . Here, &lt; 9,8 is measured by cosine similarity: &lt; 9,8 = 2&gt;B(b 9 , b 8 ). Let 2 R W?d enote a binary adjacency matrix, where each element 0 9,8 in is a binary-valued variable that indicates whether the unseen class ; D 9 and the seen class ; B 8 are connected. We formulate this problem of subgraph selection as below. The goal is to find the assignments to the variables 0 9,8 in , which maximizes the objective function, with the space of the solutions bounded by a set of linear constraints. (1)</p><formula xml:id="formula_5">s.t. 0 9,8 ? 1 ; B 8 2N ; D 9 ,<label>(2)</label></formula><formula xml:id="formula_6">0 9,8 ? ? A ( 9, 8),<label>(3)</label></formula><formula xml:id="formula_7">' 8 0 9,8 ? d,<label>(4)</label></formula><p>where represents the element-wise multiplication; 1 is an index function; N ; D 9 denotes ; D 9 's K-nearest neighbors according to a distance function 1 &lt; 9,8 ; ? A ( 9, 8) is used to impose a relative distance constraint; d is a threshold. We define ? A ( 9, 8) as:</p><formula xml:id="formula_8">? A ( 9, 8) = ( 1, otherwise 0, min :&lt; 9 ( 1 &lt; :,8 1 &lt; 9,8 ) ? o,<label>(5)</label></formula><p>where o is a threshold. With the constraint in Eq. 2, only relatively close-related nodes are considered. Eq. 3 and Eq. 4 together promote the composited visual prototypes of unseen classes to be distinctive from one another. We use an Integer Programming (IP) <ref type="bibr" target="#b11">[12]</ref> to perform our search. With the IP solution? , each row in? corresponds to an optimal subgraph. Specifically, in the 9 th row, all of the seen class ; B 8 with 0 9,8 = 1 are selected to composite the visual prototype of the unseen class? D 9 . To preserve the semantic relatedness,? D 9 is calculated as follows:</p><formula xml:id="formula_9">i D 9 = 1 ? 0 9,8 ? &lt; 9,8' 8=1 0 9,8 ? &lt; 9,8 ? i B 8 .<label>(6)</label></formula><p>We follow a common protocol to compute the semantic embeddings of class labels <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b54">52]</ref>. Given a class label with # 2 words 2 1 , ..., 2 # 2 , we adopt Word2Vec [41] to calculate the semantic embeddings b of the class 9 by averaging the word vectors to obtain a single fixed-size embedding vector:</p><formula xml:id="formula_10">b 9 = 1 # 2 ? # 2 :=1 F2E(2 : ) 2 R 300 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Zero-shot evaluation</head><p>With {? D 9 }, we can now perform the ZSAR in the learned space. For any unseen video instance (E D , r D ), we first input r D into the learned model % to extract its visual representa-</p><formula xml:id="formula_11">tion G D . G D = %(z D 0 ), where z D 0 = [[CLS], r u , [MASK]].</formula><p>The zero-shot recognition of E D is achieved by evaluating its distance to the composite visual representations of the unseen classes. We assign the unseen label H ? 2 ! D to the unseen video instance E D as follows: H ? = arg min 9 3 (G D ,? D 9 ), where 3 (?) denotes cosine distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets and training/evaluation protocol. We train our model on the Kinetics dataset <ref type="bibr" target="#b28">[28]</ref> and perform evaluations on three action recognition datasets: UCF101 <ref type="bibr" target="#b57">[55]</ref>, HMDB51 <ref type="bibr" target="#b34">[33]</ref>, and ActivityNet <ref type="bibr" target="#b6">[7]</ref>. For a fair comparison, we adopt the same protocol introduced in <ref type="bibr" target="#b4">[5]</ref> to remove a source category if its label is semantically identical to one of the target categories by a similarity threshold. This results in a subset of Kinetics 700 with 664 classes when classes are removed with respect to UCF [ HMDB, and a subset of 605 classes considering UCF [ HMDB [ ActivityNet <ref type="bibr" target="#b4">[5]</ref>. We refer the reader to the E2E <ref type="bibr" target="#b4">[5]</ref> paper for the detailed procedure. Our models are only trained once on the Kinetics dataset, without additional training on 50% of the target datasets. In the 0/50 (seen/unseen) split, we randomly select half classes from the test dataset, randomly generate 10 splits and report the averaged results.</p><p>Implementation details. We sample ) frames with stride 6 as a clip from each video at a random start time. Each frame is randomly cropped to a 224 ? 224 patch. We design three models, named ResT 18, ResT 34, and ResT 101, where we replace the backbone of feature encoder F with ResNet-18 (512D), ResNet-34 (512D), and ResNet-101 (2048D). To maintain a similar magnitude of computation complexity, the corresponding sampled frames for three models are 16, 8, and 4 frames. We use one clip for training and 25 clips for testing per video. The transformer in ResT consists of 12 layers with a hidden size of 768D.</p><p>All our models are trained from scratch with random initial weights. We observe that directly optimizing the tasks in the encoder F and the transformer T with randomly initialized weights is not effective. Thus, we divide the training process into two stages: warm-up and joint-training. For the warm-up, we train only F for 150 epochs using SGD with a learning rate of 0.01 and weight decay of 0.0001. Then the whole pipeline is jointly trained for 50 epochs using AdamW with a learning rate of 0.00015 and weight decay of 0.05. The cosine learning rate scheduler is used on both stages. All experiments are performed on the Nvidia Tesla V100 GPUs. The mini-batch size is 16 clips per GPU. To train ResT focusing on visual representation learning, we set the MTL loss weight, l &lt;C; , to 0.5. For semantic relatedness transfer, the coefficients (o , , d) are obtained with 5-fold cross-validation on the training class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>We compare our approach with recent state-of-the-art inductive ZSAR methods. The experimental results on 50% class split, summarized in <ref type="table" target="#tab_1">Table 1</ref>, show that our approach is competitive among these leading methods, achieving 54.7% and 39.3% top-1 accuracy on UCF101 and HMDB51 with  ResT 18. We attribute the considerable gain in accuracy to the reduced information loss in our unified cross-modal framework, which is designed to exploit visual discriminations for effective knowledge transfer. The last two sets of results demonstrate that the performance could be further improved with deeper networks while keeping similar computational complexity in GFLOPs (shown in the ablations). Due to the domain shift issue, few studies have explored the more realistic cross dataset configuration, where the model is trained on one independent dataset and is evaluated on other disjoint datasets. As both E2E <ref type="bibr" target="#b4">[5]</ref> and the proposed method adopt this configuration, we are able to perform further comparisons on 0/100 (seen/unseen) full dataset test. In <ref type="table" target="#tab_2">Table 2</ref>, we compare our models with the best-performing model of E2E, E2E R(2+1)D. Overall, our baseline ResT 18 achieves noticeable improvements on three datasets (e.g., 5.9% absolute gains in top-1 accuracy on 0/100 UCF101).</p><p>Next, we visualize a random subset of testing video in-    stances on UCF101 in <ref type="figure" target="#fig_4">Figure 4</ref>, where each color corresponds to an unseen class label. It can be seen from the t-SNE plot that most of the test samples belonging to the same classes are in the vicinity, which shows that our model could learn discriminative visual representations. Overall, we observe that while the unseen composite prototypes deviate from the centroid of the corresponding instances, they are still distinctive enough to classify various unseen classes. Qualitatively, the results provide supports for the effectiveness of the proposed semantic transfer scheme. One example of the unseen prototypes composited from seen classes is that "clap" is composited by "shouting," "singing," "saluting," and "dancing macarena" with semantic relatedness </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>For a controlled evaluation, unless noted otherwise, the ablation results are performed using the same training recipe with randomly initialized weights in ResT 18 (Kinetics 664) model on the UCF101 dataset.</p><p>Importance of the Transformer architecture. We first replace the Transformer with RNN and LSTM architec-tures to validate the rationale of choosing a Transformer as our main architecture. In the experiment with a single RNN/LSTM architecture taking both modalities, we observe the models converge to a trivial solution as the models rely on text modal for classifying actions. Thus, we experiment with a two-stream approach <ref type="bibr" target="#b76">[73]</ref>. <ref type="table" target="#tab_3">Table 3</ref>(a) shows that the RNN/LSTM models perform substantially worse than the Transformer model. The difference is that, when compared to RNN/LSTM, a great advantage in Transformer architecture for our task is the controllable attention mechanism )) (?) and the special token <ref type="bibr">[CLS]</ref>. Controlling the amount of information obtained through the attention and from the specific tokens, our model design encourages the visual and semantic representations to be learned in a shared knowledge space while still preserving visual distinctions.</p><p>Impact of CLS and MLM components. The ResT consists of two major components: classification (CLS) and masked language modeling (MLM). To evaluate the importance of each component, we train several models, turning them on and off. The proposed transfer scheme is not applicable to CLS-only or MLM-only models as two spaces are not associated. In the evaluation, a seen labelH B is first assigned to unseen E D by a cosine distance, and ZSAR is then performed by the nearest neighbor (NN) search between seenH B and unseen {H D }. <ref type="table" target="#tab_3">Table 3</ref> <ref type="bibr">(b)</ref> shows that CLS-only and MLM-only models obtain inferior results. The auxiliary MLM is served as a bridge to align semantic and visual concepts. It is designed not to directly involve in the video classification task. Although the direct benefit from MLM is modest, the inclusion of MLM is essential because it relates two spaces and enables the application of the transfer scheme. Consequently, the framework could exploit the visual capacity to a greater extent. The last row shows that our design choices of CLS, MLM, and transfer improve results and lead to higher Top-1 scores.</p><p>Attention mechanism. The way contextual information is explored via the attention mechanism is a key factor of the proposed method. To validate our design choice, we experiment with a version of ResT 18 with cross-modality atten-tion in both CLS and MLM, where both visual and word tokens attend to visual and text contexts. In <ref type="table" target="#tab_3">Table 3</ref>(c), We notice it leads to substantial performance degradation. After 20 epochs, the training accuracy reaches 100%. This is because CLS relies solely on semantic cues for the classification task, making it fail to learn informative visual representations. The benefit of single-modality attention in CLS is that the task is constrained in the same modality, rather than absorbing the complementary information from the other modality. It also prevents the model from performing unexpectedly when it takes only a single-modality video input during inference.</p><p>Performance of different feature encoder backbones. We experiment with variants, replacing the ResNet feature encoder with different 2D/3D CNN backbones: VGG16, MobileNetV2, GoogleNet and R(2+1)D. No pretrained weights are used in all variants. Table 4(a) reports the computational complexity and accuracy evaluated on 25 clips. Among the 2D backbones, the model with MobileNetV2 achieves reasonable accuracy while using much less computation cost. We can observe that the model with a 3D CNN backbone (R(2+1)D) achieves higher accuracy at the cost of higher computational complexity.</p><p>Accuracy-complexity trade-off. Table 4(b) compares the accuracy and computation cost with respect to different numbers of clips in inference. The results of another endto-end training method, E2E R(2+1)D, and the SOTA, ER <ref type="bibr" target="#b9">[10]</ref>, are included for reference. At inference, we randomly sample # 4 clips (e.g., # 4 =1 and 25) from each video and average these # 4 clip predictions to obtain the final results. The result shows that using one clip in ResT 18 is only within 1% loss in accuracy compared to using 25 clips. We hypothesize that, with discriminative visual information, one clip might contain sufficient information for recognizing actions. We also study the effect of using fewer frames for training. As stated in implementation details, to achieve computational efficiency, the number of frames sampled for training and testing in <ref type="bibr">ResT 18,</ref><ref type="bibr">ResT 34,</ref><ref type="bibr">and</ref> ResT 101 are 16, 8, and 4 frames. In <ref type="table" target="#tab_4">Table 4</ref>(b), we observe that training on a deeper network with fewer frames gives a better trade-off between computation cost and accuracy.</p><p>ZSAR in different spaces. In this experiment, we disable the transfer scheme and map each representation of unseen instance to the semantic space using different word embedding approaches (e.g., Word2Vec, GloVe <ref type="bibr" target="#b47">[45]</ref>, Sent2Vec <ref type="bibr" target="#b46">[44]</ref>). We also evaluate the proposed semantic transfer scheme using different word embeddings to perform ZSAR in the visual space. As shown in <ref type="table" target="#tab_5">Table 5</ref>, it can be seen that the classification accuracy in the semantic space is noticeably lower than that in the visual space. This could be due to the information loss in the mapping process. The semantic information is generally incomplete and less discriminative; thus, the model's generalization capability is constrained.</p><p>In contrast, our model achieves consistently higher accuracy with the proposed semantic transfer scheme using different word embeddings. We hypothesize that it is mostly because the discriminative property in the visual space is substantial. Also, because the design of the transfer scheme is to embed a combination of the most representative and distinctive information, the proposed framework is thus less prone to the hubness problem and the bias with NN search.</p><p>Generalization. While we propose a framework in which the model is not pretrained on additional datasets, the proposed model is flexible and capable of cooperating with other pretrained models. In this experiment, instead of integrating a ResNet module to encode visual features, we consider three cases to showcase the generalization of the proposed framework: (i) take pretrained visual features by a C3D model; (ii) integrate a pretrained C3D module and perform end-to-end training with our model; (iii) take pretrained object features <ref type="bibr" target="#b2">[3]</ref>. In <ref type="table">Table 6</ref>, the first two rows of results show our model is able to cooperate with concurrent models, and the end-to-end training allows our model to finetune task-specific features for performance enhancement. For example, compared to directly taking pretrained C3D features, incorporating the pretrained C3D model to train end-to-end with our model boosts the top-1 accuracy on UCF from 40.5% to 52.7%. In the object experiments, considering some unseen classes (e.g., "balance beam") are simply named for primary objects in videos, we only take object features into the model, but not object labels. This is to prevent the model achieving high accuracy by matching object names instead of recognizing actions. <ref type="table">Table 6</ref> shows that our model is able to handle contextual information in object features and make the classification of actions relatively effective. In sum, beyond the proposed e2e training scheme, it is also suitable to adopt our framework to explore other ZSAR solutions. More details and our limitations are discussed in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have explored a cross-modal transformer-based framework that could establish effective knowledge transfer for the ZSAR task, where the distributional shift, semantic gap, and hubness problem exist and affect the way by which many heretofore existing methods perform. The competitiveness of our method on three benchmark datasets suggests that preserving the discriminative capacity in the visual embedding space can be a core factor for success in ZSAR. Comprehensive ablation studies indicate several key factors and advantages of the proposed model, including Transformer model design, computational efficiency, and flexibility of e2e training with various feature encoder backbones. Further, the proposed model is capable of cooperating with concurrent pretrained models for generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This appendix presents more details of the limitations, implementation details, and extends the experimental section presented in the main manuscript.</p><p>1. Limitations. In Section A, we discuss the limitations of the proposed method. 2. Potential Negative Societal Impact. We discuss the potential negative societal impact in Section B. 3. Implementation Details. We provide network architecture of ResT, dataset statistics, and evaluation protocols in Section C. 4. Additional Ablation Study. Additional ablation studies are presented in Section D. 5. Extended Experiments. In Section E, we extend the experimental section presented in the main manuscript. We demonstrate the generalization of the proposed method using pre-trained object features as model inputs and visualize qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitations</head><p>The goal of this work is to provide a cleaner framework for zero-shot action recognition. In our setting, the model is not allowed to be pretrained on another dataset, and is evaluated on its ability to perform classification using unseen visual prototypes composited from seen visual prototypes. Although our method demonstrates its competitiveness on the benchmark datasets, it has limitations in some cases. For example, our model confuses similar actions, such as "laugh," "smile," and "chew." In these three classes, the actions mainly involve opening and shutting the jaws, but the muscle movements involved are subtle.</p><p>We also observe composite failures, e.g., for "hula hoop" where the class is named only by a noun of the main object, or for "playing daf" where the class is named by a general verb (ex: play, make, use) with a noun of a rare object. Our model is able to composite actions from other actions, but it exists a natural challenge to find relatedness for compositing out-of-distribution objects. However, if the setting of pure zero-shot is relaxed, our model could extend its capability via pretraining on another dataset, such as ImageNet. <ref type="figure" target="#fig_6">Figure 5</ref>, 6, and 7 illustrate the confusion matrixes of ResT 18 model evaluated on UCF101, HMDB51, and Ac-tivtyNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Potential Negative Societal Impact</head><p>Training and evaluating video understanding models are typically computationally intensive, which might significantly impact the environment. To alleviate this problem, we proposed a framework that reduces the computational demands for ZSAR. The potential negative impacts may include but are not limited to: (1) It poses a risk when directly applying action recognition models for decision making, especially in the health care and autonomous vehicle fields.</p><p>(2) A video action recognition model can be misused, for example for unauthorized surveillance. Ethical considerations must be addressed in a real-world application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Network architecture of ResT</head><p>We describe the detailed network architecture of ResT in this section. ResT follows the design of the transformer encoder in <ref type="bibr" target="#b64">[62]</ref>. As shown in <ref type="figure" target="#fig_5">Figure 8</ref>, the transformer encoder consists of alternating layers of multiheaded selfattention (MSA) and MLP blocks (Eq. 7, 8). Residual connections and layernorm (LN) are applied after every block. The MLP contains two fully-connected layers with a GeLU non-linearity. Our transformer consists of L layers. We denote z l as the output of ; th layers.</p><formula xml:id="formula_12">z 0 l = !# ("( (z l 1 ) + z l 1 ),<label>(7)</label></formula><formula xml:id="formula_13">z l = !# (" !%(z 0 l ) + z 0 l ),<label>(8)</label></formula><p>where ; = 1, ..., !.</p><p>ResT uses the first token, I 0 0 , to perform action classification on a source dataset. A classification head is attached to the output of the first token, I 0 ! . We append a 1-hidden-layer MLP 5 (?), which is used to predict the final video classes.</p><formula xml:id="formula_14">G = !# (I 0 ! )<label>(9)</label></formula><p>Our ResT consists of 12 transformer layers with a hidden size of 768D. The visual representation size is also 768D. The classifier weights are 768 ? 664, and 768 ? 605 corresponding to Kinetics 664 and 605 training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Datasets</head><p>We train our models on a subset of the Kinetics dataset <ref type="bibr" target="#b28">[28]</ref>, and perform evaluations on three action recognition datasets: UCF101, HMDB51, and ActivityNet. UCF101 is labeled with 101 action categories with a focus on sports and contains 13,320 videos. HMDB51 has 6,767 videos with 51 classes. ActivityNet contains 200 classes and 27,801 untrimmed videos with an emphasis on daily activities. Kinetics dataset contains 700 classes with 545,317 training videos.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Evaluation protocol</head><p>In the zero-shot evaluation, we report results on half dataset (0/50 split) and full dataset (0/100). Most prior methods use pre-trained action recognition models to extract features, followed by training a ZSL model on 50% of the target dataset and testing on the other 50% of the same dataset to alleviate domain shift problems (50/50 setting). Our work follows E2E <ref type="bibr" target="#b4">[5]</ref> to adopt a cross dataset configuration, where the models are only trained once on a source action recognition dataset and then are directly evaluated on 50% of other target datasets. The goal of 0/50 setting is to disallow tailoring ZSAR models to a specific test dataset. In the 0/50 split, we randomly choose 50% classes from the test dataset: 50 on UCF101, 25 on HMDB51, and 100 on ActivityNet. On each test set, we randomly generate 10 splits and report the averaged results. As E2E <ref type="bibr" target="#b4">[5]</ref> and our method are trained on a separate dataset, we are able to test our models on full UCF101, HMDB51, and ActivityNet datasets (0/100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Ablations</head><p>In this section, we extend the experimental section presented in the main manuscript. In this experiment, we compare our models trained on Kinetics 664 (with overlapping classes removed) with the models trained on the full Kinetics 400 and 700 datasets <ref type="bibr" target="#b28">[28]</ref> (without overlapping classes removed) to demonstrate that removing overlapping classes is a non-trivial learning constraint. The results are reported in <ref type="table" target="#tab_6">Table 7</ref>. It can be seen that the models trained on the full Kinetics dataset obtain higher Top-1 accuracy than the models trained on the sets without overlapping classes (e.g., 2.4% absolute gains in Top-1 accuracy on 0/50 configuration from K664 to K400 and 10.5% gains from K664 to K700). As discussed in the main manuscript, one has to ensure that the seen and unseen classes are disjoint and the zero-shot setting is maintained when external datasets are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Influence of removing overlapping classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Importance of constraints in semantic relatedness transfer</head><p>The design of the transfer scheme aims to embed a combination of the most representative and distinctive information for effective knowledge transfer. It follows, the proposed framework is thus less prone to the hubness problem and the bias with NN search. In this section, we discuss the importance of the constraints in the semantic relatedness transfer.</p><p>The hubness problem is related to the high-dimensional nearest neighbor search. That is, some points (hubs) frequently occur in the :-nearest neighbor set of other points. The skewness of an empirical k : distribution is typically used to measure the degree of hubness <ref type="bibr" target="#b52">[50,</ref><ref type="bibr">54]</ref>. The distribution k : is the distribution of the number of times (k : ( 9)) the 9 th prototype is in the top : nearest neighbors of the test samples. The skewness of the distribution is defined as:</p><formula xml:id="formula_15">k : B:4F=4BB = ? W 9=1 (k : ( 9) ? [k : ]) 3 +0A [k : ] 3 2 ,<label>(10)</label></formula><p>where W is the total number of test prototypes. A higher skewness value indicates a more severe hubness issue.</p><p>Here, we summarize the three constraints imposed in the semantic relatedness transfer. Constraint I is to ensure the composited unseen prototypes are representative. Constraint II and III together promote the composited visual prototypes of unseen classes to be distinctive from one another.</p><p>In <ref type="table" target="#tab_7">Table 8</ref>, we discuss the effect of the constraints in terms of the degree of hubness (k 1 B:4F=4BB ) and classification accuracy (Top-1/ Top-5). The accuracy is evaluated using one clip with ResT 18 (664/605) model. We consider five combinations: (1) Reverse transfer direction (composite semantic representation and perform ZSAR in the semantic space), <ref type="bibr" target="#b1">(2)</ref> No constraints imposed, (3) Only constraint I, (4) Only constraint II and III, (5) All constraints.</p><p>We draw several conclusions from <ref type="table" target="#tab_7">Table 8</ref>: <ref type="formula">(1)</ref> In general, we observe Top-1 accuracy is negatively affected by the presence of hubs, and the hubness problem is more likely to arise in the semantic space than the visual space. We visualize some qualitative results of ZSAR in different spaces in <ref type="figure" target="#fig_10">Figure 9.</ref> (2) Compared to 'No constraint,' all combinations of the constraints help improve the classification accuracy and alleviate the effect of the hubness problem. (3) When applying constraint I only, Top-5 accuracy is consistently higher because the constraint filters out the less related classes. (4) Constraint II and III are effective, ensuring the distinction of the composited unseen prototypes. In general, it obtains a low hubness value with these two constraints. (5) Combining all three constraints yields a filterand-refine methodology. Overall, it achieves the best Top-1 accuracy with a relatively low degree of hubness because  these three constraints together consider both representative and distinctive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extended Experiments</head><p>Although we propose a framework where no pre-training on additional datasets is performed to ensure no prior knowledge of unseen classes is acquired during training, our model is flexible and capable of cooperating with existing pre-trained models.</p><p>In the ablation study, we show the generalization of the proposed model by taking pre-trained object region features as inputs. Considering the essence of zero-shot setting, it might be arguable if using object information is incongruous with the idea of pure ZSAR because it is highly likely that some major objects occur in seen and unseen classes, and some unseen classes are simply named for objects (e.g., "yo-yo," "uneven bars," and "pommel horse"). However, modeling objects helps with the model interpretability. In this experiment, to prevent the model from achieving high accuracy by matching object names instead of recognizing actions, we only use object region features as model inputs to examine the capability of the proposed model. Detection outputs (object labels) are not used in the experiment.</p><p>In this experiment, we replace frame-level features with object region features. We start with object feature extractors, an off-the-shelf detection network, UpDown <ref type="bibr" target="#b2">[3]</ref>. The UpDown detector was trained on Visual Genome dataset <ref type="bibr" target="#b33">[32]</ref>. For a frame C sampled at time C in a video E, an amount A 0C = [A 0C 1 , ..., A 0C # C A ] of # C A object features are extracted by the detector, where A 0C : 2 R ? is a ?-dimension vector. To encode spatiotemporal information, we construct a 7-d vector B C : from the region position (normalized four corner coordinates, width, and height) and the frame index (normalized frame index offset). We concatenate object feature A 0C</p><p>: and the spatiotemporal vector B C : in order to form a spatiotemporally sensitive region vector A C : . We report the results of our model using object region features as model inputs in <ref type="table" target="#tab_8">Table 9</ref>. It shows that our model is able to handle contextual information in object features and make the classification of actions relatively effective.  <ref type="figure">Figure 10</ref>, 11, and 12 illustrate snapshots of action samples on the UCF101 <ref type="bibr" target="#b57">[55]</ref>, HMDB51 <ref type="bibr" target="#b34">[33]</ref> and ActivityNet <ref type="bibr" target="#b6">[7]</ref> that are correctly classified by our model. Each subfigure presents three sample video frames from one action clip. Each frame highlights the five most attended object regions by our network for action recognition. We observe that our model focuses on the active objects where an action is taking place and attends to the most indicative objects, e.g.,"mop handle and head" and "water bucket" in <ref type="figure">Figure  10</ref>  <ref type="figure" target="#fig_0">Figure 12</ref>(c), a sample from the class "playing beach volleyball" on ActivityNet, our model focuses on the player who sets the volleyball in the first frame, the player who steps toward the ball and bends the knee in the middle frame, and then the same player jumping and preparing for spiking the ball in the last frame. These examples demonstrate the ef-fectiveness of our transformer-based framework that learns to capture the evolution of human actions by observing the most relevant and visually descriptive objects.     Example results from our model with object region features as inputs on the classification of the "disc dog," "layup drill in basketball," "playing beach volleyball," "hitting a pinata," "throwing darts," and "tumbling" actions on ActivityNet dataset. Different bounding boxes are coded with different colors. Brighter colors depict the most attended objects. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Summary of the framework. At the training stage, ResT jointly encodes video-text pairs in a single cross-modal Transformer to learn discriminative and more semantically consistent visual representations. At inference, the semantic transfer scheme embeds the semantic relatedness information between seen and unseen class labels to composite unseen visual prototypes. The model then takes a new observation with a single-modality input (unseen video) to produce its visual representation for zero-shot action classification. (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>C</head><label></label><figDesc>sampled at time C in a video E, frame-level features A C are extracted by the feature extractor, where A C 2 R ? is a ?-dimension vector. The cross-modal transformer T takes (r, w) pairs as inputs, where r = [A 1 , A 2 , ..., A ) ] is the set of frame feature vectors and w = [F 1 , ..., F # 2 ] is the sequence of word embeddings of the corresponding class label Y with # 2 number of words. The transformer uses a constant dimension, D, through all layers. To ensure that both of the visual and word embeddings have the same dimension, a trainable linear projection layer is added to map each visual feature A C to the model dimension D. The sequence of the input features z 0 to the transformer has the form: z 0 = [[CLS], r, w], where a special token [CLS] = I 0 0 is prepended to the sequence of input for visual representation learning in 3.2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The examples of semantic relatedness matrix and binary adjacency matrix are created using subsets of label embeddings on Activi-tyNet and Kinetics. In the semantic relatedness matrix, brighter colors depict larger values. In the binary matrix, black color depicts the true value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of data points from 30 random unseen classes on UCF101 with the learned visual representations -. Each color corresponds to one unseen class. The stars denote the unseen visual prototypes, which are composited from the seen visual prototypes by the proposed semantics transfer scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>&lt; 9, 8 =</head><label>8</label><figDesc>0.42, 0.48, 0.50 and 0.50, respectively (Figure 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Confusion matrix on UCF101 by ResT 18 (K664) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Confusion matrix on HMDB51 by ResT 18 (K664) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Confusion matrix on ActivityNet by ResT 18 (K605) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Architecture of the transformer encoder in our proposed ResT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Sample results of ZSAR in visual space (V) and semantic space (S).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(b)  or "pizza dough" inFigure 10(c). For example, in Figure 11(c), a sample from the class "eat" on HMDB51, our model attends to the mouth, spoon, and hand. Similarly, in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( a )</head><label>a</label><figDesc>Action class: "Clap". (b) Action class: "Drink". (c) Action class: "Eat". (d) Action class: "Kick".(e) Action class: "Pullup".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Example results from our model with object region features as inputs on the classification of the "clap," "drink," "eat," "kick," and "pullup" actions on HMDB51 dataset. Different bounding boxes are coded with different colors. Brighter colors depict the most attended objects. Best viewed in color.(a) Action class: "Disc dog".(b) Action class: "Layup drill in basketball".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>( c )</head><label>c</label><figDesc>Action class: "Playing beach volleyball".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>( d )</head><label>d</label><figDesc>Action class: "Hitting a pinata".(e) Action class: "Throwing darts".(f) Action class: "Tumbling".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Example results from our model with object region features as inputs on the classification of the "disc dog," "layup drill in basketball," "playing beach volleyball," "hitting a pinata," "throwing darts," and "tumbling" actions on ActivityNet dataset. Different bounding boxes are coded with different colors. Brighter colors depict the most attended objects. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>B = {; B 1 , ..., ; B } with^seen classes. Given the set (, we train a model % to learn visual representations {G 8 , ..., G a } 2 -, where G 8 2 R ? denotes the ?-dimension embedding in -. Let ! D = {; D 1 , ..., ; D W } be a set of W unseen class labels. ! D \ ! B = ?. In addition, ? B = {b B 1 , ..., b B } and ? D = {b D 1 , ..., b D W } denote two sets of semantic embeddings corresponding to ! B and ! D . Given a testing video instance E D 9 , the ZSAR problem is to estimate its label H D 9 2 ! D .</figDesc><table><row><cell>H B 8 |E 2 +, H 2 ! B } be the training set for seen classes, where E B 8 is a video instance in + and H B 8 is the</cell></row><row><cell>class label in !</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with recent state-of-the-art on the 50% classes of UCF101 and HMDB51. The results are top-1 (%) with mean and standard deviation evaluated on inductive setting.</figDesc><table><row><cell>Method</cell><cell>Pre VE 1</cell><cell cols="2">SE 1 UCF101</cell><cell>HMDB51</cell></row><row><cell>GA [43], WACV18</cell><cell>C3D 2 [60]</cell><cell>W 1 A 1</cell><cell>17.3 ? 1.1 22.7 ? 1.2</cell><cell>19.3 ? 2.1 -</cell></row><row><cell>TARN [4], BMVC19</cell><cell>C3D 2 [60]</cell><cell>W A</cell><cell>19.0 ? 2.3 23.2 ? 2.9</cell><cell>19.5 ? 4.2 -</cell></row><row><cell cols="2">CEWGAN [38], CVPR19 I3D 2 [9]</cell><cell>W A</cell><cell>26.9 ? 2.8 38.3 ? 3.0</cell><cell>30.2 ? 2.7 -</cell></row><row><cell>TS-GCN [19], AAAI19</cell><cell>Obj 1,3</cell><cell>W</cell><cell>34.2 ? 3.1</cell><cell>23.2 ? 3.0</cell></row><row><cell>PS-GNN [20], PAMI20</cell><cell>Obj 1,3</cell><cell>W</cell><cell>36.1 ? 4.8</cell><cell>25.9 ? 4.1</cell></row><row><cell>E2E C3D [5], CVPR20 E2E R(2+1)D [5]</cell><cell>None</cell><cell>W</cell><cell>43.8 48.0</cell><cell>24.7 32.7</cell></row><row><cell>DASZL [29], AAAI21</cell><cell>TSM 2 [37]</cell><cell>A</cell><cell>48.9 ? 5.8</cell><cell>-</cell></row><row><cell>ER [10], ICCV21</cell><cell>S 1 + Obj 1,4</cell><cell cols="2">ED 1 51.8 ? 2.9</cell><cell>35.3 ? 4.6</cell></row><row><cell>ResT 18</cell><cell></cell><cell></cell><cell>54.7 ? 2.3</cell><cell>39.3 ? 3.5</cell></row><row><cell>ResT 34</cell><cell>None</cell><cell>W</cell><cell>55.2 ? 3.0</cell><cell>40.6 ? 3.5</cell></row><row><cell>ResT 101</cell><cell></cell><cell></cell><cell>58.7 ? 3.3</cell><cell>41.1 ? 3.7</cell></row></table><note>1 Visual embedding obtained by pretrained models (Pre VE); Semantic em- bedding (SE); Objects (Obj); Spatial features (S); Word embedding (W); Attributes (A); Elaborated descriptions by Wiki/Diction./WordNet (ED).2 Visual features from pretrained action recognition models: C3D [60] (trained on I380K [60] and Sports-1M [27]); I3D [9] (ImageNet [13] and Kinetics); TSM [37] (ImageNet [13] and Kinetics [28]).3 Object scores by GoogLeNet [58] (12,988-class ImageNet Shuffle [40]).4 Spatial features and object scores obtained by Big transfer model (BiT) [31] (ImageNet/ImageNet21K [13]).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">UCF101</cell><cell cols="2">HMDB51</cell><cell cols="2">ActivityNet</cell></row><row><cell></cell><cell>0/50</cell><cell cols="5">0/100 0/50 0/100 0/50 0/100</cell></row><row><cell>E2E R(2+1)D [5]</cell><cell>44.1</cell><cell>35.3</cell><cell>29.8</cell><cell>24.8</cell><cell>26.6</cell><cell>20.0</cell></row><row><cell>ResT 18</cell><cell>50.9</cell><cell>41.2</cell><cell>37.6</cell><cell>30.6</cell><cell>29.2</cell><cell>23.0</cell></row><row><cell>ResT 101</cell><cell>55.9</cell><cell>46.7</cell><cell>40.8</cell><cell>34.4</cell><cell>32.5</cell><cell>26.3</cell></row></table><note>Comparison with E2E [5] on all test classes (0/100) in UCF101, HMDB51, and ActivityNet datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Transformer model (a) Importance of the Transformer architecture</figDesc><table><row><cell>SE</cell><cell>ResT</cell><cell>Transfer</cell><cell cols="2">0/50 0/100</cell></row><row><cell>RNN</cell><cell></cell><cell>X</cell><cell>28.2</cell><cell>20.5</cell></row><row><cell>LSTM</cell><cell></cell><cell>X</cell><cell>29.7</cell><cell>21.8</cell></row><row><cell>Transformer</cell><cell>X</cell><cell>X</cell><cell>54.7</cell><cell>44.3</cell></row><row><cell cols="3">(b) Transformer model design</cell><cell></cell><cell></cell></row><row><cell>CLS</cell><cell>MLM</cell><cell>Transfer</cell><cell cols="2">0/50 0/100</cell></row><row><cell>X</cell><cell></cell><cell></cell><cell>47.4</cell><cell>40.0</cell></row><row><cell></cell><cell>X</cell><cell></cell><cell>45.0</cell><cell>38.7</cell></row><row><cell>X</cell><cell>X</cell><cell></cell><cell>48.9</cell><cell>41.5</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell>54.7</cell><cell>44.3</cell></row><row><cell cols="2">(c) Attention scheme</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CLS</cell><cell>MLM</cell><cell cols="2">0/50 0/100</cell></row><row><cell>Attention</cell><cell>Cross Single</cell><cell>Cross Cross</cell><cell>39.0 54.7</cell><cell>30.7 44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Feature extractor backbone (a) Performance of different feature encoder backbones</figDesc><table><row><cell>Network</cell><cell cols="3">Frames GFLOPs ? clips 0/50</cell><cell>0/100</cell></row><row><cell>VGG16</cell><cell>16</cell><cell>248? 25</cell><cell>52.5</cell><cell>42.4</cell></row><row><cell>MobileNetV2 GoogleNet R(2+1)D Archi.</cell><cell>16 16 16</cell><cell>6.9 ? 25 25.9 ? 25 41.2 ? 25</cell><cell>50.6 51.6 59.5</cell><cell>38.5 40.3 48.6</cell></row><row><cell cols="3">(b) Accuracy-complexity trade-off</cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell cols="3">Frames GFLOPs ? clips 0/50</cell><cell>0/100</cell></row><row><cell>E2E R(2+1)D E2E R(2+1)D ER S+Obj</cell><cell>16 16 8</cell><cell>40.8 ? 25 40.8 ? 1 70.2 ? 1</cell><cell>48.0 43.0 51.8</cell><cell>37.6 35.1 -</cell></row><row><cell>ResT 18 ResT 18 ResT 34 ResT 101</cell><cell>16 16 8 4</cell><cell>30.8 ? 25 30.8 ? 1 30.3 ? 1 32.0 ? 1</cell><cell>54.7 54.0 54.7 57.0</cell><cell>44.3 43.5 43.8 47.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>ZSAR in semantic or visual space</figDesc><table><row><cell>SE</cell><cell cols="3">ResT Transfer Space</cell><cell cols="2">0/50 0/100</cell></row><row><cell>W2V</cell><cell>X</cell><cell></cell><cell>S</cell><cell>48.9</cell><cell>41.5</cell></row><row><cell>GloVe</cell><cell>X</cell><cell></cell><cell>S</cell><cell>48.6</cell><cell>40.8</cell></row><row><cell>S2V</cell><cell>X</cell><cell></cell><cell>S</cell><cell>50.9</cell><cell>40.7</cell></row><row><cell>W2V</cell><cell>X</cell><cell>X</cell><cell>V</cell><cell>54.7</cell><cell>44.3</cell></row><row><cell>Glove</cell><cell>X</cell><cell>X</cell><cell>V</cell><cell>52.6</cell><cell>41.8</cell></row><row><cell>S2V</cell><cell>X</cell><cell>X</cell><cell>V</cell><cell>55.9</cell><cell>42.8</cell></row><row><cell cols="6">Table 6. with and w/o pre-trained</cell></row><row><cell cols="6">features and end-to-end training</cell></row><row><cell>VE</cell><cell cols="3">pre-trained e2e</cell><cell cols="2">UCF HMDB</cell></row><row><cell>C3D</cell><cell>X</cell><cell></cell><cell></cell><cell>40.5</cell><cell>26.5</cell></row><row><cell>C3D</cell><cell>X</cell><cell>X</cell><cell></cell><cell>52.7</cell><cell>37.1</cell></row><row><cell>Object</cell><cell>X</cell><cell></cell><cell></cell><cell>57.3</cell><cell>40.5</cell></row><row><cell cols="2">ResT 101</cell><cell>X</cell><cell></cell><cell>58.7</cell><cell>41.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Accuracy comparisons on models trained on Kinetics 664 and full Kinetics 400/700 datasets. All models are evaluated on 25 clips on the 50% of UCF101 and HMDB51 datasets.</figDesc><table><row><cell></cell><cell cols="2">UCF (0/50)</cell></row><row><cell cols="3">ResT 101 Model Top-1 Top-5</cell></row><row><cell>664 classes</cell><cell>58.7</cell><cell>75.9</cell></row><row><cell>400 classes</cell><cell>61.1</cell><cell>79.2</cell></row><row><cell>700 classes</cell><cell>69.2</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Effect of constraints in the semantic relatedness transfer scheme</figDesc><table><row><cell></cell><cell cols="2">(a) Evaluation on UCF101 dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UCF (0/50)</cell></row><row><cell>ResT 18 (664)</cell><cell cols="4">ZSAR in V or S space Skewness Top-1 Top-5</cell></row><row><cell>Reverse transfer</cell><cell>S</cell><cell>3.350</cell><cell>36.3</cell><cell>69.2</cell></row><row><cell>No constraint</cell><cell>V</cell><cell>2.235</cell><cell>38.2</cell><cell>73.9</cell></row><row><cell>Constraint I</cell><cell>V</cell><cell>1.342</cell><cell>50.7</cell><cell>81.5</cell></row><row><cell>Constraint II + III</cell><cell>V</cell><cell>1.290</cell><cell>51.8</cell><cell>74.4</cell></row><row><cell>All constraints</cell><cell>V</cell><cell>1.228</cell><cell>54.0</cell><cell>74.6</cell></row><row><cell></cell><cell cols="2">(b) Evaluation on HMDB51 dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">HMDB (0/50)</cell></row><row><cell>ResT 18 (664)</cell><cell cols="4">ZSAR in V or S space Skewness Top-1 Top-5</cell></row><row><cell>Reverse transfer</cell><cell>S</cell><cell>3.712</cell><cell>35.0</cell><cell>63.9</cell></row><row><cell>No constraint</cell><cell>V</cell><cell>1.688</cell><cell>35.1</cell><cell>64.7</cell></row><row><cell>Constraint I</cell><cell>V</cell><cell>1.379</cell><cell>37.9</cell><cell>68.5</cell></row><row><cell>Constraint II + III</cell><cell>V</cell><cell>0.849</cell><cell>38.1</cell><cell>64.5</cell></row><row><cell>All constraints</cell><cell>V</cell><cell>1.418</cell><cell>39.2</cell><cell>66.9</cell></row><row><cell></cell><cell cols="2">(c) Evaluation on ActivityNet dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ActivityNet (0/50)</cell></row><row><cell>ResT 18 (605)</cell><cell cols="3">ZSAR in V or S space Skewness Top-1</cell><cell>Top-5</cell></row><row><cell>Reverse transfer</cell><cell>S</cell><cell>2.742</cell><cell>21.9</cell><cell>40.1</cell></row><row><cell>No constraint</cell><cell>V</cell><cell>1.827</cell><cell>21.2</cell><cell>45.9</cell></row><row><cell>Constraint I</cell><cell>V</cell><cell>1.173</cell><cell>25.1</cell><cell>51.5</cell></row><row><cell>Constraint II + III</cell><cell>V</cell><cell>1.228</cell><cell>25.4</cell><cell>40.8</cell></row><row><cell>All constraints</cell><cell>V</cell><cell>1.020</cell><cell>26.2</cell><cell>47.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>ZSAR performance with ResNet and object features on the 50% of UCF101, HMDB51, and ActivityNet datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">UCF101 HMDB51 ActivityNet</cell></row><row><cell>K664</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResT 18</cell><cell>54.7</cell><cell>39.3</cell><cell>-</cell></row><row><cell>ResT 101</cell><cell>58.7</cell><cell>41.1</cell><cell>-</cell></row><row><cell>Ours obj</cell><cell>57.3</cell><cell>39.6</cell><cell>-</cell></row><row><cell>K605</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResT 18</cell><cell>50.9</cell><cell>37.6</cell><cell>29.2</cell></row><row><cell>ResT 101</cell><cell>55.9</cell><cell>40.8</cell><cell>32.5</cell></row><row><cell>Ours obj</cell><cell>55.0</cell><cell>40.5</cell><cell>34.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/microsoft/ResT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Both methods train on a subset of Kinetics with 605 classes. The subset is the result of removing all classes whose distance to any class in UCF101 [ HMDB51 [ ActivityNet is smaller than a threshold.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring synonyms as context in zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ioannis Alexiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4190" to="4194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Bishay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Zoumpourlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximating cnns with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Elaborative rehearsal for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10852</idno>
		<title level="m">Pix2seq: A language modeling framework for object detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Conforti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Cornu?jols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Zambelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">271</biblScope>
		</imprint>
	</monogr>
	<note>et al. Integer programming</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot action recognition in videos: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Valter Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">439</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04730,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to model relationships for zero-shot video classification. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallabi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirat</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12432</idno>
		<title level="m">All about knowledge graphs for actions</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cater: A diagnostic dataset for compositional actions &amp; temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Action2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meera</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00484</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Daszl: Dynamic action signatures for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Soo Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Peven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2452" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part V 16</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognizing unseen actions in a domain-adapted embedding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Hung</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4195" to="4199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint learning of attended zero-shot features and visual-semantic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic manifold alignment in visual feature space for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wegang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for generalized zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devraj</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Kumar Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaib</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9985" to="9993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Something-else: Compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1049" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The imagenet shuffle: Reorganized pre-training for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2016 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zeroshot learning for action recognition using synthesized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anubha</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema A</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiva Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2018 -Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning shared multimodal embeddings with unpaired data. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee</forename><surname>Peng Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi-Zhao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08641</idno>
		<title level="m">A review of generalized zero-shot learning methods</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Image, 2:T2, 2021. 3</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in highdimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjana</forename><surname>Ivanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>sept</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards a fair evaluation of zero-shot action recognition using external data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Haurilet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Assemblenet++: Assembling modality representations via attention connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhana</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Kangaspunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="654" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikumi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuo</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Locality and compositionality in zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Petrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Alternative semantic representations for zero-shot human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition via bidirectional latent embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="356" to="383" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot human action recognition via joint latent ranking embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Harnessing object and scene semantics for large-scale video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3112" to="3121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">S3d: Stacking segmental p3d for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trac D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Semantic embedding space for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Transductive zero-shot action recognition by word-vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="333" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multi-task zero-shot action recognition with prioritised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Visual data synthesis via gan for zero-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9436" to="9445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Action class</title>
	</analytic>
	<monogr>
		<title level="m">Baseball pitch</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Action class</title>
	</analytic>
	<monogr>
		<title level="m">Mopping floor</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Action class</title>
	</analytic>
	<monogr>
		<title level="m">Pizza tossing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Action class</title>
	</analytic>
	<monogr>
		<title level="m">Writing on board</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Example results from our model with object region features as inputs on the classification of the &quot;baseball pitch</title>
		<imprint/>
	</monogr>
	<note>Figure 10. mopping floor,&quot; &quot;pizza tossing,&quot; and &quot;writing on board&quot; actions on UCF101 dataset. Different bounding boxes are coded with different colors. Brighter colors depict the most attended objects. Best viewed in color</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
