<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ALL FOR ONE AND ONE FOR ALL: IMPROVING MUSIC SEPARATION BY BRIDGING NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-11">11 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Sawata</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sony Europe B.V</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusuke</forename><surname>Takahashi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ALL FOR ONE AND ONE FOR ALL: IMPROVING MUSIC SEPARATION BY BRIDGING NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-11">11 May 2021</date>
						</imprint>
					</monogr>
					<note>arXiv:2010.04228v4 [eess.AS]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Music source separation (MSS)</term>
					<term>Deep neural network (DNN)</term>
					<term>Loss function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes several improvements for music separation with deep neural networks (DNNs), namely a multi-domain loss (MDL) and two combination schemes. First, by using MDL we take advantage of the frequency and time domain representation of audio signals. Next, we utilize the relationship among instruments by jointly considering them. We do this on the one hand by modifying the network architecture and introducing a CrossNet structure. On the other hand, we consider combinations of instrument estimates by using a new combination loss (CL). MDL and CL can easily be applied to many existing DNN-based separation methods as they are merely loss functions which are only used during training and do not affect the inference step. Experimental results show that the performance of Open-Unmix (UMX), a well-known and state-of-the-art open-source library for music separation, can be improved by utilizing our above schemes. Our modifications of UMX are opensourced together with this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Many approaches have been researched in the field of music separation such as local Gaussian modelling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, non-negative matrix factorization <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, kernel additive modelling <ref type="bibr" target="#b5">[6]</ref> and hybrid methods combining these approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In particular, many methods have been investigated which introduce deep neural networks (DNNs) in order to improve the separation performance in recent years. There are three basic DNN architectures, namely multi-layer perceptrons (MLPs) <ref type="bibr" target="#b8">[9]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b9">[10]</ref> and recurrent neural networks (RNNs) <ref type="bibr" target="#b10">[11]</ref>, and all of them have been already introduced for the task of audio source separation. For instance, an MLP was used to separate the input spectra and then obtain separated results in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, CNNs and RNNs were used to realize source separation with improved quality than previous MLP-based methods since CNNs and RNNs can consider the temporal contexts via convolution and recurrent layers.</p><p>Although the aforementioned literature has improved the performance of music separation drastically, there are two problems with respect to training the separation networks: (1) most existing methods consider only the time or the frequency domain but not both, and, (2) they do not consider the mutual influence among output sources since loss functions are independently applied to each estimated source and the corresponding ground truth. For example, one of the state-of-the-art open-source systems for music source separation, called Open-Unmix (UMX) <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b0">1</ref> , conducts music source separation only in the frequency domain as the input and output of UMX are both spectrograms. Furthermore, UMX applies the conventional 1 https://github.com/sigsep/open-unmix- <ref type="bibr">[nnabla|pytorch]</ref> mean squared error (MSE) loss function to individual pairs of estimated and corresponding ground truth spectrograms for each instrument. In other words, UMX trains networks one-by-one for each instrument.</p><p>In order to solve these problems, we propose schemes with respect to the used loss function and model architecture. In the field of speech enhancement, which is a special case of audio source separation, methods considering time and frequency domain have been researched in recent years <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. For instance, <ref type="bibr">Kim et al. showed</ref> in <ref type="bibr" target="#b16">[17]</ref> the effectiveness of multi-domain processing via hybrid denoising networks. Furthermore, Su et al. reported in <ref type="bibr" target="#b17">[18]</ref> that building two discriminators which are responsible for time and frequency domain can realize effective denoising and dereverberation in their scheme of using generative adversarial networks (GANs). Inspired by these reports, we believe that considering both, time and frequency domain, is important to realize effective music source separation. Next, in the field of audio source separation, the effectiveness of Conv-TasNet <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, a fully-convolutional time-domain audio separation network, was reported. In particular, <ref type="bibr">D?fossez et al. reported</ref> in <ref type="bibr" target="#b19">[20]</ref> that the performance of Conv-TasNet was higher than the one of UMX when trained and evaluated on the same dataset. One of the reasons in our opinion is that the architecture of Conv-TasNet allows information sharing among sources as the convolutional layers consider all input channels when computing one output channel. In contrast, UMX does not allow such information sharing as it trains independent source extraction networks for each instrument. Therefore, it is difficult for UMX to consider the mutual influence among instruments obtained from the same input mixture.</p><p>Motivated by this discussion, we propose two new loss functions and a new model architecture which we add to UMX, called CrossNet-UMX (X-UMX) <ref type="bibr" target="#b1">2</ref> . First, we introduce a multi-domain loss (MDL) where we append an additional differentiable short-time Fourier transform (STFT) or inverse STFT (ISTFT) layer 3 during training only. The loss is computed from the estimates before and after the STFT/ISTFT layer. In this way, MDL can consider not only frequency but also time domain differences between estimates and ground truth. Second, we also propose a further loss function, named combination loss (CL), and bridging network paths for UMX. As we mentioned above, not only UMX but also almost all conventional methods for music source separation train their networks for each source independently. Thus, it is difficult to find the root-cause of performance degradation, i.e., which sources are leaking and thus producing incorrect instrument estimates.   this problem, CL considers the relationship among output sources by also producing output spectrograms for instrument combinations and applying MDL to them. If the performance of the ith source separation is insufficient, other combinations including the ith source will be adversely affected while the others which do not include the ith source are not. In addition, we bridge the network paths of UMX for different instruments and thus share information among all instruments. Our bridging operation is beneficial for networks like UMX which consists of individual extraction networks and not one joint separation network. Hence, a network like Conv-TasNet which is already crossing among sources via convolutional layers does not need this operation. The above proposed loss functions, i.e., MDL and CL, only affect the training step and, therefore, can be introduced to many conventional methods since they are merely loss functions. In addition, our bridging operation requires only a slight network modification but does not increase the number of parameters that need to be learned. Consequently, performance improvements can be gained for most DNN-based source separation methods with introducing almost no additional computational costs at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED LOSS FUNCTIONS</head><p>In this section, we describe in detail our new loss functions, i.e., MDL and CL, and discuss their merits. We assume that the timedomain mixture signal x consists of J sources, i.e.,</p><formula xml:id="formula_0">x = J j=1 y j ,<label>(1)</label></formula><p>where y j denotes the time-domain signal of the jth source and x, y j are column vectors with the samples. Furthermore, we assume that the output of the DNN is a mask M j which can extract the jth desired source from the mixture spectrum X = S{x}:</p><formula xml:id="formula_1">y j = S ?1 {? j } = S ?1 {M j ? X},<label>(2)</label></formula><p>where S and S ?1 are the forward and inverse operators of the STFT, respectively. Furthermore,? j and? j are the predicted results of time and frequency domain ground truths y j and Y j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Domain Loss (MDL)</head><p>In the scheme of MDL, we first append an additional differentiable and fixed STFT or ISTFT layer after the output layer as shown in</p><formula xml:id="formula_2">DNN Mask #1 Mask #2 Mask #3 Mask #4 4C1 4C2 4C3</formula><p>We can apply 14 (= 4C1 + 4C2 + 4C3) loss functions by using the above combination masks.</p><p>If the number of sources is 4?</p><formula xml:id="formula_3">#1 #2 #3 #4 #1,#2,#3 #1,#2,#4 #1,#3,#4 #2,#3,#4 #1,#2 #1,#3 #1,#4</formula><p>#2,#3 #2,#4 #3,#4 <ref type="figure">Fig. 2</ref>: Combination Loss (CL) for the case that the mixture consists of four sources. <ref type="figure" target="#fig_1">Fig. 1</ref>. This allows us to compute the loss in the time as well as the frequency domain. Note that the STFT or ISTFT layer does not affect the inference step since this layer is only used during training for computing the MDL. In our method, we use the mean squared error (MSE) between separated and ground truth spectrograms as frequency domain loss, and the weighted signal-to-distortion ratio (wSDR) <ref type="bibr" target="#b21">[22]</ref> as time domain loss, i.e.,</p><formula xml:id="formula_4">L J MDL = L J MSE + ?L J wSDR ,<label>(3)</label></formula><p>where ? is a scaling parameter for mixing multiple domains of loss. L J MSE and L J wSDR are respectively calculated as follows:</p><formula xml:id="formula_5">L J MSE = J j=1 t, f |Y j (t, f )| ? |? j (t, f )| 2 ,<label>(4a)</label></formula><formula xml:id="formula_6">L J wSDR = J j=1 ? ? ? ? ? ? ? ?? j y T j? j y j ? j ? (1 ? ? j ) (x ? y j ) T (x ?? j ) x ? y j x ?? j ? ? ? ? ? ? ? ,<label>(4b)</label></formula><p>where j denotes the jth source. Furthermore, t and f denote the frame and frequency bin index of the spectrogram Y j (t, f ) and its estimate? j (t, f ), respectively. In addition, ? j is the energy ratio between the jth source y j and the mixture x in the time-domain, i.e., ? j = y j 2 /( y j 2 + x ? y j 2 ). Please note that the output range of wSDR in Eq. (4b) is bounded to [?1, 1]. This range is useful for a stable training and no "log" operation is required (although SDR is traditionally calculated including the logarithm).</p><p>Using MDL, we take advantage of both domains, which provides a performance improvement as we will see in Sec. 3. As MDL is merely a loss function, it can be used for many conventional methods without requiring additional calculations during inference. Please note that we focus in this paper on frequency domain networks, e.g., UMX, as this approach is more common than time domain networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Combination Schemes</head><p>In this subsection, we introduce the new combination loss (Sec. 2.2.1) and our new bridging network architecture (Sec. 2.2.2) to help each UMX's extraction network to support each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Combination Loss (CL)</head><p>In the scheme of CL, we consider the combinations of output masks. Specifically, we combine two or more estimated masks into new ones where each of them can extract two or more sources from the mixture. By using the newly obtained combination masks, we can compute more loss functions than if we only compare each estimated  mask with its target, i.e.,</p><formula xml:id="formula_7">L = 1 N N n=1 L n MDL ,<label>(5)</label></formula><p>where N &gt; J is the total number of possible combinations except for mixing all J sources, namely N = J?1 i=1 J C i , and n denotes the index of nth combination, where J C i denote the number of possible combinations of i (= 1, 2, ? ? ? , J ? 1) instruments from the total of J instruments in the mixture, i.e., J C i is equal to the binomial coefficient J i . For example, in the situation of separating four sources, we can consider 14 (= 4 C 1 + 4 C 2 + 4 C 3 ) combinations in total as shown in <ref type="figure">Fig. 2</ref>, while conventional methods consider each source independently <ref type="bibr" target="#b3">4</ref> .</p><p>In order to illustrate the benefit of CL, let us consider the following example: Assume that we have a system with leakage of vocals into drums and other resulting in similar errors that both instruments exhibit. By considering the combination drums + other, we will notice that the two errors are correlated, resulting in an even larger leakage of vocals which we try to mitigate by using our proposed CL loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Bridging Networks</head><p>In our method, CL aims to train each network with considering the relationship among output sources by combining output masks. In addition, we observed that it is effective to cross not only the loss function via CL but also the network graphs in order to help each UMX's extraction network to support each other. Hence, we also propose UMX with a crossing architecture, named CrossNet-UMX (X-UMX). Specifically, we connect the paths to cross each source's network by adding just two average operators to the original UMX model as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Using these average operations does not change the total numbers of parameters which would not be the case if we would use a concatenation operation. Please note that this average operation is only needed for models like UMX since UMX consists of individual extraction networks.</p><p>In this way, our method can consider multiple sources together, i.e., two or more source separation, than considering each source independently. From a different viewpoint, CL particularly can be considered to provide a benefit similar to multi-task learning due to considering multiple sources jointly by computing combinational masks.</p><p>We will see in Sec. 3 that MDL and our combination schemes provide a performance improvement for UMX. We can expect such an improvement also for many conventional methods without having to introduce additional computational costs at inference time since MDL and CL are merely loss functions and the bridging is achieved with a simple average operation without learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>In this section, we conduct music separation experiments in order to confirm the validity of our method. The task is to separate a song into its four constituent instruments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Setup</head><p>In our experiments, we evaluate the proposed method on the MUSDB18 dataset <ref type="bibr" target="#b22">[23]</ref> which is comprised of 150 songs each of which is recorded at 44.1kHz sampling rate. MUSDB18 consists of two subsets ('train' and 'test') where we split the train set further into 'train' and 'valid' as defined in the 'musdb' package <ref type="bibr" target="#b4">5</ref> . For each song, the mixture and its four sources, i.e., bass, drums, other and vocals, are available. As in the original UMX, we operated our networks in the STFT magnitude domain using a Hann window of length 4096 with 75% window overlap.</p><p>Since CL needs to be applied to the joint instrument network due to the usage of the combinations of the output masks, we cannot use the original UMX implementation, which independently builds and trains a network for each instrument. Hence, we always train in the following the four separation networks jointly, even in the case that no combination scheme is used, and the loss function is merely the mean of the four individual losses for each instrument. This has the effect that the early stopping at the epoch with the smallest validation error is not done for each instrument individually (as is the case for the original UMX) but the early stopping is done at the same epoch for all four networks. Furthermore, the learning rate drops, which are determined by the 'ReduceLROnPlateau' function are done at the same epoch. Hence, the results that we obtain in (b) SAR <ref type="figure" target="#fig_3">Fig. 4</ref>: Experimental results for proposed methods.  Finally, please note that the scaling parameter ?, introduced in Eq. (3) for MDL, was set to ? = 10 in order to approximately equalize the ranges of L J MSE and L J wSDR by looking at the individual loss function's learning curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>To evaluate the performance of our method, we used the signalto-distortion ratio (SDR) and sources-to-artifacts ratio (SAR) computed with BSSEval v4 called 'museval' <ref type="bibr" target="#b5">6</ref> , which was also the official evaluation scheme for SiSEC 2018 <ref type="bibr" target="#b23">[24]</ref>.</p><p>The experimental results are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Note that C1 is almost identical to original UMX since it uses the same network architecture and loss function. However, C1's performance is inferior compared to the original UMX. As we discussed in Sec. 3.1, this is due to the difference in the early stopping and learning rate drops. First, each of our contributions, i.e., MDL, CL and Bridging, respectively enable UMX to improve performance since the results of C2-C4 outperform those of C1. Next, we can confirm that all methods that use two modifications compared to the original UMX, i.e., C5-C7, also outperformed C1. Particularly, collaborative using MDL and network bridging (C6) improved the SDR values of C1 drastically, which increases the average score from 5.18dB to 5.78dB. We can observe that the improvements of MDL and bridging linearly 6 https://github.com/sigsep/sigsep-mus-eval add up by confirming the differences of the average score as follows: C2 ? C1 = 0.22dB, C4 ? C1 = 0.37dB. Then the sum of them, i.e., 0.22dB + 0.37dB = 0.59dB, is nearly equal to the improvement from C1 to C6 (= 0.60dB). Meanwhile, we can confirm that only CL seems to be not always provide improvement in terms of SDR when it is used with another scheme by focusing on the values of C4 (w/ Bridging) and C7 (w/ Bridging and CL). On the other hand, we can observe that SAR is always improved by adding CL as shown in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>. In particular, the SARs were improved as follows: +0.08 (C1 ? C3), +0.16 (C2 ? C5), +0.13 (C4 ? C7) and +0.14 (C6 ? P). Thus, CL is effective to reduce artifacts and it is beneficial to introduce it since the improved SAR score leads to an improved sound quality as shown in <ref type="bibr" target="#b26">[27]</ref>.</p><p>Finally, we can confirm that using all our proposed modifications jointly, i.e., MDL, CL and bridging operation, which is denoted as 'P' in <ref type="figure" target="#fig_3">Fig. 4</ref>, gives the best performance in terms of SDR and SAR among all methods. In addition, we can compare our method to other music source separation public state-of-the-art systems shown in <ref type="table" target="#tab_3">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we proposed two new loss functions called multidomain loss (MDL) and combination loss (CL) and a suitable network architecture called CrossNet-UMX (X-UMX) realized by a bridging operation. We showed that MDL and CL are effective and convenient for music separation methods working in the frequency domain since both are loss functions which are used during training and thus do not change the inference step. Hence, it is easy to apply them to many other conventional DNN-based methods. In addition, if the target network is built up from sub-networks for each source, we showed that bridging the network paths by a simple averaging operation helps each extraction network to know each other better and, thus, improve performance. In this paper, we applied MDL and CL to a well-known and state-of-the-art open source library Open-Unmix (UMX), by only adding two average operators to the X-UMX model and the improved results showed the validity of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Time domain network with appended STFT layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Multi-domain loss (MDL). Note that ? is a scaling parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of network architectures used in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>for "C1" differ from the results of the original UMX. Besides the modifications mentioned above, all other experimental settings are the same as in the original UMX.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison of X-UMX with other public methods in terms of SDR ("median of frames, median of tracks").</figDesc><table><row><cell>Method</cell><cell cols="5">Bass Drums Other Vocals Avg.</cell></row><row><cell>UMX [16]</cell><cell>5.07</cell><cell>6.04</cell><cell>4.28</cell><cell>6.25</cell><cell>5.41</cell></row><row><cell>Meta-TasNet [25]</cell><cell>5.58</cell><cell>5.91</cell><cell>4.19</cell><cell>6.40</cell><cell>5.52</cell></row><row><cell>Demucs [26]</cell><cell>5.83</cell><cell>6.08</cell><cell>4.12</cell><cell>6.29</cell><cell>5.58</cell></row><row><cell>Conv-TasNet [20]</cell><cell>5.66</cell><cell>6.08</cell><cell>4.37</cell><cell>6.81</cell><cell>5.73</cell></row><row><cell cols="2">X-UMX (proposed) 5.43</cell><cell>6.47</cell><cell>4.64</cell><cell>6.61</cell><cell>5.79</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our implementation for NNabla, Sony's deep learning framework, is available at https://github.com/sony/ai-research-code/tree/ master/x-umx. Furthermore, a PyTorch version building upon Asteroid<ref type="bibr" target="#b20">[21]</ref> is available at https://github.com/asteroid-team/asteroid/ tree/master/egs/musdb18/X-UMX.<ref type="bibr" target="#b2">3</ref> If the network outputs a spectrogram, we append an ISTFT layer whereas a STFT layer is added if the network output is a time signal.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Initial experiments showed that the combination J C J , i.e., the case of i = J, is not adding further performance improvements and, hence, it is not used in Eq.<ref type="bibr" target="#b4">(5)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/sigsep/sigsep-mus-db/blob/master/ musdb/configs/mus.yaml</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Underdetermined reverberant audio source separation using a fullrank spatial covariance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q K</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1830" to="1840" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PROJET -Spatial audio separation using projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cauchy nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<meeting>of IEEE Workshop on Applications of Signal essing to Audio and Acoustics</meeting>
		<imprint>
			<publisher>WAS-PAA</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep nmf for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multichannel blind source separation based on non-negative tensor factorization in wavenumber domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel additive models for source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4298" to="4310" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multichannel nonnegative matrix factorization in convolutive mixtures for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="550" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable audio separation with light kernel additive modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms, Defense Technical Information Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multichannel music separation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 24th European Signal Processing Conference</title>
		<meeting>of 24th European Signal essing Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1748" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural network based instrument extraction from music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2135" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale multi-band densenets for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA</title>
		<meeting>of IEEE Workshop on Applications of Signal essing to Audio and Acoustics (WASPAA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open-Unmix -A reference implementation for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multidomain processing via hybrid denoising networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HiFi-GAN: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asteroid: the PyTorch-based audio source separation toolkit for researchers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olvera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mart?n-Do?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ditter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Phase-aware speech enhancement with deep complex U-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning extractors for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="816" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Demucs: Deep extractor for music sources with extra unlabeled data remixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bss eval or peass? predicting the perception of singing-voice separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wierstorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="596" to="600" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
