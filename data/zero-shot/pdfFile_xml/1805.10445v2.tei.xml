<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Age Estimation in the Wild with Attention LSTM Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
							<email>zhangkeit@ncepu.edu.cn.n.liuiswiththe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Xingfang</forename><surname>Yuan</surname></persName>
							<email>xyuan@mail.missouri.edu.x.guoiswiththe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhenbing</forename><surname>Zhao</surname></persName>
							<email>zhaozhenbing@ncepu.edu.cn.z.macorrespondingauthoriswiththe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Zhanyu</forename><forename type="middle">Ma</forename></persName>
							<email>mazhanyu@bupt.edu.cn.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Communication Engineering</orgName>
								<orgName type="institution">North China Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei, China</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic and Communication Engineer-ing</orgName>
								<orgName type="institution">Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columbia</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic and Communication Engineer-ing</orgName>
								<orgName type="institution">Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Electronic and Communication Engineer-ing</orgName>
								<orgName type="institution">Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Electronic and Communication Engineer-ing</orgName>
								<orgName type="institution">Electric Power University</orgName>
								<address>
									<postCode>071000</postCode>
									<settlement>Baoding, Hebei</settlement>
									<country>North China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">Pattern Recognition and Intelli-gent System Laboratory, Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Age Estimation in the Wild with Attention LSTM Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fine-Grained Age Estimation</term>
					<term>Attention LSTM Networks</term>
					<term>ResNets</term>
					<term>RoR</term>
					<term>LSTM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Age estimation from a single face image has been an essential task in the field of human-computer interaction and computer vision, which has a wide range of practical application values. Accuracy of age estimation of face images in the wild is relatively low for existing methods, because they only take into account the global features, while neglecting the fine-grained features of age-sensitive areas. We propose a novel method based on our attention long short-term memory (AL) network for fine-grained age estimation in the wild, inspired by the fine-grained categories and the visual attention mechanism. This method combines the residual networks (ResNets) or the residual network of residual network (RoR) models with LSTM units to construct AL-ResNets or AL-RoR networks to extract local features of age-sensitive regions, which effectively improves the age estimation accuracy. First, a ResNets or a RoR model pretrained on ImageNet dataset is selected as the basic model, which is then fine-tuned on the IMDB-WIKI-101 dataset for age estimation. Then, we fine-tune the ResNets or the RoR on the target age datasets to extract the global features of face images. To extract the local features of age-sensitive regions, the LSTM unit is then presented to obtain the coordinates of the agesensitive region automatically. Finally, the age group classification is conducted directly on the Adience dataset, and age-regression experiments are performed by the Deep EXpectation algorithm (DEX) on MORPH Album 2, FG-NET and 15/16LAP datasets. By combining the global and the local features, we obtain our final prediction results. Experimental results illustrate the effectiveness and robustness of the proposed AL-ResNets or AL-RoR for age estimation in the wild, where it achieves better state-ofthe-art performance than all other convolutional neural network (CNN) methods on the Adience, MORPH Album 2, FG-NET and 15/16LAP datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1. Fig. 1(a, b)</ref> <p>are AL-ResNets and AL-RoR architectures, where one LSTM unit is integrated into the original ResNets and RoR. <ref type="figure">Fig. 1(c)</ref> is the pipeline of our framework for fine-grained age estimation. The ResNets or RoR model is pretrained on ImageNet and IMDB-WIKI-101 datasets to obtain the optimized model first, and then fine-tuned on the target age datasets to obtain the global features. Finally, the local features of the age-sensitive regions are extracted by AL-ResNets or AL-RoR network and combined with the global features to get our final prediction results. system, advanced video surveillance, demographic information collection, soft biometrics, etc.</p><p>Most previous studies addressed the age prediction problem using hand-designed features with statistical models <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. Although many traditional statistical models <ref type="bibr" target="#b2">[3]</ref> have been proposed, hand-designed features behave unsatisfactorily on benchmarks of unconstrained images. Later research approached age estimation from face images in convolutional neural network (CNN) manner, automatically extracting feature representations for input images. There are two reasons why automatic age estimation is regarded as a very challenging task. First, discriminative feature extraction for age estimation is easily affected by large variations in facial gestures, lighting, makeup, background, noise, etc. <ref type="bibr" target="#b3">[4]</ref>. Second, the significant similarity and subtle inter-class differences in face images with adjacent ages are hardly handled. Therefore, distinguishing age with only global features of face may not achieve better results, while searching for age-sensitive regions (wrinkles, hair, liver spots, etc.) can provide more distinctive features for age estimation. Inspired by fine-grained image categories and attention conceptions <ref type="bibr" target="#b4">[5]</ref>, we argue that combining the local features of age-sensitive regions with global features may be helpful to age estimation. In contrast to traditional fine-grained category datasets, where manual annotation of a specific part is the dominant strategy for the target, a wide array of fine-grained classification methods exist <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref> [8] <ref type="bibr" target="#b8">[9]</ref> and are still deployed with additional supervisory information which increases computational complexity. Currently, none of the several public large age datasets mark certain image parts, which severely limits the development of fine-grained age estimation. Therefore, there is an urgent demand on how to automatically obtain position information of age-sensitive regions.</p><p>To solve such problems, we propose a method of finegrained age estimation based on our attention LSTM (AL) network to improve the abilities of feature extraction. As shown in <ref type="figure">Fig. 1(a, b)</ref>, the long short-term memory (LSTM) unit is seamlessly inserted between the residual group and the fully connected layer of the residual network (ResNets) or the residual network of residual network (RoR) to form an AL-ResNets or AL-RoR network. The network is designed to effectively combine the ResNets or RoR with LSTM unit to generate feature representations on the critical agesensitive regions. In the context of fine-grained age estimation, extracting features can be regarded as a two-level process, where one is image-level and the other one is part-level. <ref type="figure">Fig. 1(c)</ref> shows the pipeline of our framework. First, to improve the performance and alleviate the over-fitting problem on small-scale datasets, we train ResNets or RoR model on ImageNet <ref type="bibr" target="#b9">[10]</ref>; then we fine-tune it on the IMDB-WIKI-101 dataset <ref type="bibr" target="#b26">[27]</ref>. Second, we use the model to further fine-tune it on target age datasets to extract the global features of the images. Third, on the premise of image-level features extraction, an AL-ResNets or AL-RoR network based on ResNets or RoR is constructed, where the local features of the agesensitive regions on target images are extracted; then the final prediction results are obtained by combining the predictions of the global and local features together. Finally, through abun-dant experiments on several popular age datasets, our models achieved new state-of-the-art results on Adience <ref type="bibr" target="#b3">[4]</ref>, MORPH Album 2 <ref type="bibr" target="#b35">[36]</ref>, FG-NET <ref type="bibr" target="#b23">[24]</ref> and 15/16LAP datasets <ref type="bibr" target="#b50">[51]</ref>  <ref type="bibr" target="#b60">[61]</ref>.</p><p>Our main contribution is threefold: 1. Our method employs fine-grained categories and visual attention mechanism in the age estimation field for the first time. The proposed Attention LSTM network extracts local age-sensitive regions and more distinctive features automatically, which can effectively improve the accuracy of age estimation.</p><p>2. The proposed Attention LSTM network has few extra parameters compared with the original CNN models and is easily trained, so our method is not only effective but also practical for age estimation.</p><p>3. Through massive experiments, we analyzed the effects of different training strategies, network structures (ResNets and RoR) and network depths for age estimation, then developed reasonable strategies and achieved the new state-ofthe-art results on different datasets. Moreover, the results of experiments based on ResNets and RoR verified the robustness of our method with different network structures.</p><p>The rest of the paper is organized as follows. Section II briefly reviews related work for age estimation methods. Section III describes the proposed AL-ResNets or AL-RoR age estimation method. Section IV presents experimental results and analysis leading to conclusions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Automatic face analysis is a research topic that is currently receiving much attention from the computer vision and pattern recognition communities. Age has been investigated as a soft biometric <ref type="bibr" target="#b0">[1]</ref> and facial attribute <ref type="bibr" target="#b25">[26]</ref>, and age estimation has historically been one of the most challenging problems within the field of facial analysis. Age estimation used to extract facial features manually in the past, but now CNN methods <ref type="bibr" target="#b26">[27]</ref> are preferred due to achievements in training CNN directly on age datasets. Face age datasets are divided into biological age datasets and apparent age datasets, and diverse age datasets can apply different age estimation methods. Much research has been devoted to age estimation from a face image under the more familiar biological age estimation. Adience, MORPH Album 2 and FG-NET are prevalent benchmarks for biological age estimation; their image labels are marked by the age group or actual age; thus the predicted output is the biological age of a person. In contrast, apparent age estimation research is still in its infancy. Only one publicly available dataset is used in the context of apparent age estimation: Chalearn LAP including 15LAP and 16LAP. What distinguishes it from other datasets is that the age of each image in this dataset is labeled by the average of annotators' subjective opinions, so apparent age is the visual age based on the perspective of human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Biological Age Estimation</head><p>In the past 20 years, biological age estimation from face image has benefited tremendously from the evolutionary development in facial analysis. Based on hand-designed features, regression and classification methods were used to predict the age of face images. AGing pattErn Subspace(AGES) <ref type="bibr" target="#b27">[28]</ref> was constructed to model the aging pattern, which was implemented for automatic age estimation. Subsequently, Chang et al. <ref type="bibr" target="#b28">[29]</ref> proposed an ordinal hyperplane ranking algorithm called ordinal hyperplanes ranker (OHRank) for estimating human age via facial images. Wang et al. <ref type="bibr" target="#b29">[30]</ref> proposed a new framework for age feature extraction based on a manifold learning algorithm and the deep learned aging pattern (DLA), which greatly improved the age estimation performance. Chen et al. <ref type="bibr" target="#b30">[31]</ref> proposed a cumulative attribute concept based on support vector regression (SVR) for learning a regression model, and it gained a notable advantage based on its accuracy for age estimation. Guo et al. <ref type="bibr" target="#b32">[33]</ref> proposed a kernel canonical correlation analysis (KCCA) method, which could derive an extremely low dimensionality in estimating age, but the amount of kernel calculation was tremendous. All of these methods had the same scope of applicability, which was only proven effective on constrained benchmarks but could not achieve acceptable results on benchmarks for images captured in the wild.</p><p>Recent research on CNN showed that CNN models <ref type="bibr">[</ref>  <ref type="bibr" target="#b18">[19]</ref> could learn compact and discriminative feature representations when the size of training data is sufficiently large, so an increasing number of researchers have started to use CNN for age estimation. Levi et al. <ref type="bibr" target="#b19">[20]</ref> applied DCNN for the first time to age classification on an unconstrained Adience benchmark. Yi et al. <ref type="bibr" target="#b33">[34]</ref> proposed a multi-scale convolution neural network based on the traditional face analysis method. Hou et al. <ref type="bibr" target="#b20">[21]</ref> proposed a VGG-16 model with smooth adaptive activation function (SAAF) to predict age group on the Adience benchmark. Then they used the exact squared Earth Movers Distance (EMD2) <ref type="bibr" target="#b21">[22]</ref> as the loss function for CNN training and obtained better age estimation results. Rothe et al. <ref type="bibr" target="#b34">[35]</ref> combined the VGG-16 network pretrained on ImageNet dataset, using the principal component analysis (PCA) method to obtain a lower mean absolute error (MAE) value on MORPH Album 2. Then, they transformed the age regression into an age classification problem through the Deep EXpectation (DEX) method <ref type="bibr" target="#b22">[23]</ref> and achieved better results. Recently, Hou et al. <ref type="bibr" target="#b24">[25]</ref> used the R-SAAFc2+IMDB-WIKI method to obtain best results on a FG-NET dataset. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> proposed an age-and-gender estimation method combining a multi-level residual network (RoR) with two modest mechanisms, which they actively presented to achieve state-of-the-art results on the Adience benchmark. Gao et al. <ref type="bibr" target="#b36">[37]</ref> proposed a deep label distribution learning (DLDL) method, which effectively utilized the label ambiguity in both feature learning and classifier learning; thus, the best MAE value on the MORPH dataset was achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Apparent Age Estimation</head><p>Face age estimation made a breakthroughs through the development of the convolution neural network; yet, researchers are not confined to the study of biological age estimation. Apparent age estimation was originally inspired by the 2015 ChaLearn Looking at People (LAP) competition <ref type="bibr" target="#b50">[51]</ref>, where the apparent age dataset was released. A method called logistic boosting regression (Logit Boost) <ref type="bibr" target="#b37">[38]</ref> was proposed, which realized the network optimization progressively. Xu et al. <ref type="bibr" target="#b38">[39]</ref> proposed a deep label distribution method with distributionbased loss functions and used the Coc-DPM algorithm <ref type="bibr" target="#b39">[40]</ref> and face point detector <ref type="bibr" target="#b40">[41]</ref> for face search. Zhu et al. <ref type="bibr" target="#b41">[42]</ref> used the Microsoft Project Oxford API <ref type="bibr" target="#b43">[44]</ref> and Face ++ API <ref type="bibr" target="#b42">[43]</ref> to preprocess LAP dataset, and then got GoogleNet pretrained on several other datasets. Kuang et al. <ref type="bibr" target="#b44">[45]</ref> studied the age-related discriminative performance over multiple age datasets of MORPH, FG-NET, Adience, FACES <ref type="bibr" target="#b45">[46]</ref>, and mixed with random forest and quadratic regression as well as local adjustment methods. Lin et al. <ref type="bibr" target="#b46">[47]</ref> fused real-word value-based regression models and Gaussian label distribution based classification models, which were pretrained on CA-SIA WebFace <ref type="bibr" target="#b47">[48]</ref>, CACD(computer aided conceptual design) <ref type="bibr" target="#b48">[49]</ref>, WebFaceAge <ref type="bibr" target="#b49">[50]</ref> and MORPH datasets, and were finetuned on the ChaLearn LAP dataset. Deep EXpectation (DEX) formulation <ref type="bibr" target="#b51">[52]</ref> was proposed for apparent age estimation and won the LAP 2015 challenge. Recently, Agustsson et al. <ref type="bibr" target="#b52">[53]</ref> proposed a nonlinear regression network called Anchored Regression Network (ARN), which achieved the state-of-theart results on 15LAP validation set.</p><p>The 2016 ChaLearn LAP Apparent Age Estimation (AAE) competition <ref type="bibr" target="#b60">[61]</ref> had been completed and expanded the dataset scale based on the 15LAP dataset. Gurpinar et al. <ref type="bibr" target="#b53">[54]</ref> proposed a two-level system for estimating the apparent age of facial images, where the samples were classified into eight age groups. Duan et al. <ref type="bibr" target="#b54">[55]</ref> proposed a CNN2ELM method, where apparent age was estimated by the Race-Net + Age-Net + Gender-Net + ELM Classifier + ELM Regression (RAGN). Malli et al. <ref type="bibr" target="#b55">[56]</ref> divided the LAP dataset into age groups and age-shifted groups and used these groups to train the VGG-16 model. Uricar et al. <ref type="bibr" target="#b56">[57]</ref> extracted the deep features and formulated a SO-SVM multi-class classifier on top of it. Huo et al. <ref type="bibr" target="#b57">[58]</ref> proposed a novel method called deep age distribution learning (DADL) to use the deep CNN model to predict the age distribution. Dehghan et al. <ref type="bibr" target="#b58">[59]</ref> introduced a large dataset of 4 million face recognition images to pretrain their model, and then predicted apparent age on the age dataset. Antipov et al. <ref type="bibr" target="#b59">[60]</ref> employed different age encoding strategies for training general and children networks, including 11 "general" models and 3 "children" models, which achieved the state-of-the-art results using on 16LAP dataset.</p><p>In conclusion, whether biological age estimation or apparent age estimation, one of the pivotal issues in age estimation is how to learn the distinctive features of face age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we describe the proposed AL-ResNets and AL-RoR architecture with the Attention LSTM network for age estimation. Our methodology is essentially composed of three steps: (1) Constructing AL-ResNets or AL-RoR architecture for improving the discrimination of the model; <ref type="bibr" target="#b1">(2)</ref> Pretraining the CNN model of ResNets or RoR on ImageNet and fine-tuning on the IMDB-WIKI-101 dataset to alleviate over-fitting problem, and then training for global features on the target age datasets; (3) Extracting local features of the agesensitive regions by LSTM to further improve the performance Our classifier is not restricted to the raw image but rather its regional information.</p><p>of age estimation. In the following, we will describe the three main components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AL-ResNets and AL-RoR Architectures</head><p>It is widely acknowledged that the performance of CNNbased age estimation relies heavily on the optimization ability of the CNN architecture, where deeper and deeper CNNs have been constructed. Particularly, ResNets <ref type="bibr" target="#b16">[17]</ref> won the first place at the ILSVRC 2015 classification task, which had achieved tremendous success in various computer vision tasks. RoR <ref type="bibr" target="#b17">[18]</ref> was constructed by adding identity shortcuts levelby-level based on original residual networks. It is noteworthy to mention that recently RoR also succeeded in the study of age estimation <ref type="bibr" target="#b10">[11]</ref> [27] for its outstanding performance. In order to get state-of-the-art performance and verify the robustness of our method with different network architectures, we construct new network structures named AL-ResNets and AL-RoR based on the ResNets and RoR network architectures.</p><p>To train the ResNets models for image classification tasks, the input RGB images need to be cast into an ordered preprocess procedure. The input images are first resized to a fixed-size of 256?256, followed by a random cut to further reduce image size to 224?224 before entering the network. ResNets are built on four groups of residual blocks, where their basic components (conv, BN and ReLU) operate on shortcut levels. ResNets use shortcuts to propagate information only between neighboring layers in residual blocks. The LSTM unit is not only suitable for shallow ResNets, but also fits in nicely with other various deep residual networks. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, AL-ResNets-34 and AL-ResNets-152 have different residual block structures, where each residual block can be expressed in a general form:</p><formula xml:id="formula_0">y l = h(x l ) + F (x l , W l ), x l+1 = f (y l )<label>(1)</label></formula><p>where x l and x l+1 are input and output of the l-th block, respectively. F is a residual mapping function, h(x l ) = x l is an identity mapping function, and f is a ReLU function.</p><p>ResNets transform the learning of y l into the learning of F (x l , W l ) (single-level residual mapping) by residual block structure. Compared with ResNets, RoR <ref type="bibr" target="#b17">[18]</ref> transfers the learning problem to learning the residual mapping of residual mapping (multi-level residual mapping), which is simpler and easier than the original residual mapping to learn. In addition, RoR creates several direct paths for propagating information between different original residual blocks by adding extra shortcuts, so layers in upper blocks can propagate information to layers in lower blocks. By information propagation, RoR can alleviate the vanishing gradients problem. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the basic structure of a 34-layer AL-RoR based on RoR, which owns root-level, middle-level, and final-level shortcuts. Each residual block group contains residual blocks of 3, 4, 6 and 3 in order, and the junctions of multilevel residual mapping are located at the end of each residual block group and can be expressed by the following formulations.</p><formula xml:id="formula_1">x 3+1 = g(x 1 ) + h(x 3 ) + F (x 3 , W 3 ) x 3+4+1 = g(x 3+1 ) + h(x 3+4 ) + F (x 3+4 , W 3+4 ) x 3+4+6+1 = g(x 3+4+1 ) + h(x 3+4+6 ) +F (x 3+4+6 , W 3+4+6 ) x 3+4+6+3+1 = g(x 1 ) + g(x 3+4+6+1 ) +h(x 3+4+6+3 ) + F (x 3+4+6+3 , W 3+4+6+3 )<label>(2)</label></formula><p>where x l and x l+1 are input and output of the l-th block, and F is a residual mapping function, h(x l ) = x l and g(x l ) = x l are both identity mapping functions. g(x l ) expresses the identity mapping of first-level and second-level shortcuts, and h(x l ) denotes the identity mapping of the final-level shortcuts.</p><p>When the images are trained on the ResNets or RoR network, effective global facial features can be obtained by extracting the output feature map of the last convolutinal layer. One of the reasonable assumptions is that the facial age prediction is not only represented by the form of global characteristics but also can be related to many age-sensitive facial parts. So it is possible to introduce the local features of age-sensitive regions to enhance discrimination for finegrained age estimation further. In this work, we build an AL-ResNets or AL-RoR network to locate age-sensitive regions for extracting local features, which are based on the LSTM unit and CNN models (ResNets or RoR), as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> and <ref type="figure" target="#fig_1">Fig. 3</ref>. The output feature map of the ResNets or RoR last residual block is used as both the input of the original fully connected layer and the input of the LSTM unit. The LSTM unit locates the positions of age-sensitive regions and extracts the most significant local features for softmax classification in forward-propagation, and it is optimized in backward-propagation according to the cross entropy loss function. For combining global image-level features with local attention features in the AL-ResNets or AL-RoR network, age predictions were done by the two kinds of features separately, and then the final age prediction was determined by weighted average of the above predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pretraining for Global Features</head><p>In order to obtain the global and local features of facial images on Adience, MORPH Album 2, FG-NET and LAP datasets, the training phase is divided into two stages: pretraining for global features, training for local features of the agesensitive regions. First, we extract global features by ResNets or RoR.</p><p>Due to the use of small-scale target age datasets for age estimation, the over-fitting problem will occur easily if training directly on them. Drawing on the idea of transfer learning, we use ResNets or RoR network pretraining on ImageNet dataset to learn basic image feature representation, which can efficiently reduce the over-fitting problem. Moreover, the accuracy of age estimation relates to both the scale and the age distribution of the dataset. So the large-scale facial age dataset IMDB-WIKI-101 is used to fine-tune the models pretrained on ImageNet dataset for further learning the feature expression of facial age images and alleviating the over-fitting problem. IMDB-WIKI <ref type="bibr" target="#b22">[23]</ref> is the largest publicly available dataset for age estimation of people in the wild, containing more than 0.5 million images with accurate age labels. However, there are many poor-quality images in the IMDB-WIKI dataset. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> first cleaned this dataset and divided them into 101 categories, and then renamed it as IMDB-WIKI-101 used to fine-tune the network models for adapting to the distribution of facial age images.</p><p>In this research, the datasets for pretraining consisted of two large datasets, ImageNet and IMDB-WIKI-101. By the two stages pretraining, the model transition from general image classification to face age classification was accomplished. Finally, we fine-tuned the pretrained ResNets or RoR models on target facial age datasets (Adience, MORPH Album 2, FG-NET and LAP datasets) to get global features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training For Local Features of the Age-Sensitive Regions</head><p>The uppermost dilemma of age estimation is the similarity of the adjacent ages, we concluded that it is possible to use local features of age-sensitive regions to improve the ability of age estimation. Thus, age estimation in the wild can be treated as a fine-grained classification problem, which locates age-sensitive regions to get local features for age estimation. In this paper, we introduced the attention idea proposed by Mnih et al. <ref type="bibr" target="#b31">[32]</ref> to construct the AL-ResNets or AL-RoR network to extract local features of age-sensitive regions. Fine-grained age estimation conveniently enables part-based approaches rather than confining to global, image-level features, which improves the cohesion of contextual age information to further reduce age prediction error.</p><p>The AL-ResNets or AL-RoR model is set up to automatically find age-sensitive regions and discriminative local features, and is grounded in the ResNets or RoR network to get the global features of the target age datasets. We use face global features to update the internal state of the LSTM unit and extract age-sensitive position information, and the supervised learning method is used to make our networks locate age-sensitive region automatically. In the training stage for local features of age-sensitive regions, we adopt Cross Entropy Loss as the supervised signal to train the LSTM unit module and location network module by backwardpropagation algorithm. Training for local features with the AL-ResNets or AL-RoR network consists of several parts, as shown in <ref type="figure">Fig. 4, which</ref>   <ref type="figure">Fig. 4</ref>. First, the global features are extracted by input feature module; Second, LSTM unit module is used to extract the location information of age-sensitive regions; Third, the location information is used to extract the coordinate of age-sensitive region; Next, the local features are extracted by cropping the global features based on the coordinate; Finally, we calculate the final age prediction by combining the global and local age predictions in output module.</p><p>RoR model as the input features of LSTM unit, except to reduce the possibility of local information confusion caused by over-enriching semantic information in the upper layers. There are two other reasons for using the feature map of the last convolution layer as the input of LSTM unit. The first reason is that the cropped region is a local area on the feature map, which is much smaller than the size of cropping the original image directly. So it only requires a fraction of computational cost compared with the entire network. The second reason is that cropping features on the feature map can share the same basic network so that there is no need to use a separate network training for features cropping. The size of output feature map at the last layer in the fourth residual block group of ResNets or RoR is identified as 512?7?7. It then goes through 7?7 average pooling operation, so the LSTM unit gets a 512dimension input based on the number of output channels. The input feature module is used to generate 512-dimensional feature vector as the input of the LSTM unit. These input features have already been trained through the basic DCNN network (ResNets or RoR), so the basic networks for global features does not employ the gradient descent algorithm at the stage of local feature training, which means that the basic networks are fixed. LSTM unit module: LSTM unit module is used to extract the location information of age-sensitive regions. We consider that only using the features of the current image to locate the age-sensitive region is not enough. Because the locations of age-sensitive regions in different face images are similar and regular, the location information of the age-sensitive region of other face images may be helpful to locate the age-sensitive region of the current image. Considering this assumption, we adopt LSTM unit to achieve the features for locating the age-sensitive regions. LSTM unit can automatically retain the location information of other images similar to the current image by the long-term and short-term memory mechanism. The features for locating the age-sensitive regions extracted by our Attention LSTM networks not only take into account the features of the current image, but also draw on the location information of other similar images, so these features are more comprehensive for locating the age-sensitive regions.</p><p>The LSTM unit controls the cell state through the structure of "gates," which is divided into input gate, forget gate, and output gate. A typical gate approach consists of two fundamental parts: a sigmoid layer and a pointwise operation. Its main implementation is as follows: First, the forget gate applies to select the information from state output at the last moment C prev , and is followed by the input gate and a new candidate vector C in?tan generated by the tanh layer to create a product value. Then two sources of information are combined for status update, where the process is to abandon unnecessary information and add new information. Furthermore, the hidden layer status output of the LSTM is acquired using cell status which is maintained by the tanh layer at -1 to 1 and multiplied by the output value of the output gate. The LSTM unit performs the following computation:</p><formula xml:id="formula_2">C next = f orget gate C prev + in gate C in?tan h next = out gate tanh(C next ) C in?tan = tanh(W C [h prev , x input ] + b C )<label>(3)</label></formula><p>where C prev and C next are the output cell state of the LSTM at the previous and current moment, respectively, h next is the hidden layer output state of LSTM, and all of them have the same feature dimension of 128. C in?tan is the candidate vector for updating the cell state. W C is weight parameter, and b C is bias. x input is the 512 dimensional feature as the input of the LSTM unit.</p><p>The age-sensitive regions location of different faces has some similarity. By LSTM unit, both C next and h next contain long term and short-term memory information, which combine the location information of current image with previous images. LSTM unit can automatically determine the information which is more suitable for locating age-sensitive regions by the three gates. So we use both C next and h next as the location information instead of only using x input , the 256 dimensional vector combining C next with h next is the output of LSTM unit, named as S next .</p><p>Location network module: The location network module consists of a convolution layer followed by a sigmoid activation function, and the output S next of LSTM unit is considered as the input of the convolution layer. The output of the convolution layer is a 4 dimensional vector l 1?4 as follows <ref type="bibr" target="#b3">(4)</ref>, which is used to generate the coordinate (x, y), the width and height of the age-sensitive bounding box. We share the same strategy of loss functions in backward propagation process to update the LSTM unit module and location network module, as is done in the cross entropy criterion.</p><formula xml:id="formula_3">l 1?4 = L(W * S next )<label>(4)</label></formula><p>Where S next is the joint output of LSTM with a feature dimension of 256, and W denotes the overall parameters. The specific form of L(?) is represented as a convolution layer. Feature cropping module: This module adopts a pooling strategy to simplify the computational complexity of the network. The module first crops the features on the 512?7?7 output feature map of the ResNets or RoR convolution layer according to the location coordinates. Then, an average pooling operation is performed on the cropped feature map. Finally, we get a 512 dimensional feature vector, which is the local feature of age-sensitive region.</p><p>Output module: Finally, a key problem to solved is how to combine the global features with local features for final age prediction. In order to simply the model and reduce more training computation, we use a weighted method to combine the global prediction with local prediction in output module, and the weighted values are set to 1 and 0.5. The global features come from the feature map of the last residual block on original ResNets or RoR model, and they are used for global age prediction(P global ). The local features are the features of age-sensitive regions, which are used for local age prediction(P local ). The global and local features pass through their respective full connection layer and softmax layer to get global and local age prediction, respectively. The final age prediction (P f inal ) is the weighted average of the two prediction results, shown as:</p><formula xml:id="formula_4">P f inal (x) = P global + 0.5P local<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head><p>We empirically demonstrated the effectiveness of AL-ResNets and AL-RoR on a series of benchmark datasets: Adience, MORPH, FGNET and 15/16LAP datasets. Through experiments, we analyze the effects of different training strategies, network structures and network depths to develop the best strategies, and then achieve the new state-of-the-art performance on different age datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>The ResNets <ref type="bibr" target="#b62">[63]</ref> or RoR <ref type="bibr" target="#b17">[18]</ref> network pretrained on the ImageNet dataset is used as the fundamental model. When fine-tuning the ResNets or RoR model, the IMDB-WIKI-101 dataset is randomly divided into two parts of training and testing with the size of 90% and 10%, and the number of output of the softmax classifier is changed from 1000 to 101. The learning rate starts from 0.01, and is divided by a factor of 10 at epoch 60 and 90.</p><p>In the target age experiments, we use the oversampling method <ref type="bibr" target="#b19">[20]</ref> by taking a ten-crop way to crop and mirror images in testing phase. We introduce deep expectation algorithm <ref type="bibr" target="#b22">[23]</ref> to deal with the problem of age regression. The network is trained for classification with M output neurons, where the number of output neurons M is set to 62, 70, and 101 for training MORPH, FGNET and LAP datasets, respectively, and where each neuron corresponds to an integer age. The weight decay is set to 1e-4 and the momentum is 0.9. The total epoch number for the Adience dataset is 60, and the learning rate starts from 0.0001. When experimenting on the MORPH Album dataset, the epoch is 120 with a learning rate of 0.001. The learning rates for the two datasets are divided by a factor of 10 after epoch 60. For training the global and local features of the FG-NET/LAP dataset, the epoch number is set to 90 and 120, respectively. The learning rate is set to <ref type="bibr">Fig. 5</ref>. Four different face image datasets are used in this paper. Images in MORPH and FG-NET are under constrained conditions, both of which can be applied in estimating the biological age of a person. The images in Adience and LAP datasets are in the wild, with the Adience dataset used to estimate the age group, and the LAP dataset used to predict the apparent age of a person. 0.001, and the former is divided by a factor of 10 after epoch 30, while the latter is after epoch 60. Our implementations are based on Torch 7 with one NVIDIA GeForce GTX Titan X. For data augmentation, we all use scale and aspect ratio augmentation <ref type="bibr" target="#b62">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>MORPH: MORPH Album 2 is one of the largest publicly available age datasets. There are 55,134 face images, whose age range from 16 to 77. This dataset contains multiple races. In order to reduce the age difference between different races or ethnic origins, we randomly selected 5,475 face images among Caucasians to avoid the influence of ethnic differences and randomly divided them into 80% for training and 20% for testing.</p><p>FG-NET: FG-NET is a small dataset that only includes 1,002 images of 82 individuals in the wild, ranging from 0 to 69 years old with about 12 images per person. To ensure that everyone could provide pictures of different ages, images are collected by scanning paper documents of personal collections beside the digital images from recent years. We followed the standard leave-one-out-protocol (LOOP) for FG-NET and reported the average performance over the 82 splits. Because MORPH and FG-NET are not in the wild, we do not use any alignment method on these datasets.</p><p>Adience: The entire Adience collection includes 26,580 256?256 color facial images of 2,284 subjects, with eight age group classes (0-2, 4-6, <ref type="bibr">8-13, 15-20, 25-32, 38-43, 48-53, 60-100)</ref>. Adience dataset comes from images that people automatically upload to network albums from smart phones. These images are not artificially filtered before uploading, and they are completely unconstrained as they were taken under different variations. We use the in-plane face aligned method to align faces, originally used in <ref type="bibr" target="#b3">[4]</ref>. Testing for age classification is performed using a standard five-fold, subjectexclusive cross-validation protocol, defined in <ref type="bibr" target="#b19">[20]</ref>, and the accuracy of five folds are averaged to be the final age group classification result.</p><p>LAP: The ChaLearn 15/16LAP datasets are mainly used to study the apparent age estimation of face images. Each image label consists of the average age and the standard deviation. Most images are under unconstrained conditions (such as different background, character rotation, partial occlusion, etc.), which need face detection, alignment and cropping preprocessing. We rotated the input image in the interval of [?60 ? , 60 ? ] in 5 ? steps and also by ?90 ? , 90 ? and 180 ? . The face box with the strongest detection score detected by the face detector <ref type="bibr" target="#b39">[40]</ref> was taken, then the face box size was enlarged by 40% in both width and height and the face image was cropped. For robustness, we did not delete those unaligned images, but instead, we kept them. The entire images were eventually condensed to size of 256?256. The 15LAP dataset is a relatively small dataset with a total of 4,691 images, including 2,476 for training, 1,136 for validation and 1,079 for testing. The 16LAP dataset is an expanded version of 15LAP, which adds nearly 3,000 images to 15LAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Protocol</head><p>Different datasets adopt different evaluation protocols in testing phase, and there are three kinds of evaluation protocols as follows.</p><p>1) Accuracy and 1-off Accuracy: This evaluation measures utilized in the Adience experiments are exact accuracy and 1-off accuracy, where the exact accuracy computes the correctness for the estimated age group, and the 1-off accuracy measures the results when the algorithm gives the correct or adjacent age-groups.</p><p>2) Mean Absolute Error (MAE): The results are evaluated in the MORPH Album 2 and FG-NET experiments by using the MAE measure. The MAE computes the error between the predicted age and the real one as follows <ref type="bibr" target="#b5">(6)</ref>, where y j , y j represent the actual age and the estimated age, respectively. N represents the number of all test images.</p><formula xml:id="formula_5">M AE = 1 N N j=1 y j ? y j<label>(6)</label></formula><p>3) -Error: For age estimation on LAP dataset, we employ -error evaluation metric besides MAE, which is a result of the uncertainty introduced by standard deviation ?. -error is mainly affected by mean ? and standard deviation ?, as well as the network prediction output value, where they are subject to a normal distribution. The expression of -error is shown in <ref type="bibr" target="#b6">(7)</ref>, where x j , ? j , ? j are the predicted age, the apparent age value and the standard deviation, respectively, and N is the number of all test images.</p><formula xml:id="formula_6">? error = 1 N N j=1 (1 ? exp(? (xj ??j ) 2 2? 2 j ))<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Age Group Classification Experiments</head><p>By information shortcut propagation, ResNets can alleviate the vanishing gradients problem. RoR based on ResNets benefits from the standpoint of optimization through RoR residual mapping and the extra shortcuts provided to expedite information propagation between distant layers. In order to analyze the robustness of our method in different networks, we used ResNets and RoR as the base models. To find the optimal model of the 34-layer network, we carried out a lot of comparative experiments on the Adience dataset. There are eight training methods which were used on fold4 of the Adience dataset and analyzed in terms of classification accuracy and 1-off accuracy:</p><p>(1) ReNets-34: Use solely Adience to train ResNets-34 network.</p><p>(2) I-ResNets-34: After pretrained on ImageNet dataset, fine-tune the I-ResNets-34 (ImageNet-ResNets-34) network with Adience.</p><p>(3) Ft-101-RoR-34: After pretrained on ImageNet and IMDB-WIKI-101 datasets, fine-tune the Ft-101-RoR-34 (FineTune-IMDBWIKI101-RoR-34) network with Adience.</p><p>(4) Ft-101-ResNets-34: After pretrained on ImageNet and IMDB-WIKI-101 datasets, fine-tune the Ft-101-ResNets-34 (FineTune-IMDBWIKI101-ResNets-34) network with Adience. The results of different methods tested on fold4 of the Adience dataset are shown in <ref type="table" target="#tab_2">Table I</ref>. We evaluated the effect of each step in the proposed method on age group classification.</p><p>The learning rate of ResNets-34 begins at 0.1 and I-ResNets-34 at 0.01. For Ft-101-RoR-34, Ft-101-ResNets-34, AL-RoR-34 and AL-ResNets-34, learning rate starts from 0.0001, and all of epochs are set to 160. Compared with the result of ResNets-34, the I-ResNets-34 obtains higher accuracy because of the basic image feature expression acquisition by ImageNet pretraining. The result of Ft-101-RoR-34 or Ft-101-ResNets-34 is obviously superior to that of I-ResNets-34, which reveals that the network first pretrained by ImageNet, and then finetuned through the IMDB-WIKI-101 dataset <ref type="bibr" target="#b26">[27]</ref> to achieve transfer learning and alleviate the over-fitting problem, which works better than pretraining only on ImageNet.</p><p>AL-ResNets-34(Only local features) and AL-RoR-34(Only local features) get good performance, which illustrate that the local features of age-sensitive regions are sensitive to predict age. The results of ResNets-34 and RoR-34 are better than AL-ResNets-34(Only local features) and AL-RoR-34(Only local features), we argue that the global features are more important than local features, so we set higher weight to the prediction with global features. AL-ResNets-34 outperforms Ft-101-ResNets-34 performance, and the same experimental performance also matches the AL-RoR-34 and Ft-101-RoR-34 model results. These results prove the robustness and effectiveness of our attention LSTM method in different networks. Since AL-ResNets-34 based on Ft-101-ResNets-34 is constructed to train the partial regions with distinctive features, improvement of age group classification results benefit from both global features and local features of the input face images. A definite improvement has come about in the ability to train effective feature vectors of the face images as a result of the proposed method. The local features of age-sensitive regions extracted by attention LSTM is efficient, which results in the further improvement on classification accuracy.</p><p>From the results shown in <ref type="table" target="#tab_2">Table I</ref>, we can see that our approach can consistently improve the age estimation performance. Therefore, further experiments on five folds of Adience use AL-ResNets-34 and AL-RoR-34 networks for age group classification, all of epochs are set to 120. ResNets or RoR is pretrained on ImageNet first, fine-tuned on IMDB-WIKI-101 dataset and Adience dataset, then constructed using AL-ResNets-34 and AL-RoR-34 networks to train Adience.  Furthermore, the over-sampling method (ten-crop) is applied on Adience dataset for better performance. <ref type="table" target="#tab_2">Table II</ref> shows the effectiveness of the proposed method. From the results of <ref type="table" target="#tab_2">Table I and Table II</ref>, we can see that the results of RoR are slightly worse than ResNets, which is due to the fact that the stochastic depth algorithm <ref type="bibr" target="#b63">[64]</ref> can not play a role in improving the accuracy when using large datasets to fine-tune the model; thus, RoR without a stochastic depth algorithm and ResNets had similar performances in these experiments <ref type="bibr" target="#b26">[27]</ref>. The proposed model achieves the better results with shallow AL-ResNets in terms of accuracy. We can expand it to a deeper AL-ResNets-152 network that is able to obtain more accurate age classification results under unconstrained situations; meanwhile, AL-ResNets-152 can boost to an accuracy of 67.83% and a 1-off accuracy of 97.53% on five folds of the Adience dataset. To sufficiently evaluate the performance of our proposed Network, we compared it with other six other state-ofthe-art methods, including: 4c2f-CNN <ref type="bibr" target="#b19">[20]</ref>, R-SAAFc2 <ref type="bibr" target="#b20">[21]</ref>, DEX <ref type="bibr" target="#b22">[23]</ref>, EMD <ref type="bibr" target="#b21">[22]</ref>, RSAAFc2(IMDB-WIKI) <ref type="bibr" target="#b24">[25]</ref> and RoR+IMDB-WIKI with two mechanisms <ref type="bibr" target="#b26">[27]</ref>. The age group classification results of various methods are shown in <ref type="table" target="#tab_2">Table III</ref>. Relying on two mechanisms, gender and weight loss layers, the RoR+IMDB-WIKI <ref type="bibr" target="#b26">[27]</ref> method achieved a significant improvement in classification accuracy. However, our AL-ResNets-152 (ten-crop), trained as a single-model architecture, provided the new state-of-the-art age classification results. The success of the above experiments should be credited to the use of the AL-ResNets-152 model with attention LSTM unit, which can extract age-sensitive local facial features for age prediction. Extensive comparisons on the Adience dataset verify the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Age Value Estimation Experiments</head><p>According to the preceding experiments in above section, we analyze that our proposed model can improve the performance of age group classification task. In this section, in order to prove the generalization ability of our method, we use our proposed method with the Deep EXpectation (DEX) method <ref type="bibr" target="#b22">[23]</ref> for age regression task on other age datasets.</p><p>The performance comparison on the 15LAP dataset is summarized in <ref type="table" target="#tab_2">Table IV</ref>  Because LAP datasets are relatively small, overfitting can be a critical problem for the 15LAP dataset. The over-depth of the network may also cause the overfitting problems to be even more severe on small age datasets, so we employ the shallow RoR-34 model to alleviate the overfitting problem on LAP and other small age datasets. The RoR-34 is used to train the global features and the local features of age-sensitive regions are extracted by using the attention LSTM in the AL-RoR-34 model. Our single AL-RoR-34 model achieves 0.2683 -error in the validation phase, and 0.2548 -error in the test phase. The MAE reaches 3.137 in the validation phase. Notably, the proposed method achieves the new state-of-the-art results, and surpassed the winners(achieving 1st place) <ref type="bibr" target="#b51">[52]</ref> of the ChaLearn LAP 2015. <ref type="figure">Fig. 6</ref> shows some representative examples of validation images on 15LAP dataset where our method significantly outperforms DEX <ref type="bibr" target="#b51">[52]</ref>, and the error ranges of age predictions can be greatly reduced compared with RoR-34. Some validation images with the minimum age prediction errors are shown in <ref type="figure">Fig. 7</ref>, which indicates that the more accurate age values than RoR-34 can be obtained in different age ranges by our proposed method. From <ref type="figure">Fig. 6 and Fig 7,</ref> we find that the age predictions by AL-RoR-34 are more accurate than RoR-34, because the local features of age-sensitive regions play an important role for age estimation.</p><p>We also performed an additional experiment on the 16LAP dataset to demonstrate the superiority of our model. <ref type="table" target="#tab_7">Table V</ref> summarizes the results for the 2016 ChaLearn challenge on   <ref type="bibr" target="#b59">[60]</ref>. This is because we rounded the label value on the 16LAP dataset to satisfy classification requirements, but it has an impact on the range of predicted age errors. In addition, OrangeLabs <ref type="bibr" target="#b59">[60]</ref> introduced an additional private dataset to address the shortcomings in the supply of children images, and the final test result depended on the combined results of multiple models, while our results are based only on one 34-layer model. The proposed model also gave a superior performance on MORPH Album 2 and FG-NET datasets, where the attention LSTM network is essential for age estimation too. As   <ref type="figure">Fig. 7</ref>. Examples of validation images where our proposed method obtained the smallest absolute error. shown in <ref type="table" target="#tab_2">Table VI</ref>, by training on MORPH Album 2 dataset using the AL-RoR-34 network, the best MAE value of 2.36 years is achieved, which is an improvement of 0.06 over the DLDL <ref type="bibr" target="#b36">[37]</ref> methods. On FG-NET dataset, we achieve a new state-of-the-art MAE of 2.39 years. Compared to only using global features on both datasets, the addition of local features further reduces the age prediction error. All these experiments on different age datasets show that combining global and local features can result in better performance for age estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes an AL-ResNets and an AL-RoR architectures based on the attention LSTM network for the task of facial age estimation. Fine-grained age estimation method effectively learns the discriminative local features of the agesensitive regions obtained by the attention LSTM unit. It combines the global and the local features on the target age datasets to achieve better performance. Pretraining on ImageNet is used to learn the basic image feature representation. Further fine-tuning on IMDB-WIKI-101 helps to learn the feature expression of the facial age images. By introducing the finegrained classification and the visual attention mechanism into the age estimation task, we not only obtain the state-of-the-art performance on Adience, MORPH, FGNET and 15/16LAP datasets, but also provide new feasible ideas for face age estimation and face analysis research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>AL-ResNets is a combination of the ResNets and LSTM unit. ResNets-34 and ResNets-152 are composed of different residual block structures. The rose branch indicates the global features extracted by ResNets on each image, and the colored areas (red, cyan, and purple) on the feature map of AL-ResNets represents the age-sensitive region extracted by LSTM on different images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The AL-RoR model is constructed by adding the LSTM unit to Multilevel Residual Networks. The shortcut on the left is a root-level shortcut, and the remaining shortcuts made up of the four yellow shortcuts are middlelevel shortcuts. The blue shortcuts are final-level shortcuts. The LSTM unit is inserted between the residual group and the fully connected layer, which need to be applied to select the useful relative patch (red, cyan, and purple) on different images for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 5 )</head><label>5</label><figDesc>AL-RoR-34 (Only local features): Based on (3), then train the AL-RoR-34 network with Adience and predict age only based on local features. (6) AL-ResNets-34 (Only local features): Based on (4), then train the AL-ResNets-34 network with Adience and predict age only based on local features. (7) AL-RoR-34: Based on (5), predict age based on global and local features. (8) AL-ResNets-34: Based on (6), predict age based on global and local features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>13</head><label>13</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="3">AGE CLASSIFICATION RESULTS(%) TESTED ON FOLD4(1-CROP):</cell></row><row><cell cols="3">RESNETS-34, I-RESNETS-34, FT-101-ROR-34, FT-101-RESNETS-34,</cell></row><row><cell cols="3">AL-ROR-34(ONLY LOCAL FEATURES), AL-RESNETS-34(ONLY LOCAL</cell></row><row><cell cols="3">FEATURES), AL-ROR-34 AND AL-RESNETS-34 STAND FOR EIGHT</cell></row><row><cell cols="2">TRAINING METHODS RESPECTIVELY.</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy(%)</cell><cell>1-off(%)</cell></row><row><cell>(1) ResNets-34</cell><cell>56.96</cell><cell>90.28</cell></row><row><cell>(2) I-ResNets-34</cell><cell>60.18</cell><cell>90.51</cell></row><row><cell>(3) Ft-101-RoR-34</cell><cell>65.46</cell><cell>96.85</cell></row><row><cell>(4) Ft-101-ResNets-34</cell><cell>65.71</cell><cell>96.90</cell></row><row><cell>(5) AL-RoR-34(Only local features)</cell><cell>65.25</cell><cell>96.76</cell></row><row><cell>(6) AL-ResNets-34(Only local features)</cell><cell>65.33</cell><cell>96.81</cell></row><row><cell>(7) AL-RoR-34</cell><cell>65.77</cell><cell>97.01</cell></row><row><cell>(8) AL-ResNets-34</cell><cell>66.03</cell><cell>97.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II AGE</head><label>II</label><figDesc>CLASSIFICATION RESULTS ON ADIENCE(10-CROP)</figDesc><table><row><cell>Method</cell><cell>Accuracy(%)</cell><cell>1-off(%)</cell></row><row><cell>Ft-101-RoR-34</cell><cell>66.74?2.69</cell><cell>97.38?0.65</cell></row><row><cell>Ft-101-ResNets-34</cell><cell>66.63?3.04</cell><cell>97.20?0.65</cell></row><row><cell>AL-RoR-34</cell><cell>66.82?2.79</cell><cell>97.36?0.70</cell></row><row><cell>AL-ResNets-34</cell><cell>67.47?2.83</cell><cell>97.33?0.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. To evaluate the impact of the attention LSTM of our models, we trained a ResNets-34 model as a baseline using global features of the LAP dataset. The visual attention component in the AL-ResNets-34 model provides a dynamic strategy to highlight the important and discriminative region of the image. To prove its effectiveness, we compare the MAE and -error results of ResNets-34 model, both of which are significantly improved. The reason is mainly derived from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III THE</head><label>III</label><figDesc>error had gains of 11.4% and 3.57% respectively, which shows the power of network depth and attention LSTM.The introduction of RoR can improve the optimization ability of ResNets by adding a few identity shortcuts. To achieve better age estimation results, it is important to choose a suitable RoR basic model for a satisfying performance.</figDesc><table><row><cell cols="3">COMPARISON RESULTS ON ADIENCE</cell></row><row><cell>Method</cell><cell>Accuracy(%)</cell><cell>1-off(%)</cell></row><row><cell>4c2f-CNN [20]</cell><cell>50.7?5.1</cell><cell>84.7?2.2</cell></row><row><cell>R-SAAFc2 [21]</cell><cell>53.5</cell><cell>87.9</cell></row><row><cell>DEX w/o IMDB-WIKI</cell><cell>55.6?6.1</cell><cell>89.7?1.8</cell></row><row><cell>Pretrain [23]</cell><cell></cell><cell></cell></row><row><cell>DEX w/ IMDB-WIKI Pre-</cell><cell>64.0?4.2</cell><cell>96.60?0.90</cell></row><row><cell>train [23]</cell><cell></cell><cell></cell></row><row><cell>V GG F -XEMD2 [22]</cell><cell>61.1</cell><cell>94.0</cell></row><row><cell>RES F -EMD [22]</cell><cell>62.2</cell><cell>94.3</cell></row><row><cell>R-SAAFc2(IMDB-</cell><cell>67.3</cell><cell>97.4</cell></row><row><cell>WIKI) [25]</cell><cell></cell><cell></cell></row><row><cell>RoR34+IMDB-WIKI with</cell><cell>66.91?2.51</cell><cell>97.49?0.76</cell></row><row><cell>two mechanisms [27]</cell><cell></cell><cell></cell></row><row><cell>RoR152+IMDB-WIKI</cell><cell>67.34?3.56</cell><cell>97.51?0.67</cell></row><row><cell>with two mechanisms [27]</cell><cell></cell><cell></cell></row><row><cell>AL-ResNets-34</cell><cell>67.47?2.83</cell><cell>97.33?0.65</cell></row><row><cell>AL-ResNets-152</cell><cell>67.83?2.98</cell><cell>97.53?0.59</cell></row><row><cell cols="3">the fact that AL-ResNets-34 network captures the age-sensitive</cell></row><row><cell cols="3">local features. Compared with the AL-ResNets-34 model, we</cell></row><row><cell cols="3">can obtain superior results by AL-ResNets-152, where relative</cell></row><row><cell>MAE and -</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV THE</head><label>IV</label><figDesc>Fig. 6. Examples of validation images where our method significantly outperforms DEX<ref type="bibr" target="#b51">[52]</ref> and RoR-34.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">COMPARISON RESULTS ON 15LAP</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>MAE(Val)</cell><cell cols="2">-error(Val)</cell><cell>-error(Test)</cell></row><row><cell cols="2">Logit Boost [38]</cell><cell>7.2949</cell><cell>0.5483</cell><cell></cell><cell>-</cell></row><row><cell cols="2">DADL [39]</cell><cell>-</cell><cell>0.3806</cell><cell></cell><cell>0.3057</cell></row><row><cell cols="2">age group [42]</cell><cell>-</cell><cell>0.3162</cell><cell></cell><cell>0.2948</cell></row><row><cell cols="2">Rich Coding [45]</cell><cell>3.29</cell><cell>0.3273</cell><cell></cell><cell>0.2872</cell></row><row><cell cols="2">AgeNet [47]</cell><cell>3.334</cell><cell>0.2922</cell><cell></cell><cell>0.2706</cell></row><row><cell>DEX1 [23]</cell><cell></cell><cell>3.252</cell><cell>0.282</cell><cell></cell><cell>0.2649</cell></row><row><cell>DEX2 [52]</cell><cell></cell><cell>3.221</cell><cell>0.278</cell><cell></cell><cell>0.2649</cell></row><row><cell>ARN [53]</cell><cell></cell><cell>3.153</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">ResNets-34</cell><cell>3.712</cell><cell>0.3167</cell><cell></cell><cell>0.3003</cell></row><row><cell cols="2">AL-ResNets-34</cell><cell>3.357</cell><cell>0.2810</cell><cell></cell><cell>0.2772</cell></row><row><cell cols="2">AL-ResNets-152</cell><cell>3.243</cell><cell>0.2778</cell><cell></cell><cell>0.2668</cell></row><row><cell cols="2">AL-RoR-34</cell><cell>3.137</cell><cell>0.2683</cell><cell></cell><cell>0.2548</cell></row><row><cell>Apparent age:</cell><cell>15</cell><cell>1 1</cell><cell>20</cell><cell>40</cell><cell>23</cell></row><row><cell>DEX:</cell><cell>27.25</cell><cell>26.35</cell><cell>34.07</cell><cell>26.63</cell><cell>35.81</cell></row><row><cell>RoR-34:</cell><cell>16.68</cell><cell>28.96</cell><cell>23.32</cell><cell>30.83</cell><cell>24.37</cell></row><row><cell>AL-RoR-34:</cell><cell>15.99</cell><cell>25.30</cell><cell>20.09</cell><cell>31.26</cell><cell>23.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V THE</head><label>V</label><figDesc>Our AL-RoR-34 model achieved a test -error of 0.2859 thereby obtaining the second place. Our result is slightly worse than OrangeLabs</figDesc><table><row><cell cols="3">COMPARISON RESULTS ON 16LAP</cell></row><row><cell>Method</cell><cell>-score(Test)</cell><cell>single model?</cell></row><row><cell>DeepAge</cell><cell>0.4573</cell><cell>Yes</cell></row><row><cell>MIPAL SNU</cell><cell>0.4569</cell><cell>No</cell></row><row><cell>Bogazici [54]</cell><cell>0.3740</cell><cell>No</cell></row><row><cell>CNN2ELM [55]</cell><cell>0.3679</cell><cell>No</cell></row><row><cell>ITU SiMiT [56]</cell><cell>0.3668</cell><cell>No</cell></row><row><cell>WYU CVL</cell><cell>0.3405</cell><cell>No</cell></row><row><cell>cmp+ETH [57]</cell><cell>0.3361</cell><cell>No</cell></row><row><cell>palm seu [58]</cell><cell>0.3214</cell><cell>No</cell></row><row><cell>DNN [59]</cell><cell>0.319</cell><cell>Yes</cell></row><row><cell>OrangeLabs [60]</cell><cell>0.2411</cell><cell>No</cell></row><row><cell>AL-RoR-34</cell><cell>0.2859</cell><cell>Yes</cell></row><row><cell>apparent age estimation.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc></figDesc><table><row><cell cols="3">RESULTS (MAE) FOR AGE ESTIMATION ON MORPH</cell></row><row><cell cols="2">ALBUM 2 AND FG-NET</cell><cell></cell></row><row><cell>Method</cell><cell>MORPH Album 2</cell><cell>FG-NET</cell></row><row><cell>AGES [28]</cell><cell>8.83</cell><cell>6.77</cell></row><row><cell>OHRank [29]</cell><cell>6.07</cell><cell>4.48</cell></row><row><cell>CA-SVR [31]</cell><cell>5.88</cell><cell>4.67</cell></row><row><cell>DLA [30]</cell><cell>4.77</cell><cell>4.26</cell></row><row><cell>BIF+KCCA [33]</cell><cell>3.98</cell><cell>-</cell></row><row><cell>CNN (Multi-Task) [34]</cell><cell>3.63</cell><cell>-</cell></row><row><cell>MF+VisReg [35]</cell><cell>3.45</cell><cell>-</cell></row><row><cell>DEX [23]</cell><cell>3.25</cell><cell>4.63</cell></row><row><cell>DEX(IMDB-WIKI) [23]</cell><cell>2.68</cell><cell>3.09</cell></row><row><cell>R-SAAFc2(IMDB-WIKI) [25]</cell><cell>-</cell><cell>3.01</cell></row><row><cell>DLDL [37]</cell><cell>2.42</cell><cell>-</cell></row><row><cell>AL-RoR-34</cell><cell>2.36</cell><cell>2.39</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What else does your biometric data reveal? A survey on soft biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Elia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="467" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Impact of facial cosmetics on automatic gender and age estimation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision Theory and Applications</title>
		<meeting>the IEEE Conference on Computer Vision Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational bayesian learning for dirichlet process mixture of inverted dirichlet distributions in Non-Gaussian image feature modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nerural Network and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="463" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video summarization with attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology, Early Access</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06878</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Age group classification in the wild with deep RoR architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generic object detection with dense neural patterns and regionlets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4316</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Residual Networks of Residual Networks: Multilevel Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1303" to="1314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pyramidal RoR for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster Computing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06557</idno>
		<title level="m">Neural networks with smooth adaptive activation functions for regression</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Squared Earth Mover&apos;s Distancebased Loss for Training Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gool</forename><forename type="middle">L</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<ptr target="http://www-prima.inrialpes.fr/FGnet" />
		<title level="m">FG-NET Aging Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ConvNets with smooth adaptive activation functions for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="430" to="439" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stages based facial demographic attributes combination for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zighem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ouafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="236" to="249" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Age Group and Gender Estimation in the Wild With Deep RoR Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="22492" to="22503" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic age estimation based on facial aging patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ordinal hyperplanes ranker with cost sensitivities for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deeply-Learned Feature for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="534" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cumulative Attribute Space for Age and Crowd Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint estimation of age, gender and ethnicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Age estimation by multi-scale convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Some like it hot-visual guidance for preference prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gool</forename><forename type="middle">L</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MORPH: A Longitudinal Image Database of Normal Adult Age-Progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Logistic boosting regression for label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep label distribution learning for apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="102" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Study on Apparent Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Face++</forename><surname>Api</surname></persName>
		</author>
		<ptr target="http://www.faceplusplus.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://www.projectoxford.ai" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeply learned rich coding for cross-dataset facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">FACESA database of facial expressions in young, middle-aged, and older women and men: Development and validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riediger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="351" to="362" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Agenet: Deeply learned regressor and classifier for robust apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-age reference coding for age-invariant face recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="768" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Web image and video mining towards universal and robust age estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1217" to="1229" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Chalearn looking at people 2015: Apparent age and cultural event recognition datasets and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gool</forename><forename type="middle">L</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Anchored Regression Networks applied to Age Estimation and Super Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gool</forename><forename type="middle">L</forename><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1643" to="1652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Kernel ELM and CNN based facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gurpinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="80" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An Ensemble CNN2ELM for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="758" to="772" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Apparent age estimation using ensemble of deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Malli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aygn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="714" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Structured output svm prediction of apparent age, gender and smile from deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uricr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep age distribution learning for apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Dager: Deep age, gender and emotion recognition using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04280</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Apparent age estimation from face images combining general and children-specialized deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Berrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Chalearn looking at people and faces of the world: Face analysis workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">ResNet models trainined on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cubbee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gross</surname></persName>
		</author>
		<ptr target="https://github.com/facebook/fb.resnet.torch/tree/master/pretrained" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
