<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
							<email>jinliang@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
							<email>andreas.lugmayr@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
							<email>kai.zhang@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<email>martin.danelljan@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
							<email>timofter@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyunliang</forename><forename type="middle">/</forename><surname>Com/</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hcflow</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalizing flows have recently demonstrated promising results for low-level vision tasks. For image superresolution (SR), it learns to predict diverse photo-realistic high-resolution (HR) images from the low-resolution (LR) image rather than learning a deterministic mapping. For image rescaling, it achieves high accuracy by jointly modelling the downscaling and upscaling processes. While existing approaches employ specialized techniques for these two tasks, we set out to unify them in a single formulation. In this paper, we propose the hierarchical conditional flow (HCFlow) as a unified framework for image SR and image rescaling. More specifically, HCFlow learns a bijective mapping between HR and LR image pairs by modelling the distribution of the LR image and the rest high-frequency component simultaneously. In particular, the high-frequency component is conditional on the LR image in a hierarchical manner. To further enhance the performance, other losses such as perceptual loss and GAN loss are combined with the commonly used negative loglikelihood loss in training. Extensive experiments on general image SR, face image SR and image rescaling have demonstrated that the proposed HCFlow achieves state-ofthe-art performance in terms of both quantitative metrics and visual quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Normalizing flows <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> are powerful deep generative probabilistic models that allow for efficient and exact likelihood calculation and sampling. They have been used in the generation of image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>, blur kernel <ref type="bibr" target="#b23">[24]</ref>, and audio <ref type="bibr" target="#b15">[16]</ref> data. Recently, in the low-level vision community, normalizing flows have attracted much interest and have achieved promising progress for image super-resolution (SR) <ref type="bibr" target="#b27">[28]</ref> and image rescaling <ref type="bibr" target="#b38">[39]</ref>. <ref type="bibr">*</ref> Corresponding author.  <ref type="figure">Figure 1</ref>: The comparison between SRFlow <ref type="bibr" target="#b27">[28]</ref>, IRN <ref type="bibr" target="#b38">[39]</ref> and the proposed HCFlow. x, y and z denote HR image, LR image and the latent variable, respectively. Blue boxes are invertible neural networks, while green ones are non-invertible models (e.g., CNN). Solid bi-directional arrows denote bijective mappings, while dashed arrows represent conditional relations. SRFlow <ref type="bibr" target="#b27">[28]</ref> is a seminal flow-based model for image SR. Unlike previous CNN-based models that learn a deterministic mapping from the low-resolution (LR) image to the high-resolution (HR) image, SRFlow learns the distribution of HR images and is able to generate diverse photo-realistic HR images. However, as shown in <ref type="figure">Fig. 1(a)</ref>, it treats the LR image as an external conditional prior and thus is not fully invertible between HR and LR image pairs, making it hard to be used for image rescaling. Another work IRN <ref type="bibr" target="#b38">[39]</ref> employs an invertible neural network to learn downscaling and upscaling for image rescaling. Since the model is bijective, it can recover the input HR image with high accuracy after downscaling. Nevertheless, as shown in <ref type="figure">Fig. 1(b)</ref>, it assumes the high-frequency and low-frequency components of the image are independent to each other and thus lacks the ability to exploit their dependency for image SR.</p><p>In this paper, we propose a hierarchical conditional flow (HCFlow) as a unified framework for both image SR and rescaling. As shown in <ref type="figure">Fig. 1(c)</ref>, HCFlow is an invertible flow-based model for modelling the HR-LR relationship, in which the high-frequency component is hierarchically conditional on the low-frequency component of the image. More specifically, in the forward propagation, HCFlow learns to decompose the input HR image into the LR image and a latent variable. In the inverse propagation, it generates HR images based on the LR input and random samples of the latent variable. The modelling of the latent variable (high-frequency component) is conditional on the generated LR image (low-frequency component) in a hierarchical manner.</p><p>When trained for image SR, HCFlow is optimized by minimizing the negative log-likelihood loss on the basis of tractable Jacobian determinant computation. To further improve visual quality, we integrate a pixel loss, perceptual loss, and GAN loss in the inverse propagation to constrain the learned HR space. Moreover, HCFlow can be used for the image rescaling task. It can decompose the HR image to a visually-pleasing LR image and a latent variable that follows a simple distribution. In this case, HCFlow is trained as an encoder-decoder framework, in which the forward and inverse processes are jointly optimized. As HCFlow is bijective, it can recover the HR image faithfully by sampling from the latent space given the generated LR image.</p><p>Our contributions can be summarized as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we will briefly review image SR and image rescaling with a particular focus on two highly related flow-based methods, i.e., SRFlow <ref type="bibr" target="#b27">[28]</ref> and IRN <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image SR</head><p>Image SR aims to reconstruct the HR image given the LR image. Since the pioneer work SRCNN <ref type="bibr" target="#b6">[7]</ref>, many CNNbased models have been proposed in recent years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42]</ref>. Most of them focus on delicate feature extraction module design and generate over-smoothed images when trained with the pixel loss. To remedy this, the perceptual loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref> and GAN loss <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45]</ref> are introduced to improve the perceptual quality. Despite of above progresses, they usually learn a deterministic mapping between the LR image and HR image, which is unnatural for image SR since one LR image may correspond to multiple HR images. SRFlow <ref type="bibr" target="#b27">[28]</ref>. Normalizing flows <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> provide a new possible solution for image SR. SRFlow designs a conditional flow to model the distribution of HR images, conditional on LR images. It can generate diverse photo-realistic images by sampling different latent variables. Our proposed HCFlow differs from SRFlow in two main aspects: First, SRFlow uses the LR image as an external conditional prior and maps the HR distribution to a simple latent distribution. Therefore, it cannot generate LR image and thus is not applicable for image rescaling. In contrast, HCFlow models the LR image and treats it as part of the latent space. Second, SRFlow basically follows the flow framework proposed in <ref type="bibr" target="#b5">[6]</ref>, while HCFlow proposes a new framework with hierarchical conditional mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image Rescaling</head><p>Image rescaling aims to downscale the HR image to a visually meaningful LR image, and then recover the HR image plausibly. Different from image SR that works on a given LR image space, image rescaling tries to maintain as much information from the HR image as possible for a better subsequent reconstruction, for the purpose of reducing the storage and bandwidth cost. In other words, it can define its own LR image space which is expected to be more informative than that by simple downscaling such as bicubic downscaling. In general, in image rescaling, the downscaling and upscaling processes are jointly modelled by an encoder-decoder framework <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>, so that the downscaling model is optimized for the later upscaling operation. IRN <ref type="bibr" target="#b38">[39]</ref>. Recently, IRN proposes to use a bijective invertible neural network to model the downscaling and upscaling processes. High-frequency component is well-captured and transformed to a structured latent space in training. In testing, the HR image can be recovered by inputting the generated LR image and a randomly sampled latent variable. In particular, IRN assumes the LR image and high-frequency component is independent to each other. These two components are divided apart and learned separately. By contrast, HCFlow assumes the removed high-frequency component is dependent on the LR image and thus employs a hierarchical conditional framework to model the LR image and the conditional distribution of the high-frequency component. Besides, although IRN designs a bijective mapping between HR and LR image pairs, it can only be trained by Monte Carlo simulation rather than maximum likelihood estimation (MLE). HCFlow can be trained in the same way for image rescaling, but it further models the LR image distribution and allows for tractable Jacobian determinant computation, making it possible for probabilistic modelling of HR and LR images when trained by MLE.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Flow-based models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24]</ref> aim to learn a bijective mapping between the target space and the latent space. For a high-dimensional random variable (e.g., an image) x with distribution x ? p(x) and a latent variable z with simple tractable distribution z ? p(z) (e.g., multivariate Gaussian distribution), flow models generally use an invertible neural network f ? to transform x to z: z = f ? (x). Conversely, x can be recovered from z by the inverse mapping</p><formula xml:id="formula_0">x = f ?1 ? (z). Generally, f ? is composed of a series of invertible trans- formations: f ? = f 1 ? ? f 2 ? ? ? ? ? ? f K ? . The intermediate vari- ables are defined as h k = f k ? (h k?1 ) for k ? {1, .</formula><p>.., K}. The input h 0 and output h N of f ? are x and z, respectively. Concretely, f k ? are flow layers such as squeeze layer, batch normalization layer, affine coupling layer, etc.</p><p>According to the change of variable formula and the chain rule, for a sample x, the log probability log(x) can be calculated as</p><formula xml:id="formula_1">log p(x) = log p(f ? (x)) + K k=1 log det ?f k ? (h k?1 ) ?h k?1 ,<label>(1)</label></formula><p>where log det</p><formula xml:id="formula_2">?f k ? (h k?1 ) ?h k?1</formula><p>is the logarithm of the absolute value of the determinant of the Jacobian of f k ? at h k?1 . The flow model can thereby be optimized by minimizing the negative log-likelihood loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Specification</head><p>Both image SR and image rescaling try to reconstruct the HR image x given a LR image. Since the image degradation process (or image downscaling) is the inverse of image super-resolution (or image upscaling), we can model these two processes with an invertible bijective transformation:</p><p>x ? [y, a], where y and a are the generated LR image and the rest high-frequency component, respectively. As modelling the probability of natural images is a non-trivial task, it is reasonable to design a flow model conditional on the ground-truth LR image y * as, p(x|y * ) ? p(y, a|y * ) = p(y|y * )p(a|y, y * ).</p><p>(</p><p>Ideally, we hope the model can generate exactly the same LR image as the ground-truth LR image. This can be formulated as a Dirac delta function ?(y ? y * ) and further approximated by a multivariate Gaussian distribution as,</p><formula xml:id="formula_4">p(y|y * )p(a|y, y * ) = ?(y ? y * )p(a|y) = lim ??0 N (y|y * , ?)p(a|y),<label>(3)</label></formula><p>where ? is a diagonal covariance matrix with all diagonal elements close to zero. Note that y is nearly equal to y * in this case. By further mapping p(a|y) to a standard multivariate Gaussian distribution p(z) = N (z|0, I), the flow model is defined as,</p><formula xml:id="formula_5">p(x|y * ) ? lim ??0 N (y|y * , ?)N (z|0, I).<label>(4)</label></formula><p>As we can see, part of the latent space is constrained to be the LR image space. In particular, decomposed highfrequency component a is conditional on another decomposed component y. Once trained, following the forward direction, HCFlow can decompose the HR image x into LR image y and latent variable z that follows a simple distribution. Following the inverse direction, HCFlow can generate x given the LR image input y * and a random sample z from the latent distribution, as it is an invertible bijective model. Note that this model regards y * as an input or output, rather than as an external conditional prior. Therefore, it is not explicitly conditional on y * and is fully invertible between HR and LR image pairs. Besides, by approximating the the distribution of y with a multi-variate Gaussian distribution, it allows for tractable Jacobian determinant computation, so that the model can be optimized by maximum likelihood estimation (MLE). Similarly, y1 is decomposed to y2 (i.e., the LR image in this case) and a2 in the next level. a1 and a2 are transformed to latent variables z1 and z2, conditional on ?1([?2(y2), y1]) and ?2(y2) (note that ?1 and ?2 are feature extractors, e.g., CNN) respectively, in a hierarchical manner. The model is trained by negative log-likelihood loss, and can be further enhanced by pixel loss, perceptual loss and GAN loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Architecture</head><p>The multi-scale architecture proposed in RealNVP <ref type="bibr" target="#b5">[6]</ref> is a popular normalizing flow architecture <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9]</ref>. It consists of L levels and at the end of each level, half of the dimensions are factored out. Generally, the factored out dimensions are directly Gaussianized for the computation of negative log-likelihood loss, lacking sufficient modelling of these dimensions. Therefore, based on the multi-scale architecture, we take a further step to model factored out dimensions conditional on the reserved dimensions.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, at each level l, y l?1 is decomposed to low-frequency component y l and high-frequency component a l . Then, a l is modelled by an additional flow that is conditional on the concatenation of tensors y L , y L?1 , ..., y l from multiple flow levels. By this design, the reconstruction of high-frequency component is hierarchically conditional on frequencies reconstructed from all previous levels. In forward propagation, similar to the depth-first traversal of a binary tree, we first compute y 1 , y 2 , ..., y L in order. Then, we model the factored out dimensions in a reverse order: a L , a L?1 , ..., a 1 . In inverse propagation, we compute y l and a l level by level, from level L to level 1. Note that the determinant of Jacobian of the whole flow can still be efficiently computed, since the conditional relations between a 1 , a 2 , ..., a L and y 1 , y 2 , ..., y L can be represented as an upper triangle block matrix.</p><p>The detailed architecture of HCFlow is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. For each level, the first layer is the squeeze layer, which transforms the H ? W ? C input to a H 2 ? W 2 ? 4C tensor by trading spatial size for number of channels. Then, K flow-steps are used for transforming the tensor and decomposing it into different components. More specifically, each flow-step consists of a sequence of three layers: Actnorm layer, invertible 1 ? 1 convolution layer and affine coupling layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>. After that, the split layer is used to evenly split the tensor into two tensors y l ? R</p><formula xml:id="formula_6">H 2 ? W 2 ?2C</formula><p>and a l ? R H 2 ? W 2 ?2C along the channel dimension. Note that, for the last level, we only keep 3 channels for y l to make it fit the RGB space of the LR image. Next, y l is fed to the next level, while a l is input into an additional flow.</p><p>In the l-th additional flow, a l is transformed to the latent variable z l by P flow-steps. Different from above flowsteps, we use conditional affine coupling layer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref> rather than ordinary affine coupling layer to obtain a conditional flow. In particular, we first upscale the conditional feature c l+1 from level l + 1 by ?2 nearest neighbor interpolation, and concatenate it with y l . Then, we use a feature extractor ? l to extract image features, which act as the conditional feature c l for level l. Note that the feature extractor only provides scale and shift for an affine coupling during both forward and inverse propagation. Hence the constrains on being invertible and having a tractable Jacobian do not hold for this part. More formally, the hierarchical conditional mechanism of HCFlow is formulated as follows,</p><formula xml:id="formula_7">c l = ? l (y l ) l = L ? l ([c L , c L?1 , ..., c l+1 , y l ]) l = L ? 1, ..., 1 ,<label>(5)</label></formula><p>where conditional features of different levels are computed in a reverse order, from c L to c 1 . Particularly, for the last level, we directly model y L by a Dirac delta function ?(y ? y * ) instead of transforming it to another latent variable. This constrains part of the latent space to be the LR image space and implicitly makes the model be conditional on y * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Objectives</head><p>Image SR. When HCFlow is used for image SR, it can be trained by minimizing the negative log-likelihood loss,</p><formula xml:id="formula_8">L nll = ? log p(x),<label>(6)</label></formula><p>which is unsupervised and converges stably. However, in practice, this loss converges slowly and does not provide strong supervision for image SR. To achieve better HR image PSNR, we can add L 1 pixel loss on the generated SR image in inverse propagation, leading to a loss function as follows,</p><formula xml:id="formula_9">L = ? 1 L nll (x) + ? 2 L pixel (x, x ? =0 ),<label>(7)</label></formula><p>where x is the ground-truth HR image and x ? =0 is the generated SR image by inputting the ground-truth LR image y * and sampling the latent variable z with temperature ? = 0.</p><p>The added pixel loss can help the flow to learn the SR manifold centered around the PSNR-oriented SR image. Furthermore, we can add perceptual loss <ref type="bibr" target="#b12">[13]</ref> and GAN loss <ref type="bibr" target="#b7">[8]</ref> on the generated SR image to improve the visual quality. This is formulated as,</p><formula xml:id="formula_10">L =? 1 L nll (x) + ? 2 L pixel (x, x ? =0 ) + ? 3 L percep (x, x ? =?0 ) + ? 4 L gan (x, x ? =?0 ),<label>(8)</label></formula><p>where x ? =?0 is the generated SR image by inputting y * and sampling z with ? = ? 0 . Note that unlike the pixel loss that uses ? = 0, ? 0 is set to 0.8 or 0.9 to preserve the diversity of HR images.</p><p>Image rescaling. Different from image SR, image rescaling aims to recover exactly the same HR image. Following <ref type="bibr" target="#b38">[39]</ref>, we regard the invertible HCFlow as an encoderdecoder framework, in which the forward and inverse processes correspond to the encoding and decoding stages, respectively. The loss is as follows,</p><formula xml:id="formula_11">L =? 1 L pixel hr (x, x ? =1 ) + ? 2 L pixel lr (y * , y) + ? 3 L latent (z),<label>(9)</label></formula><p>where L pixel hr is the L 1 pixel loss to ensure that, after downscaling and upscaling, the reconstructed image x ? =1 is close to the input x. Note that this loss would dramatically decrease the diversity of generated images. Besides, L pixel lr is the L 2 pixel loss on the LR image, which guides y to be close to the bicubic LR image y * , so as to generate visually-pleasing LR images in downscaling. The last term L latent (z) is the L 2 regularization on the latent variable z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We conduct experiments on general image SR, face image SR and image rescaling to show the effectiveness of HCFlow. For image SR experiments, we train the model by three loss combinations: L nll , L nll + L pixel and L nll + L pixel + L percep &amp;L gan . The corresponding learned models are denoted as HCFlow, HCFlow+ and HCFlow++, respectively.</p><p>Image SR. For general image SR (?4), we set L, K, P to 2, 13 and 13, respectively. Two 13-block RRDB networks <ref type="bibr" target="#b35">[36]</ref> are used as feature extractors. More details on the architecture are provided in the supplementary. The model is trained on the training set of DIV2K <ref type="bibr" target="#b0">[1]</ref> and Flickr2K <ref type="bibr" target="#b34">[35]</ref> with random flips. The crop patch size and mini-batch size are set to 160 ? 160 and 16, respectively. Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with ? 1 = 0.9 and ? 2 = 0.99 is used for optimization. For HCFLow (with only L nll ), the learning rate is 2.5 ? 10 ?4 and reduced by half at 50%, 75%, 90% and 95% of 300k iterations. We fine-tune HCFLow+ (with L nll + L pixel ) for 50k iterations from the pretrained HCFlow. The weight of L nll and L pixel are ? 1 = 2 ? 10 ?3 and ? 2 = 1, respectively. It is worth pointing out that we can achieve even higher PSNR (about 0.2dB) if we train HCFlow+ from scratch. Similarly, we can fine-tune HCFlow++ by further adding L percep and L gan . The loss weighting parameters are ? 1 = 2 ? 10 ?3 , ? 2 = 1, ? 3 = 5 ? 10 ?2 and ? 4 = 5 ? 10 ?1 .</p><p>For face image SR (?8), L, K, P are set to 3, 13 and 13, respectively. Three 8-block RRDB networks are used as feature extractors. We train the model on the CelebA training set <ref type="bibr" target="#b26">[27]</ref> and test it using first 5,000 images from the testing set. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>, we crop and resize the HR images to the resolution of 160 ? 160, and flip them randomly for data augmentation. Other training details are the same as general image SR.</p><p>Image rescaling. For image rescaling (?4), we set L, K, P to 2, 8 and 6, respectively. Two 3-block RRDB networks are used as feature extractors. In particular, we use Haar transformation to replace the squeeze layer and remove invertible 1 ? 1 convolution layers. Details on data preparation and optimizer are the same as general image SR. The learning rate is initialized as 2.5 ? 10 ?4 and halved at [100k, 200k, 300k, 400k] (500k iterations in total). The loss weighting parameters are ? 1 = 1, ? 2 = 5 ? 10 ?2 and ? 3 = 1 ? 10 ?5 , respectively.</p><p>Performance evaluation. Following SRFlow <ref type="bibr" target="#b27">[28]</ref> and IRN <ref type="bibr" target="#b38">[39]</ref>, we evaluate PSNR and SSIM on the RGB color space for image SR, and on the Y channel of the YCbCr color space for image rescaling. We also use perceptual metric LPIPS <ref type="bibr" target="#b43">[44]</ref> and two no-reference metrics, NIQE <ref type="bibr" target="#b30">[31]</ref> and BRISQUE <ref type="bibr" target="#b29">[30]</ref>, for better visual quality comparison. Pixel standard deviation of 5 samples are used to compare the diversity of results. In addition, Consistency (PSNR between the downscaled SR image and the ground-truth LR image) and LR-PSNR (PSNR between the generated LR image in forward propagation and the ground-truth LR image) are also reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Fitting to the LR image space. To learn a fully invertible flow between HR and LR image pairs, HCFlow constrains   part of the latent space to be the LR image space, instead of using the LR image as an external prior. To show the impact, we remove the LR image y 2 from the latent space as shown in case 1 and 2 of <ref type="table" target="#tab_1">Table 1</ref>. When there is no conditional prior (case 1), the model fails to converge as it does not have enough information for SR . When we replace y 2 with ground-truth LR image y * as a conditional prior (case 2, similar to SRFlow <ref type="bibr" target="#b27">[28]</ref>), it achieves slightly better performance than HCFlow although they have almost the same conditional information. The underlying reason might be that it has a larger latent space than HCFlow.</p><p>Ground-truth LR image as a conditional prior. HCFlow is conditional on y 1 and y 2 , which are generated during propagation. When we use the ground-truth LR image y * as a conditional prior to replace y 2 (case 3, <ref type="table" target="#tab_1">Table 1)</ref>, the model achieves similar performance as HCFlow. In fact, since we model the distribution of y 2 as a Dirac delta function ?(y 2 ? y * ), y 2 would be nearly equal to y * after model convergence, which is confirmed by the high LR-PSNR. Therefore, conditional on the generated y 2 and the external y * have similar effects.</p><p>Hierarchical conditional mechanism. As shown in case 4 of <ref type="table" target="#tab_1">Table 1</ref>, similar to IRN <ref type="bibr" target="#b38">[39]</ref>, we assume the LR image and the rest high-frequency component is independent by removing all conditional priors. It yields significantly worse performance because the reconstruction of HR image (highfrequency component) is highly conditional on the LR image (low-frequency component) for image SR. Despite this, it has better results than case 1, as fitting to the LR image space could partly play the role of conditional prior. In case 5, we change from hierarchical conditional mechanism to single-scale conditional mechanism, by removing y 2 from level 1. In this case, z l (l = 1, 2) is only conditional on y l from the same level. The performance drops in terms of all kinds of metrics, which shows that the hierarchical conditional mechanism can better model the conditional relations between high-frequency and low-frequency components.  and RRDB <ref type="bibr" target="#b35">[36]</ref>, perception-oriented ESRGAN <ref type="bibr" target="#b35">[36]</ref> and RankSRGAN <ref type="bibr" target="#b44">[45]</ref>, as well as SRFlow <ref type="bibr" target="#b27">[28]</ref>. All methods are trained on the same training dataset. From <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure">Fig. 4</ref>  Set14 <ref type="bibr" target="#b39">[40]</ref> BSD100 <ref type="bibr" target="#b28">[29]</ref> Urban100 <ref type="bibr" target="#b10">[11]</ref> DIV2K <ref type="bibr" target="#b0">[1]</ref> include PSNR-oriented RRDB, perception-oriented ESR-GAN and the flow-based SRFlow. As shown in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure">Fig. 5</ref>, similar observations as in general image SR can be concluded for face image SR. HCFlow achieves best quantitative and visual performance compared with competing methods. In particular, HCFlow generates sharp faces with natural details, especially on eyes, teeth and hairs. By comparison, other methods suffer from either over-smoothed results or obvious artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Image SR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Image Rescaling</head><p>As a unified framework for image SR and image rescaling, HCFlow also achieves state-of-the-art performance in image rescaling. We compare it with three kinds of rescaling methods: (1) bicubic interpolation &amp; state-of-the-art SR models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26]</ref>; (2) encoder-decoder models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>; (3) invertible neural networks <ref type="bibr" target="#b38">[39]</ref>.</p><p>As can be seen from <ref type="table" target="#tab_6">Table 4</ref>, when the downscaling process is fixed (i.e., bicubic interpolation), performances of different state-of-the-art SR models are similar and limited. When the downscaling models are optimized for the upscaling models, the results are largely improved. IRN further boosts the performance by joint optimization based on the invertible architecture. Compared with IRN, the proposed HCFlow achieves better performance on all testing datasets with an increased PSNR of 0.10 ? 0.35dB. Besides, as shown in <ref type="figure">Fig. 6</ref>, HCFlow can better preserve image details and generates sharper edges than IRN. Since these two models have same number of parameters, HCFlow is more efficient than IRN for image rescaling, which can be attributed to the conditional modelling between highfrequency and low-frequency components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a unified framework, i.e., hierarchical conditional flow (HCFlow), for both image superresolution and image rescaling. It learns a fully invertible mapping between HR image and LR image as well as the latent variable. Particularly, we learn the LR image space and IRN HCFlow Ground Truth <ref type="figure">Figure 6</ref>: Visual results of image rescaling (?4) on the DIV2K <ref type="bibr" target="#b0">[1]</ref> validation set. More results are shown in the supplementary. design a hierarchical conditional mechanism between the latent variable (high-frequency component) and the LR image (low-frequency component). For image SR, HCFLow is trained by the negative log-likelihood loss, and is further enhanced by pixel loss, perceptual loss and GAN losses for better performance. For image rescaling, it is trained as an encoder-decoder framework, where the forward and inverse progresses are jointly optimized. Experiments demonstrate that HCFlow achieves state-of-the-art performance on general image SR, face image SR and image rescaling, in terms of both quantitative metrics and visual quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Schematic computational graphs of the hierarchical conditional flow (HCFlow) with 3 flow levels. On level l, y l?1 (note that y0 = x) is decomposed to low-frequency component y l and high-frequency component a l . The transformation between a l and z l is conditional on [yL, yL?1, ..., y l ], as indicated by the blue arrows. The computation orders in forward and inverse propagation are shown on the top of each node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of the hierarchical conditional flow (HCFlow) with 2 flow levels. For a HR image x, we first squeeze, transform and split it to low-frequency component y1 and high-frequency component a1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visual results of general image SR (?4) on the DIV2K [1] validation set. Visual results of face image SR (?8) on the CelebA [27] testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on latent space and conditional priors for general image SR (?4). Results are tested on DIV2K<ref type="bibr" target="#b0">[1]</ref> validation set.</figDesc><table><row><cell>Case</cell><cell>Latent Space</cell><cell>Conditional Prior (l = 2)</cell><cell>Conditional Prior (l = 1)</cell><cell>PSNR? (? = 0)</cell><cell>SSIM? (? = 0)</cell><cell>LPIPS? (? = 0.9)</cell><cell>Consistency ? (? = 0.9)</cell><cell>LR-PSNR?</cell></row><row><cell>1</cell><cell>z 2 , z 1</cell><cell>-</cell><cell>-</cell><cell>4.76</cell><cell>0.34</cell><cell>0.863</cell><cell>10.56</cell><cell>-</cell></row><row><cell>2</cell><cell>z 2 , z 1</cell><cell>y  *</cell><cell>y  *  , y 1</cell><cell>28.73</cell><cell>0.81</cell><cell>0.123</cell><cell>41.97</cell><cell>-</cell></row><row><cell>3</cell><cell>y 2 , z 2 , z 1</cell><cell>y  *</cell><cell>y  *  , y 1</cell><cell>28.71</cell><cell>0.81</cell><cell>0.124</cell><cell>41.79</cell><cell>52.77</cell></row><row><cell>4</cell><cell>y 2 , z 2 , z 1</cell><cell>-</cell><cell>-</cell><cell>18.95</cell><cell>0.47</cell><cell>0.361</cell><cell>40.79</cell><cell>53.88</cell></row><row><cell>5</cell><cell>y 2 , z 2 , z 1</cell><cell>y 2</cell><cell>y 1</cell><cell>28.60</cell><cell>0.80</cell><cell>0.126</cell><cell>41.94</cell><cell>52.19</cell></row><row><cell>HCFlow</cell><cell>y 2 , z 2 , z 1</cell><cell>y 2</cell><cell>y 2 , y 1</cell><cell>28.71</cell><cell>0.81</cell><cell>0.124</cell><cell>42.01</cell><cell>53.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>General image SR (?4) results on DIV2K<ref type="bibr" target="#b0">[1]</ref> validation set. For SRFlow and our method, the mean results of 5 draws are reported.</figDesc><table><row><cell>Method</cell><cell>#Param</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>LPIPS?</cell><cell>NIQE?</cell><cell cols="2">BRISQUE? Diversity?</cell><cell cols="2">Consistency? LR-PSNR?</cell></row><row><cell>Bicubic</cell><cell>-</cell><cell>26.70</cell><cell>0.77</cell><cell>0.409</cell><cell>5.20</cell><cell>53.8</cell><cell>0</cell><cell>38.70</cell><cell>-</cell></row><row><cell>EDSR [25]</cell><cell>43.1M</cell><cell>28.98</cell><cell>0.83</cell><cell>0.270</cell><cell>4.46</cell><cell>43.3</cell><cell>0</cell><cell>54.89</cell><cell>-</cell></row><row><cell>RRDB [36]</cell><cell>16.7M</cell><cell>29.44</cell><cell>0.84</cell><cell>0.253</cell><cell>5.08</cell><cell>52.4</cell><cell>0</cell><cell>49.20</cell><cell>-</cell></row><row><cell>ESRGAN [36]</cell><cell>16.7M</cell><cell>26.22</cell><cell>0.75</cell><cell>0.124</cell><cell>2.61</cell><cell>22.7</cell><cell>0</cell><cell>39.03</cell><cell>-</cell></row><row><cell>RankSRGAN [45]</cell><cell>13.7M</cell><cell>26.55</cell><cell>0.75</cell><cell>0.128</cell><cell>2.45</cell><cell>17.2</cell><cell>0</cell><cell>42.33</cell><cell>-</cell></row><row><cell>SRFlow, ? = 0 [28]</cell><cell>39.5M</cell><cell>29.07</cell><cell>0.81</cell><cell>0.254</cell><cell>5.20</cell><cell>39.4</cell><cell>0</cell><cell>55.13</cell><cell>-</cell></row><row><cell>SRFlow, ? = 0.9 [28]</cell><cell>39.5M</cell><cell>27.09</cell><cell>0.76</cell><cell>0.121</cell><cell>3.57</cell><cell>17.8</cell><cell>5.6</cell><cell>49.96</cell><cell>-</cell></row><row><cell>HCFlow, ? = 0</cell><cell>23.2M</cell><cell>28.71</cell><cell>0.81</cell><cell>0.285</cell><cell>4.61</cell><cell>44.1</cell><cell>0</cell><cell>42.03</cell><cell>53.37</cell></row><row><cell>HCFlow, ? = 0.9</cell><cell>23.2M</cell><cell>27.02</cell><cell>0.76</cell><cell>0.124</cell><cell>2.79</cell><cell>21.7</cell><cell>4.8</cell><cell>42.01</cell><cell>53.37</cell></row><row><cell>HCFlow+, ? = 0</cell><cell>23.2M</cell><cell>29.25</cell><cell>0.83</cell><cell>0.212</cell><cell>4.45</cell><cell>43.2</cell><cell>0</cell><cell>51.11</cell><cell>53.95</cell></row><row><cell>HCFlow++, ? = 0.9</cell><cell>23.2M</cell><cell>26.61</cell><cell>0.74</cell><cell>0.110</cell><cell>2.85</cell><cell>22.0</cell><cell>5.2</cell><cell>50.07</cell><cell>52.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Face image SR (?8) results on CelebA<ref type="bibr" target="#b26">[27]</ref> testing set. For SRFlow and our method, the mean results of 5 draws are reported.</figDesc><table><row><cell>Method</cell><cell>#Param</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>LPIPS?</cell><cell>NIQE?</cell><cell cols="2">BRISQUE? Diversity?</cell><cell cols="2">Consistency? LR-PSNR?</cell></row><row><cell>Bicubic</cell><cell>-</cell><cell>23.15</cell><cell>0.63</cell><cell>0.517</cell><cell>7.82</cell><cell>58.6</cell><cell>0</cell><cell>35.19</cell><cell>-</cell></row><row><cell>RRDB [36]</cell><cell>16.7M</cell><cell>26.59</cell><cell>0.77</cell><cell>0.230</cell><cell>6.02</cell><cell>49.7</cell><cell>0</cell><cell>48.22</cell><cell>-</cell></row><row><cell>ESRGAN [36]</cell><cell>16.7M</cell><cell>22.88</cell><cell>0.63</cell><cell>0.120</cell><cell>3.46</cell><cell>23.7</cell><cell>0</cell><cell>34.04</cell><cell>-</cell></row><row><cell>SRFlow, ? = 0 [28]</cell><cell>40.0M</cell><cell>26.74</cell><cell>0.76</cell><cell>0.216</cell><cell>5.74</cell><cell>40.4</cell><cell>0</cell><cell>56.57</cell><cell>-</cell></row><row><cell>SRFlow, ? = 0.8 [28]</cell><cell>40.0M</cell><cell>25.24</cell><cell>0.71</cell><cell>0.110</cell><cell>4.20</cell><cell>23.2</cell><cell>5.2</cell><cell>50.85</cell><cell>-</cell></row><row><cell>HCFlow, ? = 0</cell><cell>27.0M</cell><cell>26.66</cell><cell>0.77</cell><cell>0.210</cell><cell>6.42</cell><cell>48.0</cell><cell>0</cell><cell>51.83</cell><cell>54.50</cell></row><row><cell>HCFlow, ? = 0.8</cell><cell>27.0M</cell><cell>24.99</cell><cell>0.71</cell><cell>0.104</cell><cell>4.34</cell><cell>31.6</cell><cell>5.9</cell><cell>51.81</cell><cell>54.50</cell></row><row><cell>HCFlow+, ? = 0</cell><cell>27.0M</cell><cell>27.02</cell><cell>0.78</cell><cell>0.212</cell><cell>6.04</cell><cell>49.5</cell><cell>0</cell><cell>51.11</cell><cell>53.95</cell></row><row><cell>HCFlow++, ? = 0.8</cell><cell>27.0M</cell><cell>24.83</cell><cell>0.69</cell><cell>0.090</cell><cell>3.87</cell><cell>23.8</cell><cell>4.0</cell><cell>51.57</cell><cell>51.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, we have several observations as follows. First, when sampling HR images with temperature ? = 0, HCFlow acts like a PSNR-oriented model, achieving similar performance as EDSR and RRDB. Adding the HR pixel loss (i.e., HCFlow+) can further improve the PSNR and SSIM by large margins. Second, when ? = 0.9, the perceptual metrics of HCFlow are boosted dramatically. With perceptual loss and GAN loss (i.e., HCFlow++), the perceptual metrics are further improved by significant margins in terms of LPIPS and BRISQUE, which is confirmed by the visual results. Note that, unlike ESRGAN and RankSR-GAN, the generated HR images of HCFlow++ are still diversified. Third, HCFlow achieves state-of-the-art performance in terms of both quantitative metrics and visual quality. It generates sharp images with few artifacts. In contrast, RRDB and SRFlow tend to produce blurry images, while ESRGAN and RankSRGAN suffer from over-sharpen artifacts and distortions. In addition, HCFlow only has about half of the number of parameters compared with SRFlow.</figDesc><table /><note>Face image SR. We also test HCFlow on face image SR (?8) to show its effectiveness. The compared methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Image rescaling (?4) results (Y-channel PSNR / SSIM) on different datasets. For IRN<ref type="bibr" target="#b38">[39]</ref> and our method, the mean results of 5 draws are reported. Differences of PSNR / SSIM of different samples are less than 0.02.</figDesc><table><row><cell>Downscaling &amp; Upscaling</cell><cell>Param</cell><cell>Set5 [4]</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Dr. Suryansh Kumar for helpful discussion. This work was partially supported by the ETH Zurich Fund (OK), a Huawei Technologies Oy (Finland) project, the China Scholarship Council and a Microsoft Azure grant. Special thanks goes to Yijue Chen.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing inverse problems with invertible neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Lynton Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?the</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynton</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>L?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>K?the</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02392</idno>
		<title level="m">Guided image generation with conditional invertible neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Christine Guillemot, and Marie line Alberi Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="135" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00779</idno>
		<title level="m">Neural autoregressive flows</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyank</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Selby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02325</idno>
		<title level="m">Sum-ofsquares polynomial flow</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Task-aware image downscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="399" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Gil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02155</idno>
		<title level="m">Flowavenet: A generative flow for raw audio</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for image compact-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1092" to="1107" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SwinIR: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mutual affine network for spatially variant kernel estimation in blind image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flow-based kernel prior with application to blind super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Residual feature aggregation network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2359" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blind/referenceless image spatial quality evaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="723" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Making a &quot;completely blind&quot; image quality analyzer. IEEE Signal processing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="209" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyank</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02731</idno>
		<title level="m">Survae flows: Surjections to bridge the gap between vaes and flows</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learned image downscaling for upscaling using content adaptive resampler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4027" to="4040" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00042</idno>
		<title level="m">Learning likelihoods with conditional normalizing flows</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deflow: Learning complex image degradations from unpaired data with conditional flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Invertible image rescaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="126" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep unfolding network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3217" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Plug-and-play image restoration with deep denoiser prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Designing a practical degradation model for deep blind image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ranksrgan: Generative adversarial networks with ranker for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
