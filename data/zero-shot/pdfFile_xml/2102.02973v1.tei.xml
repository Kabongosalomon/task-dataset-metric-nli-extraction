<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Show, Attend and Distill: Knowledge Distillation via Attention-based Feature Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingi</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advenced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
							<email>bh.heo@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI LAB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
							<email>sungrae.park@navercorp.com</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CLOVA AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Show, Attend and Distill: Knowledge Distillation via Attention-based Feature Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation extracts general knowledge from a pretrained teacher network and provides guidance to a target student network. Most studies manually tie intermediate features of the teacher and student, and transfer knowledge through predefined links. However, manual selection often constructs ineffective links that limit the improvement from the distillation. There has been an attempt to address the problem, but it is still challenging to identify effective links under practical scenarios. In this paper, we introduce an effective and efficient feature distillation method utilizing all the feature levels of the teacher without manually selecting the links. Specifically, our method utilizes an attention-based meta-network that learns relative similarities between features, and applies identified similarities to control distillation intensities of all possible pairs. As a result, our method determines competent links more efficiently than the previous approach and provides better performance on model compression and transfer learning tasks. Further qualitative analyses and ablative studies describe how our method contributes to better distillation. The implementation code is available at github.com/clovaai/attention-feature-distillation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge distillation is the technique for transferring knowledge from a source neural network to a target neural network <ref type="bibr" target="#b5">(Hinton, Vinyals, and Dean 2015)</ref>. The source network, referred to as a teacher, indicates a large network that is highly regularized via pre-training, and the target network, referred to as a student, is a smaller network for a specific task. The pre-trained teacher directly informs the student of the solution and intermediate process of a problem, and this informative supervision enables fast and effective learning of the student. Based on knowledge distillation, recent studies have shown significant improvements in model compression <ref type="bibr" target="#b5">(Hinton, Vinyals, and Dean 2015;</ref><ref type="bibr" target="#b22">Romero et al. 2014;</ref><ref type="bibr" target="#b29">Yim et al. 2017;</ref><ref type="bibr" target="#b23">Tian, Krishnan, and Isola 2019)</ref>, crossdomain transfer learning <ref type="bibr" target="#b17">(Orbes-Arteainst et al. 2019;</ref><ref type="bibr" target="#b2">Asami et al. 2017)</ref>, and continual learning <ref type="bibr" target="#b15">(Li and Hoiem 2017;</ref><ref type="bibr">Hou et al. 2018)</ref>.</p><p>For the success of knowledge distillation, various distillation methods were introduced. Starting from transfer-ring output probability distributions of the teacher <ref type="bibr" target="#b5">(Hinton, Vinyals, and Dean 2015)</ref>, intermediate features representations <ref type="bibr" target="#b22">(Romero et al. 2014)</ref> and their variants <ref type="bibr" target="#b30">(Zagoruyko and Komodakis 2016a;</ref><ref type="bibr" target="#b18">Park et al. 2019;</ref><ref type="bibr" target="#b23">Tian, Krishnan, and Isola 2019)</ref> are investigated to identify what knowledge of the teacher helps to build a better student. However, most studies manually links the teacher and student features and perform distillation through the links individually. This manual link selection does not consider the similarity between the teacher and student features, so there is a risk of forcing an incorrect intermediate process to the student. Furthermore, the link selection has a limitation on fully utilizing the whole knowledge of the teacher by choosing a few of all possible links.</p><p>To compensate for the limitation, Jang et al. <ref type="bibr" target="#b9">(Jang et al. 2019</ref>) apply a meta-networks, "learning to transfer (L2T)", automatically determining the links. In more details, the metanetwork consists of individual gates for all possible links, and each gate determines whether distillation through the link contributes to decreasing the classification loss of the student. Their results prove that knowledge distillation with the identified links provides better performance than those with manually selected links. However, the individual gates are not aware of each other although the distillation through the gates simultaneously affect the student. Moreover, their metalearning scheme requires expensive inner-loop procedures to learn their meta-networks, thus its application can be limited under practical scenarios.</p><p>In this paper, we introduce a new feature linking method based on an attention mechanism <ref type="bibr" target="#b27">(Xu et al. 2015;</ref><ref type="bibr" target="#b25">Vaswani et al. 2017)</ref>, which is called attention-based feature distillation (AFD). Specifically, AFD utilizes an attention-based meta-network that identifies similar features between the teacher and student. The identified similarities are applied to control the distillation intensities for all possible feature pairs. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of our proposed distillation method to provide graphical descriptions.</p><p>When comparing from L2T, our proposed method considers the granularity of the teacher and student features to identify the importance of their links while L2T only uses information for a single pair in a narrow perspective. In addition, AFD learns from feature similarities without any innerloop procedure but L2T learns from the classification loss, which requires expensive Hessian computation. In our exper- iment, we observe that L2T and ours have distinct linking results from the different objectives, but our method shows better or comparable results on multiple tasks with more efficient computation.</p><p>We conduct experiments for model compression on three image classification tasks such as CIFAR-100 <ref type="bibr" target="#b13">(Krizhevsky, Hinton et al. 2009</ref>), tinyImageNet, and ImageNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref>) and for domain transfer on four specific tasks such as CUB200 <ref type="bibr" target="#b26">(Wah et al. 2011)</ref>, MIT67 <ref type="bibr" target="#b21">(Quattoni and Torralba 2009</ref>), Stanford40 <ref type="bibr">Stanford Dogs (Khosla et al. 2011</ref>) with a pre-trained large network on ImageNet. As a result, our method shows performance gains in most of experiments and the analyses on the identified feature links explains how our method works. Further ablation studies and sensitivity analysis provide a guideline to use our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Hinton et al. introduced the concept of knowledge distillation <ref type="bibr" target="#b5">(Hinton, Vinyals, and Dean 2015)</ref> by utilizing the output probability distributions of the teacher as a soft label to transfer knowledge. Also, intermediate features from the teacher have been proved to hold additional knowledge that can contribute to improving the student performance. <ref type="bibr">Romero et al. (FitNet (Romero et al. 2014)</ref>) proposed feature-based distillation that couples the teacher and student features and induce the student to mimic the paired teacher features. However, due to a capacity gap between the teacher and the student, it is challenging for the student to mimic the exact teacher features.</p><p>To address the problem, recent works focused on propagating core knowledge from the teacher features. <ref type="bibr" target="#b30">Zagoruyko et al. (Zagoruyko and Komodakis 2016a)</ref> simplified teacher features by applying channel-wise summations and led the student to learn core knowledge of refined teacher features. <ref type="bibr" target="#b11">Kim et al. (Kim, Park, and Kwak 2018)</ref> extracted low-dimensional representations of the features via multiple auto-encoders and transfer them to the student. Relational knowledge dis-tillation aims to transfer relation knowledge between data instances <ref type="bibr" target="#b18">(Park et al. 2019;</ref><ref type="bibr" target="#b24">Tung and Mori 2019;</ref><ref type="bibr" target="#b19">Peng et al. 2019;</ref><ref type="bibr" target="#b16">Liu et al. 2019</ref>). In the other direction of feature refinement methods, the distillation regularization terms have been explored to allow the student to accept more knowledge. Ahn et al. <ref type="bibr" target="#b0">(Ahn et al. 2019</ref>) transferred knowledge by maximizing the mutual information between the feature of the teacher and student. <ref type="bibr" target="#b8">Huang et al. (Huang and Wang 2017)</ref> utilized the maximum mean discrepancy to propagate knowledge from the teacher features. Tian et al. <ref type="bibr" target="#b23">(Tian, Krishnan, and Isola 2019)</ref> applied the contrastive learning scheme on relational knowledge distillation.</p><p>Although distillation methods on how to refine and propagate knowledge have been continuously advanced as the above, it is still remaining problem how to link intermediate features between the teacher and student. Our method is placed to solve the problem as like L2T <ref type="bibr" target="#b9">(Jang et al. 2019</ref>). However, L2T and ours have different properties to identify the links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based Feature Distillation</head><p>Let h T = {h T 1 , ..., h T T } be a set of the feature candidates from the teacher and h S = {h S 1 , ..., h S S } be a set of feature candidates from the student where T and S indicate the numbers of the candidates from the teacher and student, respectively. Each candidate has its own feature map size and channel dimension as h ? R H?W ?d where H, W , and d indicate the height, width, and channel dimension, respectively. When two sets of the candidates are given, AFD aims to identify similarities for all possible combinations (S ? T pairs) and transfer knowledge of the teacher candidates to the student with the identified similarities. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overview of our proposed network. As can be seen, the feature candidates are compared in two directions with the two pooling methods: global average pooling and channel-wise pooling. The similarity identified by two globally pooled features is used as an intensity for transferring knowledge through the distance defined by the channel-wisely averaged features. In order to identify the similarity between h T t and h S s , AFD adopts a query-key concept of the attention mechanism <ref type="bibr" target="#b27">(Xu et al. 2015;</ref><ref type="bibr" target="#b25">Vaswani et al. 2017)</ref>. Specifically, each teacher feature generates a query, q t , and each student feature identifies a key, k s . The followings describe q t and k s in mathematical expressions;</p><formula xml:id="formula_0">q t = f Q (W Q t ? ? HW (h T t )), k s = f K (W K s ? ? HW (h S s )).</formula><p>(1)</p><p>Here, ? HW (?) indicates a global average pooling. f Q and f K are activation function of the query and key. W Q t ? R d?d T t and W K s ? R d?d S s are linear transition parameters for the t-th query and the s-th key. It should be noted that the features have different transition weights since they have different properties through their different levels, i.e. a low-level visual feature can represent a line and a high-level visual features can represent an object. Therefore, we apply different transition weights to each features.</p><p>By utilizing the queries and keys, attention values that represent relations between teacher and student candidates are calculated with a "softmax" function;</p><formula xml:id="formula_1">? t = softmax([(q t W Q-K 1 k t,1 + (p T t ) p S 1 )/ ? d, ? ? ? , (q t W Q-K S k t,S + (p T t ) p S S )/ ? d]).</formula><p>(2)</p><p>Here, we introduce additional weight parameters; a bilinear weight, W Q-K t ? R d?d , and positional encodings, p T t ? R d and p S s ? R d . The bilinear weight is applied to generalize the attention value from different source ranks since the query and key are identified from different dimensional features <ref type="bibr" target="#b20">(Pirsiavash, Ramanan, and Fowlkes 2009;</ref><ref type="bibr" target="#b12">Kim, Jun, and Zhang 2018)</ref>. The positional encodings are utilized to share common information over different instances <ref type="bibr" target="#b25">(Vaswani et al. 2017)</ref>. ? t is the attention vector that capture relation between the t-th teacher feature and whole student features. By utilizing ? t , the teacher feature, h T t , enables to transfer its knowledge selectively to student features.</p><p>The final distillation term forms as</p><formula xml:id="formula_2">L AFD = ? t ? s ? t,s ? C (h T t ) ?? C (? S s ) 2 ,<label>(3)</label></formula><p>where? C indicates a combined function of a channel-wise average pooling layer with L2 normalization, v/ v 2 , by following <ref type="bibr" target="#b30">(Zagoruyko and Komodakis 2016a)</ref>. In addition, h S s is up-sampled or down-sampled from h S s to match the feature map size to those of the teacher features.</p><p>Finally, the regularization term is added to the total loss function as following;</p><formula xml:id="formula_3">L Student = L cls + ?L AFD ,<label>(4)</label></formula><p>where L cls is the classification loss with ground-truth labels and ? is a trade-off parameter controlling the impact of the proposed distillation loss. We use cross entropy for L cls . Using the loss, the student and the attention-based network are trained simultaneously. It should be noted that the AFD network is trained only with L AFD that represents the weighted similarities for all possible feature pairs, so AFD does not require expensive Hessian computations to connect its parameters with L cls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate our proposed method on model compression tasks that train a smaller, or better model under a limitation of the model capacity and transfer learning tasks that obtain a better model in a specific domain by utilizing a pre-trained model. Following by the quantitative evaluations, we qualitatively analyze how our method works. Finally we provide ablation studies to provide further properties of our methods. In our experiment, our baseline methods are a traditional knowledge distillation method (KD) firstly introduced by Hinton et al. <ref type="bibr" target="#b5">(Hinton, Vinyals, and Dean 2015)</ref>, three popular feature-level distillation methods (FitNet <ref type="bibr" target="#b22">(Romero et al. 2014)</ref>, ATT (Zagoruyko and Komodakis 2016a), CRD (Tian, Krishnan, and Isola 2019)) that require a manual feature matching, and one feature-level distillation method (L2T) automatically linking the teacher and student features. For CRD, we set the number of negative samples same as the batch size of each experiment. Note that KD is applied to all baselines to reveal additional gains from the feature distillation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Compression</head><p>We demonstrate the effectiveness of the proposed distillation method on model compression tasks. The experiments are conducted on three popular benchmark datasets such as CIFAR-100 <ref type="bibr" target="#b13">(Krizhevsky, Hinton et al. 2009</ref>), tinyImageNet, and ImageNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref>). We utilize Residual Network (ResNet) <ref type="bibr" target="#b4">(He et al. 2016)</ref>   <ref type="table" target="#tab_0">Table 1</ref> shows our experiment settings and results on the CIFAR-100 dataset with various network architecture. We pre-train large teacher networks and utilize them to train smaller or same-scaled student networks. The experiments are divided into two groups according to the architectural style of the teacher and student. When considering the student without knowledge distillation, all students shows the worst performance over all experiment settings. With manually linked feature pairs, the baseline models including KD, FitNet, ATT, RKD and CRD show better performances than the vanilla students. When applying L2T that identifies feature links with individual gates, we observed worse performance than other baseline methods even though it identifies beneficial feature pairs to meet their objective. The reason of the degradation is that L2T tends to propagate high-level knowledge of the teacher to the low-level feature of the student (See Qualitative Studies on Feature Attention section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tiny ImageNet ImageNet</head><p>Teacher <ref type="formula" target="#formula_2">ResNet34</ref>   Intuitively, the low-level of a small network cannot mimic the high-level of a large network, which adversely affects performance. Our method that utilizes an attention mechanism to identify similar features between the teacher and student shows the best performance over all experiment settings.</p><p>In particular, our method shows an improvement over ATT which uses the same feature distance for distillation. In other words, the proposed linking method significantly contributes to distillation performance. In order to validate our method on more real world environment, we compare our method from other baseline methods on tinyImageNet and ImageNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref> For tinyImageNet, we pad the images to 72 ? 72 and then randomly cropped to 64 ? 64 and flipped for data augmentation. We set the batch size as 128 and the maximum iteration as 200 epochs. All models are trained with stochastic gradient descent with momentum 0.9. We set weight decay as 5?10 ?4 , initial learning rate as 0.1, and we divide the learning rate by 5 at 60, 120, 150 and 180 epochs. We adopt ResNet34 for the teacher and utilize ResNet34 and ResNet18 both for the student. Unlike the ResNet34 and ResNet18 architecture for large image classification, we resize the first convolutional filter size from 7 to 3. For ImageNet, we randomly crop size of 224 ? 224 of each of images and flipped for data augmentation. We optimize the student with initial learning rate as 0.1, divide it by 10 at 30, 60, 90 epochs, and set the maximum iteration as 100 epochs. We set weight decay as 10 ?4 and the batch size as 256. We utilize ResNet34 for the teacher and ResNet18 for the student. For tinyImageNet and ImageNet, we set the hyperparameter, ?, as 50. <ref type="table" target="#tab_2">Table 2</ref> shows the experiment results on tinyImageNet and ImageNet. The proposed method achieve better performance on large-scale datasets than other baseline knowledge distillation methods. It should be noted that we do not conduct L2T on these large-scaled datasets due to the heavy time complexity to update their meta-network. In addition, our method shows constantly better performances than ATT that holds the same distillation loss for the manually selected feature pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Learning</head><p>Transfer learning with knowledge distillation utilizes the teacher pre-trained on a source domain task to train the student for a target domain task. We investigate the effectiveness of our method on transfer learning tasks. We adopt a ResNet34 pre-trained on ImageNet as the teacher network and transfer its knowledge into the students for four tasks,    <ref type="table" target="#tab_5">Table 3</ref> summarizes the experiment results for the multidomain transfer learning tasks. For most datasets, our method shows better performance than L2T and ATT. In transfer learning, dataset from target tasks give limited information. Therefore, training without any knowledge transfer (scratch) shows the worst performance with large margin. Comparing ATT with L2T and our method, it can be seen that identifying the feature links is effective in transfer learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Studies on Feature Attention</head><p>Feature Links. Here, we analyze the attention values, ?, learned from our proposed method in order to provide its further properties. First, we observe the attention maps of various architecture pairs, such as the same architectural style, different architectural style and identical architecture (self). <ref type="figure" target="#fig_2">Figure 3</ref> shows the feature links ? during the training phases. As can be seen, the traditional knowledge distillation methods manually link the teacher and student features and transfer teacher's knowledge only through the pre-defined links. In contrast to the manual feature links, our method identifies feature links in a data-driven way and thus the feature links ? are changed over training steps and converged at the end. In the case of the same architectural style, low and mid-levels of the teacher features are linked with a low-level student feature. This indicates our method affects the student to use more layers to learn high-level of the teacher features. In the case of the different architectural style, the lowest-level and the highest-level features are linked among themselves and mid-level features are smoothly connected. These results show that it is difficult to manually create feature links in different architecture styles. In the case of the identical architectures, the features are linked in the order of the levels but the student tends to use more layers to extract high-level features. Based on observations, we can infer that AFD tends to link features in the same level. However, when the teacher and student have different architectures, the connection is extended to other levels. It is an advantage of our method that the feature links can be changed and extended according to the difference between the teacher and the student's architecture, and improves the distillation performance in various architecture settings. <ref type="figure">Figure 4</ref>: Activation map corresponding to each distillation methods and the teacher. The teacher is ResNet56 and the students are ResNet20. The colored boxes indicate the links from the teacher to the student. <ref type="figure">Figure 4</ref> shows activation maps of a teacher and a students trained with knowledge distillation methods such as ATT with manual feature pairs, L2T, and ours. When comparing links between the teacher and student features, ATT has manually set ordered links through the levels of the features. In the L2T, the identified links switch the order of the levels; the last feature of the teacher is connected with the mid-level feature of the student (green box), and the mid-level feature of the teacher is linked to the last feature of the student (blue box). The identified links of the L2T may hinder the student training by transferring different order of features learned by the teacher, see green and blue boxes. In contrast to both, our proposed method spreads the high-level features of the teacher to various feature levels of the student (blue, red, and green boxes) and identify links for the student to train while keeping the order of the features from the teacher. More interestingly, compared with ATT, the low-level and the mid-level features of the AFD student tends to mimic activated regions of the high-level features of the teacher. To investigate the impact of the hyperparameter of AFD, ?, we evaluate our model by varying the value of ?. ?, is used to train the attention map, ?, which determines the links of the AFD network, and to decide the degree of how much the student mimic the teacher features. We perform the sensitivity analysis for ? with the model compression and the transfer learning tasks. <ref type="figure" target="#fig_4">Figure 5</ref> shows the accuracy of each task. For the model compression task, we use WRN-40-2 as the teacher network and WRN-16-2 as the student network with CIFAR-100 dataset. As can be seen from the red line in <ref type="figure" target="#fig_4">Figure 5</ref>, the accuracy decreases when ? is more than 1,000 compared to the interval between 30 to 200. For the transfer learning task, we use ResNet34 as the teacher network and ResNet18 as the student network with MIT67 dataset. In contrast with model compression task, the transfer learning task shows better performance when the ? value is relatively large. However, we observe that the different result from the model compression task using the same network architecture of transfer learning setting with ImageNet dataset (we use ? as 50 for ImageNet). The rationale behind the gap of the hyperparameter lies in the degree of reliance on teachers' knowledge cased by the size of the dataset and the relevance between source and target tasks <ref type="bibr" target="#b24">(Tung and Mori 2019;</ref><ref type="bibr" target="#b0">Ahn et al. 2019;</ref><ref type="bibr" target="#b18">Park et al. 2019)</ref> Ablation Studies Linking methods. In order to reveal the benefits of the similarity-based links, we compare ours from other linking  <ref type="bibr">AFD (pre-trained)</ref> 0.7121 ? 0.001 <ref type="bibr">AFD (co-train)</ref> 0.7135 ? 0.002 <ref type="table">Table 4</ref>: Ablation studies on selecting the teacher candidates and linking them to the student features. All experiments are repeated 5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation Map</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity Analysis</head><p>methods in <ref type="table">Table 4</ref>. In this experiment, we set the numbers of the teacher and student candidates same as 9 for the same architectural style (ResNet56?ReNet20) to use all possible student features. We choose the teacher candidates in two ways; "Random" that randomly selects the candidates from all features upon all residual blocks and "Equal interval" that selects the features that are sequentially equidistant between themselves. For the linking method, we evaluate three linking methods; "Random link", "Ordered", "AFD". As shown in <ref type="table">Table 4</ref>, "Ordered" shows better performance than "Random link". It should be noted that the combination of "Equal interval" and "Ordered" is usually used when manually selecting links between the teacher and student features. Interestingly, when applying the links identified from the pre-trained AFD, we observe the performance improvement although we only change links from the usually link setting. The result proves that there is more effective way to set the links than manually decided links. In addition, when training AFD together, AFD shows the best performances. This experiments prove the superiority of AFD on identifying links of the feature pairs and transferring teacher's knowledge to the student.  <ref type="table">Table 5</ref>: Ablation studies on the numbers of the candidates for both the teacher and the student. The candidates are set as the output features of the residual blocks that are sequentially equidistant between themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of candidates.</head><p>Here, we analyze the impact of the numbers of the teacher and student candidates. For this experiment, the candidates are set as the output features of the residual blocks that are sequentially equidistant between themselves. <ref type="table">Table 5</ref> shows distillation performances over varying numbers of the candidates. When the student candidates are more than half of the total features, and using all teacher features (27 for ResNet56 and 16 for ResNet34) provides better student performances. In the other hand, with the small number of the student candidates, using all teacher features causes a information bottleneck and degrades the performances. This experiments provide a guidance to choose the number of the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance Metrics and Pooling Methods</head><p>We use L2 distance for distilling the teacher's feature to the student according to the trained link ?, see equation 3. However, other distance metric can used for distillation <ref type="bibr" target="#b8">(Huang and Wang 2017;</ref><ref type="bibr" target="#b0">Ahn et al. 2019;</ref><ref type="bibr" target="#b30">Zagoruyko and Komodakis 2016a)</ref> and it may affect the behavior of the student. Therfore, we explore four distance metrics, L1, L2, KL divergence, and cosine similarity, on model compression task with WRN-40-2 as the teacher network and WRN-16-2 as the student network. <ref type="table">Table 6</ref> shows that L2 distance is the optimal metric for our experiments, so we use L2 distance for the whole experiments.</p><p>Also, channel-wise pooling method,? C , applied to feature in equation 3 may affect the performance of distillation. Therefore, we compare three channel-wise pooling methods including max-pooling (max i |h i |) and average-pooling ( 1 d ? i |h i | p ). We denote A1 and A2 as average pooling methods with p = 1 and p = 2, respectively. As can be seen in the table 7, A2 shows the best performance. Therefore, we use A2 for all experiment in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1</head><p>L2 KL Cosine 0.7513 0.7547 0.7523 0.7541 <ref type="table">Table 6</ref>: Accuracy according to distance metrics. A1 A2 Max 0.7520 0.7547 0.7528 <ref type="table">Table 7</ref>: Accuracy according to pooling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have proposed an attention-based distillation method adaptively transferring knowledge of teacher features to multiple levels of the student layers. With the proposed method, the teacher features are linked with the student features with the attention map and the student learns from the teacher through the identified links. The proposed method is efficiently learned simultaneously during the student's training phase while the previous feature linking method requires an additional inner-loop procedure. Our experiment proves the benefits of the proposed method on two knowledge distillation applications such as model compression and transfer learning. Our further analysis shows that our method adjusts the feature levels of the student regardless the architectural styles of the teacher and student and provides better performance than the baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Changes made since NeurIPS Submission</head><p>To reflect questions and comments of reviewers of the NeurIPS, we have revised the NeurIPS submission paper. First of all, we fix the typos and notation errors raised by reviewers of NeurIPS. Second, we conduct the sensitivity analysis for the hyperparameter ? of AFD. We add the experiment result and discussion in Sensitivity Analysis section. Third, we analyze the difference between distance metric and channel-wise pooling methods in equation 3 of the paper to search the optimal method. The details are described in Distance Metric and Pooling Methods section.</p><p>There are some other questions raised by reviewers of NeurIPS, but we do not attach results in the paper. Our proposed method, AFD, improves the performance of existing feature distillation method ATT <ref type="bibr" target="#b30">(Zagoruyko and Komodakis 2016a)</ref>. Feature distillation is compatible with distillation method for penultimate layer, e.g. CRD, RKD. Therefore, we conducted an experiment for combination of CRD and AFD. The combined distillation achieves 0.44pp accuracy improvement over AFD only model. However, this experiment is somewhat orthogonal to our work, so we do not add the experiment result of combination of CRD and AFD setting in the paper.</p><p>Furthermore, one of reviewer had some question about fine-tuning experiment for transfer learning task. We add this experiment in supplementary material Appendix C. Fine-Tuning Approach for Transfer Learning section. The finetuning approach is also important, but we think that the results of maintaining the same experimental conditions as designed by the L2T authors are more important. Therefore we report the results of the L2T experiment setting in the paper, and the fine-tuning results are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Implementation Details of Attention-based Feature Distillation</head><p>For all experiments, we set d as 128 for the dimension of the queries, keys and positional encodings in Eq. 2. Empirically, the result does not change much depending on the dimension of d. For scaling the student feature, h S s , we use average pooling. Also, we utilize the identity and ReLU function as the activation function of the query and key, respectively. The kernel size and stride are determined according to the size of the student feature and teacher feature. To initialize all parameters in AFD network, we use Xavier initialization. The implementation code will be open-sourced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Fine-Tuning Approach for Transfer Learning</head><p>To supplement the experiment of the transfer learning task, we conduct an experiment with fine-tuning approach. We utilize the network pre-trained with ImageNet dataset. The <ref type="table" target="#tab_8">Table 8</ref> shows the accuracy of the fine-tuned model with transfer learning. First, we can see that all distillation methods still valid with fine-tuned networks with transfer learning and AFD shows the best performance. Comparing to the training from scratch, it can be seen that the difference according to the distillation method is reduced in the case of fine-tuning. However, fine-tuning approach with transfer learning is somewhat similar to the model compression task. More research is required to analyze the difference between fine-tuning and model compression task to reveal further property of pretrained network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining -Distillation</head><p>Fine-tuning 0.7067 FitNet 0.7172 ATT 0.7254 L2T 0.7485 AFD 0.7507 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Additional Results</head><p>This section provides additional results of the ablation study and the qualitative studies. <ref type="table">Table 9</ref> shows the performance according to the various linking method between the teacher and student with the different architectural style. AFD shows the best performance when the architectural style is different, such as the result of the Ablation Studies section of the paper.</p><p>Teacher candi. Linking method ResNet34 ? WRN-28-2 Random Random link 0.7567 ? 0.006 Ordered 0.7607 ? 0.005 AFD (co-train) 0.7702 ?0.004</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equal interval</head><p>Random link 0.7504 ? 0.008 Ordered 0.7544 ? 0.003 AFD (pre-trained) 0.7723 ? 0.003 AFD (co-train) 0.7726 ? 0.003 <ref type="table">Table 9</ref>: Ablation studies on selecting the teacher candidates and linking them to the student features. All experiments are repeated 5 times.</p><p>In addition, we compare manual links with converged attention-based links with various architectures. <ref type="figure">Figure 6</ref> shows the feature links with various architecture pairs. This results provide the guidance to determine the feature links of various architecture pairs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of AFD. An attention-based model determines similarities between the teacher and student features. Knowledge from each teacher feature is transferred to the student with the identified similarities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed meta-network. The globally pooled features are utilized to estimate the similarities and the channel-wisely averaged features are used to calculate the distance between the features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Manual and attention-based feature links for knowledge distillation. Rows and columns of matrices indicate the student and teacher features, respectively. Each matrix is the average overall ? at the corresponding training epoch. The pairs are compared on multiple distillation settings; ResNet56 ? ResNet20 (Same architectural style), ResNet34 ? WRN-28-2 (Different architectural style), and WRN-40-2 ? WRN-40-2 (Self). The manual feature links are not changed once they are selected, but AFD are adaptively selected during the distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>such as Caltech-UCSD Bird (CUB 200)<ref type="bibr" target="#b26">(Wah et al. 2011)</ref>, MIT Indoor Scene Recognition (MIT67)<ref type="bibr" target="#b21">(Quattoni and Torralba 2009</ref>), Stanford 40 Actions (Stanford40) and Stanford Dogs. CUB 200 consists of 5k training and 6k validation images with 200 bird species. MIT67 contains 5k training and 1k validation images with 67 type of indoor scenes. Stanford40 has 4k training and 5k validation images with 40 human actions. Stanford Dogs consists of 12k training and 8k validation images with 120 dog species. The images of transfer learning datasets consists of large scale images. The student architecture for the specific target tasks are set as ResNet18. All experiment settings are followed by those of L2T<ref type="bibr" target="#b9">(Jang et al. 2019</ref>) and our hyper-parameter, ?, is set as 1,000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sensitivity analysis for ?. Red line indicates accuracy of model compression task (WRN-40-2 ? WRN-16-2). Blue line indicates accuracy of transfer learning task (ResNet34 ? ResNet18)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>and Wide Residual Network Performance comparison on CIFAR-100. The teachers and the students have same or different architectural style. All experiments are repeated 5 times.(WRN) (Zagoruyko and Komodakis 2016b) architectural styles. First, we conduct an experiment on CIFAR-100 that consists of 32 ? 32 sized color images for 100 object classes and has 50K training and 10K validation images. For data augmentation, the horizontal flipping and random cropping are applied. We set the batch size as 64 and the maximum iteration as 240 epochs. All models are trained with stochastic gradient descent with 0.9 of momentum, weight decay as 5 ? 10 ?4 , initial learning rate as 0.05, and divide it by 10 at 150, 180, 210 epochs. For the baseline methods, we use their official code and their hyper-parameters. For the distillation loss of our model, we apply {30, 50, 100, 200} of beta, ?, and choose the best performer. We provide how the performance changes depending on the value of ? in Sensitivity Analysis section. For the feature candidates of our method, we use all output features of the teacher and student residual blocks as candidate for all experiments, except ResNet110. For ResNet110, we skip one for every two residual blocks and used only half of the entire residual blocks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Same style</cell><cell></cell><cell></cell><cell cols="2">Different style</cell></row><row><cell cols="8">Teacher ResNet56 ResNet110 ResNet110 WRN-40-2 WRN-40-2 WRN-40-2 ResNet34</cell></row><row><cell cols="8">Student ResNet20 ResNet20 ResNet56 WRN-16-2 WRN-40-2 ResNet56 WRN-28-2</cell></row><row><cell cols="2">Teacher 0.7254</cell><cell>0.7409</cell><cell>0.7409</cell><cell>0.7620</cell><cell>0.7620</cell><cell>0.7620</cell><cell>0.7860</cell></row><row><cell cols="2">Student 0.6940</cell><cell>0.6940</cell><cell>0.7254</cell><cell>0.7289</cell><cell>0.7620</cell><cell>0.7254</cell><cell>0.7532</cell></row><row><cell>KD</cell><cell>0.7098</cell><cell>0.7081</cell><cell>0.7483</cell><cell>0.7499</cell><cell>0.7763</cell><cell>0.7497</cell><cell>0.7648</cell></row><row><cell>FitNet</cell><cell>0.7005</cell><cell>0.7002</cell><cell>0.7411</cell><cell>0.7522</cell><cell>0.7766</cell><cell>0.7506</cell><cell>0.7644</cell></row><row><cell>ATT</cell><cell>0.7054</cell><cell>0.7081</cell><cell>0.7488</cell><cell>0.7520</cell><cell>0.7778</cell><cell>0.7516</cell><cell>0.7720</cell></row><row><cell>RKD</cell><cell>0.7043</cell><cell>0.7076</cell><cell>0.7477</cell><cell>0.7459</cell><cell>0.7762</cell><cell>0.7439</cell><cell>0.7632</cell></row><row><cell>CRD</cell><cell>0.7095</cell><cell>0.7091</cell><cell>0.7512</cell><cell>0.7515</cell><cell>0.7780</cell><cell>0.7525</cell><cell>0.7697</cell></row><row><cell>L2T</cell><cell>0.7037</cell><cell>0.7001</cell><cell>0.7457</cell><cell>0.7486</cell><cell>0.7678</cell><cell>0.7463</cell><cell>0.7640</cell></row><row><cell>Ours</cell><cell>0.7153</cell><cell>0.7138</cell><cell>0.7539</cell><cell>0.7547</cell><cell>0.7813</cell><cell>0.7540</cell><cell>0.7747</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance comparison on large-scale datasets; Tiny Ima- geNet and ImageNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). The tiny-ImageNet dataset consists of 64 ? 64 sized 100K training and 10K validation images for 200 object classes and the ImageNet dataset includes 1.2M training and 50K validation large-scale images for 1K object classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Performance comparison on multi-domain transfer learning</cell></row><row><cell>tasks; from a ResNet34 model pre-trained with ImageNet (source</cell></row><row><cell>domain) to a ResNet18 models for CUB200, MIT67, Stanford40,</cell></row><row><cell>and Stanford Dogs (target domains). Scratch, ATT and L2T are</cell></row><row><cell>referred from (Jang et al. 2019). All experiments are repeated 3</cell></row><row><cell>times.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison between transfer learning task with fine-tuning; from a ResNet34 model to a ResNet18 model for MIT67 dataset. Both models are pre-trained with ImageNet dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational Information Distillation for Knowledge Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Figure 6: Manually designed feature links and attention-based feature links. Divided into green lines, if the size or channel dimension of feature is different</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation of DNN acoustic models using knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Masumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Masataki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5185" to="5189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lifelong learning via progressive distillation and retrospection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning What and Where to Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3030" to="3039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-Scale Domain Adaptation via Teacher-Student Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2386" to="2390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge Distillation via Instance Relationship Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7096" to="7104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Knowledge distillation for semi-supervised domain adaptation. In OR 2.0 Context-Aware Operating Theaters and Machine Learning in Clinical Neuroimaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orbes-Arteainst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>S?rensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5007" to="5016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilinear classifiers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1482" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
