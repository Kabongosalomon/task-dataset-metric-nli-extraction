<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Abdominal Multi-Organ Segmentation via 3D Boundary-Constrained Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samra</forename><surname>Irshad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology</orgName>
								<address>
									<settlement>Hawthorn</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">P S</forename><surname>Gomes</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Victoria University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong</forename><forename type="middle">Tae</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Kyung Hee University</orgName>
								<address>
									<addrLine>Yongin-si</addrLine>
									<settlement>Gyeonggi-do</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Abdominal Multi-Organ Segmentation via 3D Boundary-Constrained Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Abdominal multi-organ segmentation</term>
					<term>Fully convolutional neural networks</term>
					<term>Boundary-constrained segmentation</term>
					<term>Multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background and Objective: Quantitative assessment of the abdominal region from clinically acquired CT scans requires the simultaneous segmentation of abdominal organs. Therefore, for the past two decades, automatic abdominal image segmentation has been the subject of intensive research to facilitate the health professionals easing the clinical workflow. Thanks to the availability of high-performance and powerful computational resources, deep learning-based methods have resulted in state-ofthe-art performance for the segmentation of 3D abdominal CT scans. However, the complex characterization of organs with fuzzy and weak boundaries prevents the deep learning methods from accurately segmenting these anatomical organs. Specifically, the voxels on the boundary of organs are more vulnerable to misprediction due to the highly-varying intensity of inter-organ boundaries, and the misprediction of these voxels is detrimental to overall segmentation performance. This paper investigates the possibility of improving the abdominal image segmentation performance of the existing 3D encoder-decoder networks by leveraging organ-boundary prediction as a complementary task.</p><p>Method: To address the problem of abdominal multi-organ segmentation, we train the 3D encoder-decoder network to simultaneously segment the abdominal organs and their corresponding boundaries in CT scans via multi-task learning. The network is trained end-to-end using a loss function that combines two task-specific losses, i.e., complete organ segmentation loss and boundary prediction loss. We explore two different network topologies based on the extent of weights shared between the two tasks within a unified multi-task framework. In the first topology, the whole-organ prediction task and the boundary detection task share all the layers in the encoder-decoder network except for the last task-specific prediction layers. In contrast, the second topology employs a single shared encoder but two separate task-specific decoders. To evaluate the utilization of complementary boundary prediction task in improving the abdominal multi-organ segmentation, we use three state-of-the-art encoder-decoder networks: 3D UNet, 3D UNet ++ , and 3D Attention-UNet.</p><p>Results: The effectiveness of utilizing the organs' boundary information for abdominal multi-organ segmentation is evaluated on two publically available abdominal CT datasets: Pancreas-CT and the BTCV dataset. The improvements shown in segmentation results (evaluated via Dice Score, Average Hausdorff Distance, Recall, and Precision) reveal the advantage of the multi-task training that forces the network to pay attention to ambiguous boundaries of organs. A maximum relative improvement of 3.5% and 3.6% is observed in Mean Dice Score for Pancreas-CT and BTCV datasets, respectively. All source codes are publically available on https://github.com/samra-irshad/3d-boundary-constrained-networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-organ segmentation on abdominal Computed Tomography (CT) scans is an essential prerequisite for computer-assisted surgery and organ transplantation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Particularly, quantitative assessment of abdominal regions enables accurate organ dose calculation, required in numerous faster and overcome the issues like variability in human expertise and inherent subjectivity.</p><p>Abdominal CT scans often present weak inter-organ boundaries characterized by regions of similar voxel intensities, which in turn results in low-contrast representations. Such appearances are usually caused by the representation of abdominal soft tissues in a narrow band of Hounsfield (HU) values.</p><p>Another factor that enhances the already complex representation of abdominal organs is the existence of artifacts occurring due to blood flow, respiratory, and cardiac motion. Accurate delineation of abdominal organs with unclear boundaries and complex geometrical shapes is one of the ongoing challenges that hurdles the abdominal-related clinical diagnosis. Earlier methods proposed for the abdominal multi-organ segmentation mainly were based on multi-atlas <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> or statistical models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Some methods also made use of handcrafted or learned features to segment abdominal organs <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, the recent Fully Convolutional Network (FCN) based approaches have presented better results due to the improved organ representation learning <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b10">[11]</ref>. Being able to preserve the image structure and provision of efficient learning as well as inference, FCN-based methods are currently considered state-of-the-art for abdominal multiorgan segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Specifically, these networks follow the encoder-decoder architectural design <ref type="bibr" target="#b14">[15]</ref>. In such networks, the shallow layers in the encoder aim to extract low-level features, and the deep layers encode high-level features. While the mirrored-decoder maps back the learned features to generate an output of the same size as input with skip connections assisting in retaining the crucial features extracted in the encoding path <ref type="bibr" target="#b14">[15]</ref>.</p><p>Existing FCN-based methods for abdominal multiorgan segmentation employ either 2D or 3D convolutional architectures <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref>. 2D methods process the CT scans in a slice-by-slice fashion and predict the organ labels on individual slices <ref type="bibr" target="#b12">[13]</ref>. Despite being memory-and parameter-efficient, 2D methods are unable to make full use of 3D contextual information <ref type="bibr" target="#b1">[2]</ref>. 3D methods make use of rich volumetric context by processing the whole CT volume and generating voxel-maps in a single forward propagation pass, leading to better abdominal CT segmentation performance than 2D approaches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>The existing 3D methods have primarily focused on designing better architectures for improved abdominal multiorgan representation learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b1">[2]</ref>. However, they treat all the anatomical parts within a single organ equally since they solely rely on voxel-level information and do not specifically focus on improving the segmentation of voxels in vulnerable regions/parts of organs. As an example, we highlight some of the important characteristics of abdominal organs in <ref type="figure" target="#fig_0">Fig. 1</ref>. <ref type="figure" target="#fig_0">From Figures 1a and 1b</ref>, it can be noticed that the adjacent organs have weak contours which sometimes touch each other.</p><p>As an example, observe the low-contrasted and touching boundaries between stomach ( ) and pancreas ( ). Moreover, 3D multi-organ visualization in <ref type="figure" target="#fig_0">Fig. 1c</ref> shows that the adjacent positioning of organs in the abdominal cavity aggravates the complex spatial relationship among the organs. Simultaneously segmenting the abdominal organs with soft contours and complex spatial relationships is a challenging task.</p><p>The boundaries of anatomical regions in medical scans serve as an important cue for facilitating manual and automated delineation <ref type="bibr" target="#b17">[18]</ref>. Numerous existing deep learningbased studies leveraged learning of features corresponding to boundary of regions for improved medical image segmentation via multitask learning paradigm <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In recent years, deep multitask learning paradigm has been widely used due to its potential to solve multiple tasks in one forward propagation and ability to learn better representations because of the multiple supervisory signals <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In this paper, we propose to improve the segmentation of abdominal organs on CT scans by enhancing the segmentation of boundary of organs. Particularly, we train the 3D deep learning networks to simultaneously predict the boundary and the entire region of organs. The inclusion of boundary information is motivated by the fact that the voxels on the boundary of organs are more vulnerable to misprediction because of their ambiguous appearance and complex relationship with adjacent organs. Specifically, our work makes the following contributions:</p><p>(i) We develop an end-to-end trainable 3D multi-task learning framework that simultaneously predicts the voxel-labels of abdominal organs and their corresponding boundaries. By integrating the boundary features, our proposed boundaryconstrained 3D deep learning framework focuses on the accurate prediction of the edges of organs in addition to whole organs.</p><p>(ii) Instead of relying on a single network topology, we explore and compare two network topologies for conducting multi-task learning. In the first topology, the whole encoder-decoder network is shared with separate task-specific prediction layers at the end for predicting boundaries and entire organs' maps. In the second topology, an encoder is shared with separate task-specific decoders for decoding the features, jointly learned by the shared encoder to predict the boundary and organ probability maps. With an extensive comparison, we reveal that integration of boundary features invariably improves the multi-organ segmentation performance, independent of the multi-task network design.</p><p>(iii) We utilize three state-of-the-art 3D encoder-decoder architectures, i.e., UNet <ref type="bibr" target="#b25">[26]</ref>, UNet ++ <ref type="bibr" target="#b26">[27]</ref>, and Attention-UNet <ref type="bibr" target="#b27">[28]</ref> as baseline networks for evaluating the effect of incorporating boundary information. We modifiy each baseline architecture according to our proposed multitask topologies. We demonstrate significant performance improvements with a negligible increase in trainable parameters.</p><p>(iv) We validate the performance of baseline and counterpart boundary-constrained models on two publically available datasets (Pancreas-CT <ref type="bibr" target="#b28">[29]</ref> and BTCV <ref type="bibr" target="#b29">[30]</ref>) using Dice Score, Average Hausdorff Distance, Recall, and Precision. Furthermore, we conduct additional experiments to evaluate the improvement in the segmentation of regions around the boundaries.</p><p>The results show that the boundary-constrained networks learn feature representations that focus on the accurate organs segmentation and the challenging parts around the border of the organs.</p><p>The rest of the article is organized as follows. In section 2, we review the existing methods for abdominal multi-organ segmentation.</p><p>Section 3 describes our framework for incorporating the boundary information into the 3D fully convolutional networks, including the multi-task loss function and the details of boundary-constrained network topologies. Next, we describe the dataset specifications and implementation details in section 4. We then present the experimental results, comparisons with existing single-task approaches, and indepth performance analysis of boundary-constrained models in section 5. Finally, we discuss the important highlights and some directions for future work in section 6 and present the conclusion in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Segmentation of anatomical structures from abdominal scans is a prerequisite for various high-level CT-based clinical applications. Existing computerized tools for abdominal image segmentation are either based on deep learning or non-deep learning methods. In this section, we first briefly discuss the non-deep learning methods (section 2.1) and then present a review of deep learning-based methods for abdominal multi-organ segmentation (section 2.2). We conclude this section with a discussion on multi-task deep neural networks being employed for complementary boundary learning task to improve medical image segmentation (section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Non-deep learning-based abdominal organs segmentation</head><p>Earlier methods proposed for abdominal multi-organ segmentation have primarily utilized registration-based approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Among the registration-based approaches, the widely used ones include statistical shape models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and multi-atlas label fusion techniques <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The development of statistical models requires registration of training images for estimating the shape or appearance of anatomical organs followed by fitting constructed models to test images for generating segmentations <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Multi atlas-based methods utilize an atlas created using multiple labelled images in the training set, and the test image is segmented by propagating the reference segmentations. Atlases are constructed by capturing the prior anatomical knowledge relevant to target organs. However, it is difficult to build an adequate model to capture the large variability of the deformable organs with limited data <ref type="bibr" target="#b32">[33]</ref>. Furthermore, the performance of both these approaches is restricted by image registration accuracy.</p><p>Registration-free approaches train a classifier using either handcrafted or learned features to segment abdominal images <ref type="bibr" target="#b8">[9]</ref>. Extraction of robust and deformation-invariant features relies on expert knowledge about abdominal organs <ref type="bibr" target="#b33">[34]</ref>.</p><p>Having the ability to learn the features automatically, FCNbased methods, have rapidly replaced the traditional solutions that require image registration or handcrafted features and have shown improved performance for abdominal CT segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Fully Convolutional Networks for abdominal multi-organ</head><p>segmentation In recent years, Fully Convolution Network (FCN) and its variants (e.g., UNet <ref type="bibr" target="#b14">[15]</ref>) have become a common choice for medical image segmentation. This dominancy can be attributed to their ability to learn effective task representations and efficient inference. UNet has an encoder-decoder style architecture and consists of skip connections, joining the encoding and decoding layers on the same level. Despite being trained from scratch, UNet demonstrated state-of-theart performance for various medical image segmentation tasks <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Built on top of UNet, several other modified architectures were subsequently proposed, e.g., UNet ++ <ref type="bibr" target="#b26">[27]</ref>, Attention-UNet <ref type="bibr" target="#b27">[28]</ref>, etc.</p><p>Existing deep learning-based studies for abdominal multiorgan segmentation have utilized 2D or 3D convolutional networks.</p><p>2D methods are less parameter-intensive; however, they cannot exploit the 3D contextual information and eventually provide sub-accurate organ-delineation performance. 3D convolutional networks are facilitated with 3D convolutions, 3D pooling, and 3D normalization to exploit the rich volumetric context and generate dense voxel-wise predictions <ref type="bibr" target="#b25">[26]</ref>. Advances in efficient 3D convolutional implementation and increased GPU memory have enabled the adoption of 3D convolutional models for abdominal multi-organ segmentation <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>Roth et al. <ref type="bibr" target="#b15">[16]</ref> proposed a cascaded architecture based on two 3D UNets where the first UNet is trained to separate the abdominal area from the background, and the latter utilized the output from the first UNet to simultaneously segment the abdominal organs. Peng et al. <ref type="bibr" target="#b38">[39]</ref> delineated abdominal organs using 3D UNet with residual-learning based units (ResNets) to calculate patient-specific CT organ dose. In another study <ref type="bibr" target="#b1">[2]</ref>, abdominal organs are segmented using a 3D FCN with dilated convolutions based densely connected units. Heinrich et al. <ref type="bibr" target="#b10">[11]</ref> leveraged 3D deformable convolutions to spatially adapt  <ref type="figure">Fig. 2</ref>. Multi-task topologies of 3D boundary-constrained network. (a) Multi-task topology with shared encoder-decoder network and task-specific prediction layers, and (b) Multi-task topology with shared encoder and task-specific decoders. the receptive field for abdominal multi-organ segmentation. In <ref type="bibr" target="#b39">[40]</ref>, abdominal scans were segmented using a 3D deeply supervised patch-based UNet with grid-based attention gates to encourage the network to focus on useful salient features propagated through the skip connections. Some existing methods have employed post-processing steps, including levelsets <ref type="bibr" target="#b2">[3]</ref> and graph-cut <ref type="bibr" target="#b3">[4]</ref> to refine initial segmentation maps obtained from 3D deep convolutional networks.</p><p>Through the efforts mentioned above, the existing 3D methods have mostly emphasized developing better deep learning architectures and did not attempt to improve the segmentation of challenging parts of abdominal organs, e.g., voxels that belong to the contour of organs and regions within the vicinity of organ-contour. The fuzzy appearance of the boundary of organs and low contrast between the adjacent abdominal structures makes the voxels belonging to these regions more susceptible to wrong label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Boundary-constrained medical image segmentation</head><p>Several existing deep learning-based medical image segmentation methods have utilized the boundary information of regions of interest to overcome the misprediction of boundary pixels <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b40">[41]</ref>. In these methods, the networks are trained in a multi-task learning fashion to simultaneously predict the probability maps of entire organs and their corresponding boundaries. Most of these methods have resorted to the hard-parameter sharing technique, where a single network contains shared and task-specific parameters and is jointly trained to solve multiple tasks.</p><p>Chen et al. <ref type="bibr" target="#b18">[19]</ref> segmented the glands and their corresponding boundaries via multi-task training. By training the model to learn the co-representations, the model achieved better gland segmentation performance than the single-task models. In <ref type="bibr" target="#b41">[42]</ref>, a dual-decoder-based network is presented that simultaneously detects the boundaries and predicts the semantic labels of cells. Features from the boundary-decoding path were concatenated with those learned in the entire cell region decoding path via additional skip connections. This led to the improved histopathological image segmentation performance. In <ref type="bibr" target="#b42">[43]</ref>, boundary and distance maps were used for improved polyp and optic disk segmentation, respectively. Tan et al. <ref type="bibr" target="#b19">[20]</ref> proposed a multi-task medical image segmentation network consisting of a single encoder and separate dedicated arms for decoding regions and boundaries. The study was evaluated on numerous applications, including MR femur and CT kidney segmentation. Zhang et al., <ref type="bibr" target="#b43">[44]</ref> presented a edge-based deeply supervised network for predicting the regions of interest and their corresponding boundaries. The method was validated for retinal, x-ray, and CT image segmentation. Wang et al. <ref type="bibr" target="#b44">[45]</ref> proposed a two-parallel stream model in which each of the two streams was trained to segment region and detect boundary followed by fusion of contour and region prediction maps. Lee et al. <ref type="bibr" target="#b40">[41]</ref> proposed a framework that predicts boundary keypoint maps and makes use of adversarial loss for improved boundary preserving in medical image segmentation.</p><p>Given the challenge presented by voxels on the organs' boundaries and the evidence in the literature that focusing on boundaries is beneficial for performance, we integrate the organs boundary prediction as an auxiliary task into the training of state-of-the-art 3D medical image segmentation networks. Since the design choice of network topology impacts the learning process, we explore two multi-task network designs and analyze their performance. The boundary cotraining resulted in improved performance on abdominal CT segmentation tasks compared to the several state-of-the-art 3D fully convolutional baseline architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first describe the boundary-constrained loss for training the 3D encoder-decoder network to simultaneously predict the boundaries and entire abdominal organ regions via multi-task learning (Section 3.1), followed by an exhibition of our proposed multi-task network topologies (Section 3.2). After that, we discuss the architecture of the 3D networks that we have as baselines in our work (Section 3.3). Finally, we present the architectural design of the counterpart 3D boundary-constrained models (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Boundary-Constrained Loss</head><p>Consider a 3D encoder-decoder network trained to predict the voxel labels of the abdominal CT scan with W ? H ? Z dimensions, where W, H, and Z denote the length, width, and depth of the scan, respectively. Such a network takes an abdominal multi-organ CT scan as an input and outputs a labelled voxel map of the same size as the input. To utilize the boundary information of abdominal organs for improved representation learning, we train the network to predict the 3D organ-semantic masks and 3D organ-boundaries in one forward propagation pass. We formulate this problem using a multitask learning paradigm where multiple tasks are learned jointly using shared and task-specific representations. The loss L for this multi-task learning problem is a weighted combination of per-task losses, organ segmentation loss L RS and organ boundary detection loss L BD . We use multi-class dice loss <ref type="bibr" target="#b45">[46]</ref> for evaluating the performance of the multi-organ segmentation task, given as</p><formula xml:id="formula_0">L RS = C?1 c=0 2 (? i,c ? y i,c ) y 2 i,c + y 2 i,c<label>(1)</label></formula><p>where,? i,c and y i,c denote the 3D multi-organ probability map and ground-truth mask, respectively, of the i th abdominal CT scan. C denotes the number of organ classes.</p><formula xml:id="formula_1">y i =p(x i ; ? s ) (2) p(x i ; ? s ) = N?1 n=0p (x i,n ; ? s ) = W?1 w=0 H?1 h=0 Z?1 z=0p (x i,w,h,z ; ? s ) (3)</formula><p>wherep(x i,n ) represents the label probability of n th voxel in i th scan and N refers to the total number of voxels in a scan. To evaluate the model's performance in predicting the boundaries, we use binary cross-entropy loss (shown in Eq. Eq. 4). Binary cross-entropy loss for predicting 3D boundaries is given as</p><formula xml:id="formula_2">L BD = ? N?1 n=0 e i,n log(? i,n ) + (1 ? e i,n ) log(1 ?? i,n ) = ? W?1 w=0 H?1 h=0 Z?1 z=0 p(x i,w,h,z ; ? s ) log(p(x i,w,h,z ; ? s )) + (1 ? p(x i,w,h,z ; ? s )) log(1 ?p(x i,w,h,z ; ? s ))<label>(4)</label></formula><p>e i and e i represent the edge probability map and the corresponding ground-truth.p(x i,w,h,z ) represents the edge probability of the n th voxel in i th scan. ? s represents the weights of the entire deep multi-task encoder-decoder network.</p><p>The combined total loss L is minimized with respect to the parameters ? s , as shown in Eq. 5. Thus our goal is to evaluate if a network can learn more robust features and subsequently produce improved organ segmentations by being trained to explicitly recognize the boundaries.</p><formula xml:id="formula_3">L(? s ) = M i=1 L RS + ? M i=1 L BD<label>(5)</label></formula><p>M and ? represents the total number of CT scans in the training set and the weight assigned to the edge detection loss in Eq. 5, respectively. We hypothesize that the additional boundary loss (L BD ) would impose a larger penalty on erroneous contour voxels, and it subsequently pushes the optimization of the segmentation network towards the solutions with more accurate boundaries. Thus, one would potentialize the ability of a boundaryconstrained network to extract features that account for the semantic abdominal organ regions and boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Boundary-Constrained Network Topologies</head><p>Multi-task learning is generally formulated via hardparameter sharing and soft-parameter sharing. In the hardparameter sharing paradigm, multiple tasks share a subset of jointly optimized parameters, whereas task-specific parameters are optimized separately. In soft-parameter sharing, each task is parameterized using its own set of parameters which are jointly regularized using constraints <ref type="bibr" target="#b46">[47]</ref>. In practice, hard-parameter sharing approaches incur much less parameter and computational cost. In our work, we formulate the multi-task learning problem via hard-parameter sharing to train the encoder-decoder network to do multiple tasks, i.e., organ segmentation and boundary detection. For deep neural networks, the hard-parameter sharing approach is realized by sharing some network layers between the tasks while keeping some layers task-specific.</p><p>We explore two different network topologies to conduct multi-task training, as shown in <ref type="figure">Figures 2a and 2b</ref>. The motivation to explore multiple topologies is to investigate the impact of sharing the larger and smaller number of parameters in the network between the two tasks. We explain these multitask topologies below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Task-Specific Output Layers (TSOL)</head><p>The first multi-task topology that we explore is formulated by appending two separate prediction layers for predicting the boundaries and semantic organ masks. This topology employs an encoder-decoder network whose weights are shared between the tasks, except for the last output layers, as shown in <ref type="figure">Fig. 2a</ref>. Technically, it encourages the use of compact and tightly shared feature representations. As evident, this configuration has negligibly fewer more parameters than the single-task network. We denote this configuration as TSOL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Task-Specific Decoders (TSD)</head><p>In second mutli-task topology, we modify the 3D encoderdecoder model to have a single shared encoder but two separate decoding arms for predicting the semantic regions and boundaries. The sibling-decoding arms upsample the region and boundary maps separately. This type of formulation ensures sparse representation sharing amongst the two tasks since decoders have been parameterized separately, as shown in <ref type="figure">Fig. 2b</ref>. The presence of two synthesis paths results in having significantly more parameters than its counterpart single-task network. We refer to this configuration as TSD.    . 3D UNet ++ -MTL-TSD: Multi-task learning based 3D UNet ++ with task-specific decoders for simultaneously predicting the organs and their boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Baseline models</head><p>We use UNet <ref type="bibr" target="#b25">[26]</ref>, UNet ++ <ref type="bibr" target="#b26">[27]</ref> and Attention-UNet (Att-UNet) <ref type="bibr" target="#b27">[28]</ref> as our baseline models. We illustrate these models in Supplementary material (see Figures S1 -S3). These architectures are based on encoder-decoder design and extended to segment 3D volumes by replacing the 2D convolutions, pooling, and upsampling with 3D counterparts. Each baseline model processes a 3D input scan with dimensions W ? H ? Z and outputs a predicted organ-label map of the same size as input. The encoder of the model contains five convolutional blocks with pooling layers, and the decoder comprises four upsampling layers. Each convolutional block in the encoder consists of two convolutional layers with 3 ? 3 ? 3 filters, followed by batch normalization and Exponential Linear Unit (ELU) activation <ref type="bibr" target="#b47">[48]</ref>. We use padded convolutions to keep the output dimensions of convolutional layers the same as the input dimensions. A 2 ? 2 ? 2 max pooling layer with a stride of two in each dimension is sandwiched between every two convolutional blocks for feature maps' downsampling. The bilinear interpolation layers are used in the decoder to upsample the extracted feature maps in each dimension. The feature maps in the decoder are concatenated with the equal-sized representations learned in the encoder via skip connections. The concatenated feature maps are then transformed using convolutional blocks, similar to those used in the encoder. The last 1 ? 1 ? 1 convolutional layer maps the feature channels to the class labels, followed by a softmax activation.</p><p>The resolution of the smallest feature map is 9 ? 9 ? 9 and the minimum and maximum feature count at the first and last encoding stage is 16 and 256, respectively. Note that the original UNet ++ model is trained with deep supervision 6 driven by output layers of UNet with varying depths; however, we train UNet ++ without deep supervision to constrain the computational expense. <ref type="figure">Fig. 7</ref>. 3D Att-UNet-MTL-TSOL: Multi-task learning based 3D Att-UNet with task-specific prediction layers for simultaneously predicting the organs and their boundaries. <ref type="figure">Fig. 8</ref>. 3D Att-UNet-MTL-TSD: Multi-task learning based 3D Att-UNet with task-specific decoders for simultaneously predicting the organs and their boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">3D Boundary-contrained models</head><p>To utilize the boundary information of the organs, we train the baseline models (given in Section 3.3) to predict the organ boundaries along with organs. We propose two multitask network topologies (shown in <ref type="figure">Fig. 2</ref>) for integrating the boundary information and for that, we modify each baseline model, i.e., 3D UNet, 3D UNet ++ , and 3D Att-UNet, according to two multi-task learning-based topologies.</p><p>To modify the baseline models according to the first multitask topology (TSOL) (shown in <ref type="figure">Fig. 2a</ref>), we append a separate head at the end to predict boundaries along with the organs. We refer to these models as UNet-MTL-TSOL, UNet ++ -MTL-TSOL, and Att-UNet-MTL-TSOL, as shown in <ref type="figure" target="#fig_2">Figures 3, 5</ref> and 7, respectively. To modify the baseline models according to the second boundary-constrained multi-task topology (TSD) (shown in <ref type="figure">Fig. 2b)</ref>, we modify the baseline models (3D UNet, 3D UNet ++ , and 3D Att-UNet) to have separate decoding paths followed by prediction layers for predicting boundaries and organs. We refer to the models modified according to this topology as UNet-MTL-TSD, UNet ++ -MTL-TSD, and Att-UNet-MTL-TSD and show them in Figures 4, 6 and 8, respectively. Note from <ref type="figure" target="#fig_5">Fig. 6</ref>, we only use the skip connections between the encoder with the greatest depth and boundarydecoder instead of utilizing feature maps extracted by nested-UNets of all depths. This design choice is made to constrain the number of parameters in UNet ++ -MTL-TSD. Furthermore, observe from <ref type="figure">Fig. 8</ref>, we do not employ the attention mechanism while decoding the boundary-features in Att-UNet-MTL-TSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental details</head><p>This section first describes the datasets used to validate our study and the pre-processing we perform on the datasets (Section 4.1), followed by implementation details (Section 4.2). Finally, we conclude this section by discussing the metrics used to evaluate baseline and boundary-constrained models (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Description of datasets and data preprocessing</head><p>We utilize two publically available abdominal CT datasets (Pancreas-CT and BTCV) to evaluate baseline and boundaryconstrained models. Abdominal scans in Pancreas-CT were acquired at the National Institutes of Health Clinical Center from pre-nephrectomy healthy kidney donors and subjects with neither major abdominal pathologies nor pancreatic cancer lesions <ref type="bibr" target="#b28">[29]</ref>. The BTCV dataset consists of abdominal scans acquired at the Vanderbilt University Medical Center from metastatic liver cancer patients or post-operative ventral hernia patients <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Pancreas-CT Dataset (TCIA-43)</head><p>The pancreas-CT dataset <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b48">[49]</ref> comprised 82 abdominal contrast-enhanced 3D CT scans and was initially provided with manually drawn contours of the pancreas <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Recently, 43 scans from this dataset have been re-annotated to include the segmentation of the liver, duodenum, stomach, esophagus, spleen, gallbladder, and left kidney <ref type="bibr" target="#b50">[51]</ref>. Therefore, we use only 43 scans that have been re-annotated to incorporate labels for multiple organs.</p><p>We first crop a region-of-interest from the CT scans and the corresponding ground-truth labels using the bounding box coordinates provided with the dataset <ref type="bibr" target="#b50">[51]</ref>. The cropping step ensures the models are only fed with the foreground inputs without the redundant background region. The cropped regionof-interest from the CT scans and ground-truth labels are then resampled to a common dimension of 144 ? 144 ? 144 voxels. We randomly divide the available 43 studies into 28, 5, and 10 for training, validation, and test, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">BTCV Dataset</head><p>BTCV was released <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref> as a part of a challenge held in conjunction with MICCAI 2015. The challenge compared the abdominal organs' segmentation algorithms on 3D CT scans. Our work focuses on the segmentation of eight organs from the BTCV dataset, i.e., liver, duodenum, stomach, esophagus, spleen, gallbladder, left kidney, and pancreas.</p><p>For the BTCV dataset, we utilize the bounding box coordinates given with the dataset for cropping the region-ofinterest for both the CT scans and ground-truth labels <ref type="bibr" target="#b50">[51]</ref>. Like the Pancreas-CT dataset, the cropped region-of-interest is then resampled to a common dimension of 144 ? 144 ? 144 voxels. Finally, we randomly divide the available 47 studies into 32, 5, and 10 for training, validation, and test, respectively.</p><p>For both Pancreas-CT and BTCV dataset, we applied affine random transformations to augment the data but did not observe a significant difference in segmentation performance on the validation set. Therefore, we did not use any data augmentation. We used the same dataset splits for all the experiments. To analyze the occurrence of each organ in the dataset, we present the organs' occupancy ratio in <ref type="figure" target="#fig_0">Figures 9 and 10</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>All experiments are conducted using Pytorch <ref type="bibr" target="#b51">[52]</ref> on two Nvidia Tesla P100, accessed through the HPC platform <ref type="bibr" target="#b0">1</ref> . We train all the baseline networks with a mini-batch of size 4, except for 3D UNet ++ , which is trained with one batch size. These choices have been made according to available GPU memory. All baseline (single-task) networks are trained using multi-class dice loss as expressed in Equations (1)-(3). For acquiring multi-class dice loss, the 3D predicted organsegmentation maps are compared with 3D organ ground-truth maps. All the networks (baseline and boundary-constrained) are optimized via Adam optimizer <ref type="bibr" target="#b52">[53]</ref>. The learning rate is initially set to 0.001, decaying by a factor of 0.9 after every epoch. To assess the effect of changing values of training hyperparameters on validation segmentation performance, we conduct experiments to guide us in selecting the optimal settings for baseline models. The results for these experiments are given in Tables S1 and S2 in Supplementary section. We monitor the mean dice score on the validation set during training and utilize the model for testing that results in the highest dice coefficient on the validation set.</p><p>We use a combination of multi-class dice loss and binary cross-entropy loss, as illustrated in Eq. 5 for training boundaryconstrained models.</p><p>3D organ-boundary predictions are compared with the 3D boundary ground-truths to obtain the binary cross-entropy loss. Since the datasets do not contain the boundary annotations of organs, we acquire the groundtruth boundaries by first eroding the multi-organ ground-truth labels and then taking the difference from the original groundtruth map. This process gives us boundary annotations of organs. For UNet-MTL-TSOL, UNet-MTL-TSD, Att-UNet-MTL-TSOL and Att-UNet-MTL-TSD, we use a batch of size two, whereas, for UNet ++ -MTL-TSOL and UNet ++ -MTL-TSD, we use a single batch size. These choices are made according to the available GPU memory. Note that the boundary predictions are only used in the training stage. During the validation stage, we only consider the organ predictions.</p><p>We conduct a grid search (on the range from 0 to 2 with a step of 0.5) to find the optimal value of ? (responsible for balancing the boundary detection loss). The exact value of ? selected to balance the boundary loss for each boundary-constrained model are given in <ref type="table" target="#tab_4">Tables S3 and S4</ref> in Supplementary material. These tables also present the standard deviation between validation dice scores when different values of ? are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation metrics</head><p>We compare the predicted segmentation masks with the ground-truths to evaluate the segmentation performance of baseline and boundary-constrained models on test set. To do that, we utilize Dice Score, Recall, Precision, and Average Hausdorff Distance as metrics for assessing the quality of predicted segmentation masks. These metrics are calculated for each organ individually and then an average is taken across all subjects. All metrics are calculated by taking a mean of the 5 runs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head><p>This section demonstrates the experimental results obtained from boundary-constrained abdominal segmentation models and compares them against the performance given by baseline models. For the sake of brevity, we denote the baseline and boundary-constrained models with the abbreviations below (shown in bold).</p><p>(a) 3D UNet: 3D UNet shown in <ref type="figure" target="#fig_0">Fig. S1</ref> in Supplementary material.</p><p>(b) 3D UNet-MTL-TSOL: Boundary constrained 3D UNet with task specific output layers shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>(c) 3D UNet-MTL-TSD: Boundary constrained 3D UNet with task specific decoders shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>(d) 3D UNet ++ : 3D UNet ++ shown in <ref type="figure" target="#fig_12">Fig.  S2</ref> in Supplementary material.</p><p>(e) 3D UNet ++ -MTL-TSOL: Boundary constrained 3D</p><p>UNet ++ with task specific output layers shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>(f) 3D UNet ++ -MTL-TSD: Boundary constrained 3D UNet ++ with task specific decoders shown in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>(g) 3D Att-UNet: 3D Att-UNet shown in <ref type="figure" target="#fig_2">Fig. S3</ref> in Supplementary material.</p><p>(h) 3D Att-UNet-MTL-TSOL: Boundary constrained 3D Att-UNet with task specific output layers shown in <ref type="figure">Fig. 7</ref>.</p><p>(i) 3D Att-UNet-MTL-TSD: Boundary constrained 3D Att-UNet with task specific decoders shown in <ref type="figure">Fig. 8</ref>. <ref type="table" target="#tab_1">Table 1</ref> summarizes the segmentation results for the Pancreas-CT and BTCV datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Results</head><p>We report the mean Dice Score, mean Average Hausdorff Distance (Avg. HD), mean Recall, and mean Precision computed by comparing the predicted segmentation against the ground-truth. These measures are calculated on test sets of each dataset. <ref type="table" target="#tab_1">Table 1</ref> shows that the boundary-constrained models achieve improved multi-organ segmentation on the abdominal CT scans. Firstly, the mean Dice scores are improved by 1.8% (UNet vs. UNet-MTL-TSOL) and 3.5% (Att-UNet vs. Att-UNet-MTL-TSOL), for Pancreas-CT dataset. The corresponding values of mean dice scores for BTCV dataset are improved by 3.1% (UNet vs. UNet-MTL-TSD), 3.6% (UNet ++ vs. UNet ++ -MTL-TSOL), and 3.5% (Att-UNet vs. Att-UNet-MTL-TSD). The improved overlap between predicted segmentations and manually annotated masks can be attributed to the enhanced semantic representations learned by boundary-constrained models.</p><p>Secondly, we observe that boundary-constrained models achieve a lower Avg. HDs for all the datasets than those obtained from baseline models as shown in <ref type="table" target="#tab_1">Table 1</ref>. For example, after adding boundary information, the Avg. HD values of UNet, UNet ++ , and Att-UNet are decreased by 11.5%, 14.5%, and 18.4%, respectively, for the Pancreas-CT dataset. Likewise, a decrease of 15.4%, 16.2%, and 30%, respectively, is seen for the BTCV dataset. Furthermore, we notice that the Avg. HD is still lower for the cases where boundaryconstrained models obtained lower or equivalent mean Dice score, e.g., UNet ++ -MTL-TSOL vs. UNet ++ and UNet ++ -MTL-TSD vs. UNet ++ , for Pancreas-CT results. This indicates that even for the equivalent dice overlap, the performance of the boundary-constrained models in accurately predicting the boundaries is improved.</p><p>Thirdly, our boundary-constrained models achieve higher values of mean Recall and mean Precision for all the models and datasets except for UNet ++ corresponding to the Pancreas-CT dataset, as shown in <ref type="table" target="#tab_1">Table 1</ref>. The utilization of boundary   <ref type="figure" target="#fig_0">Fig. 11</ref> shows the performance curves based on the mean Dice scores, computed by comparing the predicted multi-organ segmentation with the ground-truth masks on the validation set. <ref type="figure" target="#fig_0">Figures 11a-11c</ref> correspond to performance curves for the Pancreas-CT and <ref type="figure" target="#fig_0">Figures 11d-11f</ref> for the BTCV dataset. It can be seen that the incorporation of boundary information has improved the mean Dice score as the training progresses. <ref type="table" target="#tab_2">Table 2</ref> reports parameter count (in million M) and each method's time to segment a single CT abdominal volume in the test phase. We also highlight the increase in parameter count (given in brackets) compared with the baseline model. Among the single-task baseline models, UNet ++ is most parameterextensive with 6.87 ?10 6 parameters. The parameter count <ref type="table">Table 3</ref> Organ-wise Mean Dice scores ? Std. Dev. obtained from baseline and best performing boundary-constrained models: Bold shows the highest value corresponding to each baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Computational Complexity and Architectural Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Pancreas-CT dataset 3D UNet 0.935 ? 0.008 0.700 ? 0.035 0.927 ? 0.014 0.793 ? 0.030 0.714 ? 0.024 0.934 ? 0.009 0.829 ? 0.024 0.566 ? 0.031 3D UNet-MTL-TSD 0.931 ? 0.008 0.723 ? 0.028 0.937 ? 0.004 0.820 ? 0.028 0.718 ? 0.014 0.947 ? 0.003 0.854 ? 0.015 0.583 ? 0.021 3D UNet ++ 0.917 ? 0.006 0.613 ? 0.053 0.897 ? 0.019 0.802 ? 0.026 0.712 ? 0.013 0.899 ? 0.012 0.818 ? 0.014 0.516 ? 0.042 3D UNet ++ -MTL-TSOL 0.915 ? 0.005 0.621 ? 0.042 0.915 ? 0.013 0.774 ? 0.051 0.697 ? 0.013 0.913 ? 0.005 0.818 ? 0.032 0.519 ? 0.035 3D Att-UNet 0.927 ? 0.007 0.688 ? 0.021 0.927 ? 0.013 0.772 ? 0.037 0.700 ? 0.021 0.929 ? 0.016 0.827 ? 0.029 0.565 ? 0.036 3D Att-UNet-MTL-TSOL 0.937 ? 0.008 0.724 ? 0.010 0.927 ? 0.009 0.858 ? 0.017 0.734 ? 0.005 0.949 ? 0.004 0.847 ? 0.018 0.589 ? 0.023 BTCV dataset 3D UNet 0.878 ? 0.016 0.659 ? 0.018 0.897 ? 0.014 0.567 ? 0.020 0.698 ? 0.009 0.932 ? 0.008 0.813 ? 0.012 0.576 ? 0.023 3D UNet-MTL-TSD 0.897 ? 0.011 0.682 ? 0.009 0.912 ? 0.004 0.608 ? 0.019 0.732 ? 0.016 0.937 ? 0.007 0.837 ? 0.008 0.599 ? 0.011 3D UNet ++ 0.846 ? 0.016 0.559 ? 0.032 0.851 ? 0.030 0.552 ? 0.023 0.691 ? 0.014 0.882 ? 0.008 0.799 ? 0.015 0.538 ? 0.028 3D UNet ++ -MTL-TSOL 0.853 ? 0.023 0.625 ? 0.018 0.891 ? 0.019 0.575 ? 0.019 0.694 ? 0.014 0.918 ? 0.004 0.829 ? 0.018 0.549 ? 0.032 3D Att-UNet 0.881 ? 0.020 0.668 ? 0.016 0.882 ? 0.036 0.561 ? 0.009 0.704 ? 0.023 0.939 ? 0.006 0.805 ? 0.007 0.575 ? 0.007 3D Att-UNet-MTL-TSD 0.913 ? 0.009 0.674 ? 0.006 0.920 ? 0.015 0.603 ? 0.034 0.720 ? 0.009 0.947 ? 0.007 0.832 ? 0.022 0.608 ? 0.025 of boundary-constrained models with TSOL topology have only 17 parameters more than the baseline models; however, these models showed significantly considerable improvements in the segmentation of abdominal organs. The shared encoder and decoder in the TSOL design enable the small parameter count while capacitating the segmentation algorithm to learn the masks and boundaries using separate task-specific output layers. Observing the second multi-task topology TSD, our model requires approximately 2.4 M more parameters than the baseline. Furthermore, we can see (from <ref type="table" target="#tab_1">Table 1</ref>) that the boundaryconstrained models take more time to segment a single volume at the inference stage. This behavior can be associated with the extended parameter size of boundary-constrained models since they are trained to predict a 3D boundary of organs in addition to region masks.</p><p>Analyzing the relationship between the multi-task network design and the segmentation performance from <ref type="table" target="#tab_1">Table 1</ref>, we note there is not a single/fixed topology that led to the maximum improvement. For example, the TSD showed maximum improvement in mean DC and Avg HD corresponding to the Pancreas-CT dataset. Hd over the baseline UNet. However, when the performance of Att-UNet is compared with boundaryconstrained models, we notice that the TSOL demonstrated the best results. This reveals that the multi-task network configuration that offers the best results varies depending on each baseline architecture. All in all, we found that integration of boundary information improved the multi-organ segmentation, independent of the network topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Assesment of organ-wise segmentation performance</head><p>To assess which specific organs benefitted greatly from incorporating boundary information for the segmentation task, we examine the mean Dice scores and mean Avg. HDs achieved by baseline, and best performing boundary constrained models (from <ref type="table" target="#tab_1">Table 1</ref>) for each abdominal organ. We compute the Dice scores and Average Hausdorff distances for each organ individually and then average across all subjects. From <ref type="table">Table 3,</ref> we can see both baseline and boundary-constrained models have yielded the highest mean Dice scores for liver ( ), spleen ( ), and kidney ( ) and lowest for duodenum ( ). However, boundary-based models have led to the maximum relative improvement for the gallbladder ( ), pancreas ( ), and duodenum ( ). From <ref type="table" target="#tab_4">Table 4</ref>, we observe that the boundary-constrained models have significantly improved the mean Avg. HD distances for the spleen ( ), kidney ( ), and gallbladder ( ). Finally, relating the increase in dice overlap to the organ occurrence (shown in <ref type="figure" target="#fig_0">Figures 9 and 10)</ref>, we observe that the most significant improvement has occurred for the underweighted classes. In contrast, the boundary distances are maximally decreased for both small (e.g., gallbladder ( ) and large structures like spleen ( ). Furthermore, the boundary-constrained models led to lower standard deviations of the mean Dice scores and Avg. HDs across different subjects which show the robustness of proposed models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Segmentation performance along boundary of organs</head><p>Unclear boundaries of organs and low inter-organ contrast prevent accurate segmentation of challenging regions around the organ boundaries on abdominal CT scans. To assess if the incorporation of boundary information has improved the segmentation of voxels within the close vicinity of organ boundaries, we evaluate the quality of predicted voxel-labels within these regions and compare them against the ones acquired via baseline methods. To do this, we generate trimaps <ref type="bibr" target="#b53">[54]</ref>  <ref type="bibr" target="#b54">[55]</ref> with different voxel-bands surrounding the boundaries of organs. Trimap is a narrow region along the boundary of an object which is utilized to evaluate the quality of segmentation within a specific distance from the object's contour. First, we generate trimap regions around the boundary of organs for predicted and ground-truth segmentations, and then, we compare the 3D trimaps by computing mean Dice scores between them. We show the exemplary trimap regions on 2D abdominal axial slices in <ref type="figure" target="#fig_0">Fig. 12</ref>. For the sake of compendious presentation, we have computed the trimap Dice scores only for TSOL network topology. <ref type="figure" target="#fig_0">Fig. 13</ref> shows the mean Dice score plotted against the number of voxels the trimap contains. The top row shows trimap plots for the Pancreas-CT dataset, whereas the bottom row shows the trimap plots for the BTCV dataset. Our proposed boundary-constrained models consistently perform better than the baseline models in predicting the semantic labels within the vicinity of organboundaries, except for one case, i.e., (3D UNet ++ vs 3D UNet ++ -MTL-TSOL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 14 shows semantic segmentation predictions on a single</head><p>2D axial slice of the 3D scans. The first and second row correspond to segmentation results from the Pancreas-CT dataset whereas, the third and fourth row corresponds to results from the BTCV dataset. Each column (from left to right) illustrates the original abdominal 2D images <ref type="figure" target="#fig_0">(Fig. 14a</ref>), groundtruth masks <ref type="figure" target="#fig_0">(Fig. 14b</ref>), baseline model (UNet and Att-UNet) segmentations <ref type="figure" target="#fig_0">(Fig. 14c)</ref>, and segmentations acquired from the boundary-constrained counterparts (UNet-MTL-TSD and Att-UNet-MTL-TSOL) in <ref type="figure" target="#fig_0">(Fig. 14d</ref>). We observe that the baseline models led to under-segmentations and over-segmentations, indicated in white boxes in <ref type="figure" target="#fig_0">Fig. 14c</ref>.</p><p>Furthermore, the segmentations generated by single-task baseline models show isolated and biologically implausible organs' parts. Moreover, comparing with the corresponding boundaryconstrained segmentations <ref type="figure" target="#fig_0">(Fig. 14d)</ref>, the incorporation of boundary information has prevented the issue of mispredictions near boundary of organs and led to generation of biologically plausible segmentations. <ref type="figure" target="#fig_0">Fig. 15</ref> presents the 3D segmentations generated by baseline (UNet) and boundary-constrained model (UNet-MTL-TSD) along with the ground-truths from the Pancreas-CT (first row) and BTCV dataset (second row). Notice that the boundary-constrained segmentations (third column) are more similar to the ground-truth masks (first column) as compared to the baseline segmentations (second column). These qualitative results show the improvements induced by the use of organs borders in training the 3D fully convolutional models for abdominal organs segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Accurate segmentation of abdominal organs from CT scans is required for numerous advanced clinical procedures such as computer-assisted surgery and organ transplantation. The lowcontrasted appearance and weak edges of abdominal organs in CT scans adversely affect the accurate segmentation. In this paper, we propose to leverage boundary information of organs as an additional cue for improved 3D abdominal multi-organ segmentation. The boundary-constrained encoder-decoder network simultaneously learns to delineate the semantic abdominal regions and detect the boundaries of organs. This multi-task learning model exploits the statistics from more than one ground-truth source and subsequently retains features shared between the tasks. The boundary annotations of abdominal organs can be easily generated from the groundtruth masks and provide cost-free additional knowledge about the organs.</p><p>As reported by quantitative results in <ref type="table" target="#tab_1">Table 1</ref>, the proposed boundary-constrained 3D encoder-decoder models achieve improved multi-organ segmentation across the majority of the baselines (3D UNet, 3D UNet ++ , and 3D Att-UNet) and datasets (Pancreas-CT and BTCV). We have shown that the significant improvement in segmentation performance evaluated via Dice Score, Avg. HD, Recall, and Precision is caused by the improved segmentation of organs' boundaries and regions surrounding boundaries. Furthermore, significant improvements with a negligible increase in parameter count (0.0002% -TSOL topology) reveal the benefit of regularizing the existing encoder-decoder segmentation models using boundary information.</p><p>The reduction in Avg. HD for both datasets across all baselines depicts the advantage of informing the model about the vulnerable regions of the organ. The dramatic decrease in Avg. HD, ranging from 18% to 30%, shows that the model learned feature combinations that were expressive about the entire appearance of organs. We believe that training the models with auxiliary knowledge encourages learning more generalizable and discriminative features. Notably, the additional experiments that we have conducted to precisely assess the improvements in segmentation of regions in the vicinity of organ-boundaries further verify the superiority of training the segmentation model with complementary boundary information (shown in <ref type="figure" target="#fig_0">Fig. 15c</ref>).</p><p>Since there may exist several different configurations through which a fully convolutional architecture can be designed under a multi-task learning paradigm, we explore two network topologies based on the extent of parameter-sharing between the tasks. Through our extensive comparison, we notice that an overly-shared multi-task network (TSOL) performs on par with the network designed to have an increased number of task-specific layers (TSD) ( <ref type="table" target="#tab_1">Table 1</ref>). This indicates that models with many parameters do not necessarily correspond to higher performance. Most importantly, we also found that incorporation of boundary information improved the multiorgan segmentation performance, regardless of the network topology. One of the critical challenges in designing 3D multitask deep learning models is determining which layers should be shared while keeping the computational expense reasonable. In the future, we aim to investigate other network topologies for training the encoder-decoder model in a multi-task learning fashion. Another valuable extension of our work is to develop a mechanism/policy that can automatically dictate the sharing pattern of network layers between the two tasks.</p><p>As reported in <ref type="table" target="#tab_4">Tables 3 and 4</ref>, the improvement in the segmentation performance of organs rarely occurring in the dataset reveals the efficacy of boundary-regularized models in compensating for their rare presence. The qualitative results shown in <ref type="figure" target="#fig_0">Figures 14 and 15</ref> verify the positive impact of making the model aware of organ-boundaries during training. The ability of our model to simultaneously learn the improved representations of multiple organs is indicated by the qualitative examples in <ref type="figure" target="#fig_0">Figures 14 and 15</ref>, where the boundary-constrain has reduced the occurrence of over-and under-segmented organs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we leverage organ boundary information for an improved 3D abdominal multi-organ segmentation by addressing the challenge of unclear boundaries in low-contrasted CT scans. We demonstrate that boundary information can be seamlessly introduced in the training of 3D encoder-decoder models through different multi-task configurations. The experimental results show the boundaryconstrained multi-organ segmentation outperforms the ones obtained from several FCN-based baseline models, including 3D UNet, 3D UNet ++ , 3D Attention-UNet. Furthermore, we found that the multi-task topology that shows maximum improvement is not fixed and varies depending on the baseline architecture. This insight shows that the optimal utilization of auxiliary information cannot always be harvested through a single deep multi-task design but instead requires the exploration of different multi-task topologies. Our findings also reveal that leveraging organs boundary features improves the segmentation of underweighted organs like the gallbladder, pancreas, and duodenum with a negligible parameter-overhead. Additionally, the experimental results disclose that the boundary-constrained models improve the labelling of weak sub-parts of organs in the vicinity of boundaries. We believe the proposed 3D boundary-constrained models would be valuable for enhancing abdominal organ segmentation and utilizing those segmentations in relevant clinical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Samra Irshad a, * , Douglas P.S. Gomes b , Seong Tae Kim c a Swinburne University of Technology, Hawthorn, Australia b Victoria University, Melbourne, Australia c Kyung Hee University, Yongin-si, Gyeonggi-do, South Korea</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline models</head><p>We illustrate here the architectural design of baseline models, used in our work. Figures S1 and S2 shows the baseline 3D UNet model and 3D UNet ++ models, respectively. <ref type="figure" target="#fig_2">Figures S3 and S4</ref> presents the baseline 3D Att-UNet model and the design of attention gate used in Att-UNet model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>The experiments conducted for selecting the model training parameters for Pancreas-CT and BTCV datasets are summarized in Tables S1 and S2, respectively. We also show the variation in validation dice scores when values      <ref type="table">Table S3</ref>: Value of ? used to balance the boundary loss for Pancreas-CT dataset. The first column represents the model's name; the second column shows the ? value. The last column shows the standard deviation from the mean of validation dice scores generated using different ? values in the search grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method ?</head><p>Std. Dev. 3D UNet-MTL-TSOL 2 0.009 3D UNet-MTL-TSD 1 0.008 3D UNet ++ -MTL-TSOL 1.5 0.004 3D UNet ++ -MTL-TSD 0.5 0.004 3D Att-UNet-MTL-TSOL 0.5 0.011 3D Att-UNet-MTL-TSD 1 0.005 <ref type="table" target="#tab_4">Table S4</ref>: Value of ? used to balance the boundary loss for BTCV dataset. The first column represents the model's name; the second column shows the ? value. The last column shows the standard deviation from the mean of validation dice scores generated using different ? values in the search grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method ?</head><p>Std. Dev. 3D UNet-MTL-TSOL 1.5 0.012 3D UNet-MTL-TSD 1.5 0.011 3D UNet ++ -MTL-TSOL 0.5 0.016 3D UNet ++ -MTL-TSD 1.5 0.0004 3D Att-UNet-MTL-TSOL 2 0.017 3D Att-UNet-MTL-TSD 1 0.015</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Exemplary 2D abdominal CT image showing the visual characteristics of organs. (a) 2D abdominal image, (b) Abdominal organs annotated on CT image: pancreas ( ), spleen ( ), liver ( ), stomach ( ), gallbladder ( ), (c) 3D multi-organ voxel map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>3D UNet-MTL-TSOL: Multi-task learning based 3D UNet with task-specific output layers for simultaneously predicting the organs and their boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>3D UNet-MTL-TSD: Multi-task learning based 3D UNet with taskspecific decoders for simultaneously predicting the organs and their boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>3D UNet ++ -MTL-TSOL: Multi-task learning based 3D UNet ++ with task-specific layers for simultaneously predicting the organs and their boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6. 3D UNet ++ -MTL-TSD: Multi-task learning based 3D UNet ++ with task-specific decoders for simultaneously predicting the organs and their boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Organ occupancy ratio for BTCV dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>9 PancreasFig. 11 .</head><label>911</label><figDesc>Examining the mean dice score computed by comparing predicted multi-organ segmentation masks with the ground-truth on validation set: First two rows (a-c) and last two rows (d-f) correspond to the dice score curves for Pancreas-ct dataset and BTVC dataset, respectively. Each figure compares the mean DC of the baseline (Blue) and counterpart boundary-constrained model (Red) on the validation set, computed after each epoch. The baseline models are trained to predict the masks of the abdominal regions, whereas the boundary-constrained models are trained to predict the labels of organs and boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Trimap regions along boundary of organs shown on 2D abdominal images. (a) Original abdominal image, (b) Organ label-maps corresponding to 2D image, (c) Trimap region of 5 voxel width around boundaries of organs shown in gray color, and (d) Trimap region of 7 voxel width shown in gray color. Evaluation of boundary segmentation via trimaps. In each subfigure, we have plotted the dice scores (y-axis) obtained by comparing the predicted and groundtruth segmentations in trimap regions with varying number of voxels (x-axis). The dice scores for both the baseline (green) and boundaryconstrained counterparts (purple) are plotted. Top row of (a-c) corresponds to trimap segmentation comparison for Pancreas-CT dataset and bottom row corresponds to BTCV dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Qualitative results for multi-organ segmentation between baseline and boundary-constrained models are shown. Rows 1-2 correspond to results for the Pancreas-CT dataset, whereas Rows 3-4 show results for the BTCV dataset. Columns 1-2 show the original image and the corresponding ground-truth mask overlayed on the 2D image, i.e., spleen ( ), left kidney ( ), gallbladder ( ), liver ( ), stomach ( ), and duodenum ( ). Columns 3-4 illustrate the segmentation results related to baseline UNet and UNet-MTL-TSOL counterparts. White boxes indicate the segmented regions improved by incorporation of boundary information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 .</head><label>15</label><figDesc>Visualization of 3D segmentation predictions using Pancreas-CT and BTCV dataset. First column corresponds to abdominal groundtruth annotations, i.e., spleen ( ), pancreas ( ), left kidney ( ), gallbladder ( ), esophagus ( ), liver ( ), stomach ( ), and duodenum ( ), whereas second and third column corresponds to volume labels predicted by UNet and UNet-MTL-TSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure S1 :</head><label>S1</label><figDesc>3D UNet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure S2 :</head><label>S2</label><figDesc>3D UNet ++ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure S3 :</head><label>S3</label><figDesc>3D Att-UNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure S4 :</head><label>S4</label><figDesc>Design of attention gate utilized on 3D Att-UNet.of ? are changed for Pancreas-CT and BTCV dataset inTables S3 and S4, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 9. Organ occupancy ratio for Pancreas-ct dataset.</figDesc><table><row><cell>Duodenum 3.41%</cell><cell cols="2">Left kidney 6.58%</cell></row><row><cell></cell><cell></cell><cell>Pancreas 2.69%</cell></row><row><cell></cell><cell></cell><cell>Spleen 9.73%</cell></row><row><cell></cell><cell></cell><cell>Gallbladder 1.01%</cell></row><row><cell>60.9% Liver</cell><cell></cell><cell>Stomach 15.24%</cell></row><row><cell cols="2">Organ Occupancy Ratio: Pancreas-CT dataset</cell><cell>Esophagus 0.44%</cell></row><row><cell>Duodenum 3.12%</cell><cell cols="2">2.96% Pancreas Left kidney 5.83%</cell></row><row><cell></cell><cell></cell><cell>Spleen 9.97%</cell></row><row><cell></cell><cell></cell><cell>Gallbladder 0.83%</cell></row><row><cell></cell><cell></cell><cell>Stomach 14.59%</cell></row><row><cell>Liver 62.19%</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Esophagus 0.5%</cell></row></table><note>Organ Occupancy Ratio: BTCV dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Quantitative comparison between baseline and boundary-contrained models for abdominal multi-organ segmentation: Mean values ? Std. Dev. of Dice Score, Avg. HD, Recall, and Precision are shown. Results in BOLD indicate the best results corresponding to each baseline. ? 0.009 0.748 ? 0.130 0.807 ? 0.055 0.849 ? 0.079 0.775 ? 0.004 0.989 ? 0.095 0.770 ? 0.086 0.799 ? 0.106 3D UNet-MTL-TSD 0.814 ? 0.005 0.703 ? 0.059 0.798 ? 0.053 0.859 ? 0.079 0.776 ? 0.002 0.918 ? 0.047 0.764 ? 0.081 0.802 ? 0.107 3D UNet ++ 0.772 ? 0.009 1.369 ? 0.114 0.760 ? 0.089 0.825 ? 0.106 0.715 ? 0.011 1.419 ? 0.088 0.707 ? 0.078 0.781 ? 0.136 3D UNet ++ -MTL-TSOL 0.772 ? 0.014 1.170 ? 0.161 0.754 ? 0.073 0.829 ? 0.110 0.741 ? 0.011 1.189 ? 0.099 0.733 ? 0.080 0.789 ? 0.121 3D UNet ++ -MTL-TSD 0.771 ? 0.008 1.308 ? 0.269 0.751 ? 0.077 0.832 ? 0.111 0.723 ? 0.008 1.349 ? 0.027 0.708 ? 0.084 0.783</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Pancreas-CT dataset</cell><cell></cell><cell></cell><cell cols="2">BTCV dataset</cell><cell></cell></row><row><cell>Method</cell><cell>Dice score</cell><cell>Avg. Hd</cell><cell>Recall</cell><cell>Precision</cell><cell>Dice score</cell><cell>Avg. Hd</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>3D UNet</cell><cell cols="4">0.799 ? 0.013 0.795 ? 0.088 0.786 ? 0.049 0.850 ? 0.082</cell><cell cols="4">0.753 ? 0.008 1.085 ? 0.098 0.760 ? 0.085 0.769 ? 0.116</cell></row><row><cell>3D UNet-MTL-TSOL</cell><cell cols="8">0.813 ? 0.127</cell></row><row><cell>3D Att-UNet</cell><cell cols="4">0.792 ? 0.015 0.825 ? 0.082 0.788 ? 0.054 0.831 ? 0.093</cell><cell cols="4">0.752 ? 0.008 1.313 ? 0.238 0.754 ? 0.091 0.778 ? 0.112</cell></row><row><cell cols="5">3D Att-UNet-MTL-TSOL 0.820 ? 0.004 0.673 ? 0.035 0.822 ? 0.050 0.839 ? 0.076</cell><cell cols="4">0.769 ? 0.008 1.065 ? 0.175 0.769 ? 0.086 0.790 ? 0.095</cell></row><row><cell>3D Att-UNet-MTL-TSD</cell><cell cols="4">0.807 ? 0.005 0.732 ? 0.049 0.797 ? 0.050 0.847 ? 0.080</cell><cell cols="4">0.778 ? 0.004 0.919 ? 0.052 0.759 ? 0.077 0.810 ? 0.104</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Comparison of parameter-cost and computational time.</figDesc><table><row><cell>Method</cell><cell cols="2">No. of parameters Inference time (ms)</cell></row><row><cell>3D UNet 3D UNet-MTL-TSOL 3D UNet-MTL-TSD</cell><cell>5.89M (-) 5.89M (17) 8.24M (2.35M)</cell><cell>44 53 68</cell></row><row><cell>3D UNet ++ 3D UNet ++ -MTL-TSOL 3D UNet ++ -MTL-TSD</cell><cell>6.87M 6.87M (17) 9.22M (2.35M)</cell><cell>63 79 81</cell></row><row><cell>3D Att-UNet 3D Att-UNet-MTL-TSOL 3D Att-MTL-TSD</cell><cell>6.47M 6.47M (17) 8.82M (2.35M)</cell><cell>77 78 89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Organ-wise Mean Avg. HDs ? Std. Dev. obtained from baseline and best performing boundary-constrained models: Bold shows the highest value corresponding to each baseline. ? 0.085 0.936 ? 0.279 0.184 ? 0.143 0.803 ? 0.356 0.788 ? 0.275 0.234 ? 0.063 0.834 ? 0.234 2.321 ? 0.316 3D UNet-MTL-TSD 0.317 ? 0.083 0.911 ? 0.244 0.150 ? 0.036 0.389 ? 0.074 0.683 ? 0.072 0.259 ? 0.122 0.879 ? 0.304 2.038 ? 0.204 3D UNet ++ 0.419 ? 0.069 1.852 ? 0.554 0.265 ? 0.153 1.569 ? 0.649 1.445 ? 0.494 0.711 ? 0.218 1.371 ? 0.457 3.315 ? 0.637 3D UNet ++ -MTL-TSOL 0.389 ? 0.055 1.718 ? 0.638 0.167 ? 0.048 0.873 ? 0.229 1.187 ? 0.328 0.675 ? 0.104 1.296 ? 0.483 3.058 ? 0.948 3D Att-UNet 0.363 ? 0.063 0.928 ? 0.085 0.174 ? 0.106 0.527 ? 0.099 0.809 ? 0.277 0.442 ? 0.175 0.969 ? 0.336 2.384 ? 0.466 3D Att-UNet-MTL-TSOL 0.255 ? 0.041 0.993 ? 0.315 0.192 ? 0.033 0.305 ? 0.075 0.559 ? 0.022 0.154 ? 0.054 0.766 ? 0.274 2.162 ? 0.434 BTCV dataset 3D UNet 0.474 ? 0.161 1.741 ? 0.179 0.337 ? 0.077 1.750 ? 0.334 0.770 ? 0.140 0.636 ? 0.206 0.941 ? 0.155 2.032 ? 0.149 3D UNet-MTL-TSD 0.329 ? 0.105 1.787 ? 0.326 0.253 ? 0.059 1.076 ? 0.136 0.648 ? 0.168 0.619 ? 0.226 0.778 ? 0.123 1.849 ? 0.199 3D UNet ++ 0.780 ? 0.220 2.423 ? 0.510 0.574 ? 0.282 2.103 ? 0.284 1.034 ? 0.187 0.984 ? 0.136 1.153 ? 0.148 2.306 ? 0.206 3D UNet ++ -MTL-TSOL 0.794 ? 0.089 1.842 ? 0.518 0.328 ? 0.087 1.630 ? 0.479 0.908 ? 0.221 1.080 ? 0.063 0.860 ? 0.248 2.073 ? 0.265 3D Att-UNet 0.577 ? 0.245 2.017 ? 0.418 0.404 ? 0.154 2.889 ? 1.387 0.981 ? 0.659 0.435 ? 0.162 1.011 ? 0.133 2.193 ? 0.249 3D Att-UNet-MTL-TSD 0.273 ? 0.055 1.377 ? 0.123 0.191 ? 0.062 1.631 ? 0.456 0.887 ? 0.248 0.318 ? 0.202 0.789 ? 0.149 1.888 ? 0.318</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S1 :</head><label>S1</label><figDesc>Effect of using different values of training parameters for Pancreas-CT dataset. CE loss represents Cross-Entropy loss.</figDesc><table><row><cell>Model</cell><cell>Training parameters</cell><cell>Validation dice score</cell></row><row><cell></cell><cell>Initial</cell><cell></cell></row><row><cell>Optimizer</cell><cell>learning</cell><cell>Loss</cell></row><row><cell></cell><cell>rate</cell><cell></cell></row><row><cell cols="3">Adam RMSprop 0.01 0.001 CE Loss Dice Loss</cell></row><row><cell></cell><cell></cell><cell>0.769</cell></row><row><cell>3D UNet</cell><cell></cell><cell>0.748 0.755</cell></row><row><cell></cell><cell></cell><cell>0.728</cell></row><row><cell></cell><cell></cell><cell>0.755</cell></row><row><cell>3D UNet ++</cell><cell></cell><cell>0.751 0.732</cell></row><row><cell></cell><cell></cell><cell>0.744</cell></row><row><cell></cell><cell></cell><cell>0.769</cell></row><row><cell>3D Att-UNet</cell><cell></cell><cell>0.754 0.757</cell></row><row><cell></cell><cell></cell><cell>0.676</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S2 :</head><label>S2</label><figDesc>Effect of using different values of training parameters for BTCV dataset. CE loss represents Cross-Entropy loss.</figDesc><table><row><cell>Model</cell><cell>Training parameters</cell><cell>Validation dice score</cell></row><row><cell></cell><cell>Initial</cell><cell></cell></row><row><cell>Optimizer</cell><cell>learning</cell><cell>Loss</cell></row><row><cell></cell><cell>rate</cell><cell></cell></row><row><cell cols="3">Adam RMSprop 0.01 0.001 CE Loss Dice Loss</cell></row><row><cell></cell><cell></cell><cell>0.693</cell></row><row><cell>3D UNet</cell><cell></cell><cell>0.683 0.687</cell></row><row><cell></cell><cell></cell><cell>0.586</cell></row><row><cell></cell><cell></cell><cell>0.681</cell></row><row><cell>3D UNet ++</cell><cell></cell><cell>0.661 0.647</cell></row><row><cell></cell><cell></cell><cell>0.646</cell></row><row><cell></cell><cell></cell><cell>0.703</cell></row><row><cell>3D Att-UNet</cell><cell></cell><cell>0.620 0.626</cell></row><row><cell></cell><cell></cell><cell>0.636</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://supercomputing.swin.edu.au/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest statement:</head><p>On behalf of all authors, the corresponding author states that there is no conflict of interest.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-organ segmentation in abdominal ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Linguraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3986" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic multi-organ segmentation on abdominal ct with dense v-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bonmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gurusamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Barratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1822" to="1834" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic abdominal multi-organ segmentation using deep convolutional neural network and time-implicit level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abdominal multi-organ auto-segmentation using 3d-patch-based deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-020-63285-0</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">6204</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient multi-atlas abdominal segmentation on clinically acquired ct with simple context learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Baucom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Poulose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-organ segmentation with missing organs in abdominal ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Linguraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic multi-resolution shape modeling of multiorgan structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Cerrolaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">?</forename><surname>Gonz?lez-Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Linguraru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abdominal multi-organ segmentation from ct images using conditional shape-location and unsupervised intensity priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Linguraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic abdominal organ segmentation from ct images, ELCVIA: electronic letters on computer vision and image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Casiraghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pratissoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lombardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Segmentation of abdominal organs from ct using a multi-level, hierarchical neural network strategy, Computer methods and programs in biomedicine 113</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Selver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="830" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Obelisk-net: Fewer layers to solve 3d multi-organ segmentation with sparse deformable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouteldja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional neural networks improve abdominal organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Bobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Virostko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plassard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Assad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2018: Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10574</biblScope>
			<biblScope unit="page">105742</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale self-guided attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DCACNet: Dual context aggregation and attention-guided cross deconvolution network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno>doi:10. 1016/j.cmpb.2021.106566</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page">106566</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An application of cascaded 3d fully convolutional networks for medical image segmentation, Computerized medical imaging and graphics : the official journal of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging Society</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="90" to="99" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Performance evaluation of 2d and 3d deep learning approaches for automatic segmentation of multiple organs on ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective integration of object boundaries and regions for improving the performance of medical image segmentation by using two cascaded networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2021.106423</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">106423</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dcan: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep multi-task and task-specific feature learning network for robust shape preserved organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<idno>doi:10.1109/ ISBI.2018.8363791</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1221" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Edge-boosted u-net for 2d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<idno>doi:10.1109/ ACCESS.2019.2953727</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="171214" to="171222" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lr-net: A multitask model using relationship-based contour information to enhance the semantic segmentation of cancer regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/BIBM49941.2020.9313420</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2371" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dense contourimbalance aware framework for colon gland instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bspc.2020.101988.URLhttps:/www.sciencedirect.com/science/article/pii/S1746809420301440</idno>
		<ptr target="https://doi.org/10.1016/j.bspc.2020.101988.URLhttps://www.sciencedirect.com/science/article/pii/S1746809420301440" />
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101988</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-task deep learning based ct imaging analysis for covid-19 pneumonia: Classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Modzelewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="104037" to="104037" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crossinfonet: Multi-task information sharing based hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9888" to="9897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>I?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention gated networks: Learning to leverage salient regions in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schaap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.7937/K9/TCIA.2016.TNB1KQBU</idno>
		<title level="m">Data from pancreas-ct</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Miccai multi-atlas labeling beyond the cranial vault -workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistical shape models for 3d medical image segmentation: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Meinzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="563" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluation of six registration methods for the human abdomen on clinically acquired ct</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1563" to="1572" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Laplacian forests: semantic image segmentation by guided bagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A method of rapid quantification of patient-specific organ dose for ct using coupled deep multi-organ segmentation algorithms and gpu-accelerated monte carlo dose computing code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00360</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rbnet: A deep neural network for unified road and road boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Automatic brain tumor detection and segmentation using u-net based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno>ArXiv abs/1705.03820</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Block level skip connections across cascaded v-net for multi-organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2782" to="2793" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A method of rapid quantification of patient-specific organ doses for ct using deep-learning based multi-organ segmentation and gpu-accelerated monte carlo dose computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical physics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ct-based multi-organ segmentation using a 3d selfattention u-net network for pancreatic radiotherapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4316" to="4324" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structure boundary preserving segmentation for medical image with ambiguous boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Besnet: Boundary-enhanced segmentation of cells in histopathological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sokolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Psi-net: Shape and boundary aware joint multitask deep network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sarveswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shankaranarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sivaprakasam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>EMBC</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7223" to="7226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Et-net: A generic edge-attention guidance network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A unified two-parallel-branch deep neural network for joint gland contour and segmentation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="316" to="324" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>ArXiv abs/1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Y. Bengio, Y. LeCun</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The cancer imaging archive (TCIA): Maintaining and operating a public information repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pringle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tarbox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Prior</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10278-013-9622-7</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bonmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gurusamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Barratt</surname></persName>
		</author>
		<idno>doi:10.5281/ ZENODO.1169361</idno>
		<title level="m">Multi-organ abdominal ct reference standard segmentations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2008.4587417</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll&amp;apos;ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<title level="m">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</title>
		<imprint>
			<biblScope unit="page" from="15329" to="15337" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
