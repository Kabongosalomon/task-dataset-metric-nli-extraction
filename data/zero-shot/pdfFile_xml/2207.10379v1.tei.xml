<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Saliency Query Network for Efficient Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<postCode>SY23 3FL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Saliency Query Network for Efficient Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Recognition</term>
					<term>Transformer</term>
					<term>Temporal Sampling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient video recognition is a hot-spot research topic with the explosive growth of multimedia data on the Internet and mobile devices. Most existing methods select the salient frames without awareness of the class-specific saliency scores, which neglect the implicit association between the saliency of frames and its belonging category. To alleviate this issue, we devise a novel Temporal Saliency Query (TSQ) mechanism, which introduces class-specific information to provide fine-grained cues for saliency measurement. Specifically, we model the class-specific saliency measuring process as a query-response task. For each category, the common pattern of it is employed as a query and the most salient frames are responded to it. Then, the calculated similarities are adopted as the frame saliency scores. To achieve it, we propose a Temporal Saliency Query Network (TSQNet) that includes two instantiations of the TSQ mechanism based on visual appearance similarities and textual event-object relations. Afterward, cross-modality interactions are imposed to promote the information exchange between them. Finally, we use the class-specific saliencies of the most confident categories generated by two modalities to perform the selection of salient frames. Extensive experiments demonstrate the effectiveness of our method by achieving state-of-the-art results on ActivityNet, FCVID and Mini-Kinetics datasets. Our project page is at https://lawrencexia2008.github.io/ projects/tsqnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>Which frames are the most salient for ____ ? (tailgate party, parking cars , playing chess??)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer:</head><p>Fame 3rd 6th and 7th are more likely to be tailgate party; Frame 5th is likely to be parking car. ?? Checking the common patterns of these categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the recent years, video understanding has drawn considerable attention from the community <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b40">41]</ref> for the inexorable increase of video content on the Internet. Much progress has been achieved on the techniques to model complex video events, which can be glimpsed on promising precision on multiple benchmark datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. However, computational costs grow proportionally to the recognition accuracy. This hinders the deployment of video recognition systems in resource-constraint environments, e.g. IoT, self-driving and mobile phone applications. Hence, it is imperative to develop efficient video recognition systems to meet the rising demands of resource-efficient applications.</p><p>There are many studies that have been conducted on efficient video recognition. One set of approaches focus on designing lightweight architectures <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10]</ref>. At the other end of the spectrum are the dynamic inference-based approaches, which typically utilize a lightweight policy network to preview the video events, and allocate computation resources depending on the saliency of frames. They implant a policy network (or sampler network) inside the reinforce learning paradigm <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b21">22]</ref>, or adopt attention weight as a proxy of policy under the attention mechanism <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. The sampler networks are optimized under the assumption that the most salient frames/regions contribute most to the video representation, which produces one-size-fits-all, i.e., class-agnostic frame saliency measurements.</p><p>Actually, salient patterns are tightly associated with the category semantics. However, one-size-fits-all saliencies are not sensitive to fine-grained semantics. In particular, the sampler may overestimate the saliency of some frames which seem to be representative, but they actually belong to other categories rather than the real one of the current video. By contrast, a human can precisely elect the most informative frames with the aid of prior information about the probable category of the video. Because we can naturally build the logic connection between frame sequences and the common pattern of the predicted category, which can be understood as a query-response manner. For example, in <ref type="figure">Figure 1</ref>, one can easily select the 3rd, 6th and 7th frames from the video with the assumption that the video may belongs to Tailgate Party. By contrast, one-sizes-fit-all sampler may also be inclined to 5th frames besides those three frames for it is quite representative for another category, e.g. Parking Car.</p><p>Inspired by this observation, in this paper, we cast frame saliency measuring as a querying process, to enable discriminative class-specific saliency measurement. To this end, we present a novel Temporal Saliency Query (TSQ) mechanism, which can measure saliencies of all semantic categories over frame sequence in parallel, and select the saliency of highly-confident categories as final the result. Concretely, we formulate class-specific saliency measuring as a queryresponse task. The common patterns of the various categories are adopted as query, and frame representations gathered by category-frame similarities are taken as the response. Then, the category-frame similarities can be regarded as frame saliencies. A conceptual overview of the TSQ mechanism is shown in <ref type="figure">Figure 1</ref>. Specifically, we use cross attention in Transformer Decoder <ref type="bibr" target="#b38">[39]</ref> to model many-to-many category-frame similarities in parallel. On one hand, we represent the common pattern of a category, namely TSQ embedding, by visual prototypes. And the query process is performed over the visual feature of <ref type="figure">Fig. 1</ref>. A conceptual overview of the TSQ mechanism. We cast the saliency estimation task as a query-response task. We ask each category a question: Which frames are the most salient ones for it? As we can see in the above example, we get the answer that frames 3rd, 6th, 7th are most salient for tailgate party and the frame 5th is salient for parking car. No frame is salient for playing chess.</p><p>the frame sequence. On the other hand, to handle large intra-class variations of visual appearance, we measure saliency by textual event-object relations for complementary information. As we know, the objects in videos are closely associated with the category annotation of video. For instance, cake, candle and balloon with birthday party. To model the semantic relationships between object and category, we first employ BERT <ref type="bibr" target="#b6">[7]</ref> to represent the object with word embedding of its name. Taking the product as textual embedding, we construct another textual branch in the TSQ mechanism, where the query process is executed over the embedding sequence of object names. Doing so allows us to exploit prior knowledge from off-the-shelf word representations to supply cross-modal complementary clues to saliency measurement.</p><p>Our contributions are summarized as: First, we propose a novel Temporal Saliency Query mechanism, to alleviate the lack of class-specific information in saliency measuring for temporal sampling frameworks. Second, we present an efficient multi-modal salient frame sampler Temporal Saliency Query Network (TSQNet), which utilize both visual appearance feature and textual feature obtained by object name embeddings to measure frame saliencies in a unified framework. Third, we conduct extensive experiments on three large-scale datasets, i.e., ActivityNet, FCVID and Mini-Kinetics, which show TSQNet significantly outperforms the state-of-the-art approaches on accuracy-efficiency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Efficient Video Recognition. Efficient video recognition approaches can be roughly categorized into two directions. The first focus on elaborating new lightweight architectures by decomposing 3D convolution operations into 2D and 1D ones <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46]</ref>, channel shifting in 2D CNNs <ref type="bibr" target="#b22">[23]</ref>, etc. The others are based on a dynamic inference mechanism <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3]</ref>, which allocates computation resources on a per-sample basis based on the saliencies of frames. Wu et al. <ref type="bibr" target="#b46">[47]</ref> utilizes multi-agent reinforce learning to model parallel frame sampling and Lin et al. <ref type="bibr" target="#b23">[24]</ref> make one-step decision with holistic view. Meng et al. <ref type="bibr" target="#b26">[27]</ref> and Wang et al. <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> focus their attention on spatial redundancy. Panda et al. adaptively decide modalities for video segments. Most of the previous works are mainly based on reinforce learning or attention mechanism, which are optimized with video classification objectives. However, this paradigm makes produced adaptive sampling policy class-agnostic and lacks discrimination power in fine-grained semantics. In contrast, our temporal sampling-based framework enables discriminative class-specific frame saliency measuring and shows that class-specific mechanism combined with visual-textual multi-modal complementary measuring can push the envelope of the trade-off between accuracy and computation cost. Transformer in Vision Tasks. Transformer <ref type="bibr" target="#b38">[39]</ref> is initially proposed to solve the long-term dependence problem in machine translation. ViT <ref type="bibr" target="#b7">[8]</ref>, SwinTransformer <ref type="bibr" target="#b25">[26]</ref> and DVT <ref type="bibr" target="#b42">[43]</ref> split image to patches as words and bring Transformer Encoder to computer vision classification tasks. Query2label <ref type="bibr" target="#b24">[25]</ref> apply Transformer Decoder to multi-label classification task. DETR <ref type="bibr" target="#b1">[2]</ref> explore using Transformer Decoder for object detection task. Then Transformer Decoder for segmentation is also developed by MaskFormer <ref type="bibr" target="#b4">[5]</ref>. The role of Transformer Encoder in C-Tran <ref type="bibr" target="#b20">[21]</ref> and TransVG <ref type="bibr" target="#b5">[6]</ref> is to model relations between different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given a video of T frames X = {x i } T i=1 , x i ? R 3?H?W , our goal is to estimate the saliency score of frames S = {s i } T i=1 and sample top K frames with the highest saliency score to feed into a recognition network to obtain final video prediction P . The overview of our method is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In this section, we first introduce the Temporal Saliency Query (TSQ) mechanism in Section 3.1. Then we elaborate on the framework of our TSQNet, including two instantiations of TSQ mechanism with visual and textual modalities and cross-modality interactions of them in Section 3.2. Finally, we present the inference procedure of TSQNet in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Temporal Saliency Query Mechanism</head><p>The goal of Temporal Saliency Query (TSQ) mechanism is to perform frame saliency estimation for all categories simultaneously, which is the shared building block for two branches of visual and textual modalities in TSQNet. To expand generic saliency to class-specific version, we are potentially to ask each category a question: which frames are the most similar ones to the common pattern of it? In this way, we can convert saliency estimation task to query-response task: a learnable embedding initialized with the common pattern of each category is set as the query, and the gathered feature from frame sequence with similarities is the response. Then the similarities between each category and frame sequence Frame sequence is queried with visual and textual TSQ embeddings of categories in VQM and TQM, then TSQNet responded to the queries by gathering most salient frame representations for each category. And the resultant category-frame similarities are adopted as class-specific saliency measurements for two modalities, which are post-processed and fused for final saliency scores. Top K frames with the highest saliency score are sampled and ingested to an off-the-shelf recognition network for final recognition. Crossmodality interaction ("Interaction") is considered for information exchanging during training. The projection layer is used to reduce the dimension of input features.</p><p>can be regarded as the saliency scores. We denote the learnable embedding here as TSQ embedding. In TSQ mechanism, a TSQ layer is proposed to enable the query-response functionality and a class-specific classifier is designed to generate coarce predictions of video category and enable discriminative learning of TSQ embedding for each category at the same time. The details of TSQ mechanism are described below. TSQ Layer. The goal of TSQ layer is to model the many-to-many categoryframe similarities simultaneously and enable learning of TSQ embeddings, denoted as {E c ? R d } C c=1 , under the video classification objective. To achieve this, TSQ layer is build on an attention structure in Transformer <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_0">A c = softmax( Q 0 K 0 T ? d ), R c = A c V 0 ,<label>(1)</label></formula><p>Q 0 ? R d is a query matrix, which is obtained by projecting each TSQ embedding E c with a parameter matrix W q ? R d?d : Q 0 = E c W q . K 0 ? R T ?d and V 0 ? R T ?d are the key and value matrix, which are generated by projecting frame feature sequence X ? R T ?d with different parameter matrices W k , W v ? R d?d :</p><formula xml:id="formula_1">K 0 = XW k , V 0 = XW v .</formula><p>Then, for the TSQ embedding of the c-th category E c , the attention weight A c ? R T is produced in querying process realized by scaled dot product operation. Then the value V 0 are gathered with attention weights A c and output as response vector R c ? R d , which is fed to FFN of <ref type="bibr" target="#b38">[39]</ref>, i.e., sequential linear layers with residual connections. The output of FFN is ingested to a class-specific classifier to generate classification predictions. In addition to functioning as gathering weights, A c represent the frame saliency measurements of the c-th category for it characterizes the relations between the c-th category and all T frames. In TSQ mechanism, the more discriminative A c is, the better the response vectors {R c } C c=1 can represent the semantic information of the video, therefore the video classification objective can effectively optimize the this category-frame relation model. Class-specific Classifier. We denote the output of FFN asR ? R C?d here. The goal of class-specific classifier ("CS Classifier" in <ref type="figure" target="#fig_0">Figure 2</ref>) are twofold: <ref type="bibr" target="#b0">(1)</ref> projectR ? R C?d to a coarse video prediction z ? R C , (2) enable class-specific learning of TSQ embeddings. In class-specific classifier, instead of directly using projection layer with weight matrix W ? R 1?d as z = WR + b, we apply C projection layers with different weight matrices {W c ? R 1?d } C c=1 to eachR c separately. For the c-th category, corresponding element of p is computed as:</p><formula xml:id="formula_2">z c = W cR T c + b c ,<label>(2)</label></formula><p>where b c ? R 1 , b ? R C are the bias parameters (see Appendix for illustrative examples). This class-specific design endows the response vector of each category with exclusive classifier, which effectively reserves the characteristic of each category and make model converge more easily. z is used for calculating regular cross entropy loss with video labels. Notice here the difference between the coarse video prediction z and the final video prediction P : z is used for saliency measuring while P is the final classification result of the recognition network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Saliency Query Network</head><p>Our TSQNet mainly consists of two modules: a Visual Query module and a Textual Query module, which are instantiations of TSQ mechanism with visual and textual representations, respectively. The Visual Query module query the frame appearance sequence with the visual TSQ embedding of each category, and collect the category-frame similarities for class-specific saliency estimation. Textual Query module measures saliencies by modeling event-object (or action-object) relations on the basis of prior knowledge in off-the-shelf language models. Besides, to exchange information between two TSQ modules, cross-modality interactions are performed synchronously during training, which effective compensate scarce scene information for Textual Query module. Visual Query Module (VQM). The goal of VQM is to generate class-specific saliency measurement from pure visual perspective, which mainly consists of a video encoder, a TSQ layer and a class-specific classifier. The video encoder is a lightweight CNN or transformer backbone, e.g., MobileNetv2 <ref type="bibr" target="#b32">[33]</ref> and Mobileformer <ref type="bibr" target="#b3">[4]</ref>, which extract features from RGB frame sequence</p><formula xml:id="formula_3">{x i } T i=1 to feature sequence {x v i ? R d } T i=1</formula><p>. We further use a 1D convolutional layer to reduce the feature dimension from d to d ? , which we still denote asX v = {x v i } T i=1 for brevity.</p><p>TSQ layer takes visual TSQ embedding as query, and frame sequence as key and value, to generate saliency measurements A v ? R C?T from visual features. Classspecific classifier produce visual video coarse predictions z v , which is further used in the post-processing procedure of saliencies. Next we describe how we obtain visual TSQ embedding. Following the definition in Section 3.1, visual TSQ embedding</p><formula xml:id="formula_4">{E v c ? R d } C c=1</formula><p>here is a set of learnable embeddings initialized with common appearance patterns of categories. We propose a simple prototype based representation for common appearance patterns here. Prior works <ref type="bibr" target="#b33">[34]</ref> find that, most of the samples belonging to the same class cluster around a prototype in feature space formed by non-linear mapping of networks. We assume that category prototypes can represent the common patterns of categories. Following definitions in <ref type="bibr" target="#b33">[34]</ref>, we use the averaged features of videos belonging to each category produced by video encoder in the training set, where a video feature is obtained by top-k pooling of frame features (see Appendix A for details). A 1D convolutional layer is also used to project E v c to the same d ? -dimension space withx v i , which is still represented by E v c hereafter. Textual Query Module (TQM). The goal of TQM is to provide knowledgeaware saliency estimation by mining generic event-object relations in videos with the help of prior knowledge in off-the-shelf language models. As observed by prior works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref>, the event-object (or action-object) relations are generic in videos. Although this knowledge is typically represented in knowledge graph <ref type="bibr" target="#b39">[40]</ref>, we exploit it in a much more compact fashion, i.e., pre-trained language models. It is proved that the semantic relationships between words can be effectively captured in pre-trained word representations, e.g., Word2Vec <ref type="bibr" target="#b28">[29]</ref> and BERT <ref type="bibr" target="#b6">[7]</ref>. To model category-frame relations, we first build a object vocabulary W ? R Co?D , on a pre-defined object list, e.g., ImageNet-1K category list (C o = 1000) with word embeddings. Then we introduce a lightweight but precise object recognizer to extract appearing object scores from each frame</p><formula xml:id="formula_5">{O i ? R Co } T i=1 .</formula><p>The framelevel object embedding based feature can obtained:</p><formula xml:id="formula_6">X t = {x t i } T i=1 ,x t i = O i W . Correspondingly, the textual TSQ embedding {E t c ? R D } C c=1</formula><p>is initialized by pretrained word embeddings of the category name, to align with textual feature sequence in embedding space. Similar to VQM, we add a 1D convolutional layer to {E t c ? R D } C c=1 andX t to reduce dimensions, which are fed into a TSQ layer and class-specific classifier for textual frame saliency measurements A t ? R C?T and textual coarse video prediction z t ? R C . Cross-modality Interaction. Here we seek to enable information exchange between TSQ layers of two modalities during training and provide guidance, e.g., scene knowledge, from VQM to TQM. To achieve this, we design a novel swapattention structure, which gather the feature sequence with attention weights of the other modality in both VQM and TQM, to generate two additional response vectors:</p><formula xml:id="formula_7">R t v = A tX v , R v t = A vX t ,<label>(3)</label></formula><p>Then the two response vectors based on visual feature sequence R v and R t v are ingested to subsequent layers and compute loss as L v and L t v . The same process conducted on textual features sequence renders L t and L v t . The swapattention structure is conducive to TQM in two ways: (1) L t v help optimize scene-aware category-frame relation model <ref type="formula" target="#formula_2">(2)</ref> L v t help optimize scene-aware FFN and classifier. We weighted the existing four losses to obtain the final loss function:</p><formula xml:id="formula_8">L = L v + L t + ?L t v + ?L v t ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference of TSQNet.</head><p>During inference, to yield final saliency measurements, we aggregate the generated frame saliency estimation of high-probability predicted categories for two modalities, respectively, and fuse them for final saliency results. Saliency Aggregation. Here we only describe saliency aggregation for VQM, which is conducted for TQM with the same way. Intuitively, the higher the probability that a video belongs in c-th category, the higher the priority of the c-th row of attention weights in final saliency result. Following this intuition, we aggregate class-specific saliency measurements of VQM,</p><formula xml:id="formula_9">A v ? R C?T with the coarse video prediction z v ? R C .</formula><p>For the i-th frame, the measured saliency of VQM is:</p><formula xml:id="formula_10">s v i = C ? c=1 z v c A v c,i ,<label>(5)</label></formula><p>In practive, to filter the noise brought about by the low-confidence categories, we only aggregate saliencies of top-5 categories with highest z v to get final saliency measurements.</p><p>Multi-modality Saliency Fusion. We fuse the saliency measurements of VQM and TQM by taking the union of the top s v i frames and top s t i frames. The number of frames used for union in two modules are controlled by pre-defined proportion ? v and ? t , and the budget of selected frames K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We evaluate our method on three large-scale datasets: ActivityNet, FCVID and Mini-Kinetics. ActivityNet <ref type="bibr" target="#b0">[1]</ref> contains 200 categories, it has 10024 videos for training and 4926 videos for validation, where the average duration of videos is 117 seconds. FCVID <ref type="bibr" target="#b16">[17]</ref> includes 91,223 videos which 45,611 for training and 45,612 for validation and divided to 239 classes, where the average duration of the videos is 167 seconds. Mini-Kinetics is a small version of Kinetics <ref type="bibr" target="#b17">[18]</ref>, it consists of 121k training videos and 10k validation ones from 200 categories. Different from first two benchmarks, the videos in Mini-Kinetics are trimmed, with a average length of 10 seconds. Evaluation metrics. For all datasets above, we apply the official train-val split to experiment our method. Following the previous work, mean Average Precision  (mAP) is used as the main evaluation metric for ActivityNet and FCVID, and Top1 accuracy for Mini-Kinetics. We also evaluate the computation cost with giga floating point operations (GFLOPs). Implementation details. We adopt MobileNetv2 <ref type="bibr" target="#b32">[33]</ref> trained on target datasets as the video encoder in VQM, and Efficientnet-B0 <ref type="bibr" target="#b36">[37]</ref> trained on ImageNet-1K as the object recognizer in TQM, respectively. For fair comparisons with previous works, we adopt three backbones in ResNet <ref type="bibr" target="#b13">[14]</ref> series, e.g., ResNet-50, 101, 152 for recognition networks. For resolution of frame processed by recognition networks, we follow previous works to scale the shorter side of frames to 256 and then center cropped them to 224 ? 224 for all datasets. On ActivityNet and FCVID, the resolution of frames processed by VQM is 188 ? 188 and one for TQM is 112 ? 112 1 . On Mini-Kinetics, the resolution is 112?112 for both VQM and TQM. <ref type="table" target="#tab_0">Table 1</ref> shows decomposition of computation cost of TSQNet when adopting ResNet-50 as recognition network. Please refer to Appendix for more implementation details.</p><formula xml:id="formula_11">.55G VQM - - - -0.36G TQM - - - -0.10G Total - - - -26.09G</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Simple Baselines</head><p>We compare our TSQNet with some simple baselines with ResNet-101 without TSN-style training as the recognizer in <ref type="table" target="#tab_1">Table 2</ref>. There are multiple rule based baselines, "uniform" and "random" stand for uniformly and randomly selecting 10 frames from a video. "Dense" means using all frames of a video. For "Max-Conf", we firstly obtain the maximum confidence among all categories for every frame by applying the model along time axis, then select K frames with highest maximum confidence. We also compare with a simple sampler based baseline, "MaxConf-L", which is a lightweight version of "MaxConf" within a uniformly pre-sampled T frames, as the same as "ours". The T in "MaxConf-L" and "ours" is 50, and K in "MaxConf", "MaxConf-L" and "ours" is 5. Our TSQNet obviously presents the best accuracy with limited FLOPs. In fact, "MaxConf-L" is an ablated baseline for our class-specific motivation, which replaces our TSQ mechanism with direct frame-level classification. Comparison with "MaxConf-L" confirms the efficacy of our TSQ mechanism. Results on AcitivtyNet. We compare the proposed method with recent SOTA methods on AcitivtyNet in <ref type="table" target="#tab_2">Table 3</ref>: SCSampler <ref type="bibr" target="#b19">[20]</ref>, AR-Net <ref type="bibr" target="#b26">[27]</ref>, AdaMML <ref type="bibr" target="#b29">[30]</ref>, VideoIQ <ref type="bibr" target="#b35">[36]</ref>, AdaFcous <ref type="bibr" target="#b41">[42]</ref>, Dynamic-STE <ref type="bibr" target="#b18">[19]</ref> and FrameExit <ref type="bibr" target="#b11">[12]</ref>. Experimental result shows that our method outperforms all existing methods with ResNet50 as the main recognition network. Compared with SCSampler <ref type="bibr" target="#b19">[20]</ref> which is also a temporal sampling approach, our method surpass it by 3.7% while using 1.6? less computation overhead, which demonstrates the discrimination power of TSQ mechanism in temporal saliency estimation. Comparing to the state-of-the-art method based on early exiting, FrameExit <ref type="bibr" target="#b11">[12]</ref>, we still outperforms it by 0.5%, which shows our class-specific sampler can find more discriminative frames than this sequential early exiting framework. For a more fair comparison with above pure visual based methods, we also present the results of the visual variant of TSQNet, i.e., 'VQM-only' with comparable computes. Although without text modality, it still surpass the SotA methods, which verify the superiority of our TSM mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-arts</head><p>We further compare TSQNet with SOTA approaches in <ref type="figure" target="#fig_1">Figure 3</ref> based on Res101 backbone. Following previous works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b46">47]</ref>, ResNet-101 without TSN-style training is used as the recognizer, as the same as in Section 4.2. We calculate mAP under different budget K, which varies from 3 to 10. It is shown that our method achieves clearly superior efficiency-accuracy trade-off over all methods. And the result of pure VQM illustrates the efficiency of TSQ Mechanism.</p><p>To verify that our TSQNet can collaborate with more backbones, we present experiment results with ResNet-152 and Swin-transformer <ref type="bibr" target="#b25">[26]</ref> family as recognition networks in <ref type="table" target="#tab_3">Table 4</ref>. It is shown that our method outperforms all method with the same ResNet-152 backbones, and achieves absolute SOTA precision (88.7 Top-1 accuracy and 93.7 mAP) with Swin-Transformer architecture. Results on FCVID. To verify that performance promotion can be achieved on more untrimmed datasets, we also evaluate our method on FCVID in Table 5, which shows that our method outperforms competing methods in terms of accuracy while saving much computation cost. Compared with SOTA approach AdaFocus <ref type="bibr" target="#b41">[42]</ref>, which is motivated by selecting salient spatial regions, we achieve  higher mAP with less computation, which implies that our discriminative temporal sampler can capture more salient information of videos.</p><p>Results on Mini-Kinetics. We further test the capability of TSQNet on a short trimmed video dataset i.e., Mini-Kinetics, which is more difficult to sample salient frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we inspect different aspects of our proposed TSQNet. All ablations are completed on AcitivtyNet with ResNet-101 as recognition network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods mAP(%) FLOPs</head><p>LiteEval <ref type="bibr" target="#b49">[50]</ref> 80.0 94.3G AdaFrame <ref type="bibr" target="#b50">[51]</ref> 80.2 75.1G SCSampler <ref type="bibr" target="#b19">[20]</ref> 81.0 42.0G AR-Net <ref type="bibr" target="#b26">[27]</ref> 81. <ref type="bibr" target="#b2">3</ref> 35.1G AdaFuse <ref type="bibr" target="#b27">[28]</ref> 81. <ref type="bibr" target="#b5">6</ref> 45.0G SMART <ref type="bibr" target="#b12">[13]</ref> 82.1 -VideoIQ <ref type="bibr" target="#b35">[36]</ref> 82.7 27.0G AdaFocus <ref type="bibr" target="#b41">[42]</ref> 83  <ref type="bibr" target="#b19">[20]</ref> 70.8 42.0G AR-Net <ref type="bibr" target="#b26">[27]</ref> 71.7 32.0G AdaFuse <ref type="bibr" target="#b27">[28]</ref> 72.3 23.0G VideoIQ <ref type="bibr" target="#b35">[36]</ref> 72.3 20.4G Dynamic-STE <ref type="bibr" target="#b18">[19]</ref> 72.7 18.3G FrameExit <ref type="bibr" target="#b11">[12]</ref> 72. <ref type="bibr" target="#b7">8</ref> 19.7G AdaFocus <ref type="bibr" target="#b41">[42]</ref> 72 Effectiveness of Class-specific Designs. We investigate the effectiveness of our class-specific designs in TSQ mechanism. <ref type="table" target="#tab_11">Table 7</ref> presents the results of classspecific ("CS") version and class-agnostic ("CA") version of both the attention structure and the classifier in VQM. For attention structure, the class-agnostic version refers to setting the size of visual TSQ embedding set {E v c } C c=1 to 1. Then generated attention weight A v ? R 1?T is directly used as saliency measurement. For the classifier, the class-agnostic version is to replace existing C-projectionlayer classifier with a single-projection-layer one as aforementioned in Section 3.1. It is shown that "CS CS" (ours) significantly outperforms "CA CA" choice, which confirms the effectiveness of class-specific information in saliency measurements. Besides, "CS CA" choice presents an unpromising result, which demonstrates that class-specific classifier is critical for TSQ mechanism to function normally in class-specific setting. See Appendix for illustrative examples of these three settings and detailed explanation of comparison of their performance. Effectiveness of Multi-modal and Fusion and Interactions. To verify the effectiveness of fusion of VQM and TQM and multi-modality interactions, we present experimental results on two individual modalities with different usage of L t v and L v t in <ref type="table" target="#tab_11">Table 8</ref>. Without any interactions, fusion of two modules relatively impart improvements on TQM and VQM for 2.9% and 0.3% respectively, which verifies that two modules are complementary. L t v clearly elevate the performance of TQM for better category-frame modelling guided by visual features from VQM. The performance of VQM is also slightly improved by introducing textual-modality attention weights. L v t significantly improves the performance of TQM for better learning of textual FFN and classifier. Finally, when both losses in CIM are added, the results of both TQM and VQM branch are further promoted, and performance of overall TSQNet is obviously improved (75.3 v.s. 74.9). See Appendix C for detailed investigations on ratios of two losses.  Different Textual Feature. In <ref type="table" target="#tab_11">Table 9</ref>, we try three commonly used word embeddings, i.e., Bert <ref type="bibr" target="#b6">[7]</ref>, Glove <ref type="bibr" target="#b30">[31]</ref> and Word2Vec <ref type="bibr" target="#b28">[29]</ref>, as well as two fashions of usage of object scores O i , i.e., top-10 object categories ("Top10") and all categories ("All"). Experimental result shows that the Bert embedding with top-10 object score gain the best result, which verifies that both the quality of word embedding and noise filtering of object category count for textual instantiation of TSQ mechanism.</p><p>Impacts of Initialization of TSQ Embedding. We further explore the initialization of visual and textual TSQ embeddings in <ref type="table" target="#tab_0">Table 10</ref>. The comparison with random initialization confirms that proposed prototype based visual TSQ embedding in VQM and word embedding based textual embedding in TQM provide meaningful and effective initialization for TSQ embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Analysis</head><p>We visualize visual and textual TSQ embdding by t-SNE in <ref type="figure" target="#fig_2">Figure 4</ref>, which shows that our class-specific motivation is highly interpretable in terms of relationships between categories. We also find some categories sharing similar objects are more closer in text TSQ embeddings than in visual ones. For examples, Decorating the Christmas tree and Trimming branches or hedges share tree or tree-related objects and become closer after training. This may be because TQM measure saliency based on event-object relations, which are more robust against scene variations. In <ref type="figure" target="#fig_3">Figure 5</ref>, we exhibit some qualitative examples of Decorating Christmas tree and Golfing for sampled frames by uniform baseline, TQM, VQM and TSQNet. In the case of Decorating Christmas tree, it is shown that TQM and VQM are clearly better than uniform baseline. After fusion, TSQNet can sample further more salient frames. Another qualitative example Golfing is quite interesting. VQM captures the action moments of swinging a golf club and scenes of a golf course, while TQM captures the golf balls and a golf cart. After fusion, TSQNet select the frames of these object, actions and scenes, which implies our TQM and VQM can cooperate to build a robust sampler aware of object, scene and action information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper investigates efficient video recognition by proposing a novel Temporal Saliency Query mechanism and presents an efficient multi-modal salient frame sampler Temporal Saliency Query Network. Extensive experiments verify the proposed method significantly outperforms the state-of-the-art approaches on accuracy-efficiency trade-off. Our proposed method is model-agnostic and can be used with various network architectures. And since our salient score is class-specific, we can easily extend our method to multi-label efficient video recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Implementation Details</head><p>Here we provide some implementation details of TSQNet. We uniformly presample T frames from a video, and for those videos whose lengths are shorter than T , we repeat multiple times to padding it to T frames. Our frame sampler will select top K most salient frames in T , T and K can be adjusted to accommodate different budgets for downstream applications. We use SGD optimizer with momentum of 0.9 and train model with batch size of 64 for 100 epochs. The learning rate is 10 ?2 , decayed by the factor of 0.1 at the 25, 50, 75 epoch. Loss ratio ? and ? are both 0.6. Fusion proportion ? v and ? t are 0.6 and 0.4, respectively. We use MobileNetv2 and EfficientNet-B0 as the video encoder in VQM and object recognizer in TQM, respectively. For video encoder in TQM, we use the ImageNet pre-trained model and finetuned it on target datasets e.g., ActivityNet, etc., for 10 epochs. And for Object recognizer, we directly use the officially released ImageNet model to extract object score of the ImageNet 1000 classes. We use positional embedding on frame sequence in transformer decoder to model temporal order information. Prototype feature generation. Here we introduce how we obtain visual prototype based representation for visual TSQ embeddings initialization. First we apply a classifier to get the classification results for each frame. Then we select the top m percent of frames which can correctly predict the ground truth video category, which are then averaged to obtain the representation of each video. Finally, we pool all the video representations of each category to get the prototype representation of each category. We use m = 30 for all experiments in this paper. Saliency score fusion. We describe in detail how to fuse the VQM and TQM saliency scores into the final saliency measurement. Suppose we have the VQM salient scores S v ? R T and TQM salient scores S t ? R T of one video. We join the top saliency score frames from two modalities to get final K salient frames. Specifically, the number of selected frames from two modalities are determined by ? v K and ? t K, respectively, where ? v + ? t = 1. For example of selecting 5 frames from 16 frames with ? v = 0.6 situation, we select top 5 ? 0.6 = 3 frames from VQM and top 5 ? 3 = 2 ones are from TQM. And if there exists duplication, which results in a final result of less than 5 frames, the selection will be deferred in the VQM according to the descending order of S v until meeting the 5-frame budget. We use ? v = 0.6 and ? t = 0.4 in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Practical Inference Speed</head><p>To further verify the practical efficiency of our method, we compare the inference speed with two state-of-the-art methods FrameExit <ref type="bibr" target="#b11">[12]</ref> and AdaFocus <ref type="bibr" target="#b41">[42]</ref> on ActivityNet. FrameExit <ref type="bibr" target="#b11">[12]</ref> reduce computation cost by early stopping in temporal sequential prediction. AdaFocus <ref type="bibr" target="#b41">[42]</ref> suppose that the existing methods are spatially redundant, so it only selects salient areas to classify for each frames. We test the speed of two methods by running the official code released by the authors. We evaluate the inference speed of all methods on a NVIDIA 3090 GPU with Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz CPU. Results in two different settings with batch size of 1 and 32 are reported. Note that Frame-Exit <ref type="bibr" target="#b11">[12]</ref> exits from recognition at different time for different videos, so it cannot inference in batch setting, which we only report the latency with batch size = 1 here. Experimental results in <ref type="table" target="#tab_11">Table A</ref>.1 show that our method not only saves much theoretical computation complexity but also achieves the fastest actual inference speed (121.1 video/s) on both single-sample and batch setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Ablation Study</head><p>In this section, more ablation experiments are conducted to supplement the main paper. ResNet-101 is utilized for the recognition network as the same as in ablation studies of the main paper.   <ref type="figure">Figure 1</ref>. "CA + CA" represents the class-agnostic attention structure (with 1 query) combined with the class-agnostic classifier (with a single FC). "CS + CA", i.e. our TSQNet, the class-specific attention structure (with C queries) combined with the class-agnostic classifier (with a single FC). "CS + CS" represents the class-specific attention structure (with C queries) combined with class-agnostic classifier (with C FCs). It is interesting that the performance of "CS+CA" (68.7) is much lower than that of "CA+CA" (74.0), which seems like a more naive baseline than "CS+CA". When using the classspecific attention structure to obtain feature with shape of 200 ? 1280, the FC classifier (200 ? 1280) must have a one-to-one correspondence with each class, i.e., "CS+CS" (74.7), to achieve good results. If using one 1 ? 1280 FC, i.e., "CS+CA", to process all classes with the same parameters, discrimination power are insufficient and accuracy will decrease dramatically, which will be even lower than "CA+CA".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Ablation of class-specific classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Ablation study of ? and ?</head><p>First, we explore the appropriate values for ? and ?, i.e., the ratios of L t v and L v t in <ref type="table" target="#tab_1">Table A.2 and Table A</ref>.3, respectively. We first fix ? = 0 to find the best ?. As shown in <ref type="table" target="#tab_11">Table A</ref>.2, as ? increases, the performance of both TQM and TSQNet rises up to a maximum at ? = 0.6 and then falls down. The performance of VQM remains unchanged, which demonstrates L t v mainly benefit TQM in interactions. Then we fix ? = 0.6 to explore the impacts of ?. As presented in <ref type="table" target="#tab_11">Table A</ref>.3, the performance shows similar trend and the best results of TQM, VQM and TSQNet are achieved when ? and ? both equal to 0.6, which implies L v t benefits both TQM and VQM in interactions. After ? = 0.6, the performance of VQM breaks down, for prohibitively large ? hinders the convergence of the VQM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Detailed Ablation study for transformer decoder structures</head><p>We further ablate the structure of the standard transformer decoder, viz., selfattention, number of layers and heads. Typical transformer decoder contains a self-attention layer on the top of query matrix and multiple cross-attention layers with multi-head structure. In TSQNet, we use a quite brief version of transformer decoder, containing a single-head cross-attention layer without selfattention layers, to realize TSQ layer. Next we discuss the effectiveness of this design. Impact of Self-attention layer. On one hand, self-attention layer on queries make each TSQ embedding interact with each other, which may cause the classspecific information to mix with each other and deviates the class-specific nature of TSQ embeddings. On the other hand, self-attention layers bring in extra computation complexity of O(C 2 ) , where C is the number of categories. As shown in <ref type="table" target="#tab_11">Table A</ref>.4, adding self-attention layer presents lower performance, which demonstrates that modelling relations between TSQ embeddings of categories can not produce better saliency measuring results. Number of layers and heads. In TSQNet, the number of cross-attention layers and heads are both one. We present ablation experiments of more layers with more heads in <ref type="table" target="#tab_11">Table A</ref>.5. It is shown that both the increase of number of layers and heads make the mAP drop. For multiple cross-attention layers, the performance drop may attribute to lower discrepancy between queries in intermediate layers, which makes attention weights lack discrimination power between categories. For multi-head structure, the worse results may result from attention dimension splitting operation when calculating the similarity between query matrix and key matrix, which produces separate local similarities for multiple groups in feature dimension rather than the holistic similarity of the feature dimension. salient frames than the uniform baseline. Similar in <ref type="figure">Figure A.</ref>8, uniform baseline selects many irrelevant frames, whereas our method selects more theme-related frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Visualization of TSQ Embeddings</head><p>In this section, we provide the complete t-SNE visualization for TSQ embeddings of both the VQM and TQM on ActivityNet to supplement the local zooms visualization in Section 4.5 of the main paper. Specifically, we visualize the start and end states of training for the two modules in two different initialization fashions, i.e., random and proposed initialization, respectively. For VQM, we compare the random initialization with the visual common appearance feature initialization. For TQM, we compare the random initialization with the class name Bert embedding feature initialization. TSQ embeddings of TQM. <ref type="figure">Figure A.9</ref> shows the visualization of TSQ embeddings of TQM with class name Bert embedding feature initialization before training. <ref type="figure">Figure A.10</ref> shows the visualization of TSQ embeddings of TQM with class name Bert embedding feature initialization after training. <ref type="figure">Figure A.11</ref> shows the visualization of TSQ embeddings of TQM with random initialization before training. <ref type="figure" target="#fig_0">Figure A.12</ref> shows the visualization of TSQ embeddings of TQM with random initialization after training. TSQ embeddings of VQM. <ref type="figure" target="#fig_1">Figure A.13</ref> shows the visualization of TSQ embeddings of VQM with common appearance feature initialization before training. <ref type="figure">Figure A</ref> visualization of TSQ embeddings of VQM with random initialization before training. <ref type="figure" target="#fig_5">Figure A.16</ref> shows the visualization of TSQ embeddings of VQM with random initialization after training.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The overview of the Temporal Saliency Query Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of the pure VQM and the whole TSQNet with the state-of-the-art based on ResNet-101 recognition network on ActivityNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The visual and textual TSQ embeddings before and after training visualized by t-SNE. The category embeddings with relevant semantics cluster together after training. See Section 4.5 for detailed explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative Evaluation of Sampled frames. We visualized the most salient five frames of uniform and our proposed methods with two samples. The frames with golden border represent the identified salient frames by human intuition, and the frames with mask denote the non-salient ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>number, 1280 represents for feature dim.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. A. 6 .</head><label>6</label><figDesc>Illustrative examples of three combinations between the attention structure and the classifier of TSQNet, i.e. "CS+CA", "CA+CA" and "CS+CS".We show the illustrative examples of the combinations of the attention structure and the class-specific classifier under the situation of 200 class and 1280 feature dimensions in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A. 7 Fig. A. 7 .</head><label>77</label><figDesc>andFigure A.8show more qualitative results of TSQNet on Activ-ityNet and FCVID. For each dataset, we selected six examples, first three of which belongs to the same category and the last three belongs to different categories. InFigure A.7, we can see that our approach samples significantly more Qualitative Analysis on ActivityNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. A. 8 .</head><label>8</label><figDesc>.14 shows the visualization of TSQ embeddings of VQM with common appearance feature initialization after training.Figure A.15shows the Qualitative Analysis on FCVID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Example of FLOPs computation. Arch. Res. FLOPs/F #F FLOPs Vis.Enc. MBv2 188 0.220G 16 3.52G Obj.Rec. EN-B0 112 0.098G 16 1.56G Rec.Net. RN50 224 4.109G 5 20</figDesc><table><row><cell>Module</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with simple baselines.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP (%) FLOPs</cell></row><row><cell>Uniform</cell><cell>70.9</cell><cell>195.8G</cell></row><row><cell>Random</cell><cell>70.2</cell><cell>195.8G</cell></row><row><cell>Dense</cell><cell>71.2</cell><cell>930.8G</cell></row><row><cell>MaxConf</cell><cell>74.2</cell><cell>930.8G</cell></row><row><cell>MaxConf-L</cell><cell>71.2</cell><cell>54.9G</cell></row><row><cell>Ours</cell><cell>74.3</cell><cell>55.3G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with SOTA efficient video recognition methods with ResNet50 as recognition backbone on AcitivtyNet. 188 and 224 here represent resolutions.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mAP(%)</cell><cell>FLOPs</cell></row><row><cell>SCSampler [20]</cell><cell>ResNet50</cell><cell>72.9</cell><cell>42.0G</cell></row><row><cell>AR-Net [27]</cell><cell>ResNet18,34,50</cell><cell>73.8</cell><cell>33.5G</cell></row><row><cell>AdaMML [30]</cell><cell>ResNet50</cell><cell>73.9</cell><cell>94.0G</cell></row><row><cell>VideoIQ [36]</cell><cell>ResNet50</cell><cell>74.8</cell><cell>28.1G</cell></row><row><cell>AdaFocus [42]</cell><cell>ResNet50</cell><cell>75.0</cell><cell>26.6G</cell></row><row><cell>Dynamic-STE [19]</cell><cell>ResNet18,50</cell><cell>75.9</cell><cell>30.5G</cell></row><row><cell>FrameExit [12]</cell><cell>ResNet50</cell><cell>76.1</cell><cell>26.1G</cell></row><row><cell>Ours (VQM-only 188 )</cell><cell>ResNet50</cell><cell>75.7</cell><cell>24.3G</cell></row><row><cell>Ours (VQM-only 224 )</cell><cell>ResNet50</cell><cell>76.5</cell><cell>26.1G</cell></row><row><cell>Ours</cell><cell>ResNet50</cell><cell>76.6</cell><cell>26.1G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparisons with SOTA video recognition methods using ResNet-152 and more advanced recognition networks on AcitivtyNet.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Pretrain Accuracy(%) mAP(%)</cell></row><row><cell>P3D [32]</cell><cell>ResNet-152</cell><cell>ImageNet</cell><cell>75.1</cell><cell>78.9</cell></row><row><cell>RRA [55]</cell><cell>ResNet-152</cell><cell>ImageNet</cell><cell>78.8</cell><cell>83.4</cell></row><row><cell>MARL [47]</cell><cell>ResNet-152</cell><cell>ImageNet</cell><cell>79.8</cell><cell>83.8</cell></row><row><cell>Ours</cell><cell>ResNet-152</cell><cell>ImageNet</cell><cell>80.0</cell><cell>85.2</cell></row><row><cell cols="2">ListenToLook [11] R(2+1)D-152</cell><cell>Kinetics</cell><cell>-</cell><cell>89.9</cell></row><row><cell>MARL [47]</cell><cell cols="2">SEResNeXt152 Kinetics</cell><cell>-</cell><cell>90.1</cell></row><row><cell>Ours</cell><cell>Swin-B</cell><cell>Kinetics</cell><cell>84.7</cell><cell>91.2</cell></row><row><cell>Ours</cell><cell>Swin-L</cell><cell>Kinetics</cell><cell>88.7</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>demonstrates that our method achieves superior Top-1 accuracy (73.2 v.s. 72.9) with 2.0? less FLOPs than the state-of-the-art method<ref type="bibr" target="#b41">[42]</ref>. Practical latency. We further conduct experiments of practical efficiency, which shows that our TSQNet significantly surpasses two state-of-the-art methods in inference latency, i.e., FrameExit<ref type="bibr" target="#b11">[12]</ref> (9.8 videos/sec v.s. TSQNet 121.1 videos/sec) and AdaFocus [42] (73.8 videos/sec v.s. TSQNet 121.1 videos/sec)</figDesc><table /><note>1 . See Appendix B for more details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison with SOTA efficient video recognition methods on FCVID. TSQNet achieves the best mAP with significant computation savings. '188' and '224' are resolutions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison with stateof-the-art methods on Mini-Kinetics.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TSQNet achieves the best Top-1 ac-</cell></row><row><cell></cell><cell></cell><cell cols="2">curacy with comparable computation</cell></row><row><cell></cell><cell></cell><cell cols="2">cost with the most efficient methods.</cell></row><row><cell></cell><cell></cell><cell>Methods</cell><cell>Top-1(%) FLOPs</cell></row><row><cell></cell><cell></cell><cell>LiteEval [50]</cell><cell>61.0</cell><cell>99.0G</cell></row><row><cell></cell><cell></cell><cell>SCSampler</cell></row><row><cell></cell><cell>.4</cell><cell>26.6G</cell></row><row><cell cols="3">Ours (VQM-only 188 ) 82.9 24.4G</cell></row><row><cell cols="2">Ours (VQM-only 224 ) 83.3</cell><cell>26.2G</cell></row><row><cell>Ours</cell><cell>83.5</cell><cell>26.2G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .Table 8 .Table 9 .Table 10 .</head><label>78910</label><figDesc>Effectiveness of Class-specific designs. Effectiveness of multimodality fusion and interactions. Results of different textual feature. Impacts of initialization of TSQ embedding.</figDesc><table><row><cell cols="6">Attention Classifier mAP(%)</cell><cell></cell><cell></cell><cell></cell><cell>Lt v Lv t TQM VQM Ours</cell></row><row><cell></cell><cell>CA</cell><cell cols="2">CA</cell><cell cols="2">74.0</cell><cell></cell><cell></cell><cell></cell><cell>-?</cell><cell>--</cell><cell>72.0 74.6 74.9 72.5 74.8 75.1</cell></row><row><cell></cell><cell>CS</cell><cell cols="2">CA</cell><cell cols="2">68.7</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>?</cell><cell>72.7 74.6 75.1</cell></row><row><cell></cell><cell>CS</cell><cell cols="2">CS</cell><cell cols="2">74.7</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>? 73.1 74.8 75.3</cell></row><row><cell cols="6">Method Usage mAP(%)</cell><cell></cell><cell></cell><cell></cell><cell>Branch</cell><cell>Init</cell><cell>mAP(%)</cell></row><row><cell cols="2">W2V Glove</cell><cell cols="2">Top10 Top10</cell><cell>71.2 72.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Vis</cell><cell>Random Prototype</cell><cell>73.8 74.7</cell></row><row><cell cols="2">Bert Bert</cell><cell cols="2">All Top10</cell><cell>71.4 72.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Text</cell><cell>Random Bert Emb.</cell><cell>71.6 72.1</cell></row><row><cell>Doing kickboxing</cell><cell cols="2">Beer pong Ping-pong</cell><cell cols="2">Decorating the Christmas tree</cell><cell cols="2">Doing kickboxing Tai chi</cell><cell>Trimming branches or hedges</cell><cell cols="2">Playing water polo</cell><cell>Playing ice hockey</cell><cell>Decorating the Christmas tree</cell><cell>Playing ice hockey Curling</cell></row><row><cell></cell><cell>Tai chi</cell><cell>Tango</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ballet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Curling</cell><cell>Tango</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ballet</cell></row><row><cell>Decorating the Christmas tree</cell><cell cols="3">Trimming branches or hedges</cell><cell>Trimming branches or hedges</cell><cell>Tango Ballet</cell><cell>Beer pong Ping-pong</cell><cell cols="2">Decorating the Christmas tree</cell><cell>Swimming Scuba diving</cell><cell>Tango Ballet</cell><cell>Playing water polo Swimming Scuba diving Trimming branches or hedges</cell></row><row><cell cols="3">(a) Textual before training</cell><cell></cell><cell cols="3">(b) Textual after training</cell><cell cols="3">(c) Visual before training</cell><cell>(d) Visual after training</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A . 1 .</head><label>A1</label><figDesc>Comparisons of practical inference speed with state-of-the-art methods on ActivityNet.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP (% ) FLOPs (G)</cell><cell>Throughput(bs=1) (videos/s)</cell><cell>Throughput(bs=32) (videos/s)</cell></row><row><cell>AdaFocus [42]</cell><cell>75.0</cell><cell>26.6</cell><cell>5.5</cell><cell>73.8</cell></row><row><cell>FrameExit [12]</cell><cell>76.1</cell><cell>26.1</cell><cell>9.8</cell><cell>-</cell></row><row><cell>Ours</cell><cell>76.5</cell><cell>26.1</cell><cell>17.7</cell><cell>121.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A . 2 .</head><label>A2</label><figDesc>Ablation study of ? when fixing ? = 0.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Table A.3. Ablation study of ? when</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">fixing ? = 0.6.</cell><cell></cell></row><row><cell cols="3">? TSQNet TQM VQM</cell><cell cols="3">? TSQNet TQM VQM</cell></row><row><cell cols="2">0.0 0.2 0.4 0.6 75.1 74.9 74.9 75.0</cell><cell>72.0 74.6 72.3 74.6 72.5 74.6 72.7 74.6</cell><cell>0.0 0.2 0.4</cell><cell>75.1 75.0 75.1</cell><cell>72.7 74.6 72.8 74.6 72.8 74.7</cell></row><row><cell>0.8</cell><cell>74.8</cell><cell>72.6 74.6</cell><cell cols="2">0.6 75.3</cell><cell>73.1 74.8</cell></row><row><cell>1.0</cell><cell>74.7</cell><cell>72.6 74.6</cell><cell>0.8</cell><cell>71.2</cell><cell>72.5 67.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell cols="2">.4. Ablation study of the us-</cell><cell cols="2">Table A.5. Ablation study of Trans-</cell></row><row><cell>age of self-attention.</cell><cell></cell><cell cols="2">former Decoder layers and heads.</cell></row><row><cell cols="2">Methods mAP (%)</cell><cell>Methods</cell><cell>mAP (%)</cell></row><row><cell>w/ self-atten</cell><cell>74.5</cell><cell>1 layer 8 head 2 layer 1 head</cell><cell>73.7 73.4</cell></row><row><cell cols="2">w/o self-atten 74.7</cell><cell cols="2">1 layer 1 head 74.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>TSQ embeddings of TQM with class name Bert initialization before training.Fig. A.10. TSQ embeddings of TQM with class name Bert initialization after training.Fig. A.11. TSQ embeddings of TQM with random initialization before training.</figDesc><table><row><cell>4 3 2 1 0 1</cell><cell cols="8">Applying sunscreen Bathing dog Blow-drying hair Braiding hair Breakdancing Baton twirling Beer pong Belly dance Blowing leaves Bullfighting Calf roping Camel ride Changing car wheel Cleaning shoes Assembling bicycle Chopping wood Brushing hair Brushing teeth Cleaning sink Cleaning windows Clipping cat claws Croquet Cumbia Cutting the grass Decorating the Christmas tree Discus throw Doing a powerbomb Doing crunches Drinking beer Drinking coffee Drum corps Elliptical trainer Fixing bicycle Fixing the roof Fun sliding down Arm wrestling Baking cookies Building sandcastles Beach soccer BMX Bungee jumping Carving jack-o-lanterns Cheerleading Clean and jerk Disc dog Dodgeball Doing fencing Doing karate Doing kickboxing Doing motocross Doing nails Doing step aerobics Futsal Gargling mouthwash Getting a haircut Getting a piercing Sharpening knives Shaving Chopping wood Scuba diving Throwing darts Trimming branches or hedges Shaving legs Getting a tattoo Grooming dog Grooming horse Hammer throw Hand car wash Hand washing clothes Having an ice cream High jump Hitting a pinata Hula hoop Installing carpet Ironing clothes Javelin throw Laying tile Layup drill in basketball Long jump Longboarding Making a cake Making a lemonade Making a sandwich Making an omelette Mixing drinks Mooping floor Mowing the lawn Paintball Peeling potatoes Ping-pong Plataform diving Playing accordion Playing badminton Curling Archery Ballet Hopscotch Horseback riding Kayaking Kite flying Kneeling Canoeing Knitting Painting Painting fence Painting furniture Ice fishing Hanging wallpaper Cricket Hurling Plastering Playing bagpipes Playing beach volleyball Playing blackjack Playing congas Playing drums Playing field hockey Playing flauta Playing guitarra Playing harmonica Playing ice hockey Playing kickball Playing lacrosse Playing piano Playing polo Playing pool Playing racquetball Playing rubik cube Playing saxophone Playing squash Playing ten pins Playing violin Playing water polo Pole vault Polishing forniture Polishing shoes Powerbocking Preparing pasta Preparing salad Putting in contact lenses Putting on makeup Putting on shoes Rafting Raking leaves Removing curlers Removing ice from car Riding bumper cars River tubing Rock climbing Rock-paper-scissors Rollerblading Roof shingle removal Rope skipping Running a marathon Sailing Scuba diving Shot put Shoveling snow Shuffleboard Skateboarding Skiing Slacklining Smoking a cigarette Smoking hookah Snatch Snow tubing Snowboarding Spinning Spread mulch Springboard diving Starting a campfire Sumo Surfing Swimming Swinging at the playground Table soccer Tai chi Tango Tennis serve with ball bouncing Throwing darts Trimming branches or hedges Triple jump Tug of war Tumbling Using parallel bars Using the balance beam Using the monkey bar Using the pommel horse Using the rowing machine Using uneven bars Vacuuming floor Volleyball Wakeboarding Walking the dog Washing dishes Washing face Washing hands Waterskiing Waxing skis Welding Windsurfing Wrapping presents Zumba Making a sandwich Sharpening knives Archery Doing karate Installing carpet Playing field hockey Running a marathon Applying sunscreen Arm wrestling Assembling bicycle BMX Baking cookies Ballet Bathing dog Baton twirling Beach soccer Beer pong Croquet Curling Decorating the Christmas tree Discus throw Drinking coffee Having an ice cream Playing flauta Playing lacrosse Playing piano Powerbocking Playing squash Rock-paper-scissors Horseback riding Belly dance Blow-drying hair Blowing leaves Braiding hair Breakdancing Brushing hair Brushing teeth Building sandcastles Bullfighting Bungee jumping Calf roping Camel ride Canoeing Capoeira Carving jack-o-lanterns Changing car wheel Cheerleading Clean and jerk Cleaning shoes Cleaning sink Cleaning windows Clipping cat claws Cricket Cumbia Cutting the grass Disc dog Dodgeball Doing a powerbomb Doing crunches Doing fencing Doing kickboxing Doing motocross Doing nails Doing step aerobics Drinking beer Drum corps Elliptical trainer Fixing bicycle Fixing the roof Fun sliding down Futsal Gargling mouthwash Getting a haircut Getting a piercing Getting a tattoo Grooming dog Grooming horse Hammer throw Hand car wash Hand washing clothes Hanging wallpaper Laying tile Layup drill in basketball Paintball Painting fence Playing badminton Playing drums Playing harmonica Playing pool Playing ten pins Playing violin Playing water polo Polishing shoes Removing ice from car Skateboarding Skiing Sumo Tai chi Swinging at the playground Preparing pasta Putting on shoes Rafting Sailing Snatch Springboard diving Tango Triple jump Windsurfing Waterskiing Using the pommel horse Starting a campfire Tennis serve with ball bouncing Riding bumper cars Rock climbing Rope skipping Walking the dog Smoking hookah Kneeling High jump Hitting a pinata Hopscotch Javelin throw Raking leaves Smoking a cigarette Playing beach volleyball Shaving Wakeboarding Hula hoop Hurling Ice fishing Making a lemonade Mixing drinks Playing polo Playing saxophone Shot put Using the rowing machine Playing blackjack Welding Peeling potatoes Vacuuming floor Preparing salad Washing face Ironing clothes Kayaking Kite flying Knitting Long jump Longboarding Making a cake Tug of war Making an omelette Mooping floor Mowing the lawn Painting Painting furniture Ping-pong Plastering Plataform diving Playing accordion Playing bagpipes Playing congas Playing guitarra Playing ice hockey Playing rubik cube Polishing forniture Playing kickball Shuffleboard Slacklining Snow tubing Washing dishes Table soccer Tumbling Volleyball Playing racquetball Pole vault Putting in contact lenses Putting on makeup Removing curlers River tubing Rollerblading Roof shingle removal Using the monkey bar Shaving legs Shoveling snow Snowboarding Spinning Spread mulch Surfing Swimming Using parallel bars Using uneven bars Washing hands Waxing skis Wrapping presents Zumba</cell></row><row><cell>5</cell><cell>3</cell><cell>2</cell><cell>1 Using the balance beam</cell><cell>0</cell><cell>Capoeira</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell cols="9">10 Cleaning shoes Assembling bicycle Breakdancing 2 Bathing dog Blowing leaves 5 Baking cookies Building sandcastles Camel ride Carving jack-o-lanterns Changing car wheel Clean and jerk Calf roping 0 Ballet 0 Capoeira Cleaning sink Cleaning windows Clipping cat claws Cumbia Curling Cutting the grass 4 Applying sunscreen Fig. A.9. 15 10 5 0 5 10 Blow-drying hair Braiding hair Brushing hair Brushing teeth Chopping wood Decorating the Christmas tree Disc dog Doing a powerbomb Doing crunches Doing fencing Cheerleading Dodgeball Doing karate Doing nails Doing step aerobics 5 2 Arm wrestling Belly dance Baton twirling Archery 10 4 BMX Bungee jumping Canoeing Cricket Beach soccer Beer pong Bullfighting Croquet Discus throw 15 6 Doing kickboxing Doing motocross Drinking beer Drinking coffee Drum corps Elliptical trainer Fixing bicycle Fixing the roof Fun sliding down Futsal Gargling mouthwash Getting a haircut Getting a piercing Getting a tattoo Grooming dog Grooming horse Hammer throw Hand car wash Hand washing clothes Hanging wallpaper Having an ice cream High jump Hitting a pinata Hopscotch Horseback riding Hula hoop Hurling Ice fishing Installing carpet Ironing clothes Javelin throw Kayaking Kite flying Kneeling Knitting Laying tile Layup drill in basketball Long jump Longboarding Making a cake Making a lemonade Making a sandwich Making an omelette Mixing drinks Mooping floor Mowing the lawn Paintball Painting Painting fence Painting furniture Peeling potatoes Ping-pong Plastering Plataform diving Playing accordion Playing badminton Playing bagpipes Playing beach volleyball Playing blackjack Playing congas Playing drums Playing field hockey Playing flauta Playing guitarra Playing harmonica Playing ice hockey Playing kickball Playing lacrosse Playing piano Playing polo Playing pool Playing racquetball Playing rubik cube Playing saxophone Playing squash Playing ten pins Playing violin Playing water polo Pole vault Polishing forniture Polishing shoes Powerbocking Preparing pasta Preparing salad Putting in contact lenses Putting on makeup Putting on shoes Rafting Raking leaves Removing curlers Removing ice from car Riding bumper cars River tubing Rock climbing Rock-paper-scissors Rollerblading Roof shingle removal Rope skipping Running a marathon Tango Chopping wood Skiing Volleyball Swimming Sailing Scuba diving Sharpening knives Shaving Shaving legs Shot put Shoveling snow Shuffleboard Skateboarding Slacklining Smoking a cigarette Smoking hookah Snatch Snow tubing Snowboarding Spinning Spread mulch Springboard diving Starting a campfire Sumo 6 Surfing Swinging at the playground Table soccer Tai chi Tennis serve with ball bouncing Throwing darts Trimming branches or hedges Triple jump Tug of war Tumbling Welding Cumbia Using parallel bars Using the balance beam Using the monkey bar Using the pommel horse Using the rowing machine Using uneven bars Vacuuming floor Wakeboarding Walking the dog Washing dishes Washing face Washing hands Waterskiing Waxing skis Windsurfing Wrapping presents Zumba 4 2 0 2 Brushing hair Bungee jumping Getting a tattoo Javelin throw Playing blackjack Playing pool Playing squash 4 Applying sunscreen Archery BMX Beer pong Blowing leaves Cleaning windows Clipping cat claws Doing a powerbomb Elliptical trainer Fun sliding down Peeling potatoes Plataform diving Playing field hockey Playing kickball Sailing Playing harmonica Playing ten pins Rock climbing Shot put Doing crunches Hanging wallpaper High jump Hitting a pinata Ice fishing Longboarding Playing bagpipes Playing congas Putting on shoes Arm wrestling Assembling bicycle Baking cookies Ballet Bathing dog Baton twirling Beach soccer Decorating the Christmas tree Discus throw Hula hoop Sharpening knives Scuba diving Sumo Having an ice cream Layup drill in basketball Skateboarding Kite flying Belly dance Blow-drying hair Braiding hair Breakdancing Brushing teeth Building sandcastles Bullfighting Calf roping Camel ride Canoeing Capoeira Carving jack-o-lanterns Changing car wheel Cheerleading Clean and jerk Cleaning shoes Cleaning sink Cricket Croquet Curling Cutting the grass Disc dog Dodgeball Doing fencing Doing karate Doing kickboxing Doing motocross Doing nails Doing step aerobics Drinking beer Drinking coffee Drum corps Fixing bicycle Fixing the roof Futsal Gargling mouthwash Getting a haircut Getting a piercing Grooming dog Grooming horse Hammer throw Hand car wash Hand washing clothes Hopscotch Horseback riding Hurling Installing carpet Ironing clothes Kayaking Kneeling Knitting Laying tile Long jump Making a cake Making a lemonade Making a sandwich Making an omelette Mixing drinks Mooping floor Mowing the lawn Paintball Painting Painting fence Painting furniture Ping-pong Plastering Playing accordion Playing badminton Playing beach volleyball Pole vault Polishing forniture Putting on makeup Raking leaves Shoveling snow Slacklining River tubing Smoking a cigarette Playing violin Removing ice from car Rafting Running a marathon Shaving Smoking hookah Playing rubik cube Playing lacrosse Playing polo Playing racquetball Playing water polo Roof shingle removal Surfing Preparing pasta Removing curlers Shaving legs Snatch Spinning Rollerblading Playing drums Playing flauta Playing guitarra Playing ice hockey Playing piano Playing saxophone Polishing shoes Powerbocking Preparing salad Putting in contact lenses Riding bumper cars Snow tubing Snowboarding Shuffleboard Starting a campfire Rock-paper-scissors Springboard diving Rope skipping Skiing Spread mulch Swimming Swinging at the playground</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>TSQ embeddings of TQM with random initialization after training.Fig. A.13. TSQ embeddings of VQM with appearance initialization before training.Fig. A.14. TSQ embeddings of VQM with common appearance initialization after training.</figDesc><table><row><cell>10 5 0 5 10 15 1 0 1 2 3 4 2</cell><cell>Applying sunscreen Arm wrestling Braiding hair Blow-drying hair Brushing hair Brushing teeth Building sandcastles Beer pong Canoeing Carving jack-o-lanterns Baking cookies Bathing dog Blowing leaves Changing car wheel Chopping wood Cleaning shoes Cleaning sink Bullfighting Archery BMX Bungee jumping Calf roping Assembling bicycle Beach soccer Camel ride Cleaning windows Clipping cat claws Croquet Cutting the grass Decorating the Christmas tree Disc dog Doing crunches Doing motocross Doing nails Drinking beer Drinking coffee Elliptical trainer Drum corps Cricket Fixing bicycle Fixing the roof Fun sliding down Gargling mouthwash Getting a haircut Getting a piercing Getting a tattoo Grooming dog Grooming horse Hand car wash Hand washing clothes Hanging wallpaper Having an ice cream Hitting a pinata Horseback riding Hula hoop Hurling Ice fishing Installing carpet Ironing clothes Kayaking Kite flying Kneeling Clean and jerk Capoeira Discus throw Breakdancing Baton twirling Belly dance Ballet Cheerleading Cumbia Curling Dodgeball Doing a powerbomb Doing fencing Doing karate Doing kickboxing Doing step aerobics Futsal Hammer throw High jump Javelin throw Knitting Laying tile Layup drill in basketball Long jump Longboarding Making a cake Making a lemonade Making a sandwich Making an omelette Mixing drinks Mooping floor Mowing the lawn Paintball Painting Painting fence Painting furniture Peeling potatoes Plastering Plataform diving Hopscotch Ping-pong Playing accordion Playing badminton Playing bagpipes Playing beach volleyball Playing blackjack Playing congas Playing drums Playing field hockey Playing flauta Playing guitarra Playing harmonica Playing ice hockey Playing kickball Playing lacrosse Playing piano Playing polo Playing pool Playing racquetball Playing rubik cube Playing saxophone Playing squash Playing ten pins Playing violin Playing water polo Pole vault Polishing forniture Polishing shoes Powerbocking Preparing pasta Preparing salad Putting in contact lenses Putting on makeup Putting on shoes Rafting Raking leaves Removing curlers Removing ice from car Riding bumper cars River tubing Rock climbing Rock-paper-scissors Rollerblading Roof shingle removal Rope skipping Running a marathon Sailing Scuba diving Sharpening knives Shaving Shaving legs Shot put Shoveling snow Shuffleboard Skateboarding Skiing Slacklining Smoking a cigarette Smoking hookah Snatch Snow tubing Snowboarding Spinning Spread mulch Springboard diving Starting a campfire Sumo Surfing Swimming Swinging at the playground Table soccer Tai chi Tango Tennis serve with ball bouncing Throwing darts Trimming branches or hedges Triple jump Tug of war Tumbling Using parallel bars Using the balance beam Using the monkey bar Using the pommel horse Using the rowing machine Using uneven bars Vacuuming floor Volleyball Wakeboarding Walking the dog Washing dishes Washing face Washing hands Waterskiing Waxing skis Welding Windsurfing Wrapping presents Zumba Brushing hair Layup drill in basketball Preparing salad Applying sunscreen Archery Arm wrestling Assembling bicycle BMX Baking cookies Ballet Bathing dog Baton twirling Beach soccer Breakdancing Changing car wheel Cheerleading Doing a powerbomb Kite flying Paintball Playing violin Ping-pong Surfing Hula hoop Clipping cat claws Cricket Having an ice cream Plastering Snowboarding Sailing High jump Knitting Playing racquetball Brushing teeth Doing kickboxing Playing polo Slacklining Calf roping Carving jack-o-lanterns Cumbia Doing fencing Playing water polo Playing bagpipes Playing saxophone Beer pong Blow-drying hair Blowing leaves Braiding hair Building sandcastles Bullfighting Camel ride Canoeing Capoeira Chopping wood Clean and jerk Cleaning shoes Cleaning windows Croquet Curling Cutting the grass Decorating the Christmas tree Disc dog Discus throw Doing crunches Doing karate Doing motocross Doing step aerobics Drinking beer Drum corps Elliptical trainer Fixing bicycle Fixing the roof Fun sliding down Getting a haircut Getting a piercing Getting a tattoo Grooming dog Grooming horse Hammer throw Hand washing clothes Hanging wallpaper Hitting a pinata Hopscotch Horseback riding Hurling Kayaking Kneeling Long jump Making a cake Making an omelette Mixing drinks Painting Painting fence Peeling potatoes Playing drums Playing field hockey Playing flauta Playing guitarra Playing lacrosse Riding bumper cars Running a marathon Rollerblading River tubing Sharpening knives Playing rubik cube Playing ten pins Shoveling snow Playing piano Rafting Rock climbing Roof shingle removal Shaving legs Springboard diving Shot put Smoking a cigarette Powerbocking Preparing pasta Putting on shoes Removing ice from car Shuffleboard Starting a campfire Pole vault Painting furniture Ice fishing Installing carpet Javelin throw Laying tile Longboarding Making a lemonade Mooping floor Mowing the lawn Plataform diving Playing badminton Playing beach volleyball Playing congas Shaving Removing curlers Snatch Polishing shoes Dodgeball Playing harmonica Playing kickball Spread mulch Doing nails Futsal Gargling mouthwash Hand car wash Ironing clothes Making a sandwich Playing ice hockey Raking leaves Snow tubing Swimming Spinning Rope skipping Playing pool Putting in contact lenses Polishing forniture Skateboarding Belly dance Bungee jumping Drinking coffee Playing blackjack Playing squash Putting on makeup Rock-paper-scissors Scuba diving Skiing Sumo Smoking hookah Swinging at the playground</cell></row><row><cell cols="2">Table soccer Tug of war Vacuuming floor Washing dishes 15 Baton twirling Welding 10 Ballet Cheerleading Discus throw Dodgeball Doing a powerbomb Doing fencing Using the rowing machine Throwing darts Using parallel bars Using the monkey bar Volleyball Wakeboarding Tennis serve with ball bouncing Tai chi Tumbling Using the balance beam Using uneven bars Walking the dog Washing face Washing hands Waxing skis Windsurfing Wrapping presents Zumba 5 0 5 Archery BMX Beach soccer Belly dance Blowing leaves Breakdancing Building sandcastles Bullfighting Bungee jumping Calf roping Camel ride Canoeing Capoeira Chopping wood Clean and jerk Cleaning windows Cricket Croquet Cumbia Curling Cutting the grass Disc dog Doing karate Doing kickboxing Doing motocross Doing step aerobics Drum corps Elliptical trainer Trimming branches or hedges Tango Triple jump Using the pommel horse Waterskiing 15 10 Applying sunscreen Arm wrestling Assembling bicycle Baking cookies Bathing dog Beer pong Blow-drying hair Braiding hair Brushing hair Brushing teeth Carving jack-o-lanterns Changing car wheel Cleaning shoes Cleaning sink Clipping cat claws Decorating the Christmas tree Doing crunches Doing nails Drinking beer Drinking coffee Fixing bicycle Fixing the roof Fun sliding down Futsal Running a marathon Sailing Scuba diving Sharpening knives Shaving Shaving legs Shot put Shoveling snow Shuffleboard Skateboarding Skiing Slacklining Smoking a cigarette Smoking hookah Snatch Snow tubing Snowboarding Spinning Spread mulch Springboard diving Starting a campfire Sumo Surfing Swimming Swinging at the playground Table soccer Tai chi Tango Tennis serve with ball bouncing Throwing darts Trimming branches or hedges Triple jump Tug of war Tumbling Using parallel bars Using the balance beam Using the monkey bar Using the pommel horse Using the rowing machine Using uneven bars Vacuuming floor Volleyball Wakeboarding Walking the dog Washing dishes Washing face Washing hands Waterskiing Waxing skis Welding Windsurfing Wrapping presents Zumba Fig. A.12. 10 15 10 5 0 5 10 5 0 5 10 4 3 2 1 0 1 2 3 4 15 Cleaning sink Playing accordion</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Fig. A.15. TSQ embeddings of VQM with random initialization before training.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Washing face</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Waxing skis</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tennis serve with ball bouncing</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Using parallel bars</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Trimming branches or hedges Using the pommel horse</cell><cell>Tumbling</cell><cell>Tango</cell><cell cols="3">Tai chi Washing dishes Using uneven bars</cell><cell>Wakeboarding Wrapping presents</cell><cell>Zumba</cell><cell>Using the monkey bar</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Windsurfing</cell></row><row><cell></cell><cell></cell><cell>Washing hands</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Throwing darts</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table soccer</cell></row><row><cell></cell><cell></cell><cell cols="2">Tug of war Waterskiing</cell><cell></cell><cell></cell><cell cols="2">Triple jump</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Using the balance beam Walking the dog Welding</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Vacuuming floor</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Using the rowing machine</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Volleyball</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cleaning sink</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Breakdancing Cumbia</cell><cell cols="2">Camel ride</cell><cell>Brushing teeth Calf roping</cell></row><row><cell>0</cell><cell></cell><cell>Building sandcastles Blowing leaves</cell><cell>Doing karate</cell><cell cols="2">Carving jack-o-lanterns Arm wrestling Ballet Bullfighting Doing kickboxing</cell><cell cols="2">Braiding hair Dodgeball Doing crunches Curling</cell><cell>Brushing hair Clipping cat claws Cheerleading Doing a powerbomb</cell></row><row><cell>2</cell><cell>Futsal</cell><cell cols="5">Baking cookies Fixing bicycle Doing fencing Clean and jerk Elliptical trainer Canoeing Fun sliding down Croquet Capoeira Beer pong Drum corps Assembling bicycle Baton twirling Bathing dog Chopping wood Fixing the roof Doing motocross Drinking beer</cell><cell>Cricket</cell><cell>Belly dance BMX Blow-drying hair Disc dog Cleaning shoes Cleaning windows</cell><cell>Beach soccer Applying sunscreen Bungee jumping</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Doing step aerobics</cell><cell></cell><cell>Doing nails</cell></row><row><cell>4</cell><cell></cell><cell cols="2">Decorating the Christmas tree</cell><cell></cell><cell></cell><cell cols="2">Changing car wheel</cell><cell>Archery</cell><cell>Discus throw</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cutting the grass</cell><cell></cell><cell cols="2">Drinking coffee</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the total computation cost of a 188 ? 188 frame processed by MobileNetv2 and a 112 ? 112 frame processed by EfficientNet-B0 equals to the cost of a 224 ? 224 frame processed by MobileNetv2, which is the common setting of previous works<ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b12">13]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Results are obtained on a NVIDIA 3090 GPU with an Intel Xeon E5-2650 v3 @ 2.30GHz CPU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Action keypoint network for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06304</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05895</idno>
		<title level="m">Mobile-former: Bridging mobilenet and transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transvg: End-to-end visual grounding with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1769" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mamico: Macro-to-micro semantic correspondence for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Listen to look: Action recognition by previewing audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10457" to="10467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frameexit: Conditional early exiting for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15608" to="15618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SMART frame selection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16235" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1451" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ascnet: Self-supervised video representation learning with appearance-speed consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8096" to="8105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2670560</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2670560" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="352" to="364" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient action recognition via dynamic knowledge propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13719" to="13728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16478" to="16488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2d or not 2d? adaptive 3d convolution selection for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6155" to="6164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04388</idno>
		<title level="m">Ocsampler: Compressing videos to one clip with single-step sampling</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Query2label: A simple transformer way to multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ar-net: Adaptive frame resolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05775</idno>
		<title level="m">Adafuse: Adaptive temporal fusion network for efficient action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adamml: Adaptive multi-modal learning for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05165</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic network quantization for efficient video inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7375" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Consensus-aware visual-semantic embedding for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Symbiotic attention for egocentric action recognition with object-centric alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adaptive focus for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03245</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11960" to="11973" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adafocus v2: End-to-end training of spatial dynamic networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14238</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weaklysupervised spatio-temporal anomaly detection in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mvfnet: Multi-view fusion network for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2943" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic inference: A new approach toward efficient video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="676" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Transferring textual knowledge for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<idno>ArXiv abs/2207.01297</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Liteeval: A coarse-to-fine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01601</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Nsnet: Non-saliency suppression sampler for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal action proposal generation with background constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3054" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fine-grained video categorization with redundancy reduction attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="136" to="152" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
