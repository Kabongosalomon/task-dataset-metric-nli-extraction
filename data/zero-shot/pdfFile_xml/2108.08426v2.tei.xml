<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Video Representation Learning with Meta-Contrastive Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanze</forename><surname>Lin</surname></persName>
							<email>yuanze@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Guo</surname></persName>
							<email>xunguo@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<email>yanlu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Video Representation Learning with Meta-Contrastive Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning has been successfully applied to pre-train video representations, which aims at efficient adaptation from pre-training domain to downstream tasks. Existing approaches merely leverage contrastive loss to learn instance-level discrimination. However, lack of category information will lead to hard-positive problem that constrains the generalization ability of this kind of methods. We find that the multi-task process of meta learning can provide a solution to this problem. In this paper, we propose a Meta-Contrastive Network (MCN), which combines the contrastive learning and meta learning, to enhance the learning ability of existing self-supervised approaches. Our method contains two training stages based on model-agnostic meta learning (MAML), each of which consists of a contrastive branch and a meta branch. Extensive evaluations demonstrate the effectiveness of our method. For two downstream tasks, i.e., video action recognition and video retrieval, MCN outperforms state-of-theart approaches on UCF101 and HMDB51 datasets. To be more specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8% and 54.5% for video action recognition, as well as 52.5% and 23.7% for video retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have brought unprecedented success for supervised video representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b28">29]</ref> . However, labeling large-scale video data requires huge human annotations, which is expensive and laborious. How to learn effective video representations by leveraging unlabeled videos is an important yet challenging problem. The recent progress of selfsupervised learning for image provides an efficient solution to this problem <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref>, which proposed to use contrastive loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b21">22]</ref> to discriminate different data samples. <ref type="bibr">*</ref> The work was done when the author was with MSRA as an intern. <ref type="figure">Figure 1</ref>. Comparison between models trained without and with MCN on UCF101 <ref type="bibr" target="#b38">[39]</ref>. The top row shows the activation maps produced by conv5 layer of R(2+1)D backbone using the method of <ref type="bibr" target="#b54">[55]</ref>. By using our proposed MCN, the learned representations can capture motion areas more accurately. The bottom row shows top-1 accuracies of models trained without and with MCN approach. This instance-based contrastive learning has also been applied to videos as pre-training, and achieved excellent performance on downstream tasks such as video action recognition and video retrieval <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31]</ref>. However, it has the inherent limit of lacking the common category information. The instance-based discrimination process takes each video sample as an independent class, so that distance between two video samples will be pushed away by contrastive loss even if they belong to the same category. This drawback reduces the generalization of the pre-training parameters. Consequently, the efficiency of the supervised fine-tuning for the downstream tasks will also be damaged. How to improve the generalization of contrastive selfsupervised learning and make the learned parameters easily adapt from pre-training domain to fine-tuning domain for various new tasks is still challenging.</p><p>Meta learning has demonstrated the capability of fast adaptation on new tasks with only a few training samples. The characteristic of meta learning, specifically model-agnostic meta learning (MAML) <ref type="bibr" target="#b9">[10]</ref>, might help contrastive self-supervised video learning in two aspects. Firstly, instance-based discrimination takes each video as a class, so that it is convenient to create numerous sub-tasks for meta learning to improve the model generalization. Secondly, the goal of meta learning is "learn to learn", which means that it provides good initialization for fast adaptation on a new task. This perfectly meets the requirements of contrastive video representation learning, which is taken as a pre-training method. Therefore, combining meta learning and self-supervised learning might benefit to video representation learning.</p><p>In this paper, we propose a novel Meta-Contrastive Network (MCN), which leverages meta-learning to improve the generalization and adaptation ability of contrastive selfsupervised video learning on downstream tasks. The proposed MCN contains two branches, i.e., contrastive branch and meta branch, which establishes a multi-task learning process to enhance the instance discrimination. Meanwhile, we design a two-stage training process based on MAML to improve the learning capability of MCN. Our method outperforms state-of-the-art methods and achieves significant performance boost.</p><p>The main contributions of this paper are summarized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) We propose a novel Meta-Contrastive Network</head><p>(MCN), which can significantly improve the generalization of the video representations learned in selfsupervised learning manner.</p><p>2) We fully investigate the benefits of combining meta learning with self-supervised video representation learning and conduct extensive experiments to make proposed approach better understood.</p><p>3) We evaluate our method on mainstream benchmarks for action recognition and retrieval tasks, which demonstrate that our proposed method can achieve state-of-the-art or comparable performance with other self-supervised learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pretext Tasks</head><p>Early self-supervised learning approaches mainly focus on designing handcrafted pretext tasks for images, such as predicting the rotation of transformed images <ref type="bibr" target="#b11">[12]</ref>, image jigsaw <ref type="bibr" target="#b5">[6]</ref>, count of learned features <ref type="bibr" target="#b34">[35]</ref>, image colorization <ref type="bibr" target="#b55">[56]</ref>, relative positions <ref type="bibr" target="#b5">[6]</ref> and so on.</p><p>After that, many self-supervised learning approaches about video data flourish. Due to the extra temporal dimension of video data, there are many pretext tasks specifically designed for temporal prediction, such as frame rate prediction <ref type="bibr" target="#b46">[47]</ref>, pace prediction <ref type="bibr" target="#b1">[2]</ref>, frame ordering prediction <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9]</ref> and motion statistics prediction <ref type="bibr" target="#b45">[46]</ref>.</p><p>These pretext tasks make models achieve better discriminative ability, which is important for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Self-Supervised Learning</head><p>Contrastive self-supervised learning has been proved great potential in unlabeled data <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref>. Thanks to contrastive self-supervised learning approaches, model can be empowered to distinguish samples from different domains without labels.</p><p>There are some prior works in this area. He et al. <ref type="bibr" target="#b19">[20]</ref> proposed a momentum dictionary to store and pop out learned features on the fly for images, so that the number of stored features can be extremely expanded. Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed a simplified constrastive self-supervised image learning framework including only major components that benefit the learned representations. Tian et al. <ref type="bibr" target="#b42">[43]</ref> presented a contrastive multi-view coding (CMC) approach for video representation learning, which uses different views of input videos to maximize the instance-level distinction. Our approach adopts CMC with two views, i.e., RGB view and residual view, as baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Meta Learning</head><p>Numerous research about meta learning has been presented for few-shot tasks. Finn et al. <ref type="bibr" target="#b9">[10]</ref> proposed an important meta learning method called model-agnostic meta learning (MAML), which can be combined with any learning approaches trained with gradient descent. Some variants of MAML, e.g., Reptile <ref type="bibr" target="#b32">[33]</ref> and iMAML <ref type="bibr" target="#b37">[38]</ref>, can not only significantly save the training time, but also achieve comparable performance with MAML.</p><p>Recently, researchers start to focus on applying meta learning approaches to computer vision tasks, such as object tracking and face recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref>. In this paper, we utilize MAML to improve the performance of contrastive self-supervised video learning. Different from prior efforts, we try to enhance the adaptation between selfsupervised pre-training domain and supervised fine-tuning domain, which is more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Meta-Contrastive Network</head><p>In this section, we describe our proposed metacontrastive network (MCN) in details. In section 3.1, we introduce the framework of MCN and the two-stage training process. In section 3.2, we elaborate the contrastive branch of MCN. In section 3.3, the details of meta branch are clarified. In section 3.4, we introduce the losses and optimizations in MCN. Finally, In section 3.5, implementation details are explained. <ref type="figure">Figure 2</ref>. The illustration of Meta-Contrastive Network. For simplicity, only 3 input videos are used for illustration. There are two stages of MCN, including meta-train and meta-test stages. Model is parameterized by ? initially. ?1 and ?2 represent learning rates. Note that fully connected (fc) layer in meta branch is different from that in feature extraction module. FCM is feature combination module, which generates binary classification features for meta branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>We build our framework by employing the contrastive multi-view coding (CMC) <ref type="bibr" target="#b42">[43]</ref> as baseline. Multi-view input is proved to be efficient for instance-based video contrastive learning since different views, i.e., transformations, from the same video can increase positive samples and make contrastive learning more efficient. We adopt two views in our framework, i.e., RGB view and residual view, which have been proved to be extremely efficient views <ref type="bibr" target="#b40">[41]</ref>. There are two branches in our framework as shown in <ref type="figure">Figure 2</ref>, i.e., contrastive branch and meta branch. The contrastive branch performs contrastive learning, and the meta branch performs a couple of binary classifications for efficient meta learning. A binary classification is very similar with a pretext task that predicts whether the input two features come from the same video sample.</p><p>We employ a two-stage training process including metatrain and meta-test. Training data is split into train set, i.e., support set, and test set, i.e., query set. In meta-train stage, the videos from support set are used for inner update, in which the updated parameters will be used in meta-test stage for feature extraction. In meta-test stage, the videos from query set are used with the inner updated parameters for meta-update, which updates the initial parameters of meta-train stage for the next training iteration of MCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive Branch</head><p>The contrastive branch constructs a feature bank for positive and negative samples by collecting the extracted features for both of the two views, and calculates contrastive loss, i.e., NCE loss <ref type="bibr" target="#b13">[14]</ref>. The RGB view contains the sampled RGB video frames from a video clip, and the residual view contains the differences between two consecutive RGB frames. A residual frame is calculated as:</p><formula xml:id="formula_0">F rame Res n = |F rame RGB n ? F rame RGB n+1 |,<label>(1)</label></formula><p>where F rame Res n represents residual frame; F rame RGB n represents RGB frame; n is the index of the sampled frame. The reason why residual view is efficient may be that it can reflect the motions of the video clip to some extent and provides complementary information to RGB view. For example, when there are two different video clips with the same action, they may have similar residual view. This will implicitly increase the hard positive in contrastive learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Meta Branch</head><p>Constrative learning suffers from hard-positive problem and hard-negative problem, which are even worse in selfsupervised video learning. For example, there may exist videos that contain totally different scenes and objects but the same actions and events. There also exists videos that contain similar scenes and objects but different actions and events. Theoretically, meta learning can alleviate this problem due to the multi-task learning process. For this purpose, we design the meta branch consisting of a feature combination module (FCM) and several binary classification tasks, which can predict whether a feature pair belongs to the same video clip.</p><p>As shown in <ref type="figure">Figure 2</ref>, by concatenating two features of input video samples in FCM, several instance/binary classification tasks can be constructed in meta branch. The corresponding labels can be easily created for training. <ref type="figure" target="#fig_0">Figure 3</ref> shows an example of creating classification task with FCM for two video samples v 1 and v 2 . If one concatenated feature is from the different views of the same video, the label will be true, otherwise the label will be false.</p><p>We design the binary classification lies in two reasons. Firstly, the binary classification loss can be complementary with contrastive loss to better learn the instance discrimination. Secondly, the binary classification enables efficient combination of contrastive learning branch and meta learning branch to improve the generalization through multi-task learning process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Meta Loss and Optimization</head><p>In order to ease the training process of MCN method, we incorporate both metric losses, i.e., contrastive loss and cross-entropy loss, i.e., classification loss, and propose the combined meta loss for final optimization.</p><p>Contrastive Loss. Contrastive learning aims to separate features from different samples. In our method, we employ the contrastive loss form from CMC <ref type="bibr" target="#b42">[43]</ref> as the ob-jective of the contrastive branch. Specifically, two different views of the same sample, e.g., {x 1 i , x 2 i }, are treated as positive, while the views from different samples, e.g., {x 1 i , x 2 j } (i =j), are regarded as negative.</p><p>A value function h ? is used so that positive pairs have high score, and negative pairs obtain low score. To be more concrete, after feature z 1 i is extracted by the model, function h ? is trained on a feature set Z = {z 2 1 , z 2 i , ..., z 2 k+1 }, which consists of one positive sample z 2 i and k negative samples, so that the positive sample can be easily picked out from Z. The contrastive loss can be formulated as:</p><formula xml:id="formula_1">L contrast = ?log h ? ({z 1 i , z 2 i }) k+1 j=1 h ? ({z 1 i , z 2 j }) ,<label>(2)</label></formula><p>where L contrast denotes contrast loss for contrastive branch, and k is the number of negative samples. z 1 i and z 2 i mean extracted features of two different views from ith sample. h ? (?) can be formulated as:</p><formula xml:id="formula_2">h ? ({z 1 i , z 2 j }) = exp( z 1 i ? z 2 j z 1 i ? z 2 j ? ? ),<label>(3)</label></formula><p>where h ? (?) is cosine similarity of two features and ? is the parameter for dynamically controlling the range. Classification Loss. Meta branch of MCN performs instance/binary classification. We use binary cross-entropy loss (BCE) as our classification loss, which can be formulated as:</p><formula xml:id="formula_3">L cls = N i=1 ?y i log? ? (1 ? y i )log(1 ?? i ),<label>(4)</label></formula><p>where N is the number of concatenated features. In our approach, FCN concatenates features from 4 video clips in each batch and each video has two features. y i is the label of ith concatenated feature.? i is the output of the fully connected layer of meta branch. Meta Loss. Contrastive loss and classification loss from the two branches are combined together to get the final meta loss, which is defined as:</p><formula xml:id="formula_4">L meta = ? ? L cls + (1 ? ?) ? L contrast ,<label>(5)</label></formula><p>where ? is a hyper-parameter used to control the relative impact of binary classification loss L cls and contrast loss L contrast respectively. The meta loss is used to update weights in meta-train and meta-test stages.</p><p>Optimization. During meta-train or meta-test stage, meta loss L meta is used to optimize model parameters ? with gradient descent. In meta-train stage, L meta achieved from support set is used to get updated parameters?, which is denoted as inner update. And in meta-test stage, L meta obtained from query set is used to update ?, which is denoted as meta update.</p><p>Step sizes of gradient descent for the two optimization stages are the same as learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We will further explain implementation details of MCN in this section. In specific, our proposed MCN consists of two stages, i.e., meta-train and meta-test stages. The whole process of MCN is explained as follows.</p><p>Initialize</p><formula xml:id="formula_5">. A pre-trained model f (?) parametrized by ?, dataset D, support set D s , query set D q , batch size B, D = D s ? D q .</formula><p>Input. Sample B input videos X sup from support set D s . Sample B input videos X que from query set D q .</p><p>Meta-train.  </p><formula xml:id="formula_6">is? = ? ? ? 1 ? ? L m train meta (f ? ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our approach on three video classification datasets including UCF101 <ref type="bibr" target="#b38">[39]</ref>, HMDB51 <ref type="bibr" target="#b26">[27]</ref> and Kinetics-400 <ref type="bibr" target="#b23">[24]</ref>.</p><p>UCF101 <ref type="bibr" target="#b38">[39]</ref> is a dataset that has 101 action categories, containing 13320 videos totally. There are 3 splits on this data set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">53]</ref>. In our experiments, we use train split 1 as self-supervised pre-training dataset, and train/test split 1 for fine-tuning/evaluation.</p><p>HMDB51 <ref type="bibr" target="#b26">[27]</ref> has around 7000 videos with 51 video action classes, which is relatively small compared to UCF101 and Kinetics <ref type="bibr" target="#b23">[24]</ref>. It also has 3 splits. We use split 1 for fine-tuning and evaluation. <ref type="bibr" target="#b23">[24]</ref> is a popular benchmark for action recognition collected from Youtube, which contains 400 action categories. There are totally 300K video samples, which are divided to 240K, 20K and 40K for training, validation and test sets respectively. In our paper, we only use train split as our pre-training dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics-400</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>Data Pre-processing. We randomly sample 32 continuous frames from each video as the input of MCN. If the original videos are not long enough, the first frame will be repeated. The sampled original frames are treated as RGB view, and the residual frames generated by Equation 1 is treated as residual view. The original frames will be randomly cropped and resized into 128?128. Meanwhile, Gaussian blur, horizontal flips and color jittering are also used for augmentation.</p><p>Backbones. Three main-stream network structures, i.e., S3D <ref type="bibr" target="#b51">[52]</ref>, R3D-18 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44]</ref> and R(2+1)D <ref type="bibr" target="#b43">[44]</ref> are used as the backbones of MCN in ablation experiments. For video action recognition and video retrieval tasks, only the results of R3D-18 and R(2+1)D are reported.</p><p>Self-Supervised Learning. We train our models using 4 NVIDIA Tesla P40 around 500 epochs. Initial learning rate is 0.01 and weight decay is 0.001. ? is set to 0.2. We use the batch sizes of 28 and 80 for R(2+1)D and R3D-18 respectively.</p><p>Fine-tuning. After finishing self-supervised learning stage, we fine-tune the pre-trained models on UCF101 or HMDB51 around 300 epochs. A new fully connected layer will be added to the end of the pre-trained backbone for classification. Learning rate is set as 0.02. And the batch sizes are 72 and 200 for R(2+1)D and R3D-18 respectively.</p><p>Evaluations. Evaluations of proposed method are conducted on video action recognition and video retrieval tasks. For video action recognition, the top-1 accuracies on UCF101 <ref type="bibr" target="#b38">[39]</ref> and HMDB51 <ref type="bibr" target="#b26">[27]</ref> are reported. In order to further validate our proposed MCN, we also show the results of linear probe results, in which the weights of self-supervised learning model are fixed and only the fully-connected layers for supervised classification are finetuned. For video retrieval, top-1, top-5, top-10, top-20 and top-50 accuracies are compared with existing approachs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To fully investigate and understand the concept of MCN, we conduct ablation experiments to demonstrate how each design of MCN affects the overall performance.</p><p>Comparison with Baseline. We compare the action recognition results of self-supervised training with and without MCN in <ref type="table" target="#tab_1">Table 1</ref>. We use three backbones, i.e., S3D, R(2+1)D and R3D-18, to demonstrate the performance boost of MCN. As shown in this table, the accura-cies of baseline with S3D, R(2+1)D and R3D-18 are 76.7%, 77.3% and 78.6% on UCF101 dataset respectively. while the accuracies of MCN are 82.9%, 84.8% and 85.4% respectively. Relevant results of HMDB51 dataset can also be observed. There is consistent performance boost when using MCN on different backbones and data sets.</p><p>Furthermore, we also evaluate the linear probe results in <ref type="table">Table 2</ref>, in which only the fully connected layers are finetuned. Significant performance boost of MCN can also be observed on both UCF101 and HMDB51 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Backbone UCF101(%) HMDB51(%)  <ref type="table">Table 2</ref>. Linear probe evaluation results of different backbones on video action recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of ?. As depicted in Equation 5</head><p>, ? is introduced to modulate meta loss. We also conducted experiments to demonstrate the influence of this hyper-parameter. <ref type="table">Table 3</ref> shows the results of 4 settings of ? with R(2+1)D backbone.</p><p>Settings UCF101(%) ? = 0.1 84.1 ? = 0.2 84.8 ? = 0.3 83.4 ? = 0.4 82.7 <ref type="table">Table 3</ref>. Results of different ? settings on UCF101 dataset.</p><p>We can observe that setting ? as 0.2 shows the best performance. Therefore, we set ? as 0.2 in all our experiments.</p><p>Influence of Input Frames. For self-supervised video representation learning, the number of input frames for each video clip may affect the final performance. Therefore, we tested different numbers for quantitative analysis. We firstly pre-train models on UCF101 dataset, then fine-tune the models for video action recognition task.</p><p>In <ref type="table">Table 4</ref>, we can see that more input frames bring better performance. As the input length increases, MCN takes additional improvement. Influence of Individual Component. We also test each component of MCN to figure out their contributions to the final performance. The results of video action recognition on UCF101 are demonstrated in <ref type="table">Table 5</ref>. R(2+1)D is selected as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Input Frames UCF101(%)</head><p>As shown in <ref type="table">Table 5</ref>. CL represents contrastive loss. BL represents binary loss from proposed meta branch. Combining CL and BL without meta stages takes 1.9% accuracy improvement. By adding meta stages, additional 5.6% improvement is achieved, which proves the efficiency of meta learning. These experiments can demonstrate the effectiveness of proposed MCN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CL BL Meta Stages UCF101(%)</head><p>77.3 79.2 84.8 <ref type="table">Table 5</ref>. Ablation study for different components of MCN on video recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of MCN</head><p>In this section, we compare the performance of our proposed method with other state-of-the-art approaches. We show the evaluations on two downstream tasks including video action recognition and video retrieval. R3D-18 and R(2+1)D are used as backbones for the comparisons.</p><p>Video Action Recognition. Considering that we only use RGB information in our experiments, we didn't include the approaches with multi-modality <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31]</ref>. Co-CLR also <ref type="bibr" target="#b17">[18]</ref> demonstrates excellent performance by cotraining RGB and optical flow samples. In this paper, we only include the RGB-only results of CoCLR for fair comparison.</p><p>We first compare our linear probe evaluation results with other state-of-the-art approaches so that we can verify the transferability of the video representations learned with our approach. Results in <ref type="table" target="#tab_4">Table 6</ref> demonstrate that the proposed MCN method outperforms state-of-the-art approaches on both UCF101 and HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF101(%) HMDB51(%)</head><p>CBT <ref type="bibr" target="#b39">[40]</ref> 54.0 29.5 MemDPC <ref type="bibr" target="#b16">[17]</ref> 54.1 30.5 CoCLR <ref type="bibr" target="#b17">[18]</ref> 70.  We then compare the results of fine-tuning all parameters with other state-of-the-art methods with different pretraining datasets. In specific, we pre-train our models on both UCF101 and Kinetics-400, and then fine-tune the pretrained models on UCF101 and HMDB51. <ref type="table">Table 7</ref> and <ref type="table">Table 8</ref> show the results respectively. From the tables, we can observe that the results pre-trained on Kinetics-400 are much better than that pre-trained on UCF101. Kinetics contains much more videos than UCF101. The results demonstrates that MCN can better leverage large volume of unlabeled videos. In both tables, our method outperforms or achieves comparable performance with other state-of-theart self-supervised approaches. In <ref type="table">Table 8</ref>, CVRL shows better result than ours. This may be due to three reasons: (1) larger input image resolution (224?224) compared with ours (128?128); (2) more powerful and deeper backbone network (R3D-50) than ours (R(2+1)D and R3D-18); (3) more efficient data augmentation approaches. These experimental results can shed a light for combining meta learning with self-supervised learning approaches. Video Retrieval. In addition to video action recognition task, we also evaluate the performance of MCN on video retrieval task, which can better reflect the semanticlevel learning capability. Instead of using RGB and residual views, RGB and flow views of original video clips are considered for video retrieval, in which selected flow view is the vertical dimension of optical flow. We extract optical flow of input videos by using un-supervised TV-L1 algorithm <ref type="bibr" target="#b10">[11]</ref>. Video retrieval task is conducted with extracted features from pre-trained models without extra fine-tuning stages. We take every video from test set to query k nearest videos from the training set based on its extracted features. When the class of retrieval video is the same as that of the qeury video, this retrieval result is considered correct. The top-1, top-5, top-10, top-20, and top-50 retrieval accuracies have been shown in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>As shown in <ref type="table">Table 9</ref>, when compared with other state-ofart methods, our method achieves superior or comparable performance in UCF101 dataset. We observe that the top-1 accuracy of CoCLR is slightly better than our R(2+1)D backbone. Actually, our method is orthogonal to CoCLR. In other words, MCN can take the model trained by Co-CLR as baseline to take additional improvement. The results of HMDB51 dataset have been shown in <ref type="table" target="#tab_1">Table 10</ref>, which demonstrate the superior performance of our proposed MCN. <ref type="figure">Figure 5</ref>. Activation maps produced from conv5 layer of R(2+1)D backbone. The maps are generated with 32-frames input and the method in <ref type="bibr" target="#b54">[55]</ref> is used. 1st, 4th, 8th and 12th frames of the video clip are illustrated. The three rows represent original video clip, activation maps produced by models trained without and with MCN respectively. In <ref type="figure" target="#fig_2">Figure 4</ref>, retrieval results of models with and without MCN are visualized. R(2+1)D is used as backbone. Video clips from UCF101 test set are used to query 3 nearest videos from UCF101 training set. We can clearly observe that the learned representations with MCN can query videos with the same classes more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>In this section, we visualize activation maps of MCN in <ref type="figure">Figure 5</ref>, so that we can intuitively understand what has been improved during self-supervised learning process. We use the method in <ref type="bibr" target="#b54">[55]</ref> to visualize activation maps from conv5 layer of pre-trained R(2+1)D backbone.</p><p>It is interesting to observe that, the model trained without MCN may focus on the irrelevant areas, while MCN can accurately pay attention to the motion areas of video clips. This is essential for action recognition. For example, in the first row of <ref type="figure">Figure 5</ref>, we can clearly see that a person is doing a clean and jerk. The learned representations by MCN can focus more on his action areas, such as hands and shoulders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel Meta-Contrastive Network (MCN), which leverages meta-learning to improve the generalization and adaptation ability of contrastive selfsupervised video learning on downstream tasks. The proposed MCN contains two branches, i.e., contrastive branch and meta branch, which combine NCE loss and binary classification loss together to enhance the instance discrimination. Meanwhile, we design a two-stage training process based on MAML to improve the learning capability of MCN. Our method outperforms state-of-the-art methods and achieves significant performance boost. To our best knowledge, this is the first time that contrastive selfsupervised video learning is combined with meta learning. We also hope our work can inspire more researchers who have interests on this field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>An example of FCM. v1 and v2 are two video samples. (a) is feature space of the two videos. RGB and Res mean features extracted from RGB view and Residual view respectively. (b) is positive feature set, whose label is true. (c) is negative feature set, whose label is false.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Feed sampled videos X sup into network f (?) to extract features. Use these features to compute contrastive loss L m train contrast by Equation 2. Use FCM to combine these features and get a new feature set S m train . Set S m train is used to compute classification loss L m train cls by Equation 4, then L m train cls and L m train contrast are used to compute L m train meta by Equation 5. Inner Update. Use L m train meta to update model parameters by gradient descent. The updated process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Video retrieval examples on UCF101. The first column represents query videos from the test split, and the remaining columns are top-3 results retrieved by the models trained without and with MCN from the training split. The class name of each video is shown in bottom. Red fonts denote wrong video retrieval results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where ? 1 is the same as learning rate. Meta-test. Feed sampled videos X que into network f (?) to extract features. Use these features to compute contrastive loss L m test contrast by Equation 2. Use FCM to combine these features and get a new feature set S m test . Set S m test is used to compute classification loss L m test</figDesc><table><row><cell>4, then L m test cls</cell><cell>by Equation contrast are used to compute L m test cls and L m test meta</cell></row><row><cell>by Equation 5.</cell><cell></cell></row><row><cell cols="2">Meta Update. Use L m test meta to update the final model pa-</cell></row><row><cell cols="2">rameters by gradient descent. Corresponding updated pro-</cell></row><row><cell cols="2">cess is ? = ? ? ? 2 ?</cell></row></table><note>? L m test meta (f?), where ? 2 is the same as learning rate.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons between MCN and baseline with different backbones on video action recognition task.</figDesc><table><row><cell>Ours (Baseline)</cell><cell>S3D</cell><cell>76.7</cell><cell>45.5</cell></row><row><cell>Ours (+ MCN)</cell><cell>S3D</cell><cell>82.9</cell><cell>53.8</cell></row><row><cell>Ours (Baseline)</cell><cell>R(2+1)D</cell><cell>77.3</cell><cell>46.2</cell></row><row><cell>Ours (+ MCN)</cell><cell>R(2+1)D</cell><cell>84.8</cell><cell>54.5</cell></row><row><cell>Ours (Baseline)</cell><cell>R3D-18</cell><cell>78.6</cell><cell>47.1</cell></row><row><cell>Ours (+ MCN)</cell><cell>R3D-18</cell><cell>85.4</cell><cell>54.8</cell></row><row><cell>Methods</cell><cell cols="3">Backbone UCF101(%) HMDB51(%)</cell></row><row><cell>Ours (Baseline)</cell><cell>S3D</cell><cell>62.4</cell><cell>33.5</cell></row><row><cell>Ours (+MCN)</cell><cell>S3D</cell><cell>71.6</cell><cell>40.8</cell></row><row><cell>Ours (Baseline)</cell><cell>R(2+1)D</cell><cell>64.2</cell><cell>35.6</cell></row><row><cell>Ours (+MCN)</cell><cell>R(2+1)D</cell><cell>72.4</cell><cell>41.2</cell></row><row><cell>Ours (Baseline)</cell><cell>R3D-18</cell><cell>64.6</cell><cell>37.3</cell></row><row><cell>Ours (+MCN)</cell><cell>R3D-18</cell><cell>73.1</cell><cell>42.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Linear probe comparisons with state-of-the-art methods on UCF101 and HMDB51 datasets.</figDesc><table><row><cell>Methods</cell><cell cols="4">Backbone Resolution UCF101 HMDB51</cell></row><row><cell>Jigsaw[34]</cell><cell>UCF101</cell><cell>225</cell><cell>51.5</cell><cell>22.5</cell></row><row><cell>OPN [28]</cell><cell>VGG</cell><cell>227</cell><cell>56.3</cell><cell>22.1</cell></row><row><cell>Mars [46]</cell><cell>C3D</cell><cell>112</cell><cell>58.8</cell><cell>32.6</cell></row><row><cell>CMC [43]</cell><cell>CaffeNet</cell><cell>128</cell><cell>59.1</cell><cell>26.7</cell></row><row><cell>ST-puzzle [25]</cell><cell>R3D</cell><cell>224</cell><cell>65.0</cell><cell>31.3</cell></row><row><cell>VCP [30]</cell><cell>R(2+1)D</cell><cell>112</cell><cell>66.3</cell><cell>32.2</cell></row><row><cell>VCOP [53]</cell><cell>R(2+1)D</cell><cell>112</cell><cell>72.4</cell><cell>30.9</cell></row><row><cell>PRP [54]</cell><cell>R(2+1)D</cell><cell>112</cell><cell>72.1</cell><cell>35.0</cell></row><row><cell>IIC [42]</cell><cell>R3D</cell><cell>112</cell><cell>74.4</cell><cell>38.3</cell></row><row><cell>PP [47]</cell><cell>R(2+1)D</cell><cell>112</cell><cell>75.9</cell><cell>35.9</cell></row><row><cell>CoCLR [18]</cell><cell>S3D</cell><cell>128</cell><cell>81.4</cell><cell>52.1</cell></row><row><cell>Ours</cell><cell>R(2+1)D</cell><cell>128</cell><cell>84.8</cell><cell>54.5</cell></row><row><cell>Ours</cell><cell>R3D</cell><cell>128</cell><cell>85.4</cell><cell>54.8</cell></row><row><cell cols="5">Table 7. Comparisons with state-of-the-art methods for video ac-</cell></row><row><cell cols="5">tion recognition on UCF101 and HMDB51 datasets (models are</cell></row><row><cell cols="2">pre-trained on UCF101).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Backbone Resolution UCF101 HMDB51</cell></row><row><cell>3D-RotNet [23]</cell><cell>R3D</cell><cell>112</cell><cell>62.9</cell><cell>33.7</cell></row><row><cell>ST-Puzzle[25]</cell><cell>R3D</cell><cell>224</cell><cell>63.9</cell><cell>33.7</cell></row><row><cell>DPC [16]</cell><cell>R2D-3D</cell><cell>128</cell><cell>75.7</cell><cell>35.7</cell></row><row><cell>SpeedNet [2]</cell><cell>S3D-G</cell><cell>224</cell><cell>81.1</cell><cell>48.8</cell></row><row><cell>PP [47]</cell><cell>R(2+1)D</cell><cell>112</cell><cell>75.9</cell><cell>35.9</cell></row><row><cell>CoCLR [18]</cell><cell>S3D</cell><cell>128</cell><cell>87.9</cell><cell>54.6</cell></row><row><cell>CVRL [37]</cell><cell>R3D</cell><cell>224</cell><cell>92.1</cell><cell>65.4</cell></row><row><cell>Ours</cell><cell>R(2+1)D</cell><cell>128</cell><cell>89.2</cell><cell>58.8</cell></row><row><cell>Ours</cell><cell>R3D</cell><cell>128</cell><cell>89.7</cell><cell>59.3</cell></row><row><cell cols="5">Table 8. Comparisons with state-of-the-art methods for video ac-</cell></row><row><cell cols="5">tion recognition on UCF101 and HMDB51 datasets (models are</cell></row><row><cell cols="2">pre-trained on Kinetics-400).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 10 .</head><label>10</label><figDesc>Comparisons with state-of-the-art approaches for video retrieval on HMDB51 dataset.</figDesc><table><row><cell></cell><cell cols="5">top1 top5 top10 top20 top50</cell></row><row><cell>Jigsaw [34]</cell><cell cols="2">19.7 28.5</cell><cell>33.5</cell><cell>40.0</cell><cell>49.4</cell></row><row><cell>OPN[28]</cell><cell cols="2">19.9 28.7</cell><cell>34.0</cell><cell>40.6</cell><cell>51.6</cell></row><row><cell>B?chler [3]</cell><cell cols="2">25.7 36.2</cell><cell>42.2</cell><cell>49.2</cell><cell>59.5</cell></row><row><cell>VCOP [53]</cell><cell cols="2">10.7 25.9</cell><cell>35.4</cell><cell>47.3</cell><cell>63.9</cell></row><row><cell>VCP [30]</cell><cell cols="2">19.9 33.7</cell><cell>42.0</cell><cell>50.5</cell><cell>64.4</cell></row><row><cell>CMC [43]</cell><cell cols="2">26.4 37.7</cell><cell>45.1</cell><cell>53.2</cell><cell>66.3</cell></row><row><cell>PP [47]</cell><cell cols="2">31.9 49.7</cell><cell>59.2</cell><cell>68.9</cell><cell>80.2</cell></row><row><cell>IIC [42]</cell><cell cols="2">42.4 60.9</cell><cell>69.2</cell><cell>77.1</cell><cell>86.5</cell></row><row><cell>CoCLR [18]</cell><cell cols="2">53.3 69.4</cell><cell>76.6</cell><cell>82.0</cell><cell>-</cell></row><row><cell cols="3">Ours (R(2+1)D) 52.5 69.5</cell><cell>77.9</cell><cell>83.1</cell><cell>89.3</cell></row><row><cell>Ours (R3D)</cell><cell cols="2">53.8 70.2</cell><cell>78.3</cell><cell>83.4</cell><cell>89.7</cell></row><row><cell cols="6">Table 9. Comparisons with state-of-the-art approaches for video</cell></row><row><cell cols="2">retrieval on UCF101 dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="5">top1 top5 top10 top20 top50</cell></row><row><cell>VCOP [53]</cell><cell>7.6</cell><cell>22.9</cell><cell>34.4</cell><cell>48.8</cell><cell>68.9</cell></row><row><cell>VCP [30]</cell><cell>7.6</cell><cell>24.4</cell><cell>36.3</cell><cell>53.6</cell><cell>76.4</cell></row><row><cell>CMC [43]</cell><cell cols="2">10.2 25.3</cell><cell>36.6</cell><cell>51.6</cell><cell>74.3</cell></row><row><cell>PP [47]</cell><cell cols="2">12.5 32.2</cell><cell>45.4</cell><cell>61.0</cell><cell>80.7</cell></row><row><cell>IIC [42]</cell><cell cols="2">19.7 42.9</cell><cell>57.1</cell><cell>70.6</cell><cell>85.9</cell></row><row><cell>CoCLR [18]</cell><cell cols="2">23.2 43.2</cell><cell>53.5</cell><cell>65.5</cell><cell>-</cell></row><row><cell cols="3">Ours (R(2+1)D) 23.7 46.5</cell><cell>58.9</cell><cell>72.4</cell><cell>87.3</cell></row><row><cell>Ours (R3D)</cell><cell cols="2">24.1 46.8</cell><cell>59.7</cell><cell>74.2</cell><cell>87.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12667</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="770" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning meta face recognition in unseen domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6163" to="6172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Memoryaugmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01065</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning via video rotation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00230</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video cloze procedure for self-supervised spatio-temporal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11701" to="11708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5898" to="5906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multimodal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03800</idno>
		<title level="m">Spatiotemporal contrastive video representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04630</idno>
		<title level="m">Meta-learning with implicit gradients</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Rethinking motion representation: Residual frames with 3d convnets for better action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05661,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning using inter-intra contrastive framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tracking by instance detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6288" to="6297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning by pace prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9925" to="9934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video playback rate perception for self-supervised spatio-temporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6548" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
