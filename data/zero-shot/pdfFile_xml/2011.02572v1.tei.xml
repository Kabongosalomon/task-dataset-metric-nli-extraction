<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Multi-layer Feature Aggregation for Deep Scene Parsing Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Yongsheng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jun</forename><forename type="middle">Zhou</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Qiang</forename><forename type="middle">Wu</forename></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Multi-layer Feature Aggregation for Deep Scene Parsing Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Scene parsing</term>
					<term>Feature aggregation</term>
					<term>Spatial- semantic consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene parsing from images is a fundamental yet challenging problem in visual content understanding. In this dense prediction task, the parsing model assigns every pixel to a categorical label, which requires the contextual information of adjacent image patches. So the challenge for this learning task is to simultaneously describe the geometric and semantic properties of objects or a scene. In this paper, we explore the effective use of multi-layer feature outputs of the deep parsing networks for spatial-semantic consistency by designing a novel feature aggregation module to generate the appropriate global representation prior, to improve the discriminative power of features. The proposed module can auto-select the intermediate visual features to correlate the spatial and semantic information. At the same time, the multiple skip connections form a strong supervision, making the deep parsing network easy to train. Extensive experiments on four public scene parsing datasets prove that the deep parsing network equipped with the proposed feature aggregation module can achieve very promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Scene parsing from images is a fundamental yet challenging problem in visual content understanding. In this dense prediction task, the parsing model assigns every pixel to a categorical label, which requires the contextual information of adjacent image patches. So the challenge for this learning task is to simultaneously describe the geometric and semantic properties of objects or a scene. In this paper, we explore the effective use of multi-layer feature outputs of the deep parsing networks for spatial-semantic consistency by designing a novel feature aggregation module to generate the appropriate global representation prior, to improve the discriminative power of features. The proposed module can auto-select the intermediate visual features to correlate the spatial and semantic information. At the same time, the multiple skip connections form a strong supervision, making the deep parsing network easy to train. Extensive experiments on four public scene parsing datasets prove that the deep parsing network equipped with the proposed feature aggregation module can achieve very promising results.</p><p>Index Terms-Scene parsing; Feature aggregation; Spatialsemantic consistency</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene parsing based on semantic segmentation is a dense classification task for visual content analysis in image processing. The goal is to assign a class label to every single pixel in given images, i.e., parse a scene into different geometric regions associated with semantic categories such as sky, road and bicycles. This topic has drawn a broad research interest for many applications such as surveillance for security <ref type="bibr" target="#b0">[1]</ref>, robot sensing <ref type="bibr" target="#b1">[2]</ref> and auto-navigation <ref type="bibr" target="#b2">[3]</ref>.</p><p>The difficulty of unconstrained semantic segmentation mainly lies in the high varieties of scenes and their associated labels. Some categories are semantically confusing due to spatial-semantic inconsistencies. For example, regions of "pedestrians" and "riders" are often indistinguishable, and "cars" are usually affected by visual scales, occlusions and illuminations. Therefore, the spatial and semantic information shall be consistent to address this challenge. Furthermore, accurate label prediction at the pixel level requires high resolution of visual feature representations. <ref type="bibr">Litao</ref>  For example, in the challenging Cityscapes dataset <ref type="bibr" target="#b3">[4]</ref>, it is comparably easy to segment some large objects such as "road" and "building", but very difficult to localize and sketch the contours of small objects such as "poles" and "traffic signs". Recently, the development of deep convolutional neural networks has led to remarkable progress in scene parsing due to their powerful feature representation ability to describe the local visual properties. Deep parsing networks are often fine-tuned based on the pre-trained classification networks, e.g., deep residual networks <ref type="bibr" target="#b4">[5]</ref>. These classification networks usually stack convolution and down-sampling layers to obtain visual feature maps with rich semantics. The deeper layer features with rich semantics are crucial for accurate classification, but lead to the reduced resolution and in turn spatial information loss. Such information loss is detrimental for scene parsing because it decreases the localization accuracy. On the other hand, the spatialsensitive feature maps in the shallow layers are rarely optimized due to the vanishing gradient. Thus, how to keep the spatial-semantic consistencies becomes an open problem in scene parsing. To address this issue, several modifications to Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b5">[6]</ref> have been made. For example, in the encoder-decoder structure such as UNet <ref type="bibr" target="#b6">[7]</ref>, the encoder maps the original images into low-resolution feature representations, while the decoder mainly restores the spatial information with skip-connections. Unfortunately, the missing geometric information cannot be fully restored. Another popular method that has been widely used in segmentation is the dilated (atrous) convolution <ref type="bibr" target="#b7">[8]</ref>, which can enlarge the receptive field in the feature maps without adding more computation overhead, thus more visual details are preserved. Combining the encoder-decoder structure and dilated convolution can effectively boost the pixel-wise prediction accuracy <ref type="bibr" target="#b8">[9]</ref>, but is extremely computational demanding.</p><p>In deep parsing networks, the shallow convolution features, i.e., the early convolution outputs, encode the lowlevel spatial visual information such as edges, corners and circles. The high-level features in the deep blocks, on the other hand, carry more semantic information, including instance and category-level evidences, but lack geometric information. The outputs in the middle convolution blocks carry miscellaneous spatial and semantic properties of images, forming the mid-level feature maps. In the optimization procedure, the gradients can be easily passed through the deeper convolution blocks, but the weights in  <ref type="bibr" target="#b10">[11]</ref>. The "learning block" refers to simple or composite convolutional structures (e.g., residual block <ref type="bibr" target="#b4">[5]</ref>). The "aggregation" here means either the residual-add or simple concatenation on the channel axis.</p><p>the shallower convolution blocks are rarely updated, making a very slow convergence. At the same time, the different layer outputs of feature representations are complementary to each other, which should be re-considered for any dense prediction tasks. However, the effective aggregation of multiple feature outputs is rarely recognized as a critical step in model optimization.</p><p>In this paper, we propose a novel feature aggregation module applied on multiple layer outputs of deep parsing networks to effectively capture the long-range contextual information. In the proposed method, the module can auto-select the most useful feature maps to form a discriminative visual feature representation for dense prediction. Thus, with the neatly designed feature aggregation module, the proposed parsing network, named Spatialsemantic Aggregation Network (SANet), can take the longcontextual information of deep parsing networks into consideration, which has the following two advantages: (1) it uses multiple feature maps to form a strong supervision, in which the spatial-semantic properties are automatically correlated; and (2) the gradients are directly passed into multiple layers, making the parsing network easy to train. In the experiment, the proposed SANet achieves very promising performance on four widely used benchmark datasets, i.e., NYU Depth v2 <ref type="bibr" target="#b11">[12]</ref>, SUN RGB-D <ref type="bibr" target="#b12">[13]</ref>, ADE20K <ref type="bibr" target="#b13">[14]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref>. The spatial-semantic feature aggregation module with the long-contextual information paradigm can be extended to other image processing models to benefit the spatial-aware learning tasks.</p><p>The rest of the paper is organized as follows. Section II introduces related work. Section III elaborates the proposed feature aggregation module as well as the learning framework of SANet for scene parsing. Experimental results and analysis are presented in Section IV. Finally, Section V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Deep learning based parsing networks</head><p>Deep parsing models based on fully-convolutional networks <ref type="bibr" target="#b5">[6]</ref> have achieved significant outcomes on large-scale benchmark datasets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Following the first deep parsing model FCN <ref type="bibr" target="#b5">[6]</ref>, DeconvNet <ref type="bibr" target="#b14">[15]</ref>, SegNet <ref type="bibr" target="#b15">[16]</ref>, UNet <ref type="bibr" target="#b6">[7]</ref> and their variants <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> adopt the encoderdecoder framework, where skip-connections are frequently used to refine segmentation masks. The aim to use skipconnections is to recover the geometric information that is represented in the early convolution blocks, but such aggregations fail to effectively select the most appropriate feature components because they are mixed with higher semantic information. Moreover, it is not clear that how much information can help to preserve the geometric information and aggregate the semantics in the 2D feature maps. For scene parsing tasks, exploring the contextual information is beneficial for semantic understanding, which requires large receptive fields with adjacent pattern information in deep models. Based on this consideration, some methods use dilated convolutions to replace traditional convolution layers to enlarge the receptive field <ref type="bibr" target="#b7">[8]</ref>, or some others adopt graphical models with effective inference to analyze the surrounding visual information <ref type="bibr" target="#b19">[20]</ref>. The multi-scale processing technique is commonly used in many learning tasks such as visual localization and detection. Specific to semantic segmentation, employing multi-scale inputs <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> or applying multi-scale aggregation <ref type="bibr" target="#b22">[23]</ref> are effective ways to improve the model accuracy. In <ref type="bibr" target="#b23">[24]</ref>, the authors proposed to use neural architecture search to discover the deep model structures for semantic segmentation, which is no longer dependent on the pre-trained deep models and can obtain satisfactory results. However, its generalization capability remains an issue because the architecture is searched on a specific validation dataset rather than a universal one, so it is hard to adapt to new data environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effective utilization of multiple layer outputs</head><p>Deep neural networks have been extensively used for various image processing tasks. Specifically, the pre-trained deep networks on large-scale datasets serve as stem networks for new learning tasks. The stem networks usually contain multiple learning blocks such as residual blocks in deep residual networks <ref type="bibr" target="#b4">[5]</ref>. The densely connected network <ref type="bibr" target="#b9">[10]</ref> is a canonical architecture for semantic fusion, which is designed to better propagate features and losses through skip-connections that concatenate all the feature maps in a learning block. For spatial-aware learning tasks such as object detection, feature fusion <ref type="bibr" target="#b24">[25]</ref> is designed to equalize and standardize semantics across the levels of a pyramid feature hierarchy through top-down and lateral connections.</p><p>With the powerful GPUs, the number of layers in the stem networks can be easily extended to several hundreds or even more than one thousand, but the resulting performance does not increase linearly. In a deep convolution network, the stacked learning blocks are divided into stages according to the resolution and representation capacity of feature maps. The feature map outputs in deeper stages contain more semantic information but are spatially coarser than shallow layers. Hence, further exploration is needed on how to connect these layers or learning blocks. In <ref type="figure" target="#fig_0">Figure  1</ref>, we illustrate four feature inference schemes. The basic architecture (a) without layer aggregation or branching structure is widely used in supervised learning tasks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. To address the spatial-aware learning task, deep semantic segmentation models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref> adopt the skip-connections (b) across multiple learning stages, fusing the feature maps to restore the spatial information. The sequential inference with auxiliary loss (c) was firstly used in <ref type="bibr" target="#b28">[29]</ref> for very deep neural networks to enhance the supervision, and was also used in PSPNet <ref type="bibr" target="#b22">[23]</ref> for semantic segmentation. In <ref type="bibr" target="#b10">[11]</ref>, the authors designed a multistage aggregation without losing any intermediate feature representations to improve the model performance, as is illustrated in (d). However, the design of the aggregation relies on experience, and the basic sequential structure of the stem network has to be modified, which means the model needs to be trained from the very beginning. In our proposed aggregation structure, we design a spatialsemantic aggregation module to effectively aggregate the feature maps without changing the stem network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we start with the observation and analysis of skip-connections and auxiliary losses when applying FCN for scene parsing, which motivate the design of the multi-layer feature aggregation module, then we give the details of the whole learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-layer feature aggregation</head><p>To effectively correlate the spatial and semantic information, the feature aggregation should reserve the discriminative feature components and abandon the useless ones. Specific to a FCN, the early convolution outputs have dual functionalities: describing the geometric properties and acquiring new knowledge in deeper stages. The features of the middle and late layers are less spatial-aware but semantic indicative. To effectively use multiple feature maps from different layers in a FCN, skip-connections and auxiliary losses are commonly used. Skip-connections are usually used in deep learning-based scene parsing models such as DeepLab <ref type="bibr" target="#b8">[9]</ref> to correlate the spatial information in the early convolution layers and the semantics in the late feature outputs. Auxiliary losses, on the other hand, can help improve the discriminative power by adding extra output branches in intermediate layers and providing stronger supervision.</p><p>To observe the effectiveness of skip-connections and auxiliary losses in FCN, we conducted a simple indoor scene parsing experiment on two small datasets NYU Depth v2 <ref type="bibr" target="#b11">[12]</ref> (40 classes) and SUN RGB-D [13] (37 classes).</p><p>We used the ResNeXt50 model <ref type="bibr" target="#b29">[30]</ref> as the stem network, replacing the global average pooling layer and classification layer with two convolution layers to form a pixel-wise classifier for dense prediction.</p><p>We selected the following layer outputs to skip-connect the last feature map before the final convolution (pixel classifier): (a) the ReLU layer after the first convolution (s0); (b) the final ReLU layers at each of the four stages before resolution changes (s1, s2, s3 and s4). Similarly, we added dense layers on these layer outputs as auxiliary losses to observe their effectiveness. In the training procedure, we used the IoU (intersection over union) to evaluate the performance on the validation (test) set. The results of different skip-connections and auxiliary losses are summarised in <ref type="table" target="#tab_1">Table I</ref>. Inspecting the validation results in the table above, we can observe that some skip-connections or auxiliary losses can improve the classification accuracy, while some others cannot. Furthermore, applying the two approaches to the early convolution output (s0 in the experiment) essentially decreases the IoU. For a given stem network with very deep structures and a large-scale dataset, it is hard to select the proper intermediate layers to form the most appropriate feature representation. Based on such observations, we design a multi-layer feature selection method to fully utilize multiple feature maps thus improve the scene parsing performance.</p><p>Instead of concatenating multiple-layer outputs as is in skip-connections or applying an auxiliary loss in an intermediate layer output, we design a nonlinear feature aggregation method throughout the whole stem network, without wasting any layer information, to better fit the potential data distribution.</p><p>We consider the multi-layer outputs in a FCN as a sequence of feature maps. Given the labels as supervision information, the global feature representation should well leverage the spatial and semantic properties for the spatialaware prediction. Therefore, the layer dependency in the sequence of feature maps should be modelled to learn the spatial-aware feature representation. Here we use the long short-term memory (LSTM) as a feature selection function conducted on multi-layer outputs. LSTM can model the long-term dependencies in a sequence. Consequently, it acts as a feature selection function to form the appropriate feature representation. The core part of LSTM is a memory cell c t at the time step t that records the history of input sequence observed up to that time step, and the behaviour of the cell is controlled by an input gate i t , a forget gate f t and an output gate o t . In a fully-connected LSTM (FC-LSTM), these three gates are computed by affine mappings with non-linear activations, which control whether to forget the current cell value if it should read its input and whether to output a new cell value.</p><p>The major limitation of FC-LSTM in modelling sequential data is the usage of full connections in input-to-state and state-to-state transitions in which no spatial information is encoded. Moreover, the affine mappings are with full weight matrices, leading to high computational complexity and the over-fitting problem. To address this issue, we use 2D convolution LSTM <ref type="bibr" target="#b30">[31]</ref> (ConvLSTM) instead of FC-LSTM to aggregate the multi-layer feature outputs.</p><p>Suppose the 2D feature outputs of multiple learning blocks are x 1 , . . . , x m , where m is the number of feature map candidates. The t-th feature map (1 ? t ? m) is a 3D tensor, i.e., x t ? R wt?ht?ct , where w t , h t and c t are the width, height and number of channels, respectively. To feed the 2D feature outputs into the ConvLSTM, all feature maps should be converted to the identical-shaped tensors x 1 , . . . , x m having the same resolution and dimension. Assume shape of the target feature map is w ? h ? c , the change of the resolution from w t ? h t to w ? h is implemented by down-sampling, and the channel conversion from c t to c is computed by 1 ? 1 convolution with no bias. When c t &gt; c , the 1 ? 1 convolution acts as the dimensionality reduction, otherwise it expands the original features into a higher feature space to better fit the high-dimenional non-linearity. Formally, the mapping from x t ? R wt?ht?ct to x t ? R w ?h ?c is as follows:</p><formula xml:id="formula_0">x t = w t * ?(x t )<label>(1)</label></formula><p>where * denotes the 2D convolution, w t is the convolution kernel, and ?(?) is the down-sampling operator 1 , respectively.</p><p>The feature map x t can be considered as w t ? h t feature vectors standing on a spatial grid. The ConvLSTM learns the sequential state of a certain cell in the grid by the inputs and past states of its local neighbours, which is implemented by using a convolution operator in the stateto-state and input-to-state transitions. Similar to FC-LSTM, the input gate i t controls whether ConvLSTM considers the current input x t , the forget gate f t controls whether ConvLSTM forgets the previous memory c t?1 , and the output gate o t controls how much information will be read from memory c t to the current hidden state h t . The computation stream of ConvLSTM is as follows:</p><formula xml:id="formula_1">i t = ?(w ix * x t + w ih * h t?1 + b i ),</formula><p>(2)</p><formula xml:id="formula_2">f t = ?(w f x * x t + w f h * h t?1 + b f ), (3) o t = ?(w ox * x t + w oh * h t?1 + b o ), (4) g t = tanh(w gx * x t + w gh * h t?1 + b g ), (5) c t = f t ? c t?1 + i t ? g t , (6) h t = o t ? tanh(c t ),<label>(7)</label></formula><p>where ?(?) is the sigmoid function and ? is the elementwise multiplication, respectively. The convolution kernels w * x and w * h are the ConvLSTM state and recurrent transformations, and b * are bias matrices. Given a sequence of feature maps x 1 , . . . , x m from m learning blocks, the output of 2D ConvLSTM is a sequence of tensors. The final feature map that correlates the spatial and semantic information is calculated as:</p><formula xml:id="formula_3">y = 1 m ConvLSTM(x 1 , . . . , x m ).<label>(8)</label></formula><p>Considering the specific case of pixel-level classification that requires larger receptive fields, a higher dilation rate (e.g., 2) is used in the ConvLSTM. By adding the ConvLSTM as a feature aggregation function on multiplelayer outputs, the deep parsing structure has the following three advantages. First, the feature aggregation module is able to keep the spatial-semantic consistencies, which is not a linear combination of multiple feature maps, but is an extra learning module to acquire new knowledge thus enhance the global feature representation. Second, the feature aggregation module uses multiple feature maps at different stages as the input through the stem network, forming a strong supervision and making it easier to train, because the gradients from the late layer for pixel-wise classification can be directly passed into shallower learning blocks. Thus, compared to the deep parsing networks such as <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the convergence is faster when applying the feature aggregation module. Third, the feature aggregation module can be directly inserted in any existing FCN pipeline, which does not significantly increase the computational complexity overhead yet enhance the feature representation capability.</p><p>Recurrent modules have been widely used to improve the performance by considering the visual contextual information in some visual pattern recognition works <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b19">[20]</ref>. For example, the ConvLSTM is used to help model the motion <ref type="bibr" target="#b35">[36]</ref> and spatial-temporal dependencies in video frames <ref type="bibr" target="#b36">[37]</ref>. In our work, we use the recurrent module from another perspective for multi-layer feature aggregation for spatial-semantic consistencies, which differs from their contexts and it is the key contribution for spatial-aware learning tasks. In <ref type="bibr" target="#b37">[38]</ref>, the authors proposed to make multilevel context contrasted local features to aggregate multilayer outputs. The key difference to our model is that they adopt the gated sum to control the information flow, while we use the recurrent model to auto-aggregate the features. The gated sum is computed multiple times for every two layer aggregation, so for the whole learning framework it is less computationally efficient compared to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The overall SANet learning architecture</head><p>We now present the architecture of spatial-semantic aggregation network (SANet). The overall framework is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. In the proposed framework, the global feature representation is composed of two parallel pipelines. The first pipeline (b) is essentially a dilated FCN that consists of multiple learning blocks, during which both the feature resolution and feature dimensionality are changed. The second pipeline (c) is in parallel with (b), in which multiple feature maps are aggregated by the spatialsemantic feature selection module.</p><p>The stem network of SANet used in our work is a 50-layer ResNeXt <ref type="bibr" target="#b29">[30]</ref>. Compared with ResNet proposed in <ref type="bibr" target="#b4">[5]</ref>, ResNeXt adopts the aggregated transformation by increasing the cardinality, which can improve the classification accuracy without increasing the width of the bottleneck block or the depth of the whole network. The ResNeXt-50 has four learning stages (blocks) after the early convolution.</p><p>At each stage the number of channels doubles while the resolution halves. We choose the final ReLU activation before resolution changes, so five feature map outputs are selected as the intermediate feature candidates to feed into the spatial-semantic feature aggregation module. Given an input image (a) in <ref type="figure" target="#fig_1">Figure 2</ref>, it has two paths to arrive at the final feature representation prior. The first path is the sequential learning blocks of ResNeXt-50 in (b), and the second path is the feature map conversion and a spatialsemantic feature aggregation module in (c), respectively. We then apply a pyramid feature module (PSP) <ref type="bibr" target="#b22">[23]</ref> as a global feature representation for the pixel-label prediction of the objectives. We do not apply auxiliary loss to any intermediate layer to enhance the discriminative power because the multi-layer feature aggregation module can already conduct the feature selection and pass the gradients into multiple layers in the back-forward optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computational cost analysis</head><p>For deep learning-based models, the computational cost is mainly measured by FLOPs and memory usage of GPU. We compare our SANet based on ResNeXt-50 with some recent deep parsing networks and show the breakdown analysis in <ref type="table" target="#tab_1">Table II</ref>. When the input image resolution is 473 ? 473, our model has a moderate computational cost. The parameter efficiency of SANet is similar to PSPNet-101. On the other hand, SANet needs less GPU memory compared to PSPNet with ResNet-101 but a little more float operations. In general, it is worth increasing the FLOPs overhead in a deep parsing network to improve the accuracy of dense prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To evaluate the effectiveness of the proposed spatialsemantic aggregation model for scene parsing, we conducted comprehensive experiments on NYU Depth v2 <ref type="bibr" target="#b11">[12]</ref>, SUN RGB-D <ref type="bibr" target="#b12">[13]</ref>, Cityscapes <ref type="bibr" target="#b3">[4]</ref> and ADE20K <ref type="bibr" target="#b13">[14]</ref> datasets. In this section, we first briefly introduce the datasets and experimental settings, then report the results on the four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets NYU Depth v2 &amp; SUN RGB-D:</head><p>The two datasets are both indoor scene understanding benchmarks. The pixelwise classes are from a variety of indoor scenes as recorded by a RGB-D camera from different sensors. We use the standard training/testing splits. The depth images are not used to enhance performance. Some partial experimental results have been shown and analyzed in Section III-A.</p><p>ADE20K: This is a challenging dataset with more than 20K scene images, which has 150 classes of dense labels. The testing set has not been released, so we use 2,000 validation images for qualitative evaluation.</p><p>Cityscapes: This is a street-view dataset taken from 50 European cities, which provides fine-grained pixel-level annotations of 19 classes including buildings, pedestrians, bicycles, cars, etc. The training/validation/testing splits are with 2,975, 500 and 1,525 images, respectively. We do not use the 20,000 coarsely labelled images to pre-train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Our implementation is based on Pytorch. Specifically, we applied the following settings in the experiment:</p><p>? To leverage the pixel redundancy and output resolution, we removed the down-sampling operations in the last two learning stages (blocks) of ResNeXt-50 to preserve more visual details without adding extra parameters. Thus, the size of the final feature map is 1/8 of the input image. ? We set the dilation rate to 2 for all the 3 ? 3 convolutions when the stride is 1 to enlarge the receptive field in the third convolution block. Similarly, we set set the dilation rate to 4 in the forth convolution block. Such a setting is beneficial to improve the segmentation performance without adding computational complexity. ? The skip-connection is used multiple times in our SANet. We used the first convolution feature map before residual blocks as the input of the feature aggregation module. Due to the limited computational resource of GPU, we could only use the batch size of 8 at maximum in the training procedure. We added a pyramid pooling module proposed in <ref type="bibr" target="#b22">[23]</ref> at multi-scale spatial levels to augment the global feature representation prior. To better exploit the contextual information, we employed five bin sizes with 60 ? 60, 30 ? 30, 20 ? 20, 15 ? 15 and 10 ? 10, respectively. Such a setting is beneficial to parse very small objects or stuff by considering multiple contextual information. After that, a 3 ? 3 convolution and an up-sampling were applied to spatially adjust the feature size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation metric and experimental settings</head><p>We used intersection-over-union (IoU) and pixel accuracy to measure the parsing quality of the models in the experiment. In the training stage, we used horizontal flipping, random scaling and contrast normalization as image augmentation to further improve the model generalization ability. We used the AdamW <ref type="bibr" target="#b38">[39]</ref> optimizer with the initial learning rate 10 ?5 and followed <ref type="bibr" target="#b8">[9]</ref> to set the learning rate schedualing. In the training process, the best models were checkpointed by the minimum categorical cross-entropy losses on NYU Depth v2 and SUN RGB-D datasets. On ADE20K and Cityscapes datasets, we used the recent lov?sz-softmax loss <ref type="bibr" target="#b39">[40]</ref> to further optimize the IoU. We applied the multi-scale prediction with a single model in the inference procedure on all datasets, but did not use CRF post-processing <ref type="bibr" target="#b8">[9]</ref> to fine-tune the pixel labels after the categorical probability estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Results on NYU Depth v2 and SUN RGB-D datasets:</head><p>One of the nice properties of the proposed multi-layer feature aggregation is that the gradient can be simultaneously passed to multiple layers, making the deep parsing network easy to train. In the model training of the FCN on NYU Depth v2 dataset, we recorded the mean accuracy values for the first 80 training epochs on the validation set, which is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Compared to the FCN models with skip-connections and auxiliary losses, the proposed feature aggregation module can simultaneously pass the gradients into multiple learning blocks, including the shallower convolution blocks, which improves the optimization efficiency for dense predictions.</p><p>We first conducted the component analysis of SANet on NYU Depth v2 dataset. The proposed SANet is essentially a composition of a stem network (ResNeXt-50), a multi-layer feature aggregation module and a pyramid pooling module. In the training process, the best models were checkpointed by the minimum categorical cross-entropies, and the final evaluation of the testing set is summarized in <ref type="table" target="#tab_1">Table III</ref>. Applying a pyramid pooling module on a FCN can slightly improve the parsing performance. If the stem network is augmented by the feature aggregation module, the accuracy   We show the quantitative results of the two indoor scene parsing datasets in <ref type="table" target="#tab_1">Table IV</ref>. Even we did not incorporate the depth images in the training process, the proposed SANet reaches the best performance in most cases. Since we adopted the pyramid pooling module which is the same as PSPNet, the proposed spatial-semantic feature aggregation module outperforms the pixel accuracies by 0.7% and 0.9%, and boosts the IoU values by 0.5% and 1.1% on the two datasets, respectively.</p><p>2) Results on ADE20K dataset: We experimented on the large-scale ADE20K dataset to verify the effectiveness of the proposed SANet. The comparisons on the validation set with some recently proposed methods are reported in <ref type="table" target="#tab_5">Table V</ref>. Results show that our model achieves 82.1% in pixel accuracy and 44.7% in IoU, which achieves the best performance among these methods. Some example scene parsing results in both indoor and outdoor environments are illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>3) Results on Cityscapes dataset: For this dataset, the ground-truth of test images are withheld by the organizers, so all methods can only be tested by submitting the results to the evaluation server. The overall comparisons of our model with some recently proposed methods are summarized in <ref type="table" target="#tab_1">Table VII</ref>. Among the test results on the evaluation server, our proposed SANet outperforms all previous methods in terms of the class IoU. The per-class scene parsing results are reported in <ref type="table" target="#tab_1">Table VI</ref>. From the table, we can see that even without the pre-training on the 20,000 coarse-labelled images, our method still achieves very promising results. We also illustrate some example results for the scene parsing visualization in <ref type="figure" target="#fig_4">Figure 5</ref>. Since our model can automatically leverage the spatial and semantic information from multi-layer feature maps, some small objects (such as traffic signs) are accurately segmented by SANet, which demonstrates that the proposed feature aggregation module can well deal with the spatialaware learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussions</head><p>The goal of the proposed spatial-semantic feature aggregation module is to learn better feature maps for dense classification. Furthermore, due to the use of the proposed feature aggregation module, the deep parsing network is much easier to train because the gradients can be passed to multiple learning blocks, achieving to a faster training process. So the proposed feature aggregation module is an excellent alternative compared to simple skip-connections or auxiliary losses in the deep scene parsing models. The experimental results have proved the effectiveness of our module. In some other spatial-aware learning tasks such as object detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b50">[51]</ref> and tracking <ref type="bibr" target="#b51">[52]</ref>, the global feature representation prior that leverages the spatial and semantic information usually obtains superior performance, so it is worth trying to apply the memory cell on multiple feature outputs in deep neural networks to improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a novel spatial-semantic feature selection module for supervised scene parsing. The extra module can select the useful components in multiple feature outputs and aggregate them to form a discriminative global feature representation for accurate pixel-label prediction. Integrating the feature selection module into deep parsing networks   also forms a strong supervision to effectively suppress the over-fitting problem, making the parsing network easy to train. Extensive experiments with very promising results on four public scene parsing datasets demonstrate the effectiveness of our SANet. We believe the proposed spatialsemantic feature aggregation module can also benefit the related spatial-aware learning techniques in the community. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Different approaches to utilize multiple-layer outputs. (a) Sequential learning blocks without aggregation as default for supervised learning tasks. (b) Sequential learning blocks with an auxiliary loss branch. (c) Skip-connections through multiple learning blocks [10]. (d) Deep layer aggregation proposed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the deep parsing framework with a feature aggregation module. Given an input image (a), we use a pre-trained deep model (b) as a stem network for feature inference. At the same time, we keep the output feature maps of multiple learning blocks and send them into a feature aggregation module (c) for spatial-semantic consistencies. Finally, the aggregated feature representation goes through a pyramid pooling module to get the final per-pixel prediction (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The mean accuracy curves on the NYU Depth v2 validation set. The feature aggregation is based on a dilated FCN. Due to the multiple skip-connections, the proposed feature aggregation can effectively accelerate the training of the deep scene parsing model.can be further boosted. Specifically, the much lower loss value (categorical cross-entropy in our case) means the model can get more reliable parsing results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Scene parsing examples on ADE20K validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Scene parsing examples on Cityscapes validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yu (litao.yu@uts.edu.au), Jian Zhang (jian.zhang@uts.edu.au) and Qiang Wu (qiang.wu@uts.edu.au) are with Global Big Data Technologies Centre, University of Technology Sydney, Ultimo, 2007, NSW, Australia. Yongsheng Gao (yongsheng.gao@griffith.edu.au) and Jun Zhou (jun.zhou@griffith.edu.au) are with the Institute for Integrated and Intelligent Systems, Griffith University, Nathan, 4111, QLD, Australia. This work was supported by the Australian Research Council under Discovery Grants DP140101075 and DP180100958.</figDesc><table /><note>Manuscript received July 1, 2020.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Comparison of IoU of FCNs with different skipconnections (SC) and auxiliary losses (AL) on NYU Depth v2 and SUN RGB-D validation sets.</figDesc><table><row><cell>Layer</cell><cell cols="2">NYU Depth v2 SC AL</cell><cell cols="2">SUN RGB-D SC AL</cell></row><row><cell>None</cell><cell></cell><cell>40.2</cell><cell></cell><cell>40.6</cell></row><row><cell>s0</cell><cell>39.2</cell><cell>40.4</cell><cell>39.8</cell><cell>40.1</cell></row><row><cell>s1</cell><cell>40.0</cell><cell>40.4</cell><cell>40.4</cell><cell>40.5</cell></row><row><cell>s2</cell><cell>39.6</cell><cell>40.6</cell><cell>40.3</cell><cell>40.7</cell></row><row><cell>s3</cell><cell>39.4</cell><cell>40.4</cell><cell>40.2</cell><cell>40.6</cell></row><row><cell>s4</cell><cell>40.4</cell><cell>40.2</cell><cell>40.3</cell><cell>40.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Computaional complexity analysis.</figDesc><table><row><cell>Method</cell><cell>Stem network</cell><cell>Input size</cell><cell>Output</cell><cell>FLOPs</cell><cell>Memory</cell></row><row><cell>FCN-101</cell><cell>ResNet-101</cell><cell>512 ? 512</cell><cell>1/8</cell><cell>1.04 ? 10 8</cell><cell>3.25G</cell></row><row><cell>PSPNet-50 [23]</cell><cell>ResNet-50</cell><cell>473 ? 473</cell><cell>1/8</cell><cell>0.93 ? 10 8</cell><cell>1.96G</cell></row><row><cell>PSPNet-101 [23]</cell><cell>ResNet-101</cell><cell>473 ? 473</cell><cell>1/8</cell><cell>1.31 ? 10 8</cell><cell>3.38G</cell></row><row><cell>DeepLab v3+ [9]</cell><cell>Xception</cell><cell>512 ? 512</cell><cell>1/8</cell><cell>0.82 ? 10 8</cell><cell>5.14G</cell></row><row><cell>RefineNet [21]</cell><cell>ResNet-101</cell><cell>512 ? 512</cell><cell>1/4</cell><cell>2.61 ? 10 8</cell><cell>1.92G</cell></row><row><cell>SANet (ours)</cell><cell>ResNeXt-50</cell><cell>473 ? 473</cell><cell>1/8</cell><cell>1.65 ? 10 8</cell><cell>2.68G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Component analysis of SANet on NYU Depth v2 testing set.</figDesc><table><row><cell>Method</cell><cell>PSP</cell><cell>Feature aggregation</cell><cell>Pixel accuracy</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>71.8</cell></row><row><cell>ResNeXt-50</cell><cell></cell><cell></cell><cell>72.5</cell></row><row><cell>ResNeXt-50</cell><cell></cell><cell></cell><cell>74.3</cell></row><row><cell>ResNeXt-50</cell><cell></cell><cell></cell><cell>75.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Scene parsing results on NYU Depth v2 and SUN RGB-D testing sets.</figDesc><table><row><cell>Methods</cell><cell cols="2">NYU Depth v2 Pixel Acc. IoU</cell><cell cols="2">SUN RGB-D Pixel Acc. IoU</cell></row><row><cell>SegNet [16]</cell><cell>-</cell><cell>-</cell><cell>72.6</cell><cell>31.8</cell></row><row><cell>SEGCloud [41]</cell><cell>-</cell><cell>43.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Lin et al. [42]</cell><cell>70.0</cell><cell>40.6</cell><cell>78.4</cell><cell>42.3</cell></row><row><cell>RefineNet[21]</cell><cell>74.4</cell><cell>47.6</cell><cell>81.1</cell><cell>47.0</cell></row><row><cell>MSCI [43]</cell><cell>-</cell><cell>49.0</cell><cell>-</cell><cell>50.4</cell></row><row><cell>Pad-Net [44]</cell><cell>75.2</cell><cell>50.2</cell><cell>-</cell><cell>-</cell></row><row><cell>CCL [38]</cell><cell>-</cell><cell>-</cell><cell>81.4</cell><cell>47.1</cell></row><row><cell>SANet (ours)</cell><cell>75.9</cell><cell>50.7</cell><cell>82.3</cell><cell>51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Results on ADE20K validation set.</figDesc><table><row><cell>Method</cell><cell>Pixel Acc.</cell><cell>IoU</cell></row><row><cell>PSPNet[23]</cell><cell>81.4</cell><cell>43.3</cell></row><row><cell>SAC[45]</cell><cell>81.9</cell><cell>44.3</cell></row><row><cell>RefineNet[21]</cell><cell>-</cell><cell>42.4</cell></row><row><cell>PSANet[46]</cell><cell>81.5</cell><cell>43.8</cell></row><row><cell>EncNet[47]</cell><cell>81.7</cell><cell>44.7</cell></row><row><cell>SANet (ours)</cell><cell>82.1</cell><cell>44.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Per-class results on Cityscapes testing set. 73.2 84.0 28.5 29.0 35.7 39.8 45.2 87.0 63.8 91.8 62.8 42.8 89.3 38.1 43.1 44.1 35.8 51.9 LDN-121[48] 97.4 80.2 92.0 47.6 53.9 64.6 72.8 76.3 92.8 66.4 95.5 83.8 66.1 94.3 55.6 70.3 67.0 62.1 73.0 ResNet-38[49] 98.5 85.7 93.1 55.5 59.1 67.1 74.8 78.7 93.7 72.6 95.5 86.6 69.2 95.7 64.5 78.8 74.1 69.0 76.7 SAC[45] 98.7 86.5 93.1 56.3 59.5 65.1 73.0 78.2 93.5 72.6 95.6 85.9 70.8 95.9 71.2 78.6 66.2 67.7 76.0 Alex et al.[50] 98.4 85.2 92.8 54.1 60.8 62.4 73.4 77.5 93.3 71.5 95.1 84.9 69.5 95.3 68.5 86.2 80.0 67.8 75.6 RefineNet[21] 98.2 83.3 91.3 47.8 50.4 56.1 66.9 71.3 92.3 70.3 94.8 80.9 63.3 94.5 64.6 76.1 64.3 62.2 70.0 PSPNet[23] 98.6 86.6 93.2 58.1 63.0 64.5 75.2 79.2 93.4 72.1 95.1 86.3 71.4 96.0 73.5 90.4 80.3 69.9 76.9 AAF [35] 98.5 85.6 93.0 53.8 59.0 65.9 75.0 78.4 93.7 72.4 95.6 86.4 70.5 95.9 73.9 82.7 76.9 68.7 76.4 DANet [32] 98.6 86.1 93.5 56.1 63.3 69.7 77.3 81.3 93.9 72.9 95.7 87.3 72.9 96.2 76.8 89.4 86.5 72.2 78.2 SANet (ours) 98.7 87.1 93.6 61.6 62.4 68.1 75.9 79.5 93.8 73.1 95.8 87.3 71.5 96.2 71.9 88.1 86.1 69.4 77.2</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell></row><row><cell>SegNet[16]</cell><cell>96.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Overall results on Cityscapes testing set.</figDesc><table><row><cell>Method</cell><cell cols="4">IoU cla. iIoU cla. IoU cat. iIoU cat.</cell></row><row><cell>LDN-121[48]</cell><cell>74.3</cell><cell>51.6</cell><cell>89.7</cell><cell>79.5</cell></row><row><cell>SAC[45]</cell><cell>78.1</cell><cell>55.2</cell><cell>90.6</cell><cell>78.3</cell></row><row><cell>RefineNet[21]</cell><cell>73.6</cell><cell>47.2</cell><cell>87.9</cell><cell>70.6</cell></row><row><cell>Yu et al. [11]</cell><cell>75.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Alex et al.[50]</cell><cell>78.5</cell><cell>57.4</cell><cell>89.9</cell><cell>77.7</cell></row><row><cell>AAF [35]</cell><cell>79.1</cell><cell>56.1</cell><cell>90.8</cell><cell>78.5</cell></row><row><cell>PSPNet[23]</cell><cell>78.4</cell><cell>56.7</cell><cell>90.6</cell><cell>78.6</cell></row><row><cell>PSANet[46]</cell><cell>78.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pad-Net[44]</cell><cell>80.3</cell><cell>58.8</cell><cell>90.8</cell><cell>78.5</cell></row><row><cell>SANet (ours)</cell><cell>80.9</cell><cell>59.6</cell><cell>91.4</cell><cell>80.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the bilinear interpolation on the 2D grids to adjust the feature resolution.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Striped-texture image segmentation with application to multimedia security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="26" to="965" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic segmentation with heterogeneous sensor coverages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2639" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2174" to="2182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2403" to="2412" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3751" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The devil is in the decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scene segmentation with dag-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1480" to="1493" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent saliency transformation network: Incorporating multistage visual cues for small organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8280" to="8289" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An iterative and cooperative top-down and bottom-up inference network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5968" to="5977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="587" to="602" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A multi-level convolutional lstm model for the segmentation of left ventricle myocardium in infarcted porcine cine mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Icke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dogdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parimal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sampath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ISBI</title>
		<imprint>
			<biblScope unit="page" from="470" to="473" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Future semantic segmentation with convolutional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2393" to="2402" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The lov?szsoftmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4413" to="4421" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiscale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="675" to="684" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="267" to="283" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7151" to="7160" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ladder-style densenets for semantic segmentation of large natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7482" to="7491" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="3588" to="3597" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">New trends on moving object detection in video images captured by a moving camera: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yazdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bouwmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="157" to="177" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
