<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COMPLEX TRANSFORMER: A FRAMEWORK FOR MODELING COMPLEX-VALUED SEQUENCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muqiao</forename><surname>Yang</surname></persName>
							<email>muqiaoy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">Q</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Li</surname></persName>
							<email>dongyul@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung</forename><forename type="middle">Hubert</forename><surname>Tsai</surname></persName>
							<email>yaohungt@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COMPLEX TRANSFORMER: A FRAMEWORK FOR MODELING COMPLEX-VALUED SEQUENCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>transformer network</term>
					<term>se- quence modeling</term>
					<term>complex-valued deep neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset. The GitHub implementation to reproduce the experimental results is available at https://github.com/ muqiaoy/dl_signal. Index Terms-Deep learning, transformer network, sequence modeling, complex-valued deep neural network</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech recognition, signal processing, and audio transcription have been advanced by recent deep learning models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Those models only use the real half of the spectral input. However, developing deep learning models for complexvalued input is crucial because signal based time series modeling is naturally complex-valued through Fourier Transform (FT). In recent years, complex-valued feed-forward neural network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> has been introduced, and more advanced complex-valued deep neural nets for sequence modeling have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Those methods for sequences are based on recurrent models like recurrent neural network (RNN), Long Short-Term Memory (LSTM) <ref type="bibr" target="#b8">[9]</ref>, and Gated Recurrent Units (GRU) <ref type="bibr" target="#b9">[10]</ref>, which inherently suffer from the memory bottleneck.</p><p>Recently, attention mechanism <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> and transformer models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> have become a well-performed sequence * equal contribution. This work was supported in part by DARPA grant FA875018C0150, DARPA SAGAMORE HR00111990016, AFRL CogDe-CON, and Apple. We would also like to acknowledge NVIDIA's GPU support. model because of their capability of looking at a more extended range of input contexts and representing those in different subspaces. Attention is, therefore, promising for building a complex-valued model capable of analyzing highdimensional data with long temporal dependency. We introduce a Complex Transformer to solve complex-valued sequence modeling tasks, including prediction and generation. The Complex Transformer adapts complex representations and operations into the transformer network. We test the model with MusicNet, a dataset for music transcription <ref type="bibr" target="#b13">[14]</ref>, and a complex high-dimensional time series In-phase Quadrature (IQ) signal dataset. The specific contributions of our work are as follows:</p><p>Our Complex Transformer achieves state-of-the-art results on the MusicNet dataset <ref type="bibr" target="#b13">[14]</ref>, a dataset of classical music pieces with annotating labels at each time interval, and IQ signal dataset, a non-public dataset containing WI-FI signals with fixed-length and their corresponding device labels. Our transformer model contains several complex-valued features which boost its performance, including complex attention and complex encoder-decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATION TO PRIOR WORK</head><p>Complex-valued neural networks have attracted attentions from the deep learning community since the complex version of backpropagation algorithm <ref type="bibr" target="#b14">[15]</ref> was proposed because of its richer representational capacity. It was then applied in a variety of domains, including signal processing <ref type="bibr" target="#b15">[16]</ref> and computer vision <ref type="bibr" target="#b16">[17]</ref> where signals and images in their waveform or Fourier Transform are used as input data.</p><p>The work in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> applied complex operations on RNN, LSTM <ref type="bibr" target="#b8">[9]</ref>, and GRU <ref type="bibr" target="#b9">[10]</ref>, respectively. The work in <ref type="bibr" target="#b17">[18]</ref> implemented complex convolutional layer and used the layers to predict based on input within each time step. <ref type="bibr" target="#b7">[8]</ref> combined FT and sequence modeling methods to explore the temporal information. Our method differs from the previous ones as follows: Compared to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, which were based on recurrent networks, our work used attention-based transformer network, which has global view of input over all time steps and do not have memory bottleneck issue embedded in backbone models of RNN, LSTM and GRU; Compared to <ref type="bibr" target="#b17">[18]</ref>, we uti-lize the temporal dependencies across time steps. Another important difference we made that distinguishes this work from others is that we have performed sequence generation on both acoustic input and signal input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL ARCHITECTURE</head><p>Our complex transformer consists of an encoder-decoder structure, where the encoder maps a complex-valued input sequence into a complex-valued representation, and the decoder generates a complex-valued sequence one time step at a time given the encoder output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>Given n sequences of signal (x 1 , ..., x n ) with d time steps, we use our model to classify given inputs and generate new sequences. To start with, we transform the raw real-valued signal (x 1 , ..., x n ) ? R d?n into (a 1 + ib 1 , ..., a n + ib n ) = A + iB ? C d?n by Discrete Fourier Transform, an operation which decomposes a finite time sequence into a finite frequency sequence. The frequency sequence X = A + iB is then fed into the transformer model. For an arbitrary classification task, to classify input X to a label y, we first use a stack of encoders to produce the representation of encoder input X enc = A + iB for X, where A and B are separate latent vectors for A and B of X respectively. We then use a linear layer to predict output label probability given X enc . For the generation task, a stack of decoders in the model will be given both X enc and X dec = C + iD (decoder input), and completes the rest of X dec by generating C and D (decoder output).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Complex Encoder and Decoder</head><p>Our model architecture is shown in <ref type="figure" target="#fig_2">Fig. 1</ref>.</p><p>Complex Encoder The encoder is composed of six identical stacks, each with two sublayers. The first sublayer is a complex attention layer, and the second is a complex-valued feed-forward network. Both sublayers have residual connections <ref type="bibr" target="#b18">[19]</ref> and layer normalizations <ref type="bibr" target="#b19">[20]</ref>. We employ layer normalization before residual connections in the encoder, referred as "Norm &amp; Add" in the encoder part of <ref type="figure" target="#fig_2">Fig. 1</ref>.</p><p>Complex Decoder The decoder also has six identical stacks. Each stack has three sublayers: complex attention, complex feed-forward network, and another complex attention layer. The first complex attention layer is masked with the additional diagonal masking to prevent attending to subsequent positions. The second complex attention will be performed on the encoded representation X enc and the decoder input X dec .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Complex Attention</head><p>For a complex vector x = a + ib, we represent a and b as different input parts and simulate complex operations using real values. This is because for any complex function f : C n ? C n and any complex vector <ref type="bibr" target="#b4">[5]</ref>. This indicates any complex function could be rewritten into two separate real functions.</p><formula xml:id="formula_0">x = a + ib, we can represent f as f (a + ib) = ?(a, b) + i?(a, b) where ?, ? : R n ? R n</formula><p>A complex feed-forward network feeds a and b to separate real-valued feed-forward neural networks and ReLU activations <ref type="bibr" target="#b17">[18]</ref>. A complex convolutional neural network <ref type="bibr" target="#b17">[18]</ref> convolves a complex weight matrix W = A + iB and complex vector x as the following:</p><formula xml:id="formula_1">W * x = (A + iB) * (a + ib) = (A * a ? B * b) + i(A * b + B * a)<label>(1)</label></formula><p>Inspired by <ref type="bibr" target="#b11">[12]</ref>, we propose complex building blocks for attention mechanism in our model. Given complex input X = A+iB, we want to attend high-dimensional information at different time steps just as in real. Thus, we compute the query matrix Q = XW Q , the key matrix K = XW K , and the value matrix V = XW V (where Q, K, V are complexvalued and Ws are real-valued) and define the complex attention:</p><formula xml:id="formula_2">QK T V = (XW Q )(XW K ) T (XW V ) = (AW Q + iBW Q )(W T K A T + iW T K B T )(AW V + iBW V ) = (AW Q W T K A T AW V ? AW Q W T K B T BW V ? BW Q W T K A T BW V ? BW Q W T K B T AW V ) + i(AW Q W T K A T BW V + AW Q W T K B T AW V + BW Q W T K A T AW V ? BW Q W T K B T BW V ) = A + iB<label>(2)</label></formula><p>where A and B represent the real and the imaginary part of the complex attention result respectively. In our implementation, to have a better resolution of internal similarities between the real and the imaginary parts, for each term in the expanded version of Eq. (2), we calculate the multihead attentions, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>    </p><formula xml:id="formula_3">= Concat({Attention(QW Q i , KW K i , VW V i )} n i=1 )W O (4) Attention(Q, K, V) = Min-Max-Norm( QK T ? d k )V (5) Min-Max-Norm(X) = X ? min(X) max(X) ? min(X)<label>(6)</label></formula><p>The scaling factor d k is the feature dimension of Q and K (note that Q/K/V are complex-valued matrices, while Q/K/V are real-valued placeholder matrices that we could plug in A or B). Attention(Q, K, V) could be intuitively regarded as an extended, weight-adjusted representation of V, based on V's dependencies on Q and K. MultiHead(Q, K, V) provides a better resolution for the dependencies in different subspaces <ref type="bibr" target="#b20">[21]</ref> (in this case we have n heads, i.e. the number of attention blocks that are concatenated together). We use Min-Max-Norm instead of Softmax as in <ref type="bibr" target="#b11">[12]</ref>, because min-max-normalization prevents gradient explosion better in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Automatic Music Transcription</head><p>Automatic Music Transcription (AMT) is a challenging and significant problem, as it is a problem of forming a mapping from an audio sequence to a symbolic representation. We choose MusicNet <ref type="bibr" target="#b13">[14]</ref>, a large collection of music recordings, with raw audios as data and music notes as labels at each time step. There are in total 128 labels and each time step could contain multiple labels (i.e. a multi-label classification task). We split the dataset and resampled the raw input according to <ref type="bibr" target="#b17">[18]</ref>, and then preprocess the processed signals with FT and use the complex frequency domain data as the input of our model.</p><p>Suggested by <ref type="bibr" target="#b13">[14]</ref>, we use recordings with ids '2303', '2382', '1819' as the test set and the remaining 327 recordings as the training set. Similar to <ref type="bibr" target="#b17">[18]</ref>, we resampled the original input from 44.1kHz to 11kHz using the technique introduced by <ref type="bibr" target="#b21">[22]</ref> to improve computational efficiency.</p><p>For Complex transformer, the number of encoders is fixed as 6, and the number of heads in multi-head attention is 8. We trained our model with an initial learning rate of 10 ?3 and the optimizer is Adam. The initializer of the complex transformer is Xavier uniform. The total time steps of input are 64. For dropouts, we set the dropout following self-attention to be 0, the dropout following ReLU in residual blocks to be 0.1 and the dropout of each residual block to be 0.1.</p><p>For the networks we compare our results to, Complex Gated Recurrent Neural Network (cgRNN) <ref type="bibr" target="#b6">[7]</ref> and Deep Complex Network <ref type="bibr" target="#b17">[18]</ref> results are from the corresponding papers. Deep real network is a deep model treating real and imaginary parts as separate channels and concatenated these two as input. Concatenated Transformer is a vanilla transformer taking the same input as the deep real network. It is noteworthy that Deep Complex Network <ref type="bibr" target="#b17">[18]</ref> classifies the music data into 84 classes (piano notes only), while we classify data into 128 classes (both piano and other instruments notes), which is a more general task.</p><p>The experimental results comparison is shown in <ref type="table" target="#tab_0">Table 1</ref>, which use average precision score (APS) as the metric. As the table shows, our complex transformer achieves a higher average precision score with much fewer parameters compared to Deep Complex Network 1 , and outperforms the Complex Gated Recurrent Neural Network (cgRNN) <ref type="bibr" target="#b6">[7]</ref> and a vanilla transformer. In terms of general deep neural network architecture, we achieve the state-of-the-art result on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">In-phase Quadrature (IQ) Data Classification</head><p>We also trained our model on a non-public, In-phase Quadrature (IQ) wireless signal dataset. The task is to classify the ID of the signaling device given a fixed-length sequence of WI-FI signal. This IQ dataset contains more than 103 million segments of wireless WI-FI signals from 53,853 devices (phones, laptops, tablets, etc). We resampled the data to guarantee the distribution of device IDs is uniform.</p><p>To train our model, we only consider the first 1,600 time steps of each sequence and classify each sequence into one of 1000 classes. Thus, for each data point, the input data is a vector ? C 1600 , and the label is a one-hot vector ? R 1000 . We trained our model with comparison to a feed-forward neural network, a gated recurrent network (GRU) <ref type="bibr" target="#b9">[10]</ref> and a realvalued transformer. The experimental results are shown in <ref type="table" target="#tab_2">Table 2</ref>   than any other model we applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Music and IQ Dataset Conditional Generation</head><p>Following the idea of conditional generation in <ref type="bibr" target="#b22">[23]</ref>, we generate sequences conditioned on partial input. We split the input X into two parts. The first 60% time steps of X are input for the encoder. After getting a representation X = A + iB from the encoder input above, we then use X and the masked, remaining 40% time steps of X (masked decoder input) to generate an output sequence one step at a time.</p><p>We perform this conditional generation on MusicNet and the IQ dataset and compare our model with the LSTM encoderdecoder model <ref type="bibr" target="#b8">[9]</ref> as well as a vanilla transformer. Both the LSTM and the vanilla transformer concatenates the real and the imaginary parts of the signal and then take it as input. Lastly, all models use prediction layers to predict labels based on generated sequences. We choose binary cross entropy loss for MusicNet and cross entropy loss for IQ, since MusicNet is a multi-label classification while IQ is multi-class. As <ref type="table" target="#tab_3">Table 3</ref> shows, our complex transformer has outperformed LSTM and concatenated transformer in terms of the loss between the predicted labels and ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>The Complex Transformer we have introduced shows the power of sequence modeling in the complex domain. The model uses attention to capture dependencies between the real and the imaginary part in different time steps, and achieves better performance on automatic music transcription and generation, as well as signal prediction and generation tasks than other sequential models. By introducing complex operations to the attention network architecture, we show complex numbers capable of capturing richer temporal information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and Eq. (3)-Eq. (6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Structure of Complex Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Model Architecture Overview (Left: Encoder; Right: Decoder). ComplexAttention(X) = (MH(A, A, A) ? MH(A, B, B) ? MH(B, A, B) ? MH(B, B, A)) + i(MH(A, A, B) + MH(A, B, A) + MH(B, A, A) ? MH(B, B, B)) (3) where MH(Q, K, V) = MultiHead(Q, K, V)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experimental results for automatic music transcrip-</figDesc><table><row><cell>tion.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2"># Parameters APS (%)</cell></row><row><cell>cgRNN [7]</cell><cell>2.36M</cell><cell>53.0</cell></row><row><cell>Deep Real Network [18]</cell><cell>10.00M</cell><cell>69.8</cell></row><row><cell>Deep Complex Network [18]</cell><cell>17.14M</cell><cell>72.9</cell></row><row><cell>Concatenated Transformer</cell><cell>9.79M</cell><cell>71.30</cell></row><row><cell>Complex Transformer</cell><cell>11.61M</cell><cell>74.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and the Complex Transformer achieves better results 1 Parameter numbers of Deep Real Network and Deep Complex Network are based on empirical experiments using official Deep Complex Network GitHub code: https://github.com/ChihebTrabelsi/ deep_complex_networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Experimental results for In-phase Quadrature (IQ) data.</figDesc><table><row><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>Feed-forward neural network</cell><cell>47.12</cell></row><row><cell>Gated Recurrent Unit</cell><cell>50.38</cell></row><row><cell>Concatenated Transformer</cell><cell>57.57</cell></row><row><cell>Complex Transformer</cell><cell>59.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experimental results (in loss) for sequence generation.</figDesc><table><row><cell>Model</cell><cell cols="2">MusicNet IQ dataset</cell></row><row><cell>LSTM</cell><cell>0.0629</cell><cell>3.7086</cell></row><row><cell>Concatenated Transformer</cell><cell>0.0509</cell><cell>2.2580</cell></row><row><cell>Complex Transformer</cell><cell>0.0492</cell><cell>2.2335</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Hirose</surname></persName>
		</author>
		<title level="m">Complex-valued neural networks: theories and applications</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single-layered complex-valued neural network for real-valued classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Md Faijul Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="945" to="955" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Unitary evolution recurrent neural networks,&quot; in International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Associative long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03032</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Complex gated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Wolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08627</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fourier rnns for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Wolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning features of music from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09827</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the complex backpropagation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevio</forename><surname>Benvenuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="967" to="969" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear blind equalization schemes using complex-valued multilayer feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheolwoo</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daesik</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1442" to="1455" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep rototranslation scattering for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2865" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">Felipe</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09792</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transformer dissection: An unified understanding for transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on empirical methods in natural language processing</title>
		<meeting>the 2019 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Digital audio processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Julius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<ptr target="https://ccrma.stanford.edu/?jos/resample/" />
		<imprint>
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
