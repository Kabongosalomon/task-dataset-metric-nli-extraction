<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quasi-Dense Similarity Learning for Multiple Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlu</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff4">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quasi-Dense Similarity Learning for Multiple Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Similarity learning has been recognized as a crucial step for object tracking. However, existing multiple object tracking methods only use sparse ground truth matching as the training objective, while ignoring the majority of the informative regions on the images. In this paper, we present Quasi-Dense Similarity Learning, which densely samples hundreds of region proposals on a pair of images for contrastive learning. We can directly combine this similarity learning with existing detection methods to build Quasi-Dense Tracking (QDTrack) without turning to displacement regression or motion priors. We also find that the resulting distinctive feature space admits a simple nearest neighbor search at the inference time. Despite its simplicity, QD-Track outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external training data. Compared to methods with similar detectors, it boosts almost 10 points of MOTA and significantly decreases the number of ID switches on BDD100K and Waymo datasets. Our code and trained models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multiple Object Tracking (MOT) is a fundamental and challenging problem in computer vision, widely used in safety monitoring, autonomous driving, video analytics, and other applications. Contemporary MOT methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b54">56]</ref> mainly follow the tracking-by-detection paradigm <ref type="bibr" target="#b36">[38]</ref>. That is, they detect objects on each frame and then associate them according to the estimated instance similarity. Recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b54">56]</ref> show that if the detected objects are accurate, the spatial proximity between objects in consecutive frames, measured by Interaction of Unions (IoUs) or center distances, is a strong prior to associate the objects. However, this location heuristic only works well in simple scenarios. If the objects are occluded or the scenes are crowded, this location heuristic can easily lead to mistakes. To remedy this problem, some methods introduce motion estimation <ref type="bibr">[8,</ref><ref type="bibr" target="#b30">32]</ref> or displacement regression <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b54">56]</ref> to ensure accurate distance estimation.</p><p>However, object appearance similarity usually takes a secondary role <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b45">47]</ref> to strengthen object association or reidentify vanished objects. The search region is constrained to be local neighborhoods to avoid distractions because the appearance features can not effectively distinguish different objects. On the contrary, humans can easily associate the identical objects only through appearance. We conjecture this is because the image and object information is not fully utilized for learning object similarity. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, previous methods regard instance similarity learning as a post hoc stage after object detection or only use sparse ground truth bounding boxes as training samples <ref type="bibr" target="#b45">[47]</ref>. These processes ignore the majority of the regions proposed on the images. Because objects in an image are rarely iden-tical to each other, if the object representation is properly learned, a nearest neighbor search in the embedding space should associate and distinguish instances without bells and whistles.</p><p>We observe that besides the ground truths and detected bounding boxes, which sparsely distribute on the images, many possible object regions can provide valuable training supervision. They are either close to the ground truth bounding boxes to provide more positive training examples or in the background as negative examples. In this paper, we propose quasi-dense similarity learning, which densely matches hundreds of regions of interest on a pair of images for contrastive learning. The quasi-dense samples can cover most of the informative regions on the images, providing both more box examples and hard negatives.</p><p>Because one sample has more than one positive samples on the reference image, we extend the contrastive learning <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b47">49]</ref> to multiple positive forms that makes the quasi-dense learning feasible. Each sample is thus trained to distinguish all proposals on the other image simultaneously. This contrast provides stronger supervision than using only the handful ground truth labels and enhances the instance similarity learning.</p><p>The inference process, which maintains the matching candidates and measures the instance similarity, also plays an important role in the tracking performance. Besides similarity, MOT also needs to consider false positives, id switches, new appeared objects, and terminated tracks. To tackle the missing targets with our similarity metric, we include backdrops, the unmatched objects in the last frame, for matching and use bi-directional softmax to enforce the bi-directional consistency. The objects that do not have matching targets will lack the consistency thus has low similarity scores to any objects. To track the multiple targets, we also conduct duplicate removal to filter the matching candidates.</p><p>Quasi-dense similarity learning can be easily used with most existing detectors since generating region of interests is widely used in object detection algorithms. In this paper, we apply our method to Faster R-CNN <ref type="bibr" target="#b37">[39]</ref> along with a lightweight embedding extractor and residual networks <ref type="bibr" target="#b14">[16]</ref> and build Quasi-Dense Tracking (QDTrack) models. We conduct extensive experiments on MOT <ref type="bibr" target="#b28">[30]</ref>, BDD100K <ref type="bibr" target="#b51">[53]</ref>, Waymo <ref type="bibr" target="#b41">[43]</ref>, and TAO <ref type="bibr" target="#b7">[9]</ref> tracking benchmarks. Despite its simplicity, QDTrack outperforms all existing methods without bells and whistles. It achieves 68.7 MOTA on MOT17 at 20.3 FPS without using external training data. Moreover, it boosts almost 10 points of MOTA and significantly decreases the number of ID switches on BDD100K and Waymo datasets, establishing solid records on these brandnew large-scale benchmarks. QDTrack allows end-to-end training, thereby simplifying the training and testing procedures of multi-object tracking frameworks. The simplicity and effectiveness shall benefit further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Recent developments in multiple object tracking <ref type="bibr" target="#b22">[24]</ref> follow the tracking-by-detection paradigm <ref type="bibr" target="#b36">[38]</ref>. These approaches present different methods to estimate the instance similarity between detected objects and previous tracks, then associate objects as a bipartite matching problem <ref type="bibr" target="#b31">[33]</ref>. Location and motion in MOT The spatial proximity has been proven effective to associate objects in consecutive frames <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. However, they cannot do well in complicated scenarios such as crowd scenes. Some methods use motion priors, such as Kalman Filter <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b52">54]</ref>, optical flow <ref type="bibr" target="#b48">[50]</ref>, and displacement regression <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b15">17]</ref>, to ensure accurate distance estimations. In contrast to the old paradigm that detects objects and predicts displacements separately, Detect &amp; Track <ref type="bibr" target="#b9">[11]</ref> is the first work that jointly optimizes object detection and tracking modules. It predicts the displacements of the objects in consecutive frames and associates the objects with the Viterbi algorithm. Tracktor <ref type="bibr" target="#b1">[2]</ref> directly adopts a detector as a tracker. CenterTrack <ref type="bibr" target="#b54">[56]</ref> and Chained-Tracker <ref type="bibr" target="#b35">[37]</ref> predict the object displacements with pair-wise inputs to associate the objects. Although these methods show promising results, they <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">47]</ref> still need an extra re-identification model as complementary to re-identify vanished objects, making the entire framework complicated. Appearance similarity in MOT To exploit instance appearance similarity to strengthen tracking and re-identify vanished objects, some methods directly use an independent model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b49">51]</ref> or add an extra embedding head to the detector for end-to-end training <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b53">55]</ref>. However, they still learn the appearance similarity following the practice in image similarity learning, then measure the instance similarity by cosine distance. That is, they train the model either as a n-classes classification problem <ref type="bibr" target="#b45">[47]</ref> where n equals to the number of identities in the whole training set or using triplet loss <ref type="bibr" target="#b17">[19]</ref>. The classification problem is hard to extend to large-scale datasets, while the triplet loss only compares each training sample with two other identities. These rudimentary training samples and objectives leave instance similarity learning not fully explored in MOT. Meanwhile, they still heavily rely on motion models and displacement predictions to track objects, and the appearance similarity only takes the secondary role.</p><p>In contrast to these methods, QDTrack learns the instance similarity from dense-connected contrastive pairs and associates objects from the feature space with a simple nearest neighbor search. QDTrack has higher performance but with a simpler framework. The promising results prove the power of quasi-dense similarity learning in multiple object tracking. Contrastive learning Contrastive learning and its variants <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b47">49]</ref> have shown promising performance in self-supervised representation learning. However, it does not draw much attention when learning the instance similarity in multiple object tracking. In this paper, we  <ref type="figure">Figure 2</ref>: The training pipeline of our method. We apply dense matching between quasi-dense samples on the pair of images and optimize the network with multiple positive contrastive learning.</p><p>supervise dense matched quasi-dense samples with multiple positive contrastive learning by the inspiration of <ref type="bibr" target="#b42">[44]</ref>.</p><p>In contrast to these image-level contrastive methods, our method allows multiple positive training, while these methods can only handle the case when there is only one positive target. The promising results of our method shall draw the attention to contrastive learning in the multiple object tracking community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We propose quasi-dense similarity learning to learn the feature embedding space that can associate identical objects and distinguish different objects for online multiple object tracking. We define dense matching to be matching between box candidates at all pixels, and quasi-dense means only considering the potential object candidates at informative regions. Accordingly, sparse matching means the method only considers ground truth labels as matching candidates when learning object association. The main ingredients of using quasi-dense matching for multiple object tracking are object detection, instance similarity learning, and object association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object detection</head><p>Our method can be easily coupled with most existing detectors with end-to-end training. In this paper, we take Faster R-CNN <ref type="bibr" target="#b37">[39]</ref> with Feature Pyramid Network (FPN) <ref type="bibr" target="#b23">[25]</ref> as an example, while we can also apply other detectors with minor modifications. Faster R-CNN is a two-stage detector that uses Region Proposal Network (RPN) to generate Region of Interests (RoIs). It then localizes and classifies the regions to obtain semantic labels and locations. Based on Faster R-CNN, FPN exploits lateral connections to build the top-down feature pyramid and tackles the scale-variance problem. The entire network is optimized with a multi-task loss function</p><formula xml:id="formula_0">L det = L rpn + ? 1 L cls + ? 2 L reg ,<label>(1)</label></formula><p>where the RPN loss L rpn , classification loss L cls , regression loss L reg remain the same as the original paper <ref type="bibr" target="#b37">[39]</ref>. The loss weights ? 1 and ? 2 are set to 1.0 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quasi-dense similarity learning</head><p>We use the region proposals generated by RPN to learn the instance similarity with quasi-dense matching. As shown in <ref type="figure">Figure 2</ref>, given a key image I 1 for training, we randomly select a reference image I 2 from its temporal neighborhood. The neighbor distance is constrained by an interval k, where k ? [?3, 3] in our experiments. We use RPN to generate RoIs from the two images and RoI Align <ref type="bibr" target="#b13">[15]</ref> to obtain their feature maps from different levels in FPN according to their scales <ref type="bibr" target="#b23">[25]</ref>. We add an extra lightweight embedding head, in parallel with the original bounding box head, to extract features for each RoI. An RoI is defined as positive to an object if they have an IoU higher than ? 1 , or negative if they have an IoU lower than ? 2 . ? 1 and ? 2 are 0.7 and 0.3 in our experiments. The matching of RoIs on two frames is positive if the two regions are associated with the same object and negative otherwise.</p><p>Assume there are V samples on the key frame as training samples and K samples on the reference frame as contrastive targets. For each training sample, we can use the non-parametric softmax <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b47">49]</ref> with cross-entropy to optimize the feature embeddings  <ref type="figure">Figure 3</ref>: The testing pipeline of our method. We maintain the matching candidates and use bi-softmax to measure the instance similarity so that we can associate objects with a simple nearest neighbour search in the feature space.</p><formula xml:id="formula_1">L embed = ?log exp(v ? k + ) exp(v ? k + ) + k ? exp(v ? k ? ) ,<label>(2)</label></formula><p>where v, k + , k ? are feature embeddings of the training sample, its positive target, and negative targets in K. The overall embedding loss is averaged across all training samples, but we only illustrate one training sample for simplicity. We apply dense matching between RoIs on the pairs of images, namely, each sample on I 1 is matched to all samples on I 2 , in contrast to only using sparse sample crops, mostly ground truth boxes, to learn instance similarity in previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">19]</ref>. Each training sample on the key frame has more than one positive targets on the reference frame, so Eq. (2) can be extended as</p><formula xml:id="formula_2">L embed = ? k + log exp(v ? k + ) exp(v ? k + ) + k ? exp(v ? k ? ) .<label>(3)</label></formula><p>However, this equation does not treat positive and negative targets fairly. Namely, each negative one is considered multiple times while only once for positive counterparts. Alternatively, we can first reformulate Eq. <ref type="formula" target="#formula_1">(2)</ref> as</p><formula xml:id="formula_3">L embed = log[1 + k ? exp(v ? k ? ? v ? k + )].<label>(4)</label></formula><p>Then in the multi-positive scenario, it can be extended by accumulating the positive term as</p><formula xml:id="formula_4">L embed = log[1 + k + k ? exp(v ? k ? ? v ? k + )].<label>(5)</label></formula><p>We further adopt L2 loss as an auxiliary loss</p><formula xml:id="formula_5">L aux = ( v ? k ||v|| ? ||k|| ? c) 2 ,<label>(6)</label></formula><p>where c is 1 if the match of two samples is positive and 0 otherwise. Note the auxiliary loss aims to constrain the logit magnitude and cosine similarity instead of improving the performance.</p><p>The entire network is joint optimized under</p><formula xml:id="formula_6">L = L det + ? 1 L embed + ? 2 L aux ,<label>(7)</label></formula><p>where ? 1 and ? 2 are set to 0.25 and 1.0 by default in this paper. We sample all positive pairs and three times more negative pairs to calculate the auxiliary loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object association</head><p>Tracking objects across frames purely based on object feature embeddings is not trivial. For example, if an object has no target or more than one target during matching, the nearest search will be ambiguous. In other words, an object should have only one target in the matching candidates. However, the actual tracking process is complex. The false positives, id switches, newly appeared objects, and terminated tracks all increase the matching uncertainty. We observe that our inference strategy, including ways of maintaining the matching candidates and measuring the instance similarity, can mitigate these problems. Bi-directional softmax Our main inference strategy is bidirectional matching in the embedding space. <ref type="figure">Figure 3</ref> shows our testing pipeline. Assume there are N detected objects in frame t with feature embeddings n, and M matching candidates with feature embeddings m from the past x frames, the similarity f between the objects and matching candidates is obtained by bi-directional softmax (bi-softmax):</p><formula xml:id="formula_7">f(i, j) = [ exp(n i ? m j ) M ?1 k=0 exp(n i ? m k ) + exp(n i ? m j ) N ?1 k=0 exp(n k ? m j ) ]/2.</formula><p>(8) The high score under bi-softmax will satisfy a bi-directional consistency. Namely, the two matched objects should be No target cases Objects without a target in the feature space should not be matched to any candidates. Newly appeared objects, vanished tracks, and some false positives fall into this category. The bi-softmax can tackle this problem directly, as it is hard for these objects to obtain bi-directional consistency, leading to low matching scores. If a newly detected object has high detection confidence, it can start a new track. Moreover, previous methods often directly drop the objects that do not match any tracks. We argue that despite most of them are false positives, they are still useful regions that the following objects are likely to match. We name these unmatched objects backdrops and keep them during matching. Experiments show that backdrops can reduce the number of false positives.</p><p>Multi-targets cases Most state-of-the-art detectors only do intra-class duplicate removal by None Maximum Suppression (NMS). Consequently, some objects at the same locations might have different categories. In most cases, only one of these objects is true positive while the others not. This process can boost the object recall and contribute to a high mean Average Precision (mAP) <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b24">26]</ref>. However, it will create duplicate feature embeddings. To handle this issue, we do inter-class duplicate removal by NMS. The IoU threshold for NMS is 0.7 for objects with high detection confidence (larger than 0.5) and 0.3 for objects with low detection confidence (lower than 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments not only on the MOT <ref type="bibr" target="#b28">[30]</ref> benchmark but also on the other brand-new large-scale benchmarks including BDD100K <ref type="bibr" target="#b51">[53]</ref>, Waymo <ref type="bibr" target="#b41">[43]</ref>, and TAO <ref type="bibr" target="#b7">[9]</ref>. We hope our efforts can facilitate future multiple object tracking research to benefit from these large-scale datasets. We also show the generalization ability of our method on BDD100K segmentation tracking benchmark. More results, such as oracle analyses and failure case analyses are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOT Challenge</head><p>We perform experiments on two MOT benchmarks: MOT16 and MOT17 <ref type="bibr" target="#b28">[30]</ref>. The dataset contains 7 videos (5,316 images) for training and 7 videos (5,919 images) for testing. Only pedestrians are evaluated in this benchmark. The video frame rate is 14 -30 FPS. BDD100K We use BDD100K <ref type="bibr" target="#b51">[53]</ref>   <ref type="bibr" target="#b7">[9]</ref> annotates 482 classes in total, which are the subset of LVIS dataset <ref type="bibr" target="#b10">[12]</ref>. It has 400 videos, 216 classes in the training set, 988 videos, 302 classes in the validation set, and 1419 videos, 369 classes in the test set. The classes in train, validation, and test sets may not overlap. The videos are annotated in 1 FPS. The objects in TAO are in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We use ResNet-50 <ref type="bibr" target="#b14">[16]</ref> as the backbone by default in this paper. We select 128 RoIs from the key frame as training samples, and 256 RoIs from the reference frame with a positive-negative ratio of 1.0 as contrastive targets. We use IoU-balanced sampling <ref type="bibr" target="#b34">[36]</ref> to sample RoIs. We use 4conv-1fc head with group normalization <ref type="bibr" target="#b46">[48]</ref> to extract feature embeddings. The channel number of embedding features is set to 256 by default. We train our models with a total batch size of 16 and an initial learning rate of 0.02 for 12 epochs. We decrease the learning rate by 0.1 after 8 and 11 epochs.</p><p>Here, we first talk about the common practices if not specified mentioned afterwards. We use the original scale of the images for training and inference. We do not use any other data augmentation methods except random horizontal flipping. We use a model pre-trained on ImageNet for training. When conducting online joint object detection and tracking, we initialize a new track if its detection confidence is higher than 0.8. The backdrops are only kept for one frame. The objects can be associated only when they are classified as the same category.</p><p>For fair comparison with recent works, we follow the 1 https://github.com/cheind/py-motmetrics practice <ref type="bibr" target="#b44">[46]</ref> on MOT17 that randomly resizes and crops the longer side of the images to 1088 and does not change the aspect ratio at the training and inference time. Other data augmentation includes random horizontal flipping and color jittering, which is the common practice in <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b54">56]</ref>. We do not use extra data for training except a pre-trained model from COCO. Note that COCO is not considered as additional training data by the official rules and widely used in most methods. On TAO, we randomly select a scale between 640 to 800 to resize the shorter side of images during training. At inference time, the shorter side of the images are resized to 800. We use a LVIS <ref type="bibr" target="#b10">[12]</ref> pre-trained model, consistent with the implementation of <ref type="bibr" target="#b7">[9]</ref>. However, we observe severe over-fitting problem when training on the training videos of TAO, which hurts the detection performance. So we freeze the detection model and only fine-tune the embedding head to extract instance representations.</p><p>More details such as more hyper-parameters and momentum updating are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main results</head><p>Our method outperforms all existing methods on aforementioned benchmarks without bells and whistles. The performance are evaluated with the official metrics. MOT The results with private detectors on MOT16 and  MOT17 benchmarks are shown in <ref type="table" target="#tab_1">Table 1</ref>. Our model achieves the best MOTA of 68.7% and IDF1 of 66.3% on the MOT17. We outperform the state-of-the-art tracker Center-Track <ref type="bibr" target="#b54">[56]</ref> by 0.9 points on MOTA and 1.6 points on IDF1 respectively. Our method does not achieve a relatively low ID Sw. because we have a higher recall. The number of ID Sw. will likely increase when we have more tracks. This is also why the results with public detectors, which are shown in the supplementary material, have lower IDs, because their recall are lower (FN is higher). BDD100K The main results on BDD100K tracking validation and testing sets are in <ref type="table" target="#tab_3">Table 2</ref>. The mMOTA and mIDF1, which represent object coverage and identity consistency respectively, are 36.6% and 50.8% on the validation set, and 35.5% and 52.3% on the testing set. On the two sets, our method outperforms the baseline benchmark method by 10.7 points and 9.2 points in terms of mMOTA, and 6.3 points and 7.6 points in terms of mIDF1 respectively. We also outperform the champion of BDD100K 2020 MOT Challenge (madamada) by a large margin but with a simpler detector. The significant advancements demonstrate that our method enables more stable object tracking.</p><p>Waymo <ref type="table" target="#tab_4">Table 3</ref> shows our main results on Waymo open dataset. We report the results on the validation set following the setup of RetinaTrack <ref type="bibr" target="#b26">[28]</ref>, which only conduct experiments on the vehicle class. We also report the overall performance for future comparison. We report the results on the test set via official rules. Our method outperforms all baselines on both validation set and test set. We obtain a MOTA of 44.0% and a IDF1 of 56.8% on the validation set. We also obtain a MOTA/L1 of 49.40% and a MOTA/L2 of 43.88% on the test set. The performance of vehicle on the validation set is 10.7, 13.0, and 17.4 points higher than RetinaTrack <ref type="bibr" target="#b26">[28]</ref>, Tracktor++ <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">28]</ref>, and IoU baseline <ref type="bibr" target="#b26">[28]</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>We conduct ablation studies on BDD100K validation set, where we investigate the importance of the major model components for training and testing procedures. Importance of quasi-dense matching The results are presented in the top sub <ref type="table" target="#tab_5">-table of Table 4</ref>. MOTA and IDF1 are calculated over all instances without considering categories as overall evaluations. We use cosine distance to calculate the similarity scores during the inference procedure. Compared to learning with sparse ground truths, quasi-dense tracking improves the overall IDF1 by 4.8 points (63.0% to 67.8%). The significant improvement on IDF1 indicates quasi-dense tracking greatly improves the feature embeddings and enables more accurate associations.</p><p>We then analyze the improvements in detail. In the table, we can observe that when we match each training sample  to more negative samples and train the feature space with Eq. (2), the IDF1 is significantly improved by 3.4 points. This improvement contributes 70% to the total improved 4.8 points IDF1. This experiment shows that more contrastive targets, even most of them are negative samples, can improve the feature learning process. The multiple-positive contrastive learning following Equation (5) further improves the IDF1 by 1 point (66.8% to 67.8%). Importance of bi-softmax We investigate how different inference strategies influence the performance. As shown in the bottom part of <ref type="table" target="#tab_5">Table 4</ref>, replacing cosine similarity by bi-softmax improves overall IDF1 by 2.2 points and the IDF1 of pedestrian by 4.5 points. This experiment also shows that the one-to-one constraint further strengthens the estimated similarity. Importance of matching candidates Duplicate removal and backdrops improve IDF1 by 1.5 points. Overall, our training and inference strategies improve the IDF1 by 8.5 points (63.0% to 71.5%). The total number of ID switches is decreased by 30%. Especially, the MOTA and IDF1 of pedestrian are improved by 9.1 points and 10.5 points respectively, which further demonstrate the power of quasi-dense contrastive learning. Combinations with motion and location Finally, we try to add the location and motion priors to understand whether they are still helpful when we have good feature embeddings for similarity measure. These experiments follow the procedures in Tracktor <ref type="bibr" target="#b1">[2]</ref> and use the same detector for fair comparisons. As shown in <ref type="table" target="#tab_6">Table 5</ref>, without appearance features, the tracking performance is consistently improved with the introduction of additional information. However, these cues barely enhance the performance of our approach. Our method yields the best results when only using appearance embeddings. The results indicate that our instance feature embeddings are sufficient for multiple object tracking with the effective quasi-dense matching, which greatly simplify the testing pipeline. Inference speed To understand the runtime efficiency, we profile our method on NVIDIA Tesla V100. Because it only adds a lightweight embedding head to Faster R-CNN, our method only bring marginal inference cost overhead. With an input size of 1296 ? 720 and a ResNet-50 backbone on BDD100K, the inference FPS is 16.4. With an input size </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Embedding visualizations</head><p>We use t-SNE to visualize the embeddings trained with sparse matching and our quasi-dense matching and show them in <ref type="figure" target="#fig_3">Figure 4</ref>. The instances are selected from a video in BDD100K tracking validation set. The same instance is shown with the same color. We observe that it is easier to separate objects in the feature space of quasi-dense matching. More visualizations are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Segmentation tracking</head><p>We show the generalization ability of our method by extending it to instance segmentation tracking. BDD100K provides a subset for the segmentation tracking task. There are 154 videos in the training set, 32 videos in the validation set, and 37 videos in the test set. <ref type="table" target="#tab_8">Table 6</ref> shows the results on BDD100K segmentation tracking task. The results on the validation set are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present QDTrack, a tracking method based on quasidense matching for instance similarity learning. In contrast to previous methods that use sparse ground-truth matching as similarity supervision, we learn instance similarity from hundreds of region proposals on pairs of images, and train the feature embeddings with multiple positive contrastive learning. In the resulting feature space, a simple nearest neighbor search can distinguish instances without bells and whistles. Our method can be easily coupled with most of the existing detectors and trained end-to-end for multiple object tracking and segmentation tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, we present additional experiments, investigate oracle performance, analyze failure cases, and show some patch visualizations. In this supplementary material, we present detailed configuration of the tracker, additional experiments and ablation studies. We also investigate oracle performance, analyze failure cases, and show some patch visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyper-parameters</head><p>We show the configuration of our tracker in Algorithm 1. Some of the parameters, such as number of frames to keep backdrops and the matching metric, are fixed. For more details, please refer to our released source code. Dataset specific parameters. Our object association only relies on appearance, so it is robust to different motion patterns in different datasets. The experiments share the same tracking parameters except TAO, because TAO uses 3D mAP, instead of CLEAR MOT metrics, for evaluation. On TAO, the terms "init_score_thr" and "obj_score_thr" are set to 0.0001 to obtain a high recall. Considering the numerous tracks with these thresholds, we do not maintain backdrops in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supplementary experiments</head><p>MOT17 with public detectors Following the strategy in Tracktor <ref type="bibr" target="#b1">[2]</ref> and CenterTrack <ref type="bibr" target="#b54">[56]</ref>, we evaluate our method with public detectors on MOT17. That is, a new trajectory is only initialized from a public detection bounding box. As shown in <ref type="table" target="#tab_10">Table 7</ref>, our method outperforms existing results by a large margin. Our method outperforms CenterTrack by 3.1 points on MOTA and 5.5 points on IDF1. TAO <ref type="table" target="#tab_11">Table 8</ref> presents detailed results on the TAO <ref type="bibr" target="#b7">[9]</ref> dataset. Although QDTrack does not perform zero-shot and few-shot learning for the long-tail categories, our method is still a stronger baseline method on this dataset and paves the way for future studies. BDD100K Segmentation Tracking The results on the BDD100K segmentation tracking validation set are presented in <ref type="table" target="#tab_12">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional ablation studies</head><p>Momentum of the embeddings. Assume there is an existing track and its embedding is E 0 . This track is associated to an object on the current frame and its embedding is E 1 . The new embedding of this track will be m * E 1 + (1 ? m) * E 0 , where m is the momentum. The momentum does not improve the results too much but it considers the history of embeddings. We show the ablation studies of different values of momentum in <ref type="table" target="#tab_1">Table 10</ref>. Sensitivity of the ? 1 and ? 2 in Eq. 7. We found ? 2 does not change the final results while ? 1 does. If ? 1 is higher than 0.5, the performance will drop, but does not matter if it is lower than 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Oracle analysis</head><p>We investigate the performances of two types of oracles: detection oracle and tracking oracle on BDD100K tracking validation set. For detection oracle, we directly extract feature embeddings of the ground truth objects in each frame and associate them using our method. For tracking oracle,    we use ground truth tracking labels to associate the detected objects.</p><p>Detection oracle The results are shown in <ref type="table" target="#tab_1">Table 11</ref>. We can observe that all MOTAs are higher than 94%, and some of them are even close to 100%. This is because we use the ground truth boxes directly so that the number of false negatives and false positives are close to 0. The metric IDF1 and ID Switches can measure the performance of identity consistency. The average IDF1 over the 8 classes is 88.8%, which is 38 points higher than our result. The gaps on classes "car" and "pedestrain" are only 11.1 points and 19.3 points between oracle results and our results respectively, while gaps on other classes are exceeding 30 points. These results show that if highly accurate detection results are provided, our method can obtain robust feature embeddings and associate objects effectively. However, the huge performance gaps also indicate the demand of promoting detection algorithms in the video domain. We also notice that the total number of ID switches in the oracle experiment is higher than ours. This is due to the high object recalls in the oracle experiments, as more detected instances may introduce more ID switches accordingly. Tracking oracle The results are shown in <ref type="table" target="#tab_1">Table 12</ref>. We can observe that when associating object directly with tracking labels, the mIDF1 is only boosted by 4.3 points. This promising oracle analysis shows the effectiveness of our method and indicates that our method is bounded more by detection performance than tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Failure case analysis</head><p>Our method can distinguish different instances even they are similar in appearance. However, there are still some failure cases. We show them below with figures, in which we use yellow color to represent false negatives, red color to represent false positives, and cyan color to represent ID switches. The float number at the corner of each box indicates the detection score, while the integer indicates the object identity number. We use green dashed box to highlight the objects we want to emphasize. Object classification Inaccurate classification confidence is the main distraction for the association procedure because false negatives and false positives destroy the one-to-one matching constraint. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the false negatives are mainly small objects or occluded objects under crowd scenes. The false positives are objects that have similar appearances to annotated objects, such as persons in the mirror or advertising board, etc.</p><p>Inaccurate object category is a less frequent distraction caused by classification. The class of the instance may switch between different categories, which mostly belong to the same super-category. <ref type="figure" target="#fig_5">Figure 6</ref> shows an example. The category of the highlighted object changes from "rider" to "pedestrian" when the bicycle is occluded. Our method fails in this case because we require the associated objects have the same category.</p><p>These failure cases caused by object classification suggest the improvements on video object detection algorithms. We can exploit temporal or tracking information to improve the detectors, thus obtaining better tracking performance. Object truncation/occlusion Object truncation/occlusion causes inaccurate object localization. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, the highlighted objects are truncated by other objects. The detector detects two objects. One of them is a false positive box that only covers a part of the object. The other one is a box with a lower detection score but covers the entire object. This case may influence the association process if the two boxes have similar feature embeddings.</p><p>An instance may have totally different appearances before and after occlusion that result in low similarity scores. As shown in <ref type="figure">Figure 8</ref>, only the front of the car appears before occlusion, while only the rear of the car appears after occlusion. Our method can associate two boxes if they cover the same discriminative regions of an object, not necessarily the exact same region. However, if two boxes cover totally different regions of the object, they will have a low matching score.  Another corner case is the extreme high-level truncation. As shown in <ref type="figure">Figure 9</ref>, the highly truncated objects only appear a little when they just enter or leave the camera view. We cannot distinguish different instances effectively according to the limited appearance information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Visualizations</head><p>We show the visualizations of different instance patches during the testing procedure in <ref type="figure" target="#fig_0">Figure 10</ref>. The detected objects in each frame are matched to prior objects via bidirectional softmax. The prior objects include tracks in the consecutive frame, vanished tracks, and backdrops. We annotate them with different colors. Each detected object is enclosed by the same color of its matched object. We can observe that most false positives in the current frame are matched to backdrops, which demonstrates keeping backdrops during the matching procedure helps reduce the number of false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Qualitative results</head><p>We show some qualitative results of our method on BDD100K dataset and MOT17 dataset in <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref> respectively. The results are sampled from a certain interval for illustrative purposes.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Traditional ReID model that decouples with detector and learns with sparse ID loss; (b) joint learning ReID model with sparse ID loss; (c) joint learning ReID model with sparse triplet loss; (d) our quasi-dense similarity learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualizations of instance embeddings with (a) sparse matching and (b) quasi-dense matching using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Failure cases caused by inaccurate classification confidences. The objects enclosed by yellow rectangles are false negatives, and the objects enclosed by red rectangles are false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Failure case caused by inaccurate object category. The category of the highlighted object changes from "rider" to "pedestrian" due to the occlusion of the bicycle. They cannot be associated because they do not satisfy the category consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Inaccurate object localization caused by truncation. The red false positive box only covers part of the object, while the yellow box covers the entire object. They may have similar feature embeddings thus influencing the association procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>[ 8 ] 1 Figure 8 :</head><label>818</label><figDesc>Wongun Choi and Silvio Savarese. Multiple target tracking in world coordinate with single, minimally calibrated camera. In European Conference on Computer Vision, 2010. Two detected objects in different frames cover totally different regions of the object thus having low appearance similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Our method cannot distinguish different instances effectively according to the limited appearance information in highly truncated objects. The visualizations of different instance patches during the testing procedure. The detected objects in the current frame are matched to tracklets in the consecutive frame, vanished tracklets, and backdrops via bi-directional softmax Qualitative results of our method on BDD100K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Qualitative results of our method on MOT17 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on MOT16 and MOT17 test set with private detectors. Note that we do not use extra data for training. ? means higher is better, ? means lower is better. * means external data besides COCO and ImageNet is used.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>MOTA ?</cell><cell>IDF1 ?</cell><cell>MOTP ?</cell><cell>MT ?</cell><cell>ML ?</cell><cell>FP ?</cell><cell>FN ?</cell><cell>IDs ?</cell></row><row><cell></cell><cell>TAP [57]</cell><cell>64.8</cell><cell>73.5</cell><cell>78.7</cell><cell>292</cell><cell>164</cell><cell>12980</cell><cell>50635</cell><cell>571</cell></row><row><cell></cell><cell>CNNMTT [29]</cell><cell>65.2</cell><cell>62.2</cell><cell>78.4</cell><cell>246</cell><cell>162</cell><cell>6578</cell><cell>55896</cell><cell>946</cell></row><row><cell>MOT16</cell><cell>POI  *  [54] TubeTK_POI  *  [35]</cell><cell>66.1 66.9</cell><cell>65.1 62.2</cell><cell>79.5 78.5</cell><cell>258 296</cell><cell>158 122</cell><cell>5061 11544</cell><cell>55914 47502</cell><cell>3093 1236</cell></row><row><cell></cell><cell>CTrackerV1 [37]</cell><cell>67.6</cell><cell>57.2</cell><cell>78.4</cell><cell>250</cell><cell>175</cell><cell>8934</cell><cell>48305</cell><cell>1897</cell></row><row><cell></cell><cell>Ours</cell><cell>69.8</cell><cell>67.1</cell><cell>79.0</cell><cell>316</cell><cell>150</cell><cell>9861</cell><cell>44050</cell><cell>1097</cell></row><row><cell></cell><cell>Tracktor++v2 [2]</cell><cell>56.3</cell><cell>55.1</cell><cell>78.8</cell><cell>498</cell><cell>831</cell><cell>8866</cell><cell>235449</cell><cell>1987</cell></row><row><cell></cell><cell>Lif_T  *  [20]</cell><cell>60.5</cell><cell>65.6</cell><cell>78.3</cell><cell>637</cell><cell>791</cell><cell>14966</cell><cell>206619</cell><cell>1189</cell></row><row><cell>MOT17</cell><cell>TubeTK  *  [35] CTrackerV1 [37]</cell><cell>63.0 66.6</cell><cell>58.6 57.4</cell><cell>78.3 78.2</cell><cell>735 759</cell><cell>468 570</cell><cell>27060 22284</cell><cell>177483 160491</cell><cell>4137 5529</cell></row><row><cell></cell><cell>CenterTrack  *  [56]</cell><cell>67.8</cell><cell>64.7</cell><cell>78.4</cell><cell>816</cell><cell>579</cell><cell>18498</cell><cell>160332</cell><cell>3039</cell></row><row><cell></cell><cell>Ours</cell><cell>68.7</cell><cell>66.3</cell><cell>79.0</cell><cell>957</cell><cell>516</cell><cell>26589</cell><cell>146643</cell><cell>3378</cell></row><row><cell cols="4">each other's nearest neighbor in the embedding space. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">instance similarity f can directly associate objects with a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">simple nearest neighbor search.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>detection training set and tracking training set for training, and tracking validation/testing set for testing. It annotates 8 categories for evaluation. The detection set has 70,000 images. The tracking set has 1,400 videos (278k images) for training, 200 videos (40k images) for validation, and 400 videos (80k images) for testing. The images in the tracking set are annotated per 5 FPS with a 30 FPS video frame rate. Waymo Waymo open dataset [43] contains images from 5 cameras associated with 5 different directions: front, front left, front right, side left, and side right. There are 3,990 videos (790k images) for training, 1,010 videos (200k images) for validation, and 750 videos (148k images) for testing. It annotates 3 classes for evaluation. The videos are annotated in 10 FPS. TAO TAO dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on BDD100K tracking validation and test set. Our method outperforms all methods on this benchmark.MethodSplitmMOTA ? mIDF1 ? MOTA ? IDF1 ? FN ? FP ? ID Sw. ? MT ? ML ? mAP ?</figDesc><table><row><cell>Yu et al. [53]</cell><cell>val</cell><cell>25.9</cell><cell>44.5</cell><cell>56.9</cell><cell>66.8</cell><cell>122406 52372</cell><cell>8315</cell><cell>8396</cell><cell>3795</cell><cell>28.1</cell></row><row><cell>Ours</cell><cell>val</cell><cell>36.6</cell><cell>50.8</cell><cell>63.5</cell><cell>71.5</cell><cell>108614 46621</cell><cell>6262</cell><cell>9481</cell><cell>3034</cell><cell>32.6</cell></row><row><cell cols="2">Yu et al. [53] test</cell><cell>26.3</cell><cell>44.7</cell><cell>58.3</cell><cell>68.2</cell><cell>213220 100230</cell><cell>14674</cell><cell cols="2">16299 6017</cell><cell>27.9</cell></row><row><cell>DeepBlueAI</cell><cell>test</cell><cell>31.6</cell><cell>38.7</cell><cell>56.9</cell><cell>56.0</cell><cell>292063 35401</cell><cell>25186</cell><cell cols="2">10296 12266</cell><cell>-</cell></row><row><cell>madamada</cell><cell>test</cell><cell>33.6</cell><cell>43.0</cell><cell>59.8</cell><cell>55.7</cell><cell>209339 76612</cell><cell>42901</cell><cell cols="2">16774 5004</cell><cell>-</cell></row><row><cell>Ours</cell><cell>test</cell><cell>35.5</cell><cell>52.3</cell><cell>64.3</cell><cell>72.3</cell><cell>201041 80054</cell><cell>10790</cell><cell cols="2">17353 5167</cell><cell>31.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on Waymo tracking validation set using py-motmetrics library (top)<ref type="bibr" target="#b0">1</ref> and test set using official evaluation. * indicates methods using undisclosed detectors.</figDesc><table><row><cell>Method</cell><cell cols="2">Split Category</cell><cell>MOTA ?</cell><cell>IDF1 ?</cell><cell>FN ?</cell><cell>FP ?</cell><cell>ID Sw. ?</cell><cell>MT ?</cell><cell>ML ?</cell><cell>mAP ?</cell></row><row><cell>IoU baseline [28]</cell><cell>val</cell><cell>Vehicle</cell><cell>38.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.78</cell></row><row><cell>Tracktor++ [2, 28]</cell><cell>val</cell><cell>Vehicle</cell><cell>42.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.41</cell></row><row><cell>RetinaTrack [28]</cell><cell>val</cell><cell>Vehicle</cell><cell>44.92</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.70</cell></row><row><cell>Ours</cell><cell>val</cell><cell>Vehicle</cell><cell>55.6</cell><cell>66.2</cell><cell>514548</cell><cell>214998</cell><cell>24309</cell><cell>17595</cell><cell>5559</cell><cell>49.5</cell></row><row><cell>Ours</cell><cell>val</cell><cell>All</cell><cell>44.0</cell><cell>56.8</cell><cell>674064</cell><cell>264886</cell><cell>30712</cell><cell>21410</cell><cell>7510</cell><cell>40.1</cell></row><row><cell>Tracktor [22, 43]</cell><cell>test</cell><cell>Vehicle</cell><cell>34.80</cell><cell>10.61</cell><cell>14.88</cell><cell>39.71</cell><cell>28.29</cell><cell>8.63</cell><cell>12.10</cell><cell>50.98</cell></row><row><cell>CascadeRCNN-SORTv2*</cell><cell>test</cell><cell>All</cell><cell>50.22</cell><cell>7.79</cell><cell>2.71</cell><cell>39.28</cell><cell>44.15</cell><cell>6.94</cell><cell>2.44</cell><cell>46.46</cell></row><row><cell>HorizonMOT*</cell><cell>test</cell><cell>All</cell><cell>51.01</cell><cell>7.52</cell><cell>2.44</cell><cell>39.03</cell><cell>45.13</cell><cell>7.13</cell><cell>2.25</cell><cell>45.49</cell></row><row><cell>Ours (ResNet-50)</cell><cell>test</cell><cell>All</cell><cell>49.40</cell><cell>7.41</cell><cell>1.46</cell><cell>41.74</cell><cell>43.88</cell><cell>7.10</cell><cell>1.31</cell><cell>48.21</cell></row><row><cell cols="2">Ours (ResNet-101 + DCN) test</cell><cell>All</cell><cell>51.18</cell><cell>7.64</cell><cell>1.45</cell><cell>39.73</cell><cell>45.09</cell><cell>7.20</cell><cell>1.31</cell><cell>46.41</cell></row></table><note>Method Split Category MOTA/L1 ? FP/L1 ? MisM/L1 ? Miss/L1 ? MOTA/L2 ? FP/L2 ? MisM/L2 ? Miss/L2 ?a long-tailed distribution that half of the objects are person and 1 / 6 of the objects are car.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on quasi-dense matching and the inference strategy on the BDD100K tracking validation set. All models are comparable on detection performance. D. R. means duplicate removal. (P) means results of the class "pedestrian".</figDesc><table><row><cell cols="2">Quasi-Dense one-positive multi-positive</cell><cell>Metric</cell><cell cols="8">Matching candidates MOTA ? IDF1 ? mMOTA ? mIDF1 ? MOTA(P) ? IDF1(P) ? D. R. Backdrops</cell></row><row><cell>-</cell><cell>-</cell><cell>cosine</cell><cell>-</cell><cell>-</cell><cell>60.4</cell><cell>63.0</cell><cell>34.0</cell><cell>47.9</cell><cell>37.6</cell><cell>49.7</cell></row><row><cell></cell><cell>-</cell><cell>cosine</cell><cell>-</cell><cell>-</cell><cell>61.5</cell><cell>66.8</cell><cell>35.5</cell><cell>50.0</cell><cell>40.5</cell><cell>52.7</cell></row><row><cell>-</cell><cell></cell><cell>cosine</cell><cell>-</cell><cell>-</cell><cell>62.5</cell><cell>67.8</cell><cell>36.2</cell><cell>50.0</cell><cell>44.0</cell><cell>54.3</cell></row><row><cell>-</cell><cell></cell><cell>bi-softmax</cell><cell>-</cell><cell>-</cell><cell>62.9</cell><cell>70.0</cell><cell>35.4</cell><cell>48.5</cell><cell>45.5</cell><cell>58.8</cell></row><row><cell>-</cell><cell></cell><cell>bi-softmax</cell><cell></cell><cell>-</cell><cell>63.2</cell><cell>70.1</cell><cell>36.4</cell><cell>50.4</cell><cell>45.5</cell><cell>58.3</cell></row><row><cell>-</cell><cell></cell><cell>bi-softmax</cell><cell></cell><cell></cell><cell>63.5</cell><cell>71.5</cell><cell>36.6</cell><cell>50.8</cell><cell>46.7</cell><cell>60.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+3.1</cell><cell>+8.5</cell><cell>+2.6</cell><cell>+2.9</cell><cell>+9.1</cell><cell>+10.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablations studies on location and motion cues on the BDD100K tracking validation set.</figDesc><table><row><cell cols="5">Appearance IoU Motion Regression mMOTA ? mIDF1 ?</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.3</cell><cell>36.0</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>27.7</cell><cell>38.5</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>28.6</cell><cell>39.3</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.6</cell><cell>50.8</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>36.3</cell><cell>49.8</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>36.4</cell><cell>49.9</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>36.4</cell><cell>50.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The results are 2.9 points and 2.2 points higher than TAO's solid baseline, which are 13.2 points and 10.2 points respectively. Although we only boost the overall performance by 2 -3 points, we observe that we outperform the baseline by a large margin on frequent classes, that is, 38.6 points vs. 18.5 points on person. This improvement is buried by the average across the entire hundreds of classes. It shows that the crucial part on TAO is still how to improve the tracking on tail classes, which should be a meaningful direction for further research. Other details are presented in the supplementary material.</figDesc><table /><note>Our model with ResNet-101 and deformable convolution (DCN) has the state-of-the-art performance on the test benchmark which is on par with the champion of Waymo 2020 2D Tracking Challenge (HorizonMOT) but only with a simple single model. TAO We obtain 16.1 points and 12.4 points of AP50 on the validation and test set, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on the BDD100K segmentation tracking test set. I: ImageNet. C: COCO. S: Cityscapes. B: BDD100K. "frozen" means adopting the pretrained model from the BDD100K tracking set and only finetune the mask head.</figDesc><table><row><cell>Method</cell><cell cols="5">Pretrained mMOTSA ? mMOTSP ? mIDF1 ? ID sw. ?</cell></row><row><cell>SORT [4]</cell><cell>I, C, S</cell><cell>12.8</cell><cell>67.3</cell><cell>28.8</cell><cell>3525</cell></row><row><cell>Ours</cell><cell>I, C, S</cell><cell>24.0</cell><cell>66.3</cell><cell>42.5</cell><cell>1581</cell></row><row><cell>Ours (frozen)</cell><cell>I, B</cell><cell>30.8</cell><cell>65.5</cell><cell>50.6</cell><cell>884</cell></row><row><cell cols="6">of 1088 ? 608 and a ResNet-50 backbone on MOT17, the</cell></row><row><cell cols="2">inference FPS is 20.3.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Algorithm 1</head><label>1</label><figDesc>Configuration of the tracker in QDTrack.</figDesc><table><row><cell>tracker=dict(</cell></row><row><cell>type='QuasiDenseEmbedTracker',</cell></row><row><cell># score threshold to start a new track</cell></row><row><cell>init_score_thr=0.8,</cell></row><row><cell># score threshold to continue a track</cell></row><row><cell>obj_score_thr=0.5,</cell></row><row><cell># score threshold for data association</cell></row><row><cell>match_score_thr=0.5,</cell></row><row><cell># number of frames to keep tracks</cell></row><row><cell>memo_tracklet_frames=10,</cell></row><row><cell># number of frames to keep backdrops</cell></row><row><cell>memo_backdrop_frames=1,</cell></row><row><cell># momentum to update the embeddings</cell></row><row><cell>memo_momentum=0.8,</cell></row><row><cell># duplicate removal to tackle multi-targets cases</cell></row><row><cell>nms_backdrop_iou_thr=0.3,</cell></row><row><cell>nms_class_iou_thr=0.7,</cell></row><row><cell># the matching metric</cell></row><row><cell>match_metric='bisoftmax')</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Results on MOT17 test set with public detector. Note that we do not use extra data for training. ? means higher is better, ? means lower is better. * means external data besides COCO and ImageNet is used.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">MOTA ? IDF1 ? MOTP ?</cell><cell>MT ?</cell><cell>ML ?</cell><cell>FP ?</cell><cell>FN ?</cell><cell>IDs ?</cell></row><row><cell></cell><cell>Tracktor++v2 [2]</cell><cell>56.3</cell><cell>55.1</cell><cell>78.8</cell><cell cols="4">498 (21.1) 831 (35.3) 8866 235449 1987 (34.1)</cell></row><row><cell></cell><cell>GSM_Tracktor [27]</cell><cell>56.4</cell><cell>57.8</cell><cell>77.9</cell><cell cols="4">523 (22.2) 813 (34.5) 14379 230174 1485 (25.1)</cell></row><row><cell>Public MOT17</cell><cell>MPNTrack  *  [6] Lif_T  *  [20]</cell><cell>58.8 60.5</cell><cell>61.7 65.6</cell><cell>78.6 78.3</cell><cell cols="4">679 (28.8) 788 (33.5) 17413 213594 1185 (19.1) 637 (27.0) 791 (33.6) 14966 206619 1189 (18.8)</cell></row><row><cell></cell><cell>CenterTrackPub  *  [56]</cell><cell>61.5</cell><cell>59.6</cell><cell>78.9</cell><cell cols="4">621 (26.4) 752 (31.9) 14076 200672 2583 (40.1)</cell></row><row><cell></cell><cell>Ours</cell><cell>64.6</cell><cell>65.1</cell><cell>79.6</cell><cell cols="4">761 (32.3) 666 (28.3) 14103 182998 2652 (39.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Results on TAO challenge benchmark.</figDesc><table><row><cell>Method</cell><cell cols="7">Split AP50 AP75 AP AP50(S) AP50(M) AP50(L)</cell></row><row><cell>SORT_TAO [9]</cell><cell>val</cell><cell>13.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>val</cell><cell>16.1</cell><cell>5.0</cell><cell>7.0</cell><cell>4.8</cell><cell>13.7</cell><cell>20.0</cell></row><row><cell cols="2">SORT_TAO [9] test</cell><cell>10.2</cell><cell>4.4</cell><cell>4.9</cell><cell>7.7</cell><cell>8.2</cell><cell>15.2</cell></row><row><cell>Ours</cell><cell>test</cell><cell>12.4</cell><cell>4.5</cell><cell>5.2</cell><cell>3.7</cell><cell>8.3</cell><cell>18.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="6">: Results on the BDD100K segmentation tracking</cell></row><row><cell cols="6">validation set. I: ImageNet. C: COCO. S: Cityscapes. B:</cell></row><row><cell cols="6">BDD100K. "frozen" means adopting the pretrained model</cell></row><row><cell cols="6">from the BDD100K tracking set and only finetune the mask</cell></row><row><cell>head.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="5">Pretrained mMOTSA ? mMOTSP ? mIDF1 ? ID sw. ?</cell></row><row><cell>SORT [4]</cell><cell>I, C, S</cell><cell>11.4</cell><cell>59.7</cell><cell>22.1</cell><cell>15408</cell></row><row><cell>Ours</cell><cell>I, C, S</cell><cell>20.2</cell><cell>59.3</cell><cell>36.0</cell><cell>1681</cell></row><row><cell>Ours (frozen)</cell><cell>I, B</cell><cell>26.6</cell><cell>64.9</cell><cell>45.3</cell><cell>954</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Ablation studies of momentum of the embeddings on BDD100K tracking validation set. Note the model for this table is re-trained that the results are slightly different from the results in the main paper.</figDesc><table><row><cell>Momentum</cell><cell>mMOTA ?</cell><cell>mIDF1 ?</cell><cell>MOTA ?</cell><cell>IDF1 ?</cell></row><row><cell>0.6</cell><cell>37.0</cell><cell>50.9</cell><cell>63.3</cell><cell>71.4</cell></row><row><cell>0.7</cell><cell>37.0</cell><cell>50.9</cell><cell>63.3</cell><cell>71.3</cell></row><row><cell>0.8</cell><cell>37.0</cell><cell>50.7</cell><cell>63.3</cell><cell>71.1</cell></row><row><cell>0.9</cell><cell>37.0</cell><cell>50.6</cell><cell>63.3</cell><cell>70.8</cell></row><row><cell>1.0</cell><cell>37.0</cell><cell>50.5</cell><cell>63.3</cell><cell>70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Detection oracle analysis. The numbers in the round brackets mean the gaps between oracle results and our results.</figDesc><table><row><cell>Category</cell><cell>Set</cell><cell>MOTA ?</cell><cell>IDF1 ?</cell><cell>MOTP ?</cell><cell>FN ?</cell><cell>FP ?</cell><cell>ID Sw. ?</cell><cell>MT ?</cell><cell>ML ?</cell></row><row><cell>Pedestrian</cell><cell>val</cell><cell>94.3</cell><cell>79.5 (+19.3)</cell><cell>99.8</cell><cell>1</cell><cell>1</cell><cell>3226</cell><cell>3506</cell><cell>0</cell></row><row><cell>Rider</cell><cell>val</cell><cell>95.8</cell><cell>88.5 (+40.4)</cell><cell>99.9</cell><cell>0</cell><cell>0</cell><cell>107</cell><cell>134</cell><cell>0</cell></row><row><cell>Car</cell><cell>val</cell><cell>97.7</cell><cell>86.1 (+11.1)</cell><cell>99.9</cell><cell>0</cell><cell>0</cell><cell>7716</cell><cell>13189</cell><cell>0</cell></row><row><cell>Bus</cell><cell>val</cell><cell>99.2</cell><cell>93.0 (+31.2)</cell><cell>100.0</cell><cell>0</cell><cell>0</cell><cell>72</cell><cell>196</cell><cell>0</cell></row><row><cell>Truck</cell><cell>val</cell><cell>98.8</cell><cell>90.3 (+33.8)</cell><cell>100.0</cell><cell>0</cell><cell>0</cell><cell>340</cell><cell>726</cell><cell>0</cell></row><row><cell>Bicycle</cell><cell>val</cell><cell>88.2</cell><cell>79.5 (+31.8)</cell><cell>98.7</cell><cell>8</cell><cell>8</cell><cell>470</cell><cell>243</cell><cell>0</cell></row><row><cell>Motorcycle</cell><cell>val</cell><cell>97.0</cell><cell>94.5 (+37.8)</cell><cell>99.8</cell><cell>0</cell><cell>0</cell><cell>27</cell><cell>44</cell><cell>0</cell></row><row><cell>Train</cell><cell>val</cell><cell>99.4</cell><cell>98.7 (+98.7)</cell><cell>100.0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>6</cell><cell>0</cell></row><row><cell>All</cell><cell>val</cell><cell>96.3</cell><cell>88.8 (+38.0)</cell><cell>99.8</cell><cell>9</cell><cell>9</cell><cell>11960</cell><cell>18044</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Tracking oracle analysis. The numbers in the round brackets mean the gaps between oracle results and our results.</figDesc><table><row><cell>Category</cell><cell>Set</cell><cell>MOTA ?</cell><cell>IDF1 ?</cell><cell>MOTP ?</cell><cell>FN ?</cell><cell>FP ?</cell><cell>ID Sw. ?</cell><cell>MT ?</cell><cell>ML ?</cell></row><row><cell>Pedestrian</cell><cell>val</cell><cell>54.7</cell><cell>71.2 (+11.0)</cell><cell>77.6</cell><cell>14990</cell><cell>10095</cell><cell>755</cell><cell>1835</cell><cell>367</cell></row><row><cell>Rider</cell><cell>val</cell><cell>31.4</cell><cell>52.6 (+4.5)</cell><cell>76.6</cell><cell>1390</cell><cell>242</cell><cell>115</cell><cell>16</cell><cell>56</cell></row><row><cell>Car</cell><cell>val</cell><cell>74.3</cell><cell>82.9 (+7.9)</cell><cell>84.1</cell><cell>54585</cell><cell>31014</cell><cell>2309</cell><cell>8759</cell><cell>1141</cell></row><row><cell>Bus</cell><cell>val</cell><cell>38.2</cell><cell>65.8 (+4.0)</cell><cell>86.1</cell><cell>3532</cell><cell>2031</cell><cell>57</cell><cell>61</cell><cell>41</cell></row><row><cell>Truck</cell><cell>val</cell><cell>37.0</cell><cell>60.9 (+4.4)</cell><cell>84.7</cell><cell>12719</cell><cell>4259</cell><cell>247</cell><cell>149</cell><cell>239</cell></row><row><cell>Bicycle</cell><cell>val</cell><cell>30.6</cell><cell>55.6 (+7,9)</cell><cell>75.4</cell><cell>2031</cell><cell>714</cell><cell>125</cell><cell>60</cell><cell>58</cell></row><row><cell>Motorcycle</cell><cell>val</cell><cell>14.6</cell><cell>51.7 (-5.0)</cell><cell>76.4</cell><cell>443</cell><cell>292</cell><cell>35</cell><cell>10</cell><cell>18</cell></row><row><cell>Train</cell><cell>val</cell><cell>-0.6</cell><cell>0.0 (+0.0)</cell><cell>0.0</cell><cell>308</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>6</cell></row><row><cell>All</cell><cell>val</cell><cell>35.0</cell><cell>55.1 (+4.3)</cell><cell>70.1</cell><cell>89998</cell><cell>48649</cell><cell>3643</cell><cell>10890</cell><cell>1926</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05625</idno>
		<title level="m">Tracking without bells and whistles</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Tozeto</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Highspeed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Volker Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>AVSS</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tao: A large-scale benchmark for tracking any object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarasha</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to track at 100 FPS with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Swoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-object tracking with neural gating using bilinear LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese CNN for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tracking the trackers: an analysis of the state of the art in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02781</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gsm: Graph similarity model for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Retinatrack: Online single stage joint detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Votel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-target tracking using cnn-based features: Cnnmtt. Multimedia Tools and Applications</title>
		<editor>Nima Mahmoudi, Seyed Mohammad Ahadi, and Mohammad Rahmati</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online multi-target tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finding and tracking people from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10857</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12605</idno>
		<title level="m">Towards real-time multi-object tracking</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An online learned CRF model for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">POI: multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01888</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Online multi-target tracking with tensor-based highorder graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
