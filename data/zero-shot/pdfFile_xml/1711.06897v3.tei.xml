<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Shot Refinement Neural Network for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">GE Global Research</orgName>
								<address>
									<settlement>Niskayuna</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
							<email>xiao.bian@ge.com</email>
							<affiliation key="aff2">
								<orgName type="department">GE Global Research</orgName>
								<address>
									<settlement>Niskayuna</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-Shot Refinement Neural Network for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. Re-fineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multitask loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https: //github.com/sfzhang15/RefineDet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection has achieved significant advances in recent years, with the framework of deep neural networks (DNN). The current DNN detectors of state-of-the-art can be divided into two categories: (1) the two-stage approach, including <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>, and (2) the one-stage approach, including <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref>. In the two-stage approach, a sparse set of candidate object boxes is first generated, and then they are further classified and regressed. The two-stage meth-ods have been achieving top performances on several challenging benchmarks, including PASCAL VOC <ref type="bibr" target="#b7">[8]</ref> and MS COCO <ref type="bibr" target="#b28">[29]</ref>.</p><p>The one-stage approach detects objects by regular and dense sampling over locations, scales and aspect ratios. The main advantage of this is its high computational efficiency. However, its detection accuracy is usually behind that of the two-stage approach, one of the main reasons being due to the class imbalance problem <ref type="bibr" target="#b27">[28]</ref>.</p><p>Some recent methods in the one-stage approach aim to address the class imbalance problem, to improve the detection accuracy. Kong et al. <ref type="bibr" target="#b23">[24]</ref> use the objectness prior constraint on convolutional feature maps to significantly reduce the search space of objects. Lin et al. <ref type="bibr" target="#b27">[28]</ref> address the class imbalance issue by reshaping the standard cross entropy loss to focus training on a sparse set of hard examples and down-weights the loss assigned to well-classified examples. Zhang et al. <ref type="bibr" target="#b52">[53]</ref> design a max-out labeling mechanism to reduce false positives resulting from class imbalance.</p><p>In our opinion, the current state-of-the-art two-stage methods, e.g., Faster R-CNN <ref type="bibr" target="#b35">[36]</ref>, R-FCN <ref type="bibr" target="#b4">[5]</ref>, and FPN <ref type="bibr" target="#b26">[27]</ref>, have three advantages over the one-stage methods as follows: (1) using two-stage structure with sampling heuristics to handle class imbalance; (2) using two-step cascade to regress the object box parameters; (3) using two-stage features to describe the objects <ref type="bibr" target="#b0">1</ref> . In this work, we design a novel object detection framework, called RefineDet, to inherit the merits of the two approaches (i.e., one-stage and two-stage approaches) and overcome their shortcomings. It improves the architecture of the one-stage approach, by using two inter-connected modules (see <ref type="figure">Figure 1</ref>), namely, the anchor 2 refinement module (ARM) and the object detection dition, RefineDet is time efficient, i.e., it runs at 40.2 FPS and 24.1 FPS on a NVIDIA Titan X GPU with the input sizes 320 ? 320 and 512 ? 512 in inference.</p><p>The main contributions of this work are summarized as follows. <ref type="bibr" target="#b0">(1)</ref> We introduce a novel one-stage framework for object detection, composed of two inter-connected modules, i.e., the ARM and the ODM. This leads to performance better than the two-stage approach while maintaining high efficiency of the one-stage approach. (2) To ensure the effectiveness, we design the TCB to transfer the features in the ARM to handle more challenging tasks, i.e., predict accurate object locations, sizes and class labels, in the ODM.</p><p>(3) RefineDet achieves the latest state-of-the-art results on generic object detection (i.e., PASCAL VOC 2007 <ref type="bibr" target="#b9">[10]</ref>, PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> and MS COCO <ref type="bibr" target="#b28">[29]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Classical Object Detectors. Early object detection methods are based on the sliding-window paradigm, which apply the hand-crafted features and classifiers on dense image grids to find objects. As one of the most successful methods, Viola and Jones <ref type="bibr" target="#b46">[47]</ref> use Haar feature and AdaBoost to train a series of cascaded classifiers for face detection, achieving satisfactory accuracy with high efficiency. DPM <ref type="bibr" target="#b11">[12]</ref> is another popular method using mixtures of multiscale deformable part models to represent highly variable object classes, maintaining top results on PASCAL VOC <ref type="bibr" target="#b7">[8]</ref> for many years. However, with the arrival of deep convolutional network, the object detection task is quickly dominated by the CNN-based detectors, which can be roughly divided into two categories, i.e., the two-stage approach and one-stage approach. Two-Stage Approach. The two-stage approach consists of two parts, where the first one (e.g., Selective Search <ref type="bibr" target="#b45">[46]</ref>, EdgeBoxes <ref type="bibr" target="#b54">[55]</ref>, DeepMask <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, RPN <ref type="bibr" target="#b35">[36]</ref>) generates a sparse set of candidate object proposals, and the second one determines the accurate object regions and the corresponding class labels using convolutional networks. Notably, the two-stage approach (e.g., R-CNN <ref type="bibr" target="#b15">[16]</ref>, SPPnet <ref type="bibr" target="#b17">[18]</ref>, Fast R-CNN <ref type="bibr" target="#b14">[15]</ref> to Faster R-CNN <ref type="bibr" target="#b35">[36]</ref>) achieves dominated performance on several challenging datasets (e.g., PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> and MS COCO <ref type="bibr" target="#b28">[29]</ref>). After that, numerous effective techniques are proposed to further improve the performance, such as architecture diagram <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54]</ref>, training strategy <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref>, contextual reasoning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref> and multiple layers exploiting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref>. One-Stage Approach. Considering the high efficiency, the one-stage approach attracts much more attention recently. Sermanet et al. <ref type="bibr" target="#b37">[38]</ref> present the OverFeat method for classification, localization and detection based on deep Con-vNets, which is trained end-to-end, from raw pixels to ultimate categories. Redmon et al. <ref type="bibr" target="#b33">[34]</ref> use a single feedforward convolutional network to directly predict object classes and locations, called YOLO, which is extremely fast. After that, YOLOv2 <ref type="bibr" target="#b34">[35]</ref> is proposed to improve YOLO in several aspects, i.e., add batch normalization on all convolution layers, use high resolution classifier, use convolution layers with anchor boxes to predict bounding boxes instead of the fully connected layers, etc. Liu et al. <ref type="bibr" target="#b29">[30]</ref> propose the SSD method, which spreads out anchors of different scales to multiple layers within a ConvNet and enforces each layer to focus on predicting objects of a certain scale. DSSD <ref type="bibr" target="#b12">[13]</ref> introduces additional context into SSD via deconvolution to improve the accuracy. DSOD <ref type="bibr" target="#b38">[39]</ref> designs an efficient framework and a set of principles to learn object detectors from scratch, following the network structure of SSD. To improve the accuracy, some one-stage methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53]</ref> aim to address the extreme class imbalance problem by re-designing the loss function or classification strategies. Although the one-stage detectors have made good progress, their accuracy still trails that of twostage methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>Refer to the overall network architecture shown in Figure 1. Similar to SSD <ref type="bibr" target="#b29">[30]</ref>, RefineDet is based on a feed-forward convolutional network that produces a fixed number of bounding boxes and the scores indicating the presence of different classes of objects in those boxes, followed by the non-maximum suppression to produce the final result. RefineDet is formed by two inter-connected modules, i.e., the ARM and the ODM. The ARM aims to remove negative anchors so as to reduce search space for the classifier and also coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor, whereas ODM aims to regress accurate object locations and predict multi-class labels based on the refined anchors. The ARM is constructed by removing the classification layers and adding some auxiliary structures of two base networks (i.e., VGG-16 <ref type="bibr" target="#b42">[43]</ref> and ResNet-101 <ref type="bibr" target="#b18">[19]</ref> pretrained on Im-ageNet <ref type="bibr" target="#b36">[37]</ref>) to meet our needs. The ODM is composed of the outputs of TCBs followed by the prediction layers (i.e., the convolution layers with 3 ? 3 kernel size), which generates the scores for object classes and shape offsets relative to the refined anchor box coordinates. The following explain three core components in RefineDet, i.e., (1) transfer connection block (TCB), converting the features from the ARM to the ODM for detection; (2) two-step cascaded regression, accurately regressing the locations and sizes of objects; (3) negative anchor filtering, early rejecting well-classified negative anchors and mitigate the imbalance issue.</p><p>Transfer Connection Block. To link between the ARM and ODM, we introduce the TCBs to convert features of different layers from the ARM, into the form required by the ODM, so that the ODM can share features from the ARM. Notably, from the ARM, we only use the TCBs on the feature maps associated with anchors. Another function of the TCBs is to integrate large-scale context <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> by adding the high-level features to the transferred features to improve detection accuracy. To match the dimensions between them, we use the deconvolution operation to enlarge the high-level feature maps and sum them in the element-wise way. Then, we add a convolution layer after the summation to ensure the discriminability of features for detection. The architecture of the TCB is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Two-Step Cascaded Regression. Current one-stage methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> rely on one-step regression based on various feature layers with different scales to predict the locations and sizes of objects, which is rather inaccurate in some challenging scenarios, especially for the small objects. To that end, we present a two-step cascaded regression strategy to regress the locations and sizes of objects. That is, we use the ARM to first adjust the locations and sizes of anchors to provide better initialization for the regression in the ODM. Specifically, we associate n anchor boxes with each regularly divided cell on the feature map. The initial position of each anchor box relative to its corresponding cell is fixed. At each feature map cell, we predict four offsets of the refined anchor boxes relative to the original tiled anchors and  two confidence scores indicating the presence of foreground objects in those boxes. Thus, we can yield n refined anchor boxes at each feature map cell.</p><p>After obtaining the refined anchor boxes, we pass them to the corresponding feature maps in the ODM to further generate object categories and accurate object locations and sizes, as shown in <ref type="figure">Figure 1</ref>. The corresponding feature maps in the ARM and the ODM have the same dimension. We calculate c class scores and the four accurate offsets of objects relative to the refined anchor boxes, yielding c + 4 outputs for each refined anchor boxes to complete the detection task. This process is similar to the default boxes used in SSD <ref type="bibr" target="#b29">[30]</ref>. However, in contrast to SSD <ref type="bibr" target="#b29">[30]</ref> directly uses the regularly tiled default boxes for detection, RefineDet uses two-step strategy, i.e., the ARM generates the refined anchor boxes, and the ODM takes the refined anchor boxes as input for further detection, leading to more accurate detection results, especially for the small objects. Negative Anchor Filtering. To early reject well-classified negative anchors and mitigate the imbalance issue, we design a negative anchor filtering mechanism. Specifically, in training phase, for a refined anchor box, if its negative confidence is larger than a preset threshold ? (i.e., set ? = 0.99 empirically), we will discard it in training the ODM. That is, we only pass the refined hard negative anchor boxes and refined positive anchor boxes to train the ODM. Meanwhile, in the inference phase, if a refined anchor box is assigned with a negative confidence larger than ?, it will be discarded in the ODM for detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Inference</head><p>Data Augmentation. We use several data augmentation strategies presented in <ref type="bibr" target="#b29">[30]</ref> to construct a robust model to adapt to variations of objects. That is, we randomly expand and crop the original training images with additional random photometric distortion <ref type="bibr" target="#b19">[20]</ref> and flipping to generate the training samples. Please refer to <ref type="bibr" target="#b29">[30]</ref> for more details. Backbone Network. We use VGG-16 <ref type="bibr" target="#b42">[43]</ref> and ResNet-101 <ref type="bibr" target="#b18">[19]</ref> as the backbone networks in our RefineDet, which are pretrained on the ILSVRC CLS-LOC dataset <ref type="bibr" target="#b36">[37]</ref>. Notably, RefineDet can also work on other pretrained networks, such as Inception V2 <ref type="bibr" target="#b21">[22]</ref>, Inception ResNet <ref type="bibr" target="#b43">[44]</ref>, and ResNeXt-101 <ref type="bibr" target="#b48">[49]</ref>. Similar to DeepLab-LargeFOV <ref type="bibr" target="#b3">[4]</ref>, we convert fc6 and fc7 of VGG-16 to convolution layers conv fc6 and conv fc7 via subsampling parameters. Since conv4 3 and conv5 3 have different feature scales compared to other layers, we use L2 normalization <ref type="bibr" target="#b30">[31]</ref> to scale the feature norms in conv4 3 and conv5 3 to 10 and 8, then learn the scales during back propagation. Meanwhile, to capture high-level information and drive object detection at multiple scales, we also add two extra convolution layers (i.e., conv6 1 and conv6 2) to the end of the truncated VGG-16 and one extra residual block (i.e., res6) to the end of the truncated ResNet-101, respectively. Anchors Design and Matching. To handle different scales of objects, we select four feature layers with the total stride sizes 8, 16, 32, and 64 pixels for both VGG-16 and ResNet-101 5 , associated with several different scales of anchors for prediction. Each feature layer is associated with one specific scale of anchors (i.e., the scale is 4 times of the total stride size of the corresponding layer) and three aspect ratios (i.e., 0.5, 1.0, and 2.0). We follow the design of anchor scales over different layers in <ref type="bibr" target="#b52">[53]</ref>, which ensures that different scales of anchors have the same tiling density <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> on the image. Meanwhile, during the training phase, we determine the correspondence between the anchors and ground truth boxes based on the jaccard overlap <ref type="bibr" target="#b6">[7]</ref>, and train the whole network end-to-end accordingly. Specifically, we first match each ground truth to the anchor box with the best overlap score, and then match the anchor boxes to any ground truth with overlap higher than 0.5. Hard Negative Mining. After matching step, most of the anchor boxes are negatives, even for the ODM, where some easy negative anchors are rejected by the ARM. Similar to SSD <ref type="bibr" target="#b29">[30]</ref>, we use hard negative mining to mitigate the extreme foreground-background class imbalance, i.e., we select some negative anchor boxes with top loss values to make the ratio between the negatives and positives below 3 : 1, instead of using all negative anchors or randomly selecting the negative anchors in training. Loss Function. The loss function for RefineDet consists of two parts, i.e., the loss in the ARM and the loss in the ODM.</p><p>For the ARM, we assign a binary class label (of being an object or not) to each anchor and regress its location and size simultaneously to get the refined anchor. After that, we pass the refined anchors with the negative confidence less than the threshold to the ODM to further predict object categories and accurate object locations and sizes. With these definitions, we define the loss function as:</p><formula xml:id="formula_0">L({p i }, {x i }, {c i }, {t i }) = 1 Narm i L b (p i , [l * i ? 1]) + i [l * i ? 1]L r (x i , g * i ) + 1 Nodm i L m (c i , l * i ) + i [l * i ? 1]L r (t i , g * i )</formula><p>(1) where i is the index of anchor in a mini-batch, l * i is the ground truth class label of anchor i, g * i is the ground truth location and size of anchor i. p i and x i are the predicted confidence of the anchor i being an object and refined coordinates of the anchor i in the ARM. c i and t i are the predicted object class and coordinates of the bounding box in the ODM. N arm and N odm are the numbers of positive anchors in the ARM and ODM, respectively. The binary classification loss L b is the cross-entropy/log loss over two classes (object vs. not object), and the multi-class classification loss L m is the softmax loss over multiple classes confidences. Similar to Fast R-CNN <ref type="bibr" target="#b14">[15]</ref>, we use the smooth L1 loss as the regression loss L r . The Iverson bracket indicator function [l * i ? 1] outputs 1 when the condition is true, i.e., l * i ? 1 (the anchor is not the negative), and 0 otherwise. Hence [l * i ? 1]L r indicates that the regression loss is ignored for negative anchors. Notably, if N arm = 0, we set L b (p i , [l * i ? 1]) = 0 and L r (x i , g * i ) = 0; and if N odm = 0, we set L m (c i , l * i ) = 0 and L r (t i , g * i ) = 0 accordingly. Optimization. As mentioned above, the backbone network (e.g., VGG-16 and ResNet-101) in our RefineDet method is pretrained on the ILSVRC CLS-LOC dataset <ref type="bibr" target="#b36">[37]</ref>. We use the "xavier" method <ref type="bibr" target="#b16">[17]</ref> to randomly initialize the parameters in the two extra added convolution layers (i.e., conv6 1 and conv6 2) of VGG-16 based RefineDet, and draw the parameters from a zero-mean Gaussian distribution with standard deviation 0.01 for the extra residual block (i.e., res6) of ResNet-101 based RefineDet. We set the default batch size to 32 in training. Then, the whole network is fine-tuned using SGD with 0.9 momentum and 0.0005 weight decay. We set the initial learning rate to 10 ?3 , and use slightly different learning rate decay policy for different dataset, which will be described in details later. Inference. At inference phase, the ARM first filters out the regularly tiled anchors with the negative confidence scores larger than the threshold ?, and then refines the locations and sizes of remaining anchors. After that, the ODM takes over these refined anchors, and outputs top 400 high confident detections per image. Finally, we apply the nonmaximum suppression with jaccard overlap of 0.45 per class and retain the top 200 high confident detections per image to produce the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Experiments are conducted on three datasets: PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO. The PAS-CAL VOC and MS COCO datasets include 20 and 80 object classes, respectively. The classes in PASCAL VOC are the subset of that in MS COCO. We implement RefineDet in Caffe <ref type="bibr" target="#b22">[23]</ref>. All the training and testing codes and the trained models are available at https://github.com/ sfzhang15/RefineDet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">PASCAL VOC 2007</head><p>All models are trained on the VOC 2007 and VOC 2012 trainval sets, and tested on the VOC 2007 test set. We set the learning rate to 10 ?3 for the first 80k iterations, and decay it to 10 ?4 and 10 ?5 for training another 20k and 20k iterations, respectively. We use the default batch size 32 in training, and only use VGG-16 as the backbone network for all the experiments on the PASCAL VOC dataset, including VOC 2007 and VOC 2012.</p><p>We compare RefineDet 6 with the state-of-the-art detectors in <ref type="table" target="#tab_0">Table 1</ref>. With low dimension input (i.e., 320 ? 320), RefineDet produces 80.0% mAP without bells and whistles, which is the first method achieving above 80% mAP with such small input images, much better than several modern objectors. By using larger input size 512 ? 512, RefineDet achieves 81.8% mAP, surpassing all one-stage methods, e.g., RON384 <ref type="bibr" target="#b23">[24]</ref>, SSD513 <ref type="bibr" target="#b12">[13]</ref>, DSSD513 <ref type="bibr" target="#b12">[13]</ref>, etc. Comparing to the two-stage methods, RefineDet512 performs better than most of them except CoupleNet <ref type="bibr" target="#b53">[54]</ref>, which is based on ResNet-101 and uses larger input size (i.e., ? 1000 ? 600) than our RefineDet512. As pointed out in <ref type="bibr" target="#b20">[21]</ref>, the input size significantly influences detection accuracy. The reason is that high resolution inputs make the detectors "seeing" small objects clearly to increase successful detections. To reduce the impact of input size for a fair comparison, we use the multi-scale testing strategy to evaluate RefineDet, achieving 83.1% (RefineDet320+) and 83.8% (RefineDet512+) mAPs, which are much better than the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Run Time Performance</head><p>We present the inference speed of RefineDet and the stateof-the-art methods in the fifth column of <ref type="table" target="#tab_0">Table 1</ref>. The speed is evaluated with batch size 1 on a machine with NVIDIA Titan X, CUDA 8.0 and cuDNN v6. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we find that RefineDet processes an image in 24.8ms (40.3 FPS) and 41.5ms (24.1 FPS) with input sizes 320 ? 320 and 512 ? 512, respectively. To the best of our knowledge, RefineDet is the first real-time method to achieve detection accuracy above 80% mAP on PASCAL VOC 2007. Comparing to SSD, RON, DSSD and DSOD, RefineDet associates fewer anchor boxes on the feature maps (e.g., 24564 anchor boxes in SSD512 * <ref type="bibr" target="#b29">[30]</ref> vs. 16320 anchor boxes in RefineDet512). However, RefineDet still achieves top accuracy with high efficiency, mainly thanks to the design of two inter-connected modules, (e.g., two-step regression), which enables RefineDet to adapt to different scales and aspect ratios of objects. Meanwhile, only YOLO and SSD300 * are slightly faster than our RefineDet320, but their accuracy are 16.6% and 2.5% worse than ours. In summary, RefineDet achieves the best trade-off between accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Ablation Study</head><p>To demonstrate the effectiveness of different components in RefineDet, we construct four variants and evaluate them on VOC 2007, shown in <ref type="table" target="#tab_2">Table 3</ref>. Specifically, for a fair comparison, we use the same parameter settings and input size (320 ? 320) in evaluation. All models are trained on VOC 2007 and VOC 2012 trainval sets, and tested on VOC 2007 test set.</p><p>Negative Anchor Filtering. To demonstrate the effectiveness of the negative anchor filtering, we set the confidence threshold ? of the anchors to be negative to 1.0 in both training and testing. In this case, all refined anchors will be sent to the ODM for detection. Other parts of RefineDet remain unchanged. Removing negative anchor filtering leads to 0.5% drop in mAP (i.e., 80.0% vs. 79.5%). The reason is that most of these well-classified negative anchors will be filtered out during training, which solves the class imbalance issue to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-</head><p>Step Cascaded Regression. To validate the effectiveness of the two-step cascaded regression, we redesign the network structure by directly using the regularly paved anchors instead of the refined ones from the ARM (see the fourth column in <ref type="table" target="#tab_2">Table 3</ref>). As shown in <ref type="table" target="#tab_2">Table 3</ref>, we find that mAP is reduced from 79.5% to 77.3%. This sharp decline (i.e., 2.2%) demonstrates that the two-step anchor cascaded regression significantly help promote the performance.</p><p>Transfer Connection Block. We construct a network by cutting the TCBs in RefineDet and redefining the loss function in the ARM to directly detect multi-class of objects, just like SSD, to demonstrate the effect of the TCB. The detection accuracy of the model is presented in the fifth column in <ref type="table" target="#tab_2">Table 3</ref>. We compare the results in the fourth and fifth columns in <ref type="table" target="#tab_2">Table 3</ref> (77.3% vs. 76.2%) and find that the TCB improves the mAP by 1.1%. The main reason is  that the model can inherit the discriminative features from the ARM, and integrate large-scale context information to improve the detection accuracy by using the TCB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">PASCAL VOC 2012</head><p>Following the protocol of VOC 2012, we submit the detection results of RefineDet to the public testing server for evaluation. We use VOC 2007 trainval set and test set plus VOC 2012 trainval set (21, 503 images) for training, and test on VOC 2012 test set (10, 991 images). We use the default batch size 32 in training. Meanwhile, we set the learning rate to 10 ?3 in the first 160k iterations, and decay it to 10 ?4 and 10 ?5 for another 40k and 40k iterations. <ref type="table" target="#tab_0">Table 1</ref> shows the accuracy of the proposed RefineDet algorithm, as well as the state-of-the-art methods. Among the methods fed with input size 320 ? 320, RefineDet320 obtains the top 78.1% mAP, which is even better than most of those two-stage methods using about 1000 ? 600 input size (e.g., 70.4% mAP of Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> and 77.6% mAP of R-FCN <ref type="bibr" target="#b4">[5]</ref>). Using the input size 512 ? 512, RefineDet improves mAP to 80.1%, which is surpassing all one-stage methods and only slightly lower than CoupleNet <ref type="bibr" target="#b53">[54]</ref> (i.e., 80.4%). CoupleNet uses ResNet-101 as base network with 1000 ? 600 input size. To reduce the impact of input size for a fair comparison, we also use multi-scale testing to evaluate RefineDet and obtain the state-of-the-art mAPs of 82.7% (RefineDet320+) and 83.5% (RefineDet512+). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">MS COCO</head><p>In addition to PASCAL VOC, we also evaluate Re-fineDet on MS COCO <ref type="bibr" target="#b28">[29]</ref>. Unlike PASCAL VOC, the detection methods using ResNet-101 always achieve better performance than those using VGG-16 on MS COCO. Thus, we also report the results of ResNet-101 based Re-fineDet. Following the protocol in MS COCO, we use the trainval35k set <ref type="bibr" target="#b0">[1]</ref> for training and evaluate the results from test-dev evaluation server. We set the batch size to 32 in training <ref type="bibr" target="#b6">7</ref> , and train the model with 10 ?3 learning rate for the first 280k iterations, then 10 ?4 and 10 ?5 for another 80k and 40k iterations, respectively. <ref type="table" target="#tab_6">Table 7</ref> shows the results on MS COCO test-dev set. RefineDet320 with VGG-16 produces 29.4% AP that is better than all other methods based on VGG-16 (e.g., SSD512 * <ref type="bibr" target="#b29">[30]</ref> and OHEM++ <ref type="bibr" target="#b40">[41]</ref>). The accuracy of RefineDet can be improved to 33.0% by using larger input size (i.e., 512 ? 512), which is much better than several modern object detectors, e.g., Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> and SSD512 * <ref type="bibr" target="#b29">[30]</ref>. Meanwhile, using ResNet-101 can further improve the performance of RefineDet, i.e., RefineDet320 with ResNet-101 achieves 32.0% AP and RefineDet512 achieves 36.4% AP, exceeding most of the detection methods except Faster R-CNN w TDM <ref type="bibr" target="#b41">[42]</ref>, Deformable R-FCN <ref type="bibr" target="#b5">[6]</ref>, RetinaNet800 <ref type="bibr" target="#b27">[28]</ref>, umd det <ref type="bibr" target="#b1">[2]</ref>, and G-RMI <ref type="bibr" target="#b20">[21]</ref>. All these methods use a much bigger input images for both training and testing (i.e., 1000?600 or 800?800) than our RefineDet (i.e., 320?320 and 512 ? 512). Similar to PASCAL VOC, we also report the multi-scale testing AP results of RefineDet for fair comparison in <ref type="table" target="#tab_6">Table 7</ref>, i.e., 35.2% (RefineDet320+ with VGG-16), 37.6% (RefineDet512+ with VGG-16), 38.6% (Re- <ref type="bibr" target="#b6">7</ref> Due to the memory issue, we reduce the batch size to 20 (which is the largest batch size we can use for training on a machine with 4 NVIDIA M40 GPUs) to train the ResNet-101 based RefineDet with the input size 512 ? 512, and train the model with 10 ?3 learning rate for the first 400k iterations, then 10 ?4 and 10 ?5 for another 80k and 60k iterations. fineDet320+ with ResNet-101) and 41.8% (RefineDet512+ with ResNet-101). The best performance of RefineDet is 41.8%, which is the state-of-the-art, surpassing all published two-stage and one-stage approaches. Although the second best detector G-RMI <ref type="bibr" target="#b20">[21]</ref> ensembles five Faster R-CNN models, it still produces 0.2% lower AP than Re-fineDet using a single model. Comparing to the third and fourth best detectors, i.e., umd det <ref type="bibr" target="#b1">[2]</ref> and RetinaNet800 <ref type="bibr" target="#b27">[28]</ref>, RefineDet produces 1.0% and 2.7% higher APs. In addition, the main contribution: focal loss in RetinaNet800, is complementary to our method. We believe that it can be used in RefineNet to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">From MS COCO to PASCAL VOC</head><p>We study how the MS COCO dataset help the detection accuracy on PASCAL VOC. Since the object classes in PASCAL VOC are the subset of MS COCO, we directly fine-tune the detection models pretrained on MS COCO via subsampling the parameters, which achieves 84.0% mAP (RefineDet320) and 85.2% mAP (RefineDet512) on VOC 2007 test set, and 82.7% mAP (RefineDet320) and 85.0% mAP (RefineDet512) on VOC 2012 test set, shown in Table 4. After using the multi-scale testing, the detection accuracy are promoted to 85.6%, 85.8%, 86.0% and 86.8%, respectively. As shown in <ref type="table" target="#tab_3">Table 4</ref>, using the training data in MS COCO and PASCAL VOC, our RefineDet obtains the top mAP scores on both VOC 2007 and VOC 2012. Most important, our single model RefineNet512+ based on VGG-16 ranks as the top 5 on the VOC 2012 Leaderboard (see <ref type="bibr" target="#b8">[9]</ref>), which is the best accuracy among all one-stage methods. Other two-stage methods achieving better results are based on much deeper networks (e.g., ResNet-101 <ref type="bibr" target="#b18">[19]</ref> and ResNeXt-101 <ref type="bibr" target="#b48">[49]</ref>) or using ensemble mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we present a single-shot refinement neural network based detector, which consists of two interconnected modules, i.e., the ARM and the ODM. The ARM aims to filter out the negative anchors to reduce search space for the classifier and also coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor, while the ODM takes the refined anchors as the input from the former ARM to regress the accurate object locations and sizes and predict the corresponding multiclass labels. The whole network is trained in an end-to-end fashion with the multi-task loss. We carry out several experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO datasets to demonstrate that RefineDet achieves the state-of-the-art detection accuracy with high efficiency.</p><p>In the future, we plan to employ RefineDet to detect some other specific kinds of objects, e.g., pedestrian, vehicle, and face, and introduce the attention mechanism in RefineDet to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Complete Object Detection Results</head><p>We show the complete object detection results of the proposed RefineDet method on the PASCAL VOC 2007 test set, PASCAL VOC 2012 test set and MS COCO test-dev set in <ref type="table" target="#tab_4">Table 5</ref>, <ref type="table" target="#tab_5">Table 6 and Table 7</ref>, respectively. Among the results of all published methods, our Re-fineDet achieves the best performance on these three detection datasets, i.e., 85.8% mAP on the PASCAL VOC 2007 test set, 86.8% mAP on the PASCAL VOC 2012 test set and 41.8% AP on the MS COCO test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Qualitative Results</head><p>We show some qualitative results on the PASCAL VOC 2007 test set, the PASCAL VOC 2012 test set and the MS COCO test-dev in <ref type="figure" target="#fig_2">Figure 3</ref>, <ref type="figure" target="#fig_3">Figure 4</ref>, and <ref type="figure" target="#fig_4">Figure 5</ref>, respectively. We only display the detected bounding boxes with the score larger than 0.6. Different colors of the bounding boxes indicate different object categories. Our method works well with the occlusions, truncations, inter-class interference and clustered background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Detection Analysis on PASCAL VOC 2007</head><p>We use the detection analysis tool <ref type="bibr" target="#b7">8</ref> to understand the performance of two RefineDet models (i.e., RefineDet320 and RefineDet512) clearly. <ref type="figure" target="#fig_5">Figure 6</ref> shows that RefineDet can detect various object categories with high quality (large white area). The majority of its confident detections are correct. The recall is around 95%-98%, and is much higher with "weak" (0.1 jaccard overlap) criteria. Compared to SSD, RefineDet reduces the false positive errors at all aspects: (1) RefineDet has less localization error (Loc), indicating that RefineDet can localize objects better because it uses two-step cascade to regress the objects. (2) RefineDet has less confusion with background (BG), due to the negative anchor filtering mechanism in the anchor refinement module (ARM). (3) RefineDet has less confusion with similar categories (Sim), benefiting from using two-stage features to describe the objects, i.e., the features in the ARM focus on the binary classification (being an object or not), while the features in the object detection module (ODM) focus on the multi-class classification (background or object classes). <ref type="figure" target="#fig_6">Figure 7</ref> demonstrates that RefineDet is robust to different object sizes and aspect ratios. This is not surprising because the object bounding boxes are obtained by the twostep cascade regression, i.e., the ARM diversifies the default scales and aspect ratios of anchor boxes so that the ODM is able to regress tougher objects (e.g., extra-small, extralarge, extra-wide and extra-tall). However, as shown in <ref type="figure" target="#fig_6">Figure 7</ref>, there is still much room to improve the performance of RefineDet for small objects, especially for the chairs and tables. Increasing the input size (e.g., from 320 ? 320 to 512 ? 512) can improve the performance for small objects , but it is only a temporary solution. Large input will be a burden on running speed in inference. Therefore, detecting small objects is still a challenge task and needs to be further studied.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overview of the transfer connection block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results of RefineDet512 on the PASCAL VOC 2007 test set (corresponding to 85.2% mAP). VGG-16 is used as the backbone network. The training data is 07+12+COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of RefineDet512 on the PASCAL VOC 2012 test set (corresponding to 85.0% mAP). VGG-16 is used as the backbone network. The training data is 07++12+COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of RefineDet512 on the MS COCO test-dev set (corresponding to 36.4% mAP). ResNet-101 is used as the backbone network. The training data is COCO trainval35k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the performance of RefineDet512 on animals, vehicles, and furniture classes in the VOC 2007 test set. The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG). The solid red line reflects the change of recall with strong criteria (0.5 jaccard overlap) as the number of detections increases. The dashed red line is using the "weak" criteria (0.1 jaccard overlap). The bottom row shows the distribution of the top-ranked false positive types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Sensitivity and impact of different object characteristics on the VOC 2007 test set. The plot on the left shows the effects of BBox Area per category, and the right plot shows the effect of Aspect Ratio. Key: BBox Area: XS=extra-small; S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW =extra-wide.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detection results on PASCAL VOC dataset. For VOC 2007, all methods are trained on VOC 2007 and VOC 2012 trainval sets and tested on VOC 2007 test set. For VOC 2012, all methods are trained on VOC 2007 and VOC 2012 trainval sets plus VOC 2007 test set, and tested on VOC 2012 test set. Bold fonts indicate the best mAP.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Input size</cell><cell>#Boxes</cell><cell>FPS</cell><cell>VOC 2007</cell><cell>mAP (%)</cell><cell>VOC 2012</cell></row><row><cell>two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fast R-CNN[15]</cell><cell>VGG-16</cell><cell>? 1000 ? 600</cell><cell>? 2000</cell><cell>0.5</cell><cell>70.0</cell><cell></cell><cell>68.4</cell></row><row><cell>Faster R-CNN[36]</cell><cell>VGG-16</cell><cell>? 1000 ? 600</cell><cell>300</cell><cell>7</cell><cell>73.2</cell><cell></cell><cell>70.4</cell></row><row><cell>OHEM[41]</cell><cell>VGG-16</cell><cell>? 1000 ? 600</cell><cell>300</cell><cell>7</cell><cell>74.6</cell><cell></cell><cell>71.9</cell></row><row><cell>HyperNet[25]</cell><cell>VGG-16</cell><cell>? 1000 ? 600</cell><cell>100</cell><cell>0.88</cell><cell>76.3</cell><cell></cell><cell>71.4</cell></row><row><cell>Faster R-CNN[36]</cell><cell>ResNet-101</cell><cell>? 1000 ? 600</cell><cell>300</cell><cell>2.4</cell><cell>76.4</cell><cell></cell><cell>73.8</cell></row><row><cell>ION[1]</cell><cell>VGG-16</cell><cell>? 1000 ? 600</cell><cell>4000</cell><cell>1.25</cell><cell>76.5</cell><cell></cell><cell>76.4</cell></row><row><cell>MR-CNN[14]</cell><cell>VGG-16</cell><cell>? 1000 ? 600</cell><cell>250</cell><cell>0.03</cell><cell>78.2</cell><cell></cell><cell>73.9</cell></row><row><cell>R-FCN[5]</cell><cell>ResNet-101</cell><cell>? 1000 ? 600</cell><cell>300</cell><cell>9</cell><cell>80.5</cell><cell></cell><cell>77.6</cell></row><row><cell>CoupleNet[54]</cell><cell>ResNet-101</cell><cell>? 1000 ? 600</cell><cell>300</cell><cell>8.2</cell><cell>82.7</cell><cell></cell><cell>80.4</cell></row><row><cell>one-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>YOLO[34]</cell><cell>GoogleNet [45]</cell><cell>448 ? 448</cell><cell>98</cell><cell>45</cell><cell>63.4</cell><cell></cell><cell>57.9</cell></row><row><cell>RON384[24]</cell><cell>VGG-16</cell><cell>384 ? 384</cell><cell>30600</cell><cell>15</cell><cell>75.4</cell><cell></cell><cell>73.0</cell></row><row><cell>SSD321[13]</cell><cell>ResNet-101</cell><cell>321 ? 321</cell><cell>17080</cell><cell>11.2</cell><cell>77.1</cell><cell></cell><cell>75.4</cell></row><row><cell>SSD300  *  [30]</cell><cell>VGG-16</cell><cell>300 ? 300</cell><cell>8732</cell><cell>46</cell><cell>77.2</cell><cell></cell><cell>75.8</cell></row><row><cell>DSOD300[39]</cell><cell>DS/64-192-48-1</cell><cell>300 ? 300</cell><cell>8732</cell><cell>17.4</cell><cell>77.7</cell><cell></cell><cell>76.3</cell></row><row><cell>YOLOv2[35]</cell><cell>Darknet-19</cell><cell>544 ? 544</cell><cell>845</cell><cell>40</cell><cell>78.6</cell><cell></cell><cell>73.4</cell></row><row><cell>DSSD321[13]</cell><cell>ResNet-101</cell><cell>321 ? 321</cell><cell>17080</cell><cell>9.5</cell><cell>78.6</cell><cell></cell><cell>76.3</cell></row><row><cell>SSD512  *  [30]</cell><cell>VGG-16</cell><cell>512 ? 512</cell><cell>24564</cell><cell>19</cell><cell>79.8</cell><cell></cell><cell>78.5</cell></row><row><cell>SSD513[13]</cell><cell>ResNet-101</cell><cell>513 ? 513</cell><cell>43688</cell><cell>6.8</cell><cell>80.6</cell><cell></cell><cell>79.4</cell></row><row><cell>DSSD513[13]</cell><cell>ResNet-101</cell><cell>513 ? 513</cell><cell>43688</cell><cell>5.5</cell><cell>81.5</cell><cell></cell><cell>80.0</cell></row><row><cell>RefineDet320</cell><cell>VGG-16</cell><cell>320 ? 320</cell><cell>6375</cell><cell>40.3</cell><cell>80.0</cell><cell></cell><cell>78.1</cell></row><row><cell>RefineDet512</cell><cell>VGG-16</cell><cell>512 ? 512</cell><cell>16320</cell><cell>24.1</cell><cell>81.8</cell><cell></cell><cell>80.1</cell></row><row><cell>RefineDet320+</cell><cell>VGG-16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.1</cell><cell></cell><cell>82.7</cell></row><row><cell>RefineDet512+</cell><cell>VGG-16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.8</cell><cell></cell><cell>83.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detection results on MS COCO test-dev set. Bold fonts indicate the best performance.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell>Backbone</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fast R-CNN [15]</cell><cell>train</cell><cell>VGG-16</cell><cell>19.7</cell><cell>35.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster R-CNN [36]</cell><cell>trainval</cell><cell>VGG-16</cell><cell>21.9</cell><cell>42.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OHEM [41]</cell><cell>trainval</cell><cell>VGG-16</cell><cell>22.6</cell><cell>42.5</cell><cell>22.2</cell><cell>5.0</cell><cell>23.7</cell><cell>37.9</cell></row><row><cell>ION [1]</cell><cell>train</cell><cell>VGG-16</cell><cell>23.6</cell><cell>43.2</cell><cell>23.6</cell><cell>6.4</cell><cell>24.1</cell><cell>38.3</cell></row><row><cell>OHEM++ [41]</cell><cell>trainval</cell><cell>VGG-16</cell><cell>25.5</cell><cell>45.9</cell><cell>26.1</cell><cell>7.4</cell><cell>27.7</cell><cell>40.3</cell></row><row><cell>R-FCN [5]</cell><cell>trainval</cell><cell>ResNet-101</cell><cell>29.9</cell><cell>51.9</cell><cell>-</cell><cell>10.8</cell><cell>32.8</cell><cell>45.0</cell></row><row><cell>CoupleNet [54]</cell><cell>trainval</cell><cell>ResNet-101</cell><cell>34.4</cell><cell>54.8</cell><cell>37.2</cell><cell>13.4</cell><cell>38.1</cell><cell>50.8</cell></row><row><cell>Faster R-CNN by G-RMI [21]</cell><cell>-</cell><cell>Inception-ResNet-v2[44]</cell><cell>34.7</cell><cell>55.5</cell><cell>36.7</cell><cell>13.5</cell><cell>38.1</cell><cell>52.0</cell></row><row><cell>Faster R-CNN+++ [19]</cell><cell>trainval</cell><cell>ResNet-101-C4</cell><cell>34.9</cell><cell>55.7</cell><cell>37.4</cell><cell>15.6</cell><cell>38.7</cell><cell>50.9</cell></row><row><cell>Faster R-CNN w FPN [27]</cell><cell>trainval35k</cell><cell>ResNet-101-FPN</cell><cell>36.2</cell><cell>59.1</cell><cell>39.0</cell><cell>18.2</cell><cell>39.0</cell><cell>48.2</cell></row><row><cell>Faster R-CNN w TDM [42]</cell><cell>trainval</cell><cell>Inception-ResNet-v2-TDM</cell><cell>36.8</cell><cell>57.7</cell><cell>39.2</cell><cell>16.2</cell><cell>39.8</cell><cell>52.1</cell></row><row><cell>Deformable R-FCN [6]</cell><cell>trainval</cell><cell>Aligned-Inception-ResNet</cell><cell>37.5</cell><cell>58.0</cell><cell>40.8</cell><cell>19.4</cell><cell>40.1</cell><cell>52.5</cell></row><row><cell>umd det [2]</cell><cell>trainval</cell><cell>ResNet-101</cell><cell>40.8</cell><cell>62.4</cell><cell>44.9</cell><cell>23.0</cell><cell>43.4</cell><cell>53.2</cell></row><row><cell>G-RMI [21]</cell><cell>trainval32k</cell><cell>Ensemble of Five Models</cell><cell>41.6</cell><cell>61.9</cell><cell>45.4</cell><cell>23.9</cell><cell>43.5</cell><cell>54.9</cell></row><row><cell>one-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>YOLOv2 [35]</cell><cell>trainval35k</cell><cell>DarkNet-19[35]</cell><cell>21.6</cell><cell>44.0</cell><cell>19.2</cell><cell>5.0</cell><cell>22.4</cell><cell>35.5</cell></row><row><cell>SSD300  *  [30]</cell><cell>trainval35k</cell><cell>VGG-16</cell><cell>25.1</cell><cell>43.1</cell><cell>25.8</cell><cell>6.6</cell><cell>25.9</cell><cell>41.4</cell></row><row><cell>RON384++ [24]</cell><cell>trainval</cell><cell>VGG-16</cell><cell>27.4</cell><cell>49.5</cell><cell>27.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSD321 [13]</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>28.0</cell><cell>45.4</cell><cell>29.3</cell><cell>6.2</cell><cell>28.3</cell><cell>49.3</cell></row><row><cell>DSSD321 [13]</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>28.0</cell><cell>46.1</cell><cell>29.2</cell><cell>7.4</cell><cell>28.1</cell><cell>47.6</cell></row><row><cell>SSD512  *  [30]</cell><cell>trainval35k</cell><cell>VGG-16</cell><cell>28.8</cell><cell>48.5</cell><cell>30.3</cell><cell>10.9</cell><cell>31.8</cell><cell>43.5</cell></row><row><cell>SSD513 [13]</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>31.2</cell><cell>50.4</cell><cell>33.3</cell><cell>10.2</cell><cell>34.5</cell><cell>49.8</cell></row><row><cell>DSSD513 [13]</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>33.2</cell><cell>53.3</cell><cell>35.2</cell><cell>13.0</cell><cell>35.4</cell><cell>51.1</cell></row><row><cell>RetinaNet500 [28]</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>34.4</cell><cell>53.1</cell><cell>36.8</cell><cell>14.7</cell><cell>38.5</cell><cell>49.1</cell></row><row><cell>RetinaNet800 [28]  *</cell><cell>trainval35k</cell><cell>ResNet-101-FPN</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>RefineDet320</cell><cell>trainval35k</cell><cell>VGG-16</cell><cell>29.4</cell><cell>49.2</cell><cell>31.3</cell><cell>10.0</cell><cell>32.0</cell><cell>44.4</cell></row><row><cell>RefineDet512</cell><cell>trainval35k</cell><cell>VGG-16</cell><cell>33.0</cell><cell>54.5</cell><cell>35.5</cell><cell>16.3</cell><cell>36.3</cell><cell>44.3</cell></row><row><cell>RefineDet320</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>32.0</cell><cell>51.4</cell><cell>34.2</cell><cell>10.5</cell><cell>34.7</cell><cell>50.4</cell></row><row><cell>RefineDet512</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>36.4</cell><cell>57.5</cell><cell>39.5</cell><cell>16.6</cell><cell>39.9</cell><cell>51.4</cell></row><row><cell>RefineDet320+</cell><cell>trainval35k</cell><cell>VGG-16</cell><cell>35.2</cell><cell>56.1</cell><cell>37.7</cell><cell>19.5</cell><cell>37.2</cell><cell>47.0</cell></row><row><cell>RefineDet512+</cell><cell>trainval35k</cell><cell>VGG-16</cell><cell>37.6</cell><cell>58.7</cell><cell>40.8</cell><cell>22.7</cell><cell>40.3</cell><cell>48.3</cell></row><row><cell>RefineDet320+</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>38.6</cell><cell>59.9</cell><cell>41.7</cell><cell>21.1</cell><cell>41.7</cell><cell>52.3</cell></row><row><cell>RefineDet512+</cell><cell>trainval35k</cell><cell>ResNet-101</cell><cell>41.8</cell><cell>62.9</cell><cell>45.7</cell><cell>25.6</cell><cell>45.1</cell><cell>54.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* This entry reports the single model accuracy of RetinaNet method, trained with scale jitter and for 1.5? longer than RetinaNet500.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effectiveness of various designs. All models are trained on VOC 2007 and VOC 2012 trainval set and tested on VOC 2007 test set.</figDesc><table><row><cell>Component negative anchor filtering? two-step cascaded regression? transfer connection block?</cell><cell cols="3">RefineDet320 ! ! ! ! ! !</cell><cell></cell></row><row><cell>mAP (%)</cell><cell>80.0</cell><cell>79.5</cell><cell>77.3</cell><cell>76.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Detection results on PASCAL VOC dataset. All models are pre-trained on MS COCO, and fine-tuned on PASCAL VOC. Bold fonts indicate the best mAP.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">mAP (%) VOC 2007 test VOC 2012 test</cell></row><row><cell>two-stage:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN[36]</cell><cell>VGG-16</cell><cell>78.8</cell><cell>75.9</cell></row><row><cell>OHEM++[41]</cell><cell>VGG-16</cell><cell>-</cell><cell>80.1</cell></row><row><cell>R-FCN[5]</cell><cell>ResNet-101</cell><cell>83.6</cell><cell>82.0</cell></row><row><cell>one-stage:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD300[30]</cell><cell>VGG-16</cell><cell>81.2</cell><cell>79.3</cell></row><row><cell>SSD512[30]</cell><cell>VGG-16</cell><cell>83.2</cell><cell>82.2</cell></row><row><cell>RON384++[24]</cell><cell>VGG-16</cell><cell>81.3</cell><cell>80.7</cell></row><row><cell cols="2">DSOD300[39] DS/64-192-48-1</cell><cell>81.7</cell><cell>79.3</cell></row><row><cell>RefineDet320</cell><cell>VGG-16</cell><cell>84.0</cell><cell>82.7</cell></row><row><cell>RefineDet512</cell><cell>VGG-16</cell><cell>85.2</cell><cell>85.0</cell></row><row><cell>RefineDet320+</cell><cell>VGG-16</cell><cell>85.6</cell><cell>86.0</cell></row><row><cell>RefineDet512+</cell><cell>VGG-16</cell><cell>85.8</cell><cell>86.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Object detection results on the PASCAL VOC 2007 test set. All models use VGG-16 as the backbone network. Data mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv RefineDet320 07+12 80.0 83.9 85.4 81.4 75.5 60.2 86.4 88.1 89.1 62.7 83.9 77.0 85.4 87.1 86.7 82.6 55.3 82.7 78.5 88.1 79.4 RefineDet512 07+12 81.8 88.7 87.0 83.2 76.5 68.0 88.5 88.7 89.2 66.5 87.9 75.0 86.8 89.2 87.8 84.7 56.2 83.2 78.7 88.1 82.3 RefineDet320+ 07+12 83.1 89.5 87.9 84.9 79.7 70.0 87.5 89.1 89.8 69.8 87.1 76.4 86.6 88.6 88.4 85.3 62.4 83.7 82.3 89.0 83.1 RefineDet512+ 07+12 83.8 88.5 89.1 85.5 79.8 72.4 89.5 89.5 89.9 69.9 88.9 75.9 87.4 89.6 89.0 86.2 63.9 86.2 81.0 88.6 84.4 RefineDet320 COCO+07+12 84.0 88.9 88.4 86.2 81.5 71.7 88.4 89.4 89.0 71.0 87.0 80.1 88.5 90.2 88.4 86.7 61.2 85.2 83.8 89.1 85.5 RefineDet512 COCO+07+12 85.2 90.0 89.2 87.9 83.1 78.5 90.0 89.9 89.7 74.7 89.8 79.5 88.7 89.9 89.2 87.8 63.1 86.4 82.3 89.5 84.7 RefineDet320+ COCO+07+12 85.6 90.2 89.0 87.6 84.6 78.0 89.4 89.7 89.9 74.7 89.8 80.5 89.0 89.7 89.6 87.8 65.5 87.9 84.2 88.6 86.3 RefineDet512+ COCO+07+12 85.8 90.4 89.6 88.2 84.9 78.3 89.8 89.9 90.0 75.9 90.0 80.0 89.8 90.3 89.6 88.3 66.2 87.8 83.5 89.3 85.2</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Object detection results on the PASCAL VOC 2012 test set. All models use VGG-16 as the backbone network. Data mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv RefineDet320 07++12 78.1 90.4 84.1 79.8 66.8 56.1 83.1 82.7 90.7 61.7 82.4 63.8 89.4 86.9 85.9 85.7 53.3 84.3 73.1 87.4 73.9 RefineDet512 07++12 80.1 90.2 86.8 81.8 68.0 65.6 84.9 85.0 92.2 62.0 84.4 64.9 90.6 88.3 87.2 87.8 58.0 86.3 72.5 88.7 76.6 RefineDet320+ 07++12 82.7 92.0 88.4 84.9 74.0 69.5 86.0 88.0 93.3 67.0 86.2 68.3 92.1 89.7 88.9 89.4 62.0 88.5 75.9 90.0 80.0 RefineDet512+ 07++12 83.5 92.2 89.4 85.0 74.1 70.8 87.0 88.7 94.0 68.6 87.1 68.2 92.5 90.8 89.4 90.2 64.1 89.8 75.2 90.7 81.1 RefineDet320 COCO+07++12 82.7 93.1 88.2 83.6 74.4 65.1 87.1 87.1 93.7 67.4 86.1 69.4 91.5 90.6 91.4 89.4 59.6 87.9 78.1 91.1 80.0 RefineDet512 COCO+07++12 85.0 94.0 90.0 86.9 76.9 74.1 89.7 89.8 94.2 69.7 90.0 68.5 92.6 92.8 91.5 91.4 66.0 91.2 75.4 91.8 83.0 RefineDet320+ COCO+07++12 86.0 94.2 90.2 87.7 80.4 74.9 90.0 91.7 94.9 71.9 89.8 71.7 93.5 91.9 92.4 91.9 66.5 91.5 79.1 92.8 83.9 RefineDet512+ COCO+07++12 86.8 94.7 91.5 88.8 80.4 77.6 90.4 92.3 95.6 72.5 91.6 69.9 93.9 93.5 92.4 92.6 68.8 92.4 78.5 93.6 85.2</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Object detection results on the MS COCO test-dev set.</figDesc><table><row><cell>Method</cell><cell>Net</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AR 1</cell><cell>AR 10</cell><cell>AR 100</cell><cell>AR S</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell>RefineDet320</cell><cell>VGG-16</cell><cell>29.4</cell><cell>49.2</cell><cell>31.3</cell><cell>10.0</cell><cell>32.0</cell><cell>44.4</cell><cell>26.2</cell><cell>42.2</cell><cell>45.8</cell><cell>18.7</cell><cell>52.1</cell><cell>66.0</cell></row><row><cell>RefineDet512</cell><cell>VGG-16</cell><cell>33.0</cell><cell>54.5</cell><cell>35.5</cell><cell>16.3</cell><cell>36.3</cell><cell>44.3</cell><cell>28.3</cell><cell>46.4</cell><cell>50.6</cell><cell>29.3</cell><cell>55.5</cell><cell>66.0</cell></row><row><cell>RefineDet320</cell><cell>ResNet-101</cell><cell>32.0</cell><cell>51.4</cell><cell>34.2</cell><cell>10.5</cell><cell>34.7</cell><cell>50.4</cell><cell>28.0</cell><cell>44.0</cell><cell>47.6</cell><cell>20.2</cell><cell>53.0</cell><cell>69.8</cell></row><row><cell>RefineDet512</cell><cell>ResNet-101</cell><cell>36.4</cell><cell>57.5</cell><cell>39.5</cell><cell>16.6</cell><cell>39.9</cell><cell>51.4</cell><cell>30.6</cell><cell>49.0</cell><cell>53.0</cell><cell>30.0</cell><cell>58.2</cell><cell>70.3</cell></row><row><cell>RefineDet320+</cell><cell>VGG-16</cell><cell>35.2</cell><cell>56.1</cell><cell>37.7</cell><cell>19.5</cell><cell>37.2</cell><cell>47.0</cell><cell>30.1</cell><cell>49.6</cell><cell>57.4</cell><cell>36.2</cell><cell>62.3</cell><cell>72.6</cell></row><row><cell>RefineDet512+</cell><cell>VGG-16</cell><cell>37.6</cell><cell>58.7</cell><cell>40.8</cell><cell>22.7</cell><cell>40.3</cell><cell>48.3</cell><cell>31.4</cell><cell>52.4</cell><cell>61.3</cell><cell>41.6</cell><cell>65.8</cell><cell>75.4</cell></row><row><cell>RefineDet320+</cell><cell>ResNet-101</cell><cell>38.6</cell><cell>59.9</cell><cell>41.7</cell><cell>21.1</cell><cell>41.7</cell><cell>52.3</cell><cell>32.2</cell><cell>52.9</cell><cell>61.1</cell><cell>40.2</cell><cell>66.2</cell><cell>77.1</cell></row><row><cell>RefineDet512+</cell><cell>ResNet-101</cell><cell>41.8</cell><cell>62.9</cell><cell>45.7</cell><cell>25.6</cell><cell>45.1</cell><cell>54.1</cell><cell>34.0</cell><cell>56.3</cell><cell>65.5</cell><cell>46.2</cell><cell>70.2</cell><cell>79.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">gle most important metric, which is computed by averaging over all 10 intersection over union (IoU) thresholds (i.e., in the range [0.5:0.95] with uniform step size 0.05) of 80 categories.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For the VGG-16 base network, the conv4 3, conv5 3, conv fc7, and conv6 2 feature layers are used to predict the locations, sizes and confidences of objects. While for the ResNet-101 base network, res3b3, res4b22, res5c, and res6 are used for prediction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Due to the shortage of computational resources, we only train Re-fineDet with two kinds of input size, i.e., 320 ? 320 and 512 ? 512. We believe the accuracy of RefineDet can be further improved using larger input images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://web.engr.illinois.edu/?dhoiem/projects/ detectionAnalysis/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Leaderboard of the PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4.Online" />
		<imprint>
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.Online" />
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.Online" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DSSD : Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1701.06659</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5402</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RON: reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ME R-CNN: multi-expert region-based CNN for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DSOD: learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contextual priming and feedback for faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="330" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gated bi-directional CNN for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detecting face with densely connected face proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCBR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Faceboxes: A CPU real-time face detector with high accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">S 3 FD: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Couplenet: Coupling global structure with local parts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
