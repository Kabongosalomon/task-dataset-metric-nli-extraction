<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supporting Clustering with Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejiao</forename><surname>Zhang</surname></persName>
							<email>dejiaoz@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Nan</surname></persName>
							<email>nanfen@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
							<email>xiaokaiw@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Wen</forename><surname>Li</surname></persName>
							<email>shangwel@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
							<email>rnallapa@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">O</forename><surname>Arnold</surname></persName>
							<email>anarnld@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<email>bxiang@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Supporting Clustering with Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) -a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering and show that SCCL significantly advances the state-of-the-art results on most benchmark datasets with 3%?11% improvement on Accuracy and 4% ? 15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and top-down clustering to achieve better intracluster and inter-cluster distances when evaluated with the ground truth cluster labels 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clustering, one of the most fundamental challenges in unsupervised learning, has been widely studied for decades. Long established clustering methods such as K-means <ref type="bibr" target="#b27">(MacQueen et al., 1967;</ref><ref type="bibr" target="#b24">Lloyd, 1982)</ref> and Gaussian Mixture Models <ref type="bibr" target="#b5">(Celeux and Govaert, 1995)</ref> rely on distance measured in the data space, which tends to be ineffective for highdimensional data. On the other hand, deep neural networks are gaining momentum as an effective way to map data to a low dimensional and hopefully better separable representation space.</p><p>Many recent research efforts focus on integrating clustering with deep representation learning by optimizing a clustering objective defined in the representation space <ref type="bibr" target="#b48">(Xie et al., 2016;</ref><ref type="bibr" target="#b15">Jiang et al., 2016;</ref><ref type="bibr">Zhang et al., 2017a;</ref><ref type="bibr" target="#b44">Shaham et al., 2018)</ref>. Despite promising improvements, the clustering performance is still inadequate, especially in the presence of complex data with a large number of clusters. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, one possible reason is that, even with a deep neural network, data still has significant overlap across categories before clustering starts. Consequently, the clusters learned by optimizing various distance or similarity based clustering objectives suffer from poor purity.</p><p>On the other hand, Instance-wise Contrastive Learning (Instance-CL) <ref type="bibr" target="#b47">(Wu et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b14">He et al., 2020;</ref><ref type="bibr">Chen et al., 2020a,b)</ref> has recently achieved remarkable success in selfsupervised learning. Instance-CL usually optimizes on an auxiliary set obtained by data augmentation. As the name suggests, a contrastive loss is then adopted to pull together samples augmented from the same instance in the original dataset while pushing apart those from different ones. Essen-tially, Instance-CL disperses different instances apart while implicitly bringing similar instances together to some extent (see <ref type="figure" target="#fig_0">Figure 1</ref>). This beneficial property can be leveraged to support clustering by scattering apart the overlapped categories. Then clustering, thereby better separates different clusters while tightening each cluster by explicitly bringing samples in that cluster together.</p><p>To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) by jointly optimizing a top-down clustering loss with a bottom-up instance-wise contrastive loss. We assess the performance of SCCL on short text clustering, which has become increasingly important due to the popularity of social media such as Twitter and Instagram. It benefits many real-world applications, including topic discovery <ref type="bibr" target="#b18">(Kim et al., 2013)</ref>, recommendation <ref type="bibr" target="#b3">(Bouras and Tsogkas, 2017)</ref>, and visualization <ref type="bibr" target="#b43">(Sebrechts et al., 1999)</ref>. However, the weak signal caused by noise and sparsity poses a significant challenge for clustering short texts. Although some improvement has been achieved by leveraging shallow neural networks to enrich the representations <ref type="bibr" target="#b49">(Xu et al., 2017;</ref><ref type="bibr" target="#b13">Hadifar et al., 2019)</ref>, there is still large room for improvement.</p><p>We address this challenge with our SCCL model. Our main contributions are the following:</p><p>? We propose a novel end-to-end framework for unsupervised clustering, which advances the state-of-the-art results on various short text clustering datasets by a large margin. Furthermore, our model is much simpler than the existing deep neural network based short text clustering approaches that often require multistage independent training.</p><p>? We provide in-depth analysis and demonstrate how SCCL effectively combines the top-down clustering with the bottom-up instance-wise contrastive learning to achieve better intercluster distance and intra-cluster distance.</p><p>? We explore various text augmentation techniques for SCCL, showing that, unlike the image domain <ref type="bibr">(Chen et al., 2020a)</ref>, using composition of augmentations is not always beneficial in the text domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-supervised learning Self-supervised learning has recently become prominent in providing effective representations for many downstream tasks.</p><p>Early work focuses on solving different artificially designed pretext tasks, such as predicting masked tokens <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, generating future tokens <ref type="bibr" target="#b38">(Radford et al., 2018)</ref>, or denoising corrupted tokens <ref type="bibr" target="#b22">(Lewis et al., 2019)</ref> for textual data, and predicting colorization <ref type="bibr">(Zhang et al., 2016)</ref>, rotation <ref type="bibr" target="#b11">(Gidaris et al., 2018)</ref>, or relative patch position <ref type="bibr" target="#b9">(Doersch et al., 2015)</ref> for image data. Nevertheless, the resulting representations are tailored to the specific pretext tasks with limited generalization. Many recent successes are largely driven by instance-wise contrastive learning. Inspired by the pioneering work of <ref type="bibr" target="#b2">Becker and Hinton (1992)</ref>; <ref type="bibr" target="#b4">Bromley et al. (1994)</ref>, Instance-CL treats each data instance and its augmentations as an independent class and tries to pull together the representations within each class while pushing apart different classes <ref type="bibr" target="#b10">(Dosovitskiy et al., 2014;</ref><ref type="bibr" target="#b32">Oord et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b14">He et al., 2020;</ref><ref type="bibr">Chen et al., 2020a,b)</ref>. Consequently, different instances are well-separated in the learned embedding space with local invariance being preserved for each instance.</p><p>Although Instance-CL may implicitly group similar instances together <ref type="bibr" target="#b47">(Wu et al., 2018)</ref>, it pushes representations apart as long as they are from different original instances, regardless of their semantic similarities. Thereby, the implicit grouping effect of Instance-CL is less stable and more datadependent, giving rise to worse representations in some cases <ref type="bibr">(Khosla et al., 2020;</ref><ref type="bibr" target="#b23">Li et al., 2020;</ref><ref type="bibr" target="#b36">Purushwalkam and Gupta, 2020)</ref>.</p><p>Short Text Clustering Compared with the general text clustering problem, short text clustering comes with its own challenge due to the weak signal contained in each instance. In this scenario, BoW and TF-IDF often yield very sparse representation vectors that lack expressive ability. To remedy this issue, some early work leverages neural networks to enrich the representations <ref type="bibr" target="#b49">(Xu et al., 2017;</ref><ref type="bibr" target="#b13">Hadifar et al., 2019)</ref>, where word embeddings <ref type="bibr" target="#b30">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b0">Arora et al., 2017</ref>) are adopted to further enhance the performance.</p><p>However, the above approaches divide the learning process into multiple stages, each requiring independent optimization. On the other hand, despite the tremendous successes achieved by contextualized word embeddings <ref type="bibr" target="#b34">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019;</ref><ref type="bibr" target="#b38">Radford et al., 2018;</ref><ref type="bibr" target="#b41">Reimers and Gurevych, 2019b)</ref>, they have been left largely unexplored for short text clustering. In this work, we leverage the pretrained transformer as the back- bone, which is optimized in an end-to-end fashion. As demonstrated in Section 4, we advance the state-of-the-art results on most benchmark datasets with 3% ? 11% improvement on Accuracy and 4% ? 15% improvement on NMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We aim at developing a joint model that leverages the beneficial properties of Instance-CL to improve unsupervised clustering. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, our model consists of three components. A neural network ?(?) first maps the input data to the representation space, which is then followed by two different heads g(?) and f (?) where the contrastive loss and the clustering loss are applied, respectively. Please refer to Section 4 for details.</p><p>Our data consists of both the original and the augmented data. Specifically, for a randomly sampled minibatch B = {x i } M i=1 , we randomly generate a pair of augmentations for each data instance in B, yielding an augmented batch B a with size 2M , denoted as B a = {x i } 2M i=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Instance-wise Contrastive Learning</head><p>For each minibatch B, the Instance-CL loss is defined on the augmented pairs in B a . Let i 1 ? {1, . . . , 2M } denote the index of an arbitrary instance in augmented set B a , and let i 2 ? {1, . . . , 2M } be the index of the other instance in B a augmented from the same instance in the original set B. We refer tox i 1 ,x i 2 ? B a as a positive pair, while treating the other 2M -2 examples in B a as negative instances regarding this positive pair. Letz i 1 andz i 2 be the corresponding outputs of the head g, i.e.,z j = g(?(x j )), j = i 1 , i 2 . Then for x i 1 , we try to separatex i 2 apart from all negative instances in B a by minimizing the following</p><formula xml:id="formula_0">I i 1 = ? log exp(sim(z i 1 ,z i 2 )/? ) 2M j=1 1 j =i 1 ? exp(sim(z i 1 ,z j )/? ) .<label>(1)</label></formula><p>Here 1 j =i 1 is an indicator function and ? denotes the temperature parameter which we set as 0.5. Following <ref type="bibr">Chen et al. (2020a)</ref>, we choose sim(?) as the dot product between a pair of normalized outputs, i.e., sim</p><formula xml:id="formula_1">(z i ,z j ) =z T iz j / z i 2 z j 2 . The Instance-CL loss is then averaged over all instances in B a , L Instance-CL = 2M i=1 I i /2M .<label>(2)</label></formula><p>To explore the above contrastive loss in the text domain, we explore three different augmentation strategies in Section 4.3.1, where we find contextual augmenter <ref type="bibr" target="#b20">(Kobayashi, 2018;</ref><ref type="bibr" target="#b25">Ma, 2019)</ref> consistently performs better than the other two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clustering</head><p>We simultaneously encode the semantic categorical structure into the representations via unsupervised clustering. Unlike Instance-CL, clustering focuses on the high-level semantic concepts and tries to bring together instances from the same semantic category together. Suppose our data consists of K semantic categories, and each category is characterized by its centroid in the representation space, denoted as ? k , k ? {1, . . . , K}. Let e j = ?(x j ) denote the representation of instance x j in the original set B. Following Maaten and Hinton <ref type="formula" target="#formula_1">(2008)</ref>, we use the Student's t-distribution to compute the probability of assigning x j to the k th cluster,</p><formula xml:id="formula_2">q jk = 1 + e j ? ? k 2 2 /? ? ?+1 2 K k =1 1 + e j ? ? k 2 2 /? ? ?+1 2 .<label>(3)</label></formula><p>Here ? denotes the degree of freedom of the Student's t-distribution. Without explicit mention, we follow Maaten and Hinton (2008) by setting ? = 1 in this paper. We use a linear layer, i.e., the clustering head in <ref type="figure" target="#fig_1">Figure 2</ref>, to approximate the centroids of each cluster, and we iteratively refine it by leveraging an auxiliary distribution proposed by <ref type="bibr" target="#b48">Xie et al. (2016)</ref>. Specifically, let p jk denote the auxiliary probability defined as</p><formula xml:id="formula_3">p jk = q 2 jk /f k k q 2 jk /f k .<label>(4)</label></formula><p>Here f k = M j=1 q jk , k = 1, . . . , K can be interpreted as the soft cluster frequencies approximated within a minibatch. This target distribution first sharpens the soft-assignment probability q jk by raising it to the second power, and then normalizes it by the associated cluster frequency. By doing so, we encourage learning from high confidence cluster assignments and simultaneously combating the bias caused by imbalanced clusters.</p><p>We push the cluster assignment probability towards the target distribution by optimizing the KL divergence between them,</p><formula xml:id="formula_4">C j = KL [p j q j ] = K k=1 p jk log p jk q jk . (5)</formula><p>The clustering objective is then followed as</p><formula xml:id="formula_5">L Cluster = M j=1 C j /M<label>(6)</label></formula><p>This clustering loss is first proposed in <ref type="bibr" target="#b48">Xie et al. (2016)</ref> and later adopted by <ref type="bibr" target="#b13">Hadifar et al. (2019)</ref> for short text clustering. However, they both require expensive layer-wise pretraining of the neural network, and update the target distribution (Eq (4)) through carefully chosen intervals that often vary across datasets. In contrast, we simplify the learning process to end-to-end training with the target distribution being updated per iteration.</p><p>Overall objective In summary, our overall objective is,</p><formula xml:id="formula_6">L = L Instance-CL + ?L Cluster = M j=1 C j /M + ? 2M i=1 I i /2M .<label>(7)</label></formula><p>C j and I i are defined in Eq (5) and Eq (2), respectively. ? balances between the contrastive loss and the clustering loss of SCCL, which we set as 10 in Section 4 for simplicity. Also noted that, the clustering loss is optimized over the original data only. Alternatively, we can also leverage the augmented data to enforce local consistency of the cluster assignments for each instance. We discuss this further in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical Results</head><p>Implementation We implement our model in Py-Torch <ref type="bibr" target="#b33">(Paszke et al., 2017)</ref> with the Sentence Transformer library <ref type="bibr" target="#b40">(Reimers and Gurevych, 2019a)</ref>. We choose distilbert-base-nli-stsb-mean-tokens as the backbone, followed by a linear clustering head (f ) of size 768 ? K with K indicating the number of clusters. For the contrastive loss, we optimize an MLP (g) with one hidden layer of size 768, and output vectors of size 128. <ref type="figure" target="#fig_1">Figure 2</ref> provides an illustration of our model. The detailed experimental setup is provided in Appendix A.1. We, as in the previous work <ref type="bibr" target="#b49">Xu et al. (2017)</ref>; <ref type="bibr" target="#b13">Hadifar et al. (2019)</ref>; <ref type="bibr" target="#b39">Rakib et al. (2020)</ref>, adopt Accuracy (ACC) and Normalized Mutual Information (NMI) to evaluate different approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We assess the performance of the proposed SCCL model on eight benchmark datasets for short text clustering. <ref type="table">Table 2</ref> provides an overview of the main statistics, and the details of each dataset are as follows.</p><p>? SearchSnippets is extracted from web search snippets, which contains 12,340 snippets associated with 8 groups <ref type="bibr" target="#b35">Phan et al. (2008)</ref>.</p><p>? StackOverflow is a subset of the challenge data published by Kaggle 2 , where 20,000 question titles associated with 20 different categories are selected by <ref type="bibr" target="#b49">Xu et al. (2017)</ref>.</p><p>? Biomedical is a subset of the PubMed data distributed by BioASQ 3 , where 20,000 paper titles from 20 groups are randomly selected by <ref type="bibr" target="#b49">Xu et al. (2017)</ref>.</p><p>? AgNews is a subset of news titles <ref type="bibr">(Zhang and LeCun, 2015)</ref>, which contains 4 topics selected by <ref type="bibr" target="#b39">Rakib et al. (2020)</ref>.</p><p>? For each dataset, we use Contextual Augmenter <ref type="bibr" target="#b20">(Kobayashi, 2018;</ref><ref type="bibr" target="#b25">Ma, 2019)</ref> to obtain the augmentation set, as it consistently outperforms the other options explored in Section 4.3.1. <ref type="table" target="#tab_2">AgNews  21K  8000  23  4  1  StackOverflow  15K 20000  8  20  1  Biomedical  19K 20000  13  20  1  SearchSnippets 31K 12340  18  8  7  GooglenewsTS 20K 11109  28  152 143  GooglenewsS  18K 11109  22  152 143  GooglenewsT  8K  11109  6  152 143  Tweet  5K  2472  8  89  249   Table 2</ref>: Dataset statistics. |V |: the vocabulary size; N D : number of short text documents; Len: average number of words in each document; N C number of clusters; L/S: the ratio of the size of the largest cluster to that of the smallest cluster.</p><formula xml:id="formula_7">Dataset |V | Documents Clusters N D Len N C L/S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with State-of-the-art</head><p>We first demonstrate that our model can achieve state-of-the-art or highly competitive performance on short text clustering. For comparison, we consider the following baselines.</p><p>? STCC <ref type="bibr" target="#b49">(Xu et al., 2017)</ref> consists of three independent stages. For each dataset, it first pretrains a word embedding on a large in-domain corpus using the Word2Vec method <ref type="bibr" target="#b29">(Mikolov et al., 2013a)</ref>. A convolutional neural network is then optimized to further enrich the repre-sentations that are fed into K-means for the final stage clustering. To demonstrate that our model is robust against the noisy input that often poses a significant chal- <ref type="figure">Figure 3</ref>: Ablation study of SCCL. In SCCL-Seq, we first train the model using Instance-CL, and then optimize the clustering objective. We exclude Biomedical for better visualization, full plot can be found in Appendix A.4. lenge for short text clustering, we do not apply any pre-processing procedures on any of the eight datasets. In contrast, all baselines except BoW and TF-IDF considered in this paper either preprocessed the Biomedical dataset <ref type="bibr" target="#b49">(Xu et al., 2017;</ref><ref type="bibr" target="#b13">Hadifar et al., 2019)</ref> or all eight datasets by removing the stop words, punctuation, and converting the text to lower case <ref type="bibr" target="#b39">(Rakib et al., 2020)</ref>.</p><p>We report the comparison results in <ref type="table">Table 1</ref>. Our SCCL model outperforms all baselines by a large margin on most datasets. Although we are lagging behind <ref type="bibr" target="#b13">Hadifar et al. (2019)</ref> on Biomedical, SCCL still shows great promise considering the fact that Biomedical is much less related to the general domains on which the transformers are pretrained. In contrast, <ref type="bibr" target="#b13">Hadifar et al. (2019)</ref> learn the word embeddings on a large in-domain biomedical corpus, followed by a layer-wise pretrained autoencoder to further enrich the representations.</p><p>Rakib et al. (2020) also shows better Accuracy on Tweet and GoogleNews-T, for which we hypothesize two reasons. First, both GoogleNews and Tweet have fewer training examples with much more clusters. Thereby, it's challenging for instance-wise contrast learning to manifest its advantages, which often requires a large training dataset. Second, as implied by the clustering perfermance evaluated on BoW and TF-IDF, clustering GoogleNews and Tweet is less challenging than clustering the other four datasets. Hence, by applying agglomerative clustering on the carefully selected pairwise similarities of the preprocessed data, Rakib et al. (2020) can achieve good performance, especially when the text instances are very short, i.e., Tweet and GoogleNews-T. We also high-light the scalability of our model to large scale data, whereas agglomerative clustering often suffers from high computation complexity. We discuss this further in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To better validate our model, we run ablations in this section. For illustration, we name the clustering component described in Section 3.2 as Clustering. Besides Instance-CL and Clustering, we also evaluate SCCL against its sequential version (SCCL-Seq) where we first train the model with Instance-CL, and then optimize it with Clustering.</p><p>As shown in <ref type="figure">Figure 3</ref>, Instance-CL also groups semantically similar instances together. However, this grouping effect is implicit and data-dependent. In contrast, SCCL consistently outperforms both Instance-CL and Clustering by a large margin. Furthermore, SCCL also achieves better performance than its sequential version, SCCL-Seq. The result validates the effectiveness and importance of the proposed joint optimization framework in leveraging the strengths of both Instance-CL and Clustering to compliment each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">SCCL leads to better separated and less dispersed clusters</head><p>To further investigate what enables the better performance of SCCL, we track both the intra-cluster distance and the inter-cluster distance evaluated in the representation space throughout the learning process. For a given cluster, the intra-cluster distance is the average distance between the centroid and all samples grouped into that cluster, and the inter-cluster distance is the distance to its closest neighbor cluster. In <ref type="figure" target="#fig_3">Figure 4</ref>, we report each type  of distance with its mean value obtained by averaging over all clusters, where the clusters are defined either regarding the ground truth labels (solid lines) or the labels predicted by the model (dashed lines).  <ref type="figure" target="#fig_3">Figure 4</ref> shows Clustering achieves smaller intracluster distance and larger inter-cluster distance when evaluated on the predicted clusters. It demonstrates the ability of Clustering to tight each selflearned cluster and separate different clusters apart. However, we observe the opposite when evaluated on the ground truth clusters, along with poor Accuracy and NMI scores. One possible explanation is, data from different ground-truth clusters often have significant overlap in the embedding space before clustering starts (see upper left plot in <ref type="figure" target="#fig_0">Figure 1</ref>), which makes it hard for our distance-based clustering approach to separate them apart effectively.</p><p>Although the implicit grouping effect allows Instance-CL attains better Accuracy and NMI scores, the resulting clusters are less apart from each other and each cluster is more dispersed, as indicated by the smaller inter-cluster distance and larger intra-cluster distance. This result is unsurprising since Instance-CL only focuses on instance discrimination, which often leads to a more dispersed embedding space. In contrast, we leverage the strengths of both Clustering and Instance-CL to compliment each other. Consequently, <ref type="figure" target="#fig_3">Figure 4</ref> shows SCCL leads to better separated clusters with each cluster being less dispersed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Exploration of Data Augmentations</head><p>To study the impact of data augmentation, we explore three different unsupervised text augmentations: (1) WordNet Augmenter 5 transforms an input text by replacing its words with WordNet synonyms <ref type="bibr" target="#b31">(Morris et al., 2020;</ref><ref type="bibr" target="#b42">Ren et al., 2019)</ref>. (2) Contextual Augmenter 6 leverages the pretrained transformers to find top-n suitable words of the input text for insertion or substitution <ref type="bibr" target="#b20">(Kobayashi, 2018;</ref><ref type="bibr" target="#b25">Ma, 2019)</ref>. We augment the data via word substitution, and we choose Bertbase and Roberta to generate the augmented pairs. (3) Paraphrase via back translation 7 generates paraphrases of the input text by first translating it to another language (French) and then back to English. When translating back to English, we used the mixture of experts model <ref type="bibr" target="#b45">(Shen et al., 2019)</ref> to generate ten candidate paraphrases per input to increase diversity.</p><p>For both WordNet Augmenter and Contextual Augmenter, we try three different settings by choosing the word substitution ratio of each text instance to 10%, 20%, and 30%, respectively. As for Paraphrase via back translation, we compute the BLEU score between each text instance and its ten candidate paraphrases. We then select three pairs, achieving the highest, medium, and lowest BLEU scores, from the ten condidates of each instance. The best results 8 of each augmentation technique are summarized in <ref type="table" target="#tab_2">Table 3</ref>, where Contexual Augmenter substantially outperforms the other two. We conjecture that this is due to both Contextual Augmenter and SCCL leverage the pretrained transformers as backbones, which allows Contextual Augmenter to generate more informative augmentations. <ref type="figure" target="#fig_4">Figure 5</ref> shows the impact of using composition of data augmentations, in which we explored Contextual Augmenter and CharSwap Augmenter 9 (Morris et al., 2020). As we can see, using composition of data augmentations does boost the performance of SCCL on GoogleNews-TS where the average number of words in each text instance is 28 (see <ref type="table">Table 2</ref>). However, we observe the opposite on StackOverflow where the average number of words in each instance is 8. This result differs from what has been observed in the image domain where using composition of data augmentations is crucial for contrastive learning to attain good performance. Possible explanations is that generating high-quality augmentations for textual data is more challenging, since changing a single word can invert the semantic meaning of the whole instance. This challenge is compounded when a second round of augmentation is applied on very 8 Please refer to Appendix A.2 for details. 9 A simple technique that augments text by substituting, deleting, inserting, and swapping adjacent characters short text instances, e.g., StackOverflow. We further demonstrate this in <ref type="figure" target="#fig_4">Figure 5 (right)</ref>, where the augmented pairs of StackOverflow largely diverge from the original texts in the representation space after the second round of augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Composition of Data Augmentations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel framework leveraging instance-wise contrastive learning to support unsupervised clustering. We thoroughly evaluate our model on eight benchmark short text clustering datasets, and show that our model either substantially outperforms or performs highly comparably to the state-of-the-art methods. Moreover, we conduct ablation studies to better validate the effectiveness of our model. We demonstrate that, by integrating the strengths of both bottom-up instance discrimination and top-down clustering, our model is capable of generating high-quality clusters with better intra-cluster and inter-clusters distances. Although we only evaluate our model on short text data, the proposed framework is generic and is expected to be effective for various kinds of text clustering problems.</p><p>In this work, we explored different data augmentation strategies with extensive comparisons. However, due to the discrete nature of natural language, designing effective transformations for textual data is more challenging compared to the counterparts in the computer vision domain. One promising direction is leveraging the data mixing strategies <ref type="bibr">(Zhang et al., 2017b)</ref> to either obtain stronger augmentations <ref type="bibr">(Kalantidis et al., 2020)</ref> or alleviate the heavy burden on data augmentation <ref type="bibr">(Lee et al., 2020)</ref>. We leave this as future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Experiment Setup</head><p>We use the Adam optimizer <ref type="bibr" target="#b19">(Kingma and Ba, 2015)</ref> with batch size of 400. We use distilbert-basenli-stsb-mean-tokens in the Sentence Transformers library <ref type="bibr" target="#b40">(Reimers and Gurevych, 2019a)</ref> as the backbone, and we set the maximum input length to 32. We use a constant learning rate 5e-6 to optimize the backbone, while setting learning rate to 5e-4 to optimizing both the Clustering head and Instance-CL head. Same as <ref type="bibr" target="#b48">Xie et al. (2016)</ref>; Hadifar et al. <ref type="formula" target="#formula_0">(2019)</ref>, we set ? = 1 for all datasets except Biomedical where we use ? = 10. As mentioned in Section 3, we set ? = 0.5 for optimizing the constrastive loss. We tried different ? values in the range of (0, 1] and found using ? = 0.5 yields comparatively better yet stable performance across datasets for both Instance-CL and SCCL. For fair comparison between SCCL and its components or variants, we report the clustering performance for each of them by applying KMeans on the representations post the associated training processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Data Augmentation</head><p>Exploration of Data Augmentations As mentioned in Section 4.3.1, we tried three different augmentation strengths for both WordNet Augmenter and Contextual Augmenter by choosing the word substitution ratio of each text instance as 10%, 20%, and 30%, respectively. For each augmentation strength, we generate a pair of augmentations for each text instance. As for Paraphrase via back translation, we computed the BLEU score between each original instance and its ten candidate paraphrases. We then select three pairs, achieving the highest, medium, and lowest BLEU scores, from the ten condidates as the augmented data. For each augmentation method, we run SCCL on all three augmentation strengths independently and report the best result. For both WordNet Augmenter and Contextual Augmenter, we observe that comparatively longer text instances, i.e., those in AgNews, SearchSnippets, GoogleNewsTS, and GoogleNewsS, benefit from stronger augmentation. In contrast, Paraphrase via back translation shows better results when evaluated on the augmented pairs achieving the lowest BLEU scores with the original instance, i.e., the pair achieving the two lowest scores among all ten condidate paraphrases for each text instance.</p><p>Building Effective Data Augmentations for NLP As discussed in Section 4.3.2, using composition of data augmentations is not always beneficial for short text clustering. Because changing a single word can invert the meaning of the whole sentence, and the challenge is compounded when applied a second round data augmentation to short text data. However, we would hopefully cross the hurdle soon, as more effective approaches are keeping developed by the NLP community <ref type="bibr" target="#b37">Qu et al. (2020);</ref><ref type="bibr" target="#b12">Giorgi et al. (2020)</ref>; <ref type="bibr" target="#b28">Meng et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Alternative Clustering Loss for SCCL</head><p>In the current form of SCCL, the clustering loss is optimized on the original dataset only. However, several alternatives could be considered, we discuss two options here to encourage further explorations.</p><p>Alternative 1. Let j 1 and j 2 denote the indices of the augmented pair for the j th text instance in the original set, respectively. For the augmented instance j 1 , we then push the cluster assignment probability towards the target distribution obtained by the other instance j 2 , and vice versa. That is, we replace Eq (5) with the following C j = KL p j 1 ||q j 2 + KL p j 2 ||q j 1</p><p>Here p and q denote the target distribution and the cluster assignment probability defined in Eqs (3) <ref type="figure" target="#fig_5">Figure 6</ref>: Ablation study of our proposed SCCL model. In SCCL-Seq, we first train the model using Instance-CL, and then optimize the clustering objective. and (4), respectively.</p><p>Alternative 2. Let j 0 and j 1 , j 2 denote the indices of the original text instance and its augmented pair, respectively. We then use the original instance as anchor, and push the cluster assignments of the augmented pair towards it by optimizing the following C j = KL p j 0 ||q j 1 + KL p j 0 ||q j 2 (9)</p><p>Exploring <ref type="formula" target="#formula_8">(8)</ref> and <ref type="formula">(9)</ref> is out of the scope of this paper, however, it's worth trying when applying SCCL to solve different application problems. Especially considering that the above alternatives might lead to further performance improvement by jointly optimizing the instance-level and the cluster assignment level contrastive learning losses. time complexity, and O(N 2 /K) storage complexity. Moreover, the agglomerative clustering algorithm could require O(N 2 log(N/K)) time complexity. Therefore, HAC-SD is less feasible in presence of large scale data. In contrast, SCCL performs standard stochastic optimization, the time complexity linearly scales with N since SCCL often requires 20 ? 100 epochs to converge, which is often much smaller than the number of data examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Supplement materials for ablation study</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>TSNE visualization of the embedding space learned on SearchSnippets using Sentence Transformer<ref type="bibr" target="#b40">(Reimers and Gurevych, 2019a)</ref> as backbone. Each color indicates a ground truth semantic category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Training framework SCCL. During training, we jointly optimize a clustering loss over the original data instances and an instance-wise contrastive loss over the associated augmented pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>Self-Train<ref type="bibr" target="#b13">(Hadifar et al., 2019)</ref> enhances the pretrained word embeddings in<ref type="bibr" target="#b49">Xu et al. (2017)</ref> using SIF<ref type="bibr" target="#b0">(Arora et al., 2017)</ref>. Following<ref type="bibr" target="#b48">Xie et al. (2016)</ref>, it adopts an autoencoder obtained by layer-wise pretraining (Van Der Maaten, 2009), which is then further tuned with a clustering objective same as that in Section 3.2. Both<ref type="bibr" target="#b48">Xie et al. (2016)</ref> and<ref type="bibr" target="#b13">Hadifar et al. (2019)</ref> update the target distribution through carefully chosen intervals that vary across datasets, while we update it per iteration yet still achieve significant improvement.? HAC-SD (Rakib et al., 2020) 4 applies hierarchical agglomerative clustering on top of a sparse pairwise similarity matrix obtained by zeroing-out similarity scores lower than a chosen threshold value.? BoW &amp; TF-IDF are evaluated by applying Kmeans on top of the associated features with dimension being 1500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Cluster-level evaluation on SearchSnippets. Each plot is summarized over five random runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Impact of using composition of data augmentations. Either only Contextual Augmenter (blue) is used, or Contextual Augmenter and CharSwap Augmenter are applied sequentially (orange). (Left) Clustering accuracy versus variant augmentation strengths, the x-axis indicates the percentage of words in each instance being changed by the associated data augmentation technique. (Right) Distribution of the cosine similarity between the representations of each original text and its augmented pair at the beginning of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>provides the full version of Figure 3 in Section 4. A.5 Comparison with Rakib et al. (2020) While Rakib et al. (2020) achieve better Accuracy on Tweet and GoogleNews-T, we highlight the scalability of our model to large scale data, whereas Rakib et al. (2020) depend on agglomerative clustering which often suffers from high computation complexity. Specifically, let N denote the number of training examples, and K denote the number of clusters. The HAC-SD method proposed by Rakib et al. (2020) first computes the pairwise similarity among all possible pairs of the data, and then sorts the N 2 similarity values so as to select the top N 2 /K pairwise similarity as the input to the agglomerative clustering algorithm. Thereby, before clustering, HAC-SD could result in O(N 2 log N )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Tweet consists of 2,472 tweets with 89 categories (Yin and Wang, 2016). ? GoogleNews contains titles and snippets of 11,109 news articles related to 152 events (Yin and Wang, 2016). Following (Rakib et al., 2020), we name the full dataset as GoogleNews-TS, and GoogleNews-T and GoogleNews-S are obtained by extracting the titles and the snippets, respectively.</figDesc><table><row><cell></cell><cell cols="2">AgNews</cell><cell cols="6">SearchSnippets StackOverflow Biomedical</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell cols="2">ACC NMI</cell></row><row><cell>BoW</cell><cell>27.6</cell><cell>2.6</cell><cell>24.3</cell><cell>9.3</cell><cell>18.5</cell><cell>14.0</cell><cell>14.3</cell><cell>9.2</cell></row><row><cell>TF-IDF</cell><cell>34.5</cell><cell>11.9</cell><cell>31.5</cell><cell>19.2</cell><cell>58.4</cell><cell>58.7</cell><cell cols="2">28.3 23.2</cell></row><row><cell>STCC</cell><cell>-</cell><cell>-</cell><cell>77.0</cell><cell>63.2</cell><cell>51.1</cell><cell>49.0</cell><cell cols="2">43.6 38.1</cell></row><row><cell>Self-Train</cell><cell>-</cell><cell>-</cell><cell>77.1</cell><cell>56.7</cell><cell>59.8</cell><cell>54.8</cell><cell cols="2">54.8 47.1</cell></row><row><cell>HAC-SD</cell><cell>81.8</cell><cell>54.6</cell><cell>82.7</cell><cell>63.8</cell><cell>64.8</cell><cell>59.5</cell><cell cols="2">40.1 33.5</cell></row><row><cell>SCCL</cell><cell>88.2</cell><cell>68.2</cell><cell>85.2</cell><cell>71.1</cell><cell>75.5</cell><cell>74.5</cell><cell cols="2">46.2 41.5</cell></row><row><cell></cell><cell cols="6">GoogleNews-TS GoogleNews-T GoogleNews-S</cell><cell cols="2">Tweet</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell cols="2">ACC NMI</cell></row><row><cell>BoW</cell><cell>57.5</cell><cell>81.9</cell><cell>49.8</cell><cell>73.2</cell><cell>49.0</cell><cell>73.5</cell><cell cols="2">49.7 73.6</cell></row><row><cell>TF-IDF</cell><cell>68.0</cell><cell>88.9</cell><cell>58.9</cell><cell>79.3</cell><cell>61.9</cell><cell>83.0</cell><cell cols="2">57.0 80.7</cell></row><row><cell>STCC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Self-Train</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HAC-SD</cell><cell>85.8</cell><cell>88.0</cell><cell>81.8</cell><cell>84.2</cell><cell>80.6</cell><cell>83.5</cell><cell cols="2">89.6 85.2</cell></row><row><cell>SCCL</cell><cell>89.8</cell><cell>94.9</cell><cell>75.8</cell><cell>88.3</cell><cell>83.1</cell><cell>90.4</cell><cell cols="2">78.2 89.2</cell></row></table><note>Table 1: Clustering results on eight short text datasets. Our results are averaged over five random runs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Results of SCCL evaluated with different augmentation techniques: WordNet augmenter (WNet), para-</cell></row><row><cell>phrase via back translation (Para), and contextual augmenter (Ctxt). Each technique is detailed in Section 4.3.1.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ amazon-research/sccl.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/predict-closed-questions-onstackoverflow/download/train.zip 3 http://participants-area.bioasq.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">They further boost the performance via an iterative classification trained with high-confidence pseudo labels extracted after each round of clustering. Since the iterative classification strategy is orthogonal to the clustering algorithms, we only evaluate against with their proposed clustering algorithm for fair comparison.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/QData/TextAttack 6 https://github.com/makcedward/nlpaug 7 https://github.com/pytorch/fairseq/ tree/master/examples/paraphraser</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selforganizing neural network that discovers surfaces in random-dot stereograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanna</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6356</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving news articles recommendations via user clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Bouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Tsogkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="237" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaussian parsimonious clustering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Govaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="781" to="793" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">2020a. A simple framework for contrastive learning of visual representations</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. 2020b. Improved baselines with momentum contrastive learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>John M Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">Declutr: Deep contrastive learning for unsupervised textual representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A self-training approach for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hadifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP</title>
		<meeting>the 4th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01028</idno>
		<title level="m">Philippe Weinzaepfel, and Diane Larlus. 2020. Hard negative mixing for contrastive learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<title level="m">Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovering hot topics using twitter streaming data social topic detection and geographic clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwi-Gang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyon</forename><surname>Kyeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<imprint>
			<biblScope unit="page" from="1215" to="1220" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06201</idno>
		<title level="m">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08887</idno>
		<title level="m">Jinwoo Shin, and Honglak Lee. 2020. i-mix: A strategy for regularizing contrastive representation learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Nlp augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08473</idno>
		<title level="m">Coco-lm: Correcting and contrasting text sequences for language model pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Textattack: A framework for adversarial attacks, data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">X</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Lifland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Yong</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Grigsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>and adversarial training in nlp</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to classify short and sparse text &amp; web with hidden topics from largescale data collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susumu</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Demystifying contrastive self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13916</idno>
	</analytic>
	<monogr>
		<title level="m">variances, augmentations and dataset biases</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08670</idno>
		<title level="m">Coda: Contrast-enhanced and diversity-promoting data augmentation for natural language understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Enhancement of short text clustering by iterative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Md Rashadul Hasan Rakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Zeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Jankowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milios</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11631</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples through probability weighted word saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Shuhuai Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th annual meeting of the association for computational linguistics</title>
		<meeting>the 57th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1085" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualization of search results: a comparative evaluation of text, 2d, and 3d interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">V</forename><surname>Sebrechts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><forename type="middle">J</forename><surname>Cugini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Laskowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Vasilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Kluger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01587</idno>
		<title level="m">Spectralnet: Spectral clustering using deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mixture models for diverse machine translation: Tricks of the trade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a parametric embedding by preserving local structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="384" to="391" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-taught convolutional neural networks for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
