<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for Embedded Vision Systems and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-29">29 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tse-Wei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Canon Innovative Solution (Beijing) Co</orgName>
								<address>
									<settlement>Ltd</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Canon Innovative Solution (Beijing) Co</orgName>
								<address>
									<settlement>Ltd</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Canon Innovative Solution (Beijing) Co</orgName>
								<address>
									<settlement>Ltd</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yin</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Canon Innovative Solution (Beijing) Co</orgName>
								<address>
									<settlement>Ltd</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayuki</forename><surname>Ito</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kinya</forename><surname>Osa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Kato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Twchen@ieee</forename><surname>Org</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<orgName type="institution">Canon Inc</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for Embedded Vision Systems and Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-29">29 Apr 2021</date>
						</imprint>
					</monogr>
					<note>arXiv:2104.14126v1 [cs.CV]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field of view (FOV) of convolutional neural networks is highly related to the accuracy of inference. Dilated convolutions are known as an effective solution to the problems which require large FOVs. However, for general-purpose hardware or dedicated hardware, it usually takes extra time to handle dilated convolutions compared with standard convolutions. In this paper, we propose a network module, Cascaded and Separable Structure of Dilated (CASSOD) Convolution, and a special hardware system to handle the CAS-SOD networks efficiently. A CASSOD-Net includes multiple cascaded 2 ? 2 dilated filters, which can be used to replace the traditional 3 ? 3 dilated filters without decreasing the accuracy of inference. Two example applications, face detection and image segmentation, are tested with dilated convolutions and the proposed CASSOD modules. The new network for face detection achieves higher accuracy than the previous work with only 47% of filter weights in the dilated convolution layers of the context module. Moreover, the proposed hardware system can accelerate the computations of dilated convolutions, and it is 2.78 times faster than traditional hardware systems when the filter size is 3 ? 3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dilated convolutions in Convolutional Neural Networks (CNNs) can be applied to different kinds of applications, including audio processing <ref type="bibr" target="#b5">[6]</ref>, crowd counting <ref type="bibr" target="#b8">[9]</ref>, semantic image segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20]</ref>, image classification <ref type="bibr" target="#b6">[7]</ref>, image super-resolution <ref type="bibr" target="#b10">[11]</ref>, road extraction <ref type="bibr" target="#b22">[22]</ref>, image denoising <ref type="bibr" target="#b17">[17]</ref>, face detection <ref type="bibr" target="#b7">[8]</ref>, object detection <ref type="bibr" target="#b9">[10]</ref>, and so on. For many vision applications, the receptive field, or the field of view (FOV) <ref type="bibr" target="#b11">[12]</ref> of convolutional neural networks is highly related to the accuracy of inference. Large FOV can be obtained by increasing the filter size or by increasing the number of convolution layers. However, the computational time might also increase because the computational cost is proportional to the filter size and the number of layers. Dilated convolutions are known as an effective method to enlarge the FOV of a network without increasing the computational costs. By adjusting the dilation rate D, a 3 ? 3 filter can be enlarged to a (2D + 1) ? (2D + 1) filter with 3 ? 3 Multiply-Accumulate (MAC) operations. Deng et al. propose a context module, which is inspired by the SSH face detector <ref type="bibr" target="#b16">[16]</ref>, to increase the FOV <ref type="bibr" target="#b1">[2]</ref>. Wu et al. propose a Joint Pyramid Up-sampling (JPU) module and formulate the task of extracting high-resolution feature maps into a joint up-sampling problem <ref type="bibr" target="#b20">[20]</ref>. In the JPU structure, the generated feature maps are up-sampled and   <ref type="bibr" target="#b2">[3]</ref>. The dilation rate in the front-end module is gradually increased to extract the features from a large range, and the dilation rate in the subsequent LFE module is gradually decreased to aggregate local features generated by the front-end module. The above mentioned network models are designed for applications related to face detection and image segmentation. There are still many kinds of network architectures containing dilated convolution layers, which are used to improve the accuracy of other applications.</p><formula xml:id="formula_0">3 2 ? C 1 ? C 2 CASSOD-A 2 2 ? C 1 ? (1 + C 2 ) (1st layer: DW Conv. * ) CASSOD-C (No DW Conv. * ) 2 2 ? (C 1 + C 2 ) ? C 1 or 2 2 ? (C 1 + C 2 ) ? C 2 Dilated / Depthwise Convolution 3 2 ? C 1 CASSOD-D 2 2 ? (C 1 ? 2) (1st</formula><p>In order to implement dilated convolutions on mobile devices and embedded system platforms, it is necessary to reduce the memory size and the computational costs without decreasing the accuracy. For some hardware devices, including GPUs, convolution operations are only optimized for standard convolutions. Even though the 3 ? 3 dilated filters require only 3?3 MAC operations, the overhead to skip the pixels which do not engage in the process of convolutions is not always zero. For some dedicated hardware systems based on systolic arrays, when the dilation rate is high, the memory footprint might increase since the addresses of the pixels to be processed are not consecutive.</p><p>In this paper, we propose a new network module, Cascaded and Separable Structure of Dilated (CASSOD) Convolutions, to approximate the 3?3 dilated convolutions with low memory cost for filter weights and low computational costs for convolutions. The concept of the proposed method is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Network Module</head><p>In this section, the 2 ? 2 dilated convolutions and the proposed CASSOD module are introduced in the following subsections. The cascaded 2 ? 2 dilated convolution, which can be a separable version of the traditional 3 ? 3 dilated convolution, is an important component of the CASSOD module. In this paper, the 2 ? 2 (or 3 ? 3) dilated convolutions refer to the convolutions where the filter size is 2 ? 2 (or 3 ? 3) before the filters are dilated (D = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dilated Convolutions with Fewer Filter Weights</head><p>In previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22]</ref>, the dilated filters are generated based on 3 ? 3 filters. A total of 9 filter weights are used to compute the convolution results for one input feature map. The FOV is expanded when the dilation rate D is larger than 1. The larger the dilation rate D, the larger the FOV.</p><p>The dilated filters which are generated based on 2 ? 2 filters, are proposed in this work. The output result of the proposed dilated filters is calculated based on the following equation.</p><formula xml:id="formula_1">O (i,j) = C?1 c=0 1 x=0 1 y=0 I (c,i? D 2 +x?D,j? D 2 +y?D) ? W (c,x,y) ,<label>(1)</label></formula><p>where O (i,j) is the output feature map, I (c,i,j) is the c-th input feature map, and W (c,x,y) is the filter weight for the c-th input feature map. The parameters (i, j) represent the index of the pixels in the feature maps, and the ranges of both i and j depend on the size of feature maps. The parameter C denotes the number of channels. There are only 4 filter weights for 1 input feature map and the ranges of both x and y are [0, <ref type="bibr" target="#b0">1]</ref>.</p><p>Two examples of the 2?2 filters and dilated convolutions are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. <ref type="figure" target="#fig_2">Fig. 2</ref>(a) shows an example where the dilation rate D is 2. Two cascaded 2?2 dilated convolutions where the dilation rate D is 2 can be an approximation to a 3 ? 3 dilated convolution with the same dilation rate since the positions of zero weights are exactly the same. Similarly, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>(b), two cascaded 2 ? 2 dilated convolutions with the dilation rate D = 4 can be an approximation to a 3 ? 3 dilated convolution with the same dilation rate. The dilation rate D is always set to a multiple of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cascaded and Separable Structure of Dilated (CASSOD) Convolution</head><p>The concept of the proposed CASSOD module is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The upper part of the figure shows an example of dilated convolutions <ref type="bibr" target="#b19">[19]</ref> where the dilation rate D is 2.</p><p>There are C 1 channels in the input feature maps, and C 2 channels in the output feature maps. The number of filter weights of the dilated convolutions is 3</p><formula xml:id="formula_2">? 3 ? C 1 ? C 2 .</formula><p>The lower part of the figure shows an example of the proposed CASSOD module where the dilation rate D is 2. There are two convolution layers in this module and 3 sets of feature maps. Either of the first convolution layer or the second convolution layer can be a depthwise convolution <ref type="bibr" target="#b4">[5]</ref> layer. The variations (CASSOD-A,C,D) are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In the CASSOD-A module, there are C 1 , C 1 , and C 2 channels in the first, the second, and the third set of feature maps, respectively. The numbers of channels in the first and the second set of feature maps are the same because the first convolution layer includes depthwise separable operations. The first convolution layer includes a series of 2 ? 2 depthwise and dilated filters, which are different from the traditional 3 ? 3 dilated filters. The second convolution layers includes a series of 2 ? 2 dilated filters. The number of filter weights of the dilated convolutions is <ref type="table" target="#tab_0">Table 1</ref> also shows the comparison of the number of filter weights. It can be observed that, when C 2 is large, the number of filter weights in the CASSOD-A module is close to 44.4% (4/9) of the number of filter weights in the traditional dilated convolution.</p><formula xml:id="formula_3">2 2 ? C 1 ? (1 + C 2 ).</formula><p>In the CASSOD-C module, there are C 1 , C 1 (or C 2 ), and C 2 channels in the first, the second, and the third set of feature maps, respectively. Both convolution layers include a series of 2 ? 2 dilated filters. The number of filter weights of the dilated convolutions is  <ref type="figure">Figure 3</ref>: An example of the architecture of the proposed hardware system. C 2 , the number of filter weights of the CASSOD-C module is close to 88.9% (8/9) of the number of filter weights in the traditional dilated convolution. The CASSOD-A,C modules have lower computational costs than the traditional 3 ? 3 dilated convolutions. The number of filter weights is also proportional to the computational cost, which is equal to the number of MAC operations. Compared with the traditional 3 ? 3 dilated convolutions, 1 additional convolution layer is required to implement the CASSOD modules. The batch normalization operations and the activation functions (e.g. ReLU) can be included in the additional convolution layer if necessary. The CASSOD-A,C modules are alternatives to the dilated convolutions, and the CASSOD-D module is an alternative to the depthwise and dilated convolutions. The number of filter weights of the CASSOD-D module is close to 88.9% (8/9) of the depthwise and dilated convolutions.</p><formula xml:id="formula_4">2 2 ? (C 1 + C 2 ) ? C 1 or 2 2 ? (C 1 +C 2 )?C 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Hardware Architecture</head><p>The analysis of computational time of dilated convolutions and the proposed hardware architecture are shown in the following subsections. <ref type="table" target="#tab_2">Table 2</ref> shows an example of the computational time of dilated convolutions when the size of input image is 320 ? 320 pixels. A network with 3 layers is used to measure the computational time of CPU, and a network with 10 layers is used to measure the computational time of GPU. There are 64 input channels and 64 output channels in each layer of the networks, and the dilation rates D of the 3 ? 3 convolution layers are set to the same value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Computational Time of Dilated Convolutions</head><p>The results show that the processing time does not change much when the dilation rate D is larger than 1 for either CPU or GPU. The computational time of standard convolutions, where the dilation rate is set to 1, is the shortest, and the reason can be that the framework (e.g. CUDA li- brary) includes some optimized operations to accelerate the computations of standard convolutions. The speed of dilated convolutions may depend on the performance of system platforms, but the dilated convolutions with a dilation rate larger than 1 cannot necessarily achieve the same level of speed as standard convolutions with a dilation rate of 1.</p><p>The goal of this work is to design a hardware system which can handle both dilated convolutions and standard convolutions efficiently.</p><p>For hardware implementation, the pixel values in the feature maps are usually stored in a shift register array. One solution to handle dilated convolutions using a shift register array is to add zeros to the filter weights and compute the results of standard convolutions. To handle a dilated 3 ? 3 filter where the dilation rate D is 2, it is necessary to compute the products of 25 filter weights and 25 input pixels. A total of 16 filter weights equal to zero can be skipped. The computational time is proportional to the size of dilated filters, and the efficiency of filter processing is relatively low compared with CPU and GPU.</p><p>To solve this problem, a new hardware architecture is proposed to speed up dilated convolutions. An example of the architecture of the proposed hardware system is shown in <ref type="figure">Fig. 3</ref>, which includes 6 modules and DRAM. The filter weights stored in the "Filter Weight Memory" are sent to the "Filter Weight Cache," and the feature maps of the current convolution layer stored in the "Pixel Memory" are sent to the "Pixel Array." The filter weights and the pixels of feature maps are sent to the "Convolution Processor" to compute the convolution results, and the convolution results are sent to the "Activation and Pooling Unit" to generate the feature maps for the next convolution layer. The "Pixel Array" includes multiple hierarchical stages of pixel caches, which can generate the input pixels for dilated convolutions with a different dilation rate D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pixel Array for Dilated Convolutions</head><p>The "Pixel Array," including multiple hierarchical stages, can handle dilated convolutions with different dilation rates efficiently. The proposed hardware architecture can be implemented in a Field-Programmable Gate Array (FPGA) or an Application-Specific Integrated Circuit (ASIC). <ref type="figure" target="#fig_3">Fig. 4</ref> shows an example of the architecture and the interconnections of the proposed shift register array, where there are 3 hierarchical stages. In this example, there are 6?6 pixel buffers (or selectors) in each stage, and each pixel buffer (or selector) is connected to 4 other pixel buffers (or selectors) in the up, right, down, and left directions. It means that the pixel stored in the pixel buffer can be transferred to one of the 4 connected pixel buffers (or selectors). In the first hierarchical stage, the input of a pixel buffer is connected to the input of its neighboring pixel buffer and the pixel selector with the same position in the second stage. The difference of the shifted index and the original index of the pixel, X 0 , is 0 or 1 (2 0 ). Similarly, in the second hierarchical stage, the difference of the shifted index and the original index of the pixel, X 1 , is 0 or 2 (2 1 ), and in the third hierarchical stage, the difference of the shifted index and the original index of the pixel, X 2 , is 0 or 4 (2 2 ).</p><p>The supported dilation rate, which is equal to the total difference of the shifted index and the original index of the pixel in all hierarchical stages (X 0 , X 1 , X 2 , ... , X H?1 ) is shown in the following equation.</p><formula xml:id="formula_5">D = H?1 h=0 X h ? 2 h ,<label>(2)</label></formula><p>where H is the total number of the hierarchical stages. In this example, H is equal to 3, but H can be set to any number to support 2 H ? 1 dilation rates (D = [1, 2 H ? 1]) according to the network architectures. When the gate count of each stage is the same, the total hardware cost is proportional to the number of hierarchical stage, H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The experimental results contain 3 parts. The first part is the comparison of accuracy of face detection. The second part is the comparison of accuracy of image segmentation. The third part is the analysis of the performance of the proposed hardware architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Accuracy of Face Detection</head><p>The proposed network architecture, CASSOD-Net, is evaluated based on the RetinaFace <ref type="bibr" target="#b1">[2]</ref>, which is designed for face detection tasks. The context module, inspired by the SSH face detector <ref type="bibr" target="#b16">[16]</ref>, is used to increase the FOV and enhance the rigid context modeling power. To compare the accuracy, the convolution layers in the context modules is  replaced by the Feature Enhance Module (FEM), which includes dilated convolution structures, in the Dual Shot Face Detector (DSFD) <ref type="bibr" target="#b7">[8]</ref>. Then, the dilated convolutions in the DSFD <ref type="bibr" target="#b20">[20]</ref> are replaced by the proposed CASSOD modules. The network architecture of the modified context module is shown in <ref type="figure">Fig. 5</ref>. To keep the computational costs at the same level, the number of input channels of the FEM is changed from 256 to 64, and the number of input channels in the remaining layers is reduced by the same ratio. <ref type="figure">Fig. 5(a)</ref> shows the original context module in the Reti-naFace, and <ref type="figure">Fig. 5(b)</ref> shows the modified context module with FEM, which includes dilated convolution layers with a dilation rate (D) of 2.</p><p>MobileNet-0.25 <ref type="bibr" target="#b4">[5]</ref> is used as the backbone network. The accuracy of face detection is shown in <ref type="table" target="#tab_5">Table 3</ref>. After replacing the dilated convolutions with the CASSOD-C and CASSOD-A modules, the accuracy does not decrease. Besides, the proposed CASSOD module can achieve higher accuracy than the previous architecture with batch normalization and activation. The CASSOD-C module has better performance than the CASSOD-A module on the 3 categories (easy, medium, and hard), and the reason can be that the CASSOD-C module has more parameters than the CASSOD-A module. It is shown that the CASSOD module is a good alternative to the original dilated convolutions. The accuracy of the CASSOD-A module shows that the new network achieves higher accuracy than the FEM-based network with only 47% of filter weights in the dilated convo-lution layers of the context module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Accuracy of Image Segmentation</head><p>The proposed network architecture, CASSOD-Net, is evaluated based on the FastFCN <ref type="bibr" target="#b20">[20]</ref>, which is designed for semantic image segmentation tasks. The JPU module, which is included in the FastFCN, combines the upsampled feature maps by using 4 groups of dilated convolutions with different dilation rates. The JPU modules also contain depthwise convolution layers. It can extract multiple-scale context information and increase the accuracy of image segmentation.</p><p>To compare the accuracy, the dilated convolutions in the JPU <ref type="bibr" target="#b20">[20]</ref> are replaced by the proposed CASSOD-D modules. The network architecture of the modified JPU is shown in <ref type="figure">Fig. 6</ref>. ResNet-50 <ref type="bibr" target="#b3">[4]</ref> is used as a backbone network and trained with Pascal Context <ref type="bibr" target="#b14">[15]</ref> and ADE20K datasets <ref type="bibr" target="#b21">[21]</ref>. The accuracy of image segmentation is shown in <ref type="table" target="#tab_6">Table 4</ref>. The results <ref type="bibr" target="#b20">[20]</ref> and our re-implementation results are very close. After replacing the dilated convolutions with the CASSOD modules, the accuracy does not decrease. Besides, the proposed CASSOD module can achieve higher accuracy than the previous architecture with batch normalization and activation. It is shown that the CASSOD module is a good alternative to the original dilated convolutions for image segmentation. Also, the proposed CASSOD modules can be applied to networks with depthwise and dilated convolutions. The computational time of GPU increases after  <ref type="figure">Figure 5</ref>: (a) The context module of RetinaFace <ref type="bibr" target="#b1">[2]</ref> with the SSH structure <ref type="bibr" target="#b16">[16]</ref> and (b) the modified context module with the FEM structure <ref type="bibr" target="#b7">[8]</ref>, in which the dilated convolutions can be replaced by the proposed CASSOD modules.  <ref type="bibr" target="#b16">[16]</ref>. The parameter size of convolution layers to be replaced by dilated convolution layers and CASSOD modules is 11,520. ? ? FEM represents feature enhance module <ref type="bibr" target="#b7">[8]</ref>. * BN represents batch normalization. * * ReLU represents rectified linear unit.</p><p>applying the CASSOD modules because an extra layer is added. This problem can be solved by using the proposed hardware architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of Hardware Systems</head><p>To show the advantages of the proposed hardware system, the hardware architecture and the CASSOD modules are compared with previous works. The previous works refer to the hardware systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> which do not accelerate the algorithms with the 3 ? 3 dilated convolutions shown in Sec. 2. Since the hardware systems of the related works are not available, we re-implement a hardware system on our platforms without using the "Pixel Array" shown in Sec. 3.2 for comparison. The results are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. <ref type="figure" target="#fig_5">Fig. 7(a)</ref> shows the relation between the computational time and the dilation rate (D) with 3 ? 3 filters. The number of cycles is equivalent to the computational time. In previous works, since there are no special hardware architectures to handle dilated convolutions, it is necessary to add zero-values to the filter weights after a filter is dilated. The computational time increases as the dilation rate D increases and is roughly proportional to a square of the dilation rate (D). In the proposed hardware system, the input pixels of dilated convolutions can be adjusted according to the dilation rate (D) and dumped consecutively, and the computational time does not vary with the dilation rate D.   <ref type="figure">Figure 6</ref>: The modified JPU <ref type="bibr" target="#b20">[20]</ref> in which the dilated convolutions are replaced by the proposed CASSOD modules.</p><p>It can be observed that the proposed hardware system can handle both dilated convolutions and standard convolutions (D = 1) efficiently. When the dilation rate (D) of a 3 ? 3 filter is 2, the proposed hardware system is 2.78 times faster than the previous work. <ref type="figure" target="#fig_5">Fig. 7(b)</ref> shows the relation between the computational time and the dilation rate (D). By replacing the traditional dilated convolutions with the CASSOD module, the computational cost and the parameter size can be reduced even though 1 additional convolution layer is required. In the proposed hardware system, the time to set the parameters for 1 additional layer is relatively small. The results show that the computational time of the proposed hardware system can be further reduced by using the CASSOD module, which achieves similar accuracy as the traditional dilated convolutions. <ref type="figure" target="#fig_5">Fig. 7(c)</ref> shows the relation between the gate count of the "Pixel Array" and the maximum dilation rate (D). It can be observed that the overhead of hardware cost increases as the dilation rate D increases, but the difference of hardware cost between the previous work and the proposed work is not proportional to the maximum dilation rate, D. The proposed hardware system scales well because it can support 2 3 ? 1 dilation rates (D = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>) with less than 3 times of hardware costs.</p><p>The supported dilation rates also depend on the filter size and the interface between modules. <ref type="table" target="#tab_7">Table 5</ref> shows the specifications of the proposed hardware architecture. Different from previous works, the proposed hardware system can handle 2 ? 2 and 3 ? 3 dilated convolutions efficiently. The maximum supported dilation rate for 2 ? 2 filters is 6, and the maximum supported dilation rate for 3 ? 3 filters is 3. By replacing the traditional dilated convolutions with the CASSOD module, an approximated 3 ? 3 dilated filters with a dilation rate of 6 can be implemented with cascaded 2 ? 2 filters with a dilation rate of 6. The computational costs and memory footprints can also be reduced. Since the "Pixel Array" with 3 hierarchical stages only occupies  only 21% of the total area, the overhead to support the dilated convolutions is relatively small. A comparison of the processing speed of the proposed system is shown in Table 6. The resolution of the input image is 512 ? 512 pixels, and the network is RetinaFace <ref type="bibr" target="#b1">[2]</ref> with FEM <ref type="bibr" target="#b7">[8]</ref>, which is shown in <ref type="table" target="#tab_5">Table 3</ref>. By using the proposed hardware architecture with the shift register array, 23% of processing time can be reduced. By replacing the dilated 3 ? 3 filters with the proposed CASSOD modules, 9% of processing time can be further reduced. The result clearly shows the advantages in terms of computational speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we propose an efficient module, which is called Cascaded and Separable Structure of Dilated (CAS-SOD) Convolutions, and a special hardware system to handle the CASSOD networks efficiently.</p><p>To analyze the accuracy of algorithms, two example applications, face detection and image segmentation, are  <ref type="bibr" target="#b20">[20]</ref> can achieve the same level of accuracy after replacing the dilated convolutions in the JPU with the proposed CASSOD modules, which also contain depthwise convolutions. It is shown that the CASSOD module is a good alternative to the traditional dilated convolutions for both applications.</p><p>The performance of hardware is analyzed in terms of computational time and hardware costs. The input pixels of dilated convolutions can be adjusted according to the dilation rate (D) and dumped consecutively, the computational time does not vary with the dilation rate D. By using the proposed hardware architecture with the shift register array, 23% of processing time can be reduced for face detection applications.</p><p>The experiments clearly show that both the proposed hardware system and the proposed CASSOD modules have advantages over previous works. For future works, we plan to test the proposed hardware system and the CASSOD modules with other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Concept of the proposed CASSOD module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and 2nd layer: DW Conv. * ) * DW Conv. denotes the depthwise convolutions. concatenated, and 4 separable convolutions with different dilation rates (D = 1, 2, 4, 8) are included in the same convolution layer. Hamaguchi et al. propose a segmentation model, which includes a front-end module, a local feature extraction (LFE) module, and a head module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Example of the 2 ? 2 dilated convolutions where (a) the dilation rate D = 2 and (b) the dilation rate D = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>An example of the hardware architecture and the interconnections of the "Pixel Array" with 6 ? 6 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Analysis of (a) the computational time of 3 ? 3 filters and the dilation rate (D), (b) the computational time of the proposed CASSOD module and the dilation rate (D), and (c) the gate count of the "Pixel Array" and the maximum dilation rate (D). The computational time of 1 convolution layer is measured. The size of input feature maps is 128 ? 128 pixels, and the numbers of input channels and output channels are both set to 64. "HW" in the figure caption represents hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of Number of Filter WeightsNetwork StructureNo. of Filter Weights Dilated Convolution</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>It can be observed that, when C 1 is equal to</figDesc><table><row><cell>Pixel Cache Hierarchical Stage H</cell><cell cols="2">Pixel Cache Hierarchical Stage H-1</cell><cell>...</cell><cell>Pixel Cache Hierarchical Stage 2</cell><cell>Pixel Cache Stage 1 Hierarchical</cell><cell>Pixel Filter</cell><cell>Buffer Conv. Result</cell></row><row><cell></cell><cell></cell><cell cols="3">(H Stages)</cell><cell></cell><cell>Weight</cell><cell>Unit</cell></row><row><cell cols="2">Pixel Memory Filter Weight Memory</cell><cell cols="3">Pixel Array Filter Weight Cache</cell><cell>Convolution Processor</cell><cell cols="2">Activation/ Pooling Unit</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DRAM</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Processing Time of Dilated Convolutions GPU * * CPU * GPU * * The GPU is TITAN Xp (12 GB), and the version of cuDNN is 7.6.5.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Processing Time (ms)</cell></row><row><cell>Dilation Rate</cell><cell cols="2">Convolutions</cell><cell cols="2">Depthwise Conv.</cell></row><row><cell cols="2">CPU  D = 1 39.53</cell><cell>6.53</cell><cell>21.57</cell><cell>11.81</cell></row><row><cell>D = 2</cell><cell cols="2">862.56 19.12</cell><cell cols="2">861.99 11.83</cell></row><row><cell>D = 3</cell><cell cols="2">868.83 19.17</cell><cell cols="2">865.09 11.79</cell></row><row><cell>D = 4</cell><cell cols="2">854.23 19.21</cell><cell cols="2">862.95 11.78</cell></row><row><cell>D = 5</cell><cell cols="2">852.42 19.23</cell><cell cols="2">871.42 11.78</cell></row><row><cell cols="5">*  The CPU is Intel Xeon E5-2640 v4 (2.40 GHz) and the memory size is</cell></row><row><cell>256 GB.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>** *</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Accuracy of Face Detection [2] between Dilated Convolutions and CASSOD Modules</figDesc><table><row><cell>Context Module of RetinaFace [2]</cell><cell cols="3">Easy (%) Medium (%) Hard (%)</cell><cell>Parameter Size of Dilated Conv. Layers</cell></row><row><cell>SSH  ?</cell><cell>88.72</cell><cell>86.97</cell><cell>79.19</cell><cell>(11,520)</cell></row><row><cell>FEM  ? ?</cell><cell>88.87</cell><cell>86.74</cell><cell>80.26</cell><cell>23,040</cell></row><row><cell>FEM  ? ? -CASSOD-C</cell><cell>89.05</cell><cell>87.46</cell><cell>81.09</cell><cell>15,360</cell></row><row><cell>FEM  ? ? -CASSOD-C with BN  *</cell><cell>89.21</cell><cell>87.55</cell><cell>81.28</cell><cell>15,552</cell></row><row><cell>FEM  ? ? -CASSOD-C with BN  *  and ReLU  *  *</cell><cell>89.12</cell><cell>87.62</cell><cell>81.16</cell><cell>15,552</cell></row><row><cell>FEM  ? ? -CASSOD-A with BN  *  and ReLU  *  *</cell><cell>88.88</cell><cell>87.40</cell><cell>80.74</cell><cell>10,912</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? SSH represents single stage headless face detector</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Accuracy of Image Segmentation<ref type="bibr" target="#b20">[20]</ref> between Dilated Convolutions and CASSOD Modules Fps represents frame per second, which is the unit of processing speed of the GPU, TITAN Xp (12 GB).</figDesc><table><row><cell>Datasets</cell><cell>Networks</cell><cell cols="3">pixAcc (%) mIoU (%) Speed (fps  ? )</cell></row><row><cell></cell><cell>EncNet + JPU (Table 2, 3 in [20])</cell><cell>-</cell><cell>51.20</cell><cell>37.56</cell></row><row><cell></cell><cell>EncNet + JPU (Re-implementation)</cell><cell>77.88</cell><cell>49.44</cell><cell>35.30</cell></row><row><cell></cell><cell>EncNet + JPU-CASSOD-D (Proposed Work)</cell><cell>79.52</cell><cell>52.51</cell><cell>34.73</cell></row><row><cell>Pascal Context [15]</cell><cell>EncNet + JPU-CASSOD-D with BN and ReLU (Proposed Work)</cell><cell>79.67</cell><cell>52.72</cell><cell>34.57</cell></row><row><cell></cell><cell>EncNet + JPU-CASSOD-D with BN (Proposed Work)</cell><cell>79.75</cell><cell>52.76</cell><cell>34.12</cell></row><row><cell></cell><cell>EncNet + JPU (Table 2, 4 in [20])</cell><cell>80.39</cell><cell>42.75</cell><cell>37.56</cell></row><row><cell></cell><cell>EncNet + JPU (Re-implementation)</cell><cell>80.04</cell><cell>42.09</cell><cell>35.30</cell></row><row><cell></cell><cell>EncNet + JPU-CASSOD-D (Proposed Work)</cell><cell>80.35</cell><cell>42.72</cell><cell>34.73</cell></row><row><cell>ADE20K [21]</cell><cell>EncNet + JPU-CASSOD-D with BN and ReLU (Proposed Work)</cell><cell>80.48</cell><cell>42.86</cell><cell>34.57</cell></row><row><cell></cell><cell>EncNet + JPU-CASSOD-D with BN (Proposed Work)</cell><cell>80.42</cell><cell>42.78</cell><cell>34.12</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Specifications of Proposed Hardware Architecture</figDesc><table><row><cell>Gate Count</cell><cell>Total: 2.4 M</cell></row><row><cell>(NAND-Gates)</cell><cell>(Pixel Array: 0.5 M)</cell></row><row><cell>Process</cell><cell>28-nm CMOS technology</cell></row><row><cell>Clock Frequency</cell><cell>400 MHz</cell></row><row><cell>Supported Filter Size</cell><cell>Maximum 7 ? 7</cell></row><row><cell>Supported Dilation Rate</cell><cell>2 ? 2 filter: D = 2, 4, 6</cell></row><row><cell></cell><cell>3 ? 3 filter: D = 1, 2, 3</cell></row><row><cell>No. of Hierarchical Stages</cell><cell>H = 3</cell></row><row><cell>Memory Size</cell><cell>128 KB</cell></row><row><cell>Performance</cell><cell>409.6 GOPS*</cell></row><row><cell></cell><cell></cell></row></table><note>* GOPS represents giga operations per second. There are 2 operations in 1 MAC operation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Processing Speed of Proposed Hardware convolutions and the proposed alternatives. For face detection, the RetinaFace [2] network architecture can achieve the same level of accuracy after replacing the dilated convolutions in the context module with the proposed CASSOD modules. For image segmentation, the FastFCN</figDesc><table><row><cell></cell><cell cols="2">RetinaFace [2] RetinaFace [2]</cell></row><row><cell></cell><cell>+FEM [8]</cell><cell>+FEM [8]</cell></row><row><cell></cell><cell></cell><cell>+CASSOD-A</cell></row><row><cell>without Pixel Array</cell><cell>171 fps</cell><cell>224 fps</cell></row><row><cell>with Pixel Array</cell><cell>222 fps</cell><cell>244 fps</cell></row><row><cell>tested with dilated</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM/IEEE International Symposium on Computer Architecture (ISCA)</title>
		<meeting>ACM/IEEE International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">RetinaFace: Single-stage dense face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno>abs/1905.00641</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Effective use of dilated convolutions for segmenting small object instances in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuhei</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aito</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Hikosaka</surname></persName>
		</author>
		<idno>abs/1709.00179</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FastWave: Accelerating autoregressive convolutional neural networks on FPGA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzeen</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paarth</forename><surname>Neekhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dilated CNN model for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="124087" to="124095" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DSFD: Dual shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1810.10220</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1802.10062</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DetNet: A backbone network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1804.06215</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution using a dilated convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guimin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xixian</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1219" to="1230" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1701.04128</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ALAMO: FPGA acceleration of deep learning algorithms with a modularized RTL compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarma</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integration</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ESPNet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Sanja Fidler, Raquel Urtasun, and</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SSH: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<idno>abs/1708.03979</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enhanced CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunwei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunke</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/1810.11834</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Smoothed dilated convolutions for improved dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno>abs/1808.08931</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1805.04574</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">FastFCN: Rethinking dilated convolution in the backbone for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kongming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1903.11816</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LinkNet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. D-Linknet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
