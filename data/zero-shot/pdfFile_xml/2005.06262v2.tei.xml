<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POSE PROPOSAL CRITIC: ROBUST POSE REFINEMENT BY LEARNING REPROJECTION ERRORS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Brynte</surname></persName>
							<email>brynte@chalmers.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
							<email>fredrik.kahl@chalmers.se</email>
							<affiliation key="aff1">
								<orgName type="institution">Chalmers University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">POSE PROPOSAL CRITIC: ROBUST POSE REFINEMENT BY LEARNING REPROJECTION ERRORS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, considerable progress has been made for the task of rigid object pose estimation from a single RGB-image, but achieving robustness to partial occlusions remains a challenging problem. Pose refinement via rendering has shown promise in order to achieve improved results, in particular, when data is scarce. In this paper we focus our attention on pose refinement, and show how to push the state-of-the-art further in the case of partial occlusions. The proposed pose refinement method leverages on a simplified learning task, where a CNN is trained to estimate the reprojection error between an observed and a rendered image. We experiment by training on purely synthetic data as well as a mixture of synthetic and real data. Current state-of-the-art results are outperformed for two out of three metrics on the Occlusion LINEMOD benchmark, while performing on-par for the final metric.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurately estimating the 3D location and orientation of an object from a single image, a.k.a. rigid object pose estimation, has many important real world applications, such as robotic manipulation, augmented reality and autonomous driving. Although the problem has commonly been addressed by exploiting RGB-D cameras, e.g., <ref type="bibr" target="#b0">[1]</ref>, this introduces an increased cost of hardware, sensitivity to sunlight, and unreliable / missing depth measurements for reflective / transparent objects. In recent years, more attention has been put to RGB-only pose estimation, and although considerable progress has been made, a major challenge remains in achieving robustness to partial occlusions. To this end, rendering-based pose refinement methods have shown promise in order to achieve improved results, but their full potential remains unexplored.</p><p>In this paper we revisit pose refinement via rendering, and focus specifically on how to further improve on the robustness of such methods, in particular with respect to partial occlusions. Our method can hence be used to refine the estimates of any pose algorithm and we will give several experimental demonstrations that this is indeed achieved for different algorithms. Naturally, we will also compare to other refinement methods.</p><p>Shared among contemporary rendering-based pose refinement methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> is the approach of feeding an observed as well as a synthetically rendered image as input to a CNN model, which is trained to predict the relative pose of an object between the two images. Our key insight is that rendering-based pose refinement is possible without explicitly regressing to the parameter vector of the relative pose. Instead, estimating an error function of the relative pose is enough, since minimization of said error function w.r.t. pose can be done during inference. <ref type="figure">Figure 1</ref> shows error function estimates for two test frames. Although a larger bias is observed in the occluded case, the estimated minimum is still close to ground-truth. Our main contributions can be summarized as follows:</p><p>? A novel pose refinement method, which works well without real training data.</p><p>? Robustness to partial occlusions, by the implicit nature of our method, making it insensitive to over-or underestimations of the error function. ? State-of-the-art results for two out of three metrics on Occlusion LINEMOD <ref type="bibr" target="#b0">[1]</ref>.</p><p>Our pose refinement pipeline takes as input (a) an object CAD model and (b) an initial pose estimate, referred to as a "pose proposal". The pose proposal is assumed to be obtained from another method, and is fed to the refinement pipeline, consisting of three parts: 3. Iterative refinement of the 6D pose estimate by minimizing the reprojection error.</p><p>We will refer to our method as Pose Proposal Critic (PPC), since the heart of the method involves judging the quality of a pose proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Similarly to the work of Kendall et al. for camera localization <ref type="bibr" target="#b4">[5]</ref>, the rigid object pose estimation methods of Xiang et al. <ref type="bibr" target="#b5">[6]</ref> and Do et al. <ref type="bibr" target="#b6">[7]</ref> estimate the pose by directly regressing the pose parameters. The most successful methods for rigid object pose estimation do however make use of a two-stage pipeline, where 2D-3D correspondences are first established, and the object pose is then retrieved by solving the corresponding camera resectioning problem <ref type="bibr" target="#b7">[8]</ref>. A common approach has been to regress 2D locations of a discrete set of object keypoints, projected into the image, yielding a sparse set of correspondences <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Other methods instead output heatmaps in order to encode said keypoint locations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Among the sparse correspondence methods, Oberweger et al. <ref type="bibr" target="#b12">[13]</ref> stand out in that they address the problem of partial occlusions very carefully. They show that occluders typically have a corrupting effect on CNN activations, far beyond the occluded region itself, and that training with occluded samples might not help to overcome this problem. Instead they resort to limiting the receptive field of their keypoint detector, as a crude but effective way to limit the impact of occluders.</p><p>Dense correspondence methods on the other hand, are inherently more robust to large variances in correspondence estimates. Pixel-wise regression on the corresponding (object frame) 3D coordinates has been proposed by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, while Zakharov et al. <ref type="bibr" target="#b3">[4]</ref> take a different approach and discretize the object surface into smaller segments, and then perform classification on which segment is visible in which pixel. Hu et al. <ref type="bibr" target="#b16">[17]</ref> use a sparse set of keypoints, but yet leverage on dense correspondences due to redundantly regressing a number of 2D locations for each object keypoint. Peng et al. <ref type="bibr" target="#b17">[18]</ref> take a similar approach, but simplify the output space by regressing to the direction from each pixel to each of the projected keypoints, but not the corresponding distance. Pair-wise sampling of pixel-wise predictions then yields votes for keypoint locations.</p><p>The method of Peng et al. <ref type="bibr" target="#b17">[18]</ref> does indeed prove robust to partial occlusions, and yields accurate estimates of rotation as well as lateral translation on the Occlusion LINEMOD benchmark. Consequently, the results are state-of-the-art for the depth-insensitive metric based on reprojection errors. Nevertheless, their depth estimates are still not accurate, and suffer in the presence of partial occlusion. The rendering-based pose refinement method of DeepIM (Li et al. <ref type="bibr" target="#b2">[3]</ref>) does however perform well on Occlusion LINEMOD for all common metrics, and in particular gives a huge boost in depth estimation accuracy, yielding state-of-the-art results for the metric based on matching point clouds in 3D, and suggesting that rendering-based pose refinement is a powerful tool for accurate pose estimation in the presence of partial occlusion. We will experimentally compare to DeepIM and show how one can achieve significantly improved results for partial occlusions.</p><p>Moreover, we point out that while a multitude of approaches for increasing robustness, especially to partial occlusions, has been observed among correspondence-based pose estimation methods, we have not yet seen any directed efforts to address these issues in the literature of rendering-based pose refinement.</p><p>When it comes to rendering-based pose refinement, early work was done by Tjaden and Sch?mer <ref type="bibr" target="#b18">[19]</ref>, proposing a segmentation pipeline based on hand-crafted features, and iterative alignment of silhouettes. Rad and Lepetit <ref type="bibr" target="#b8">[9]</ref> also apply rendering-based refinement as part of the BB8 pose estimation pipeline, improving on initial estimates. BB8 is based on sparse correspondences (8 bounding box corners), and a refinement CNN is trained to regress the reprojection errors for each of the bounding box corners. Refinement is then carried out on the correspondences themselves, yielding an updated camera resectioning problem to be solved. In contrast, our method instead estimates the average reprojection error over all model points and refinement is done directly on the pose.</p><p>Manhardt et al. <ref type="bibr" target="#b1">[2]</ref>, Li et al. <ref type="bibr" target="#b2">[3]</ref> and Zakharov et al. <ref type="bibr" target="#b3">[4]</ref>, all propose a CNN-based refinement pipeline, where the model is trained to learn the relative pose between an observed image and a synthetically rendered image under a pose proposal. The main difference between their approaches and ours is that we instead choose to learn an error function of the relative pose. Among these methods, <ref type="bibr" target="#b2">[3]</ref> is the only one that handles partial occlusions well. The results of <ref type="bibr" target="#b3">[4]</ref> seem competitive at a first glance, but evaluation is only carried out on the frames for which the 2D object detector successfully detected the object of interest, and furthermore parts of the Occlusion LINEMOD dataset were used for training, which does not allow for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, the three main parts of our pipeline will be described in detail.</p><p>The core idea of our approach is that even though neural networks have an amazing capacity to learn difficult estimation tasks, the learning problem should be kept as simple as possible. Given a pose proposal, the task of our network is to determine how good the proposal is with respect to the ground truth. So, instead of trying to learn the pose parameters directly, it is only required for the network to act as a critic of different proposals. To further simplify the task, we render a synthetic image using the pose proposal, and then the network only needs to determine if the rendered image is similar to the observed image or not. As a measure of similarity, we use the average reprojection error of object CAD model points. Then, at inference, the objective is to find the pose parameters with lowest predicted reprojection error, resulting in a minimization problem which can be solved with standard optimization techniques.</p><p>It is assumed that intrinsic camera parameters are known for the observed images, and that a three-dimensional CAD model of the object of interest is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Part I: Rendering the Object Under a Pose Proposal</head><p>Similar to previous work [9, 3, 2, 4], we render a synthetic image of a detected object based on the suggested pose proposal.</p><p>Rendering is done on the GPU using OpenGL with Lambertian shading and the light source at the camera center. The background is kept black.</p><p>Rather than using all of the observed image directly, we zoom in on the detected object. Zooming is done based on the current pose proposal yielding square image patches centered at the projection of the object center. The size of the corresponding image patches (observed and rendered) is chosen as 1.2 times the projection of the object diameter, and furthermore, the observed patch is bilinearly upsampled to 512 ? 512 pixels. Note that the object will be centered in the rendered image patch, but need not be centered in the observed image unless the pose proposal is accurate.</p><p>For future reference, let Z ? denote the zoom-in operator for pose proposal ?, acting on observed image, I obs , resulting in image patch P obs = Z ? I obs . We will denote the rendered image patch by P rend . For performance reasons, the patch is rendered at 256 ? 256 resolution, and then bilinearly upsampled to 512 ? 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Part II: Learning Average Reprojection Error</head><p>We use a pretrained optical flow network as backbone, add a regression head and finetune in order to take the observed and rendered image patches as input, and output an estimate of the average reprojection error, i.e., the average image distance between the projected CAD model points using the ground truth and the pose proposal, respectively.</p><p>Let f = f (Z ? I obs , P rend (?)) be the estimated error of the neural network, where Z ? I obs is the observed image patch and P rend (?) is the rendered image patch for pose proposal ?. If P ? denotes the projection of a 3D point onto the image patch using pose ? 1 , the reprojection error to be estimated by the network f is then given by</p><formula xml:id="formula_0">1 M M i=1 P?(R?p i + t?) ? P?(R ? * p i + t ? * ) 2 ,<label>(1)</label></formula><p>where? and ? * are the estimated and true poses, respectively, and p i are the M object model points. <ref type="figure">Figure 1</ref> shows the estimated error function for two test frames of Occlusion LINEMOD, one in which the object is partially occluded.</p><p>The reprojection error is measured in image patch pixels, i.e. after zoom-in rather than before. Estimating the reprojection error before zoom-in would require the network to estimate and rescale with the absolute depth, which would introduce an unnecessary complication. The reason we choose the reprojection error is that we expect it to be relatively easy to infer from image pairs without a lot of high level reasoning, and thus providing a relatively easy learning task. Furthermore, the reprojection error is quite related to optical flow, and should fit particularly well with a pretrained optical flow backbone.</p><p>For further details on the implementation, we refer to the appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Part III: Minimizing Reprojection Error</head><p>Once the CNN f is trained for estimating reprojection errors, we may apply it for refining an initial pose proposal ? 0 of our object of interest in the observed image I obs .</p><p>Let the compound function J(?) = f (Z ? I obs , P rend (?)) encapsulate the operations of rendering, zoom-in and the CNN itself, which leads to the optimization problem: min ? J(?). We minimize J locally, initializing at ? 0 . Gradient-based optimization is carried out and although analytical differentiation is a tempting approach in the light of differentiable renderers such as <ref type="bibr" target="#b19">[20]</ref>, we observed noisy behavior in J, and we instead apply numerical differentiation for robustly estimating ? ? J.</p><p>For parameterizing the rotation, we take advantage of the Lie Algebra of SO(3). The initial rotation R 0 is used as a reference point and the parameterization is R(? r ) = e A(?r) R 0 , where the parameters ? r constitute the three elements of the 3 ? 3 skewsymmetric matrix A(? r ). The translation is split into two parts. The lateral translation ? l represents the deviation from the projection of the initial position in pixels, i.e. P ?0 (t) ? P ?0 (t 0 ), where ? 0 denotes the initial pose proposal, and t 0 denotes the translation part specifically. The depth is parameterized as d(? d ) = e ? d d 0 , where d 0 is the initial depth estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Optimization Scheme</head><p>It needs to be stressed that there may be spurious local minima in J, and for this reason care should be taken during the optimization. For better control over the procedure, we let decoupled optimizers run in parallel for the rotation / depth / lateral translation parameters, with different hyperparameters and step size decay schedules. We apply in total 100 iterations.</p><p>In order to handle non-convex and noisy behavior of J, we use stochastic optimization. In particular, the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> proved effective for handling the fact that J may be quite steep in the vicinity of the optimum, yet quite flat farther away from the optimum. This property did otherwise risk the optimizer taking too far steps when encountering "steep" points or easily getting stuck in local minima, if the step size was too high or low, respectively.</p><p>Consider for now the optimization w.r.t. rotation and depth. The optimization is roughly carried out in two phases, first w.r.t. rotation and then depth, with a smooth transition between the two. This sequential strategy is due to two reasons: (1) Although optimization w.r.t. rotation works well despite a sub-optimal depth estimate, keeping ? d fixed reduces noise for the moment estimates of the optimizer.</p><p>(2) For precise depth estimation, a good estimate of the other parameters is crucial. The reprojection error to be estimated, as well as the rendered image itself, is much less sensitive to depth perturbations than to the other pose parameters, and focusing specifically on ? d in the final stage helped to improve depth estimation.  Step size decay schedules for the different parameters ? r , ? l and ? d during inference. The decay value is relative to the respective initial step sizes.</p><p>Optimization w.r.t. lateral translation proved relatively easy, and less coupled with the other parameters, i.e. a reasonable minimum may be found despite e.g. a poor rotation estimate. Particularly fast convergence of the lateral translation is desirable for the converse reason, that optimization w.r.t. the other parameters is coupled with the lateral translation estimate, and may not work well unless this is adequate. Luckily, convergence of these parameters is achieved in just a couple of iterations when using a plain SGD optimizer with momentum 0.5 rather than Adam. The step size w.r.t. ? l is set constantly to 1.</p><p>The step size decay schedule for all parameters is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> and the exponential decay rates for the moment estimation of Adam were set to (? 1 , ? 2 ) = (0.6, 0.9) for ? r , and (? 1 , ? 2 ) = (0.4, 0.9) for ? d . The step sizes used for finite differences were 0.01, 1.0 and 0.005 for ? r , ? l , ? d , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Training Data</head><p>Experiments are carried out on LINEMOD as well as Occlusion LINEMOD.</p><p>LINEMOD is a standard benchmark for rigid object pose estimation and was introduced by Hinterstoisser et al. <ref type="bibr" target="#b21">[22]</ref>. The dataset consists of 15 object CAD models along with 15 RGB-D image sequences of an indoor scene where objects are laid out on a table with cluttered background. For each sequence there is a corresponding object of interest put at the center. Although depth images are provided, it is also a common benchmark for RGB-only pose estimation. As two of the objects suffer from low quality CAD models, they are commonly excluded from evaluation and we follow the same practice. For experiments on Occlusion LINEMOD we use real images from LINEMOD, as has conventionally been done in the literature, while the Occlusion LINEMOD images are only used for testing. With 33 % probability we sample a real training image, and with 67 % probability a synthetic one. In the synthetic case, there is a 50 % probability that 2 occluding objects are rendered, and a 50 % probability that no occluders are rendered. We carry out additional experiments on Occlusion LINEMOD where the model is trained only on synthetic data, still with 50 % of the samples being occluded.</p><p>For experiments on LINEMOD, we split training and test data exactly as <ref type="bibr" target="#b2">[3]</ref>, with the same ? 200 samples for training and 1000 samples for test. With 50 % probability we sample a real training image and with 50 % probability a synthetic one, but without any occluders.</p><p>The objects known as eggbox and glue, present in both datasets, are conventionally considered symmetric w.r.t. a 180 degree rotation, but it can be argued whether these are considered actual symmetries. Nevertheless, for these objects we duplicate the initial pose proposals with their 180 degree rotated equivalents and refine the pose using both initializations. In the end, the iterate with the least estimated error is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics for Pose Refinement</head><p>For evaluation of our pose refinement method, we use three conventional metrics, explained in the following. All of them are defined as the percentage of annotated object instances for which the pose is correctly estimated, i.e., the recall according to the specific ways of quantifying the error.</p><p>The average distance metric ADD-0.1D <ref type="bibr" target="#b21">[22]</ref> is the percentage of object instances for which the object point cloud, when transformed with the estimated pose as well as the ground-truth pose, has an average distance less than 10 % of the diameter of the object. The ADD-S-0.1D metric <ref type="bibr" target="#b21">[22]</ref> is closely related, and only differs in that the closest point distance is used, rather than the distance between corresponding points. In general ADD-0.1D is used, but ADD-S-0.1D is used for objects that are considered symmetrical, and we let ADD(-S)-0.1D refer to the two of them together. The REPROJ-5PX metric is similar to ADD-0.1D, but differs in that the transformed point clouds are projected into the image before the mean distance is computed. The acceptance threshold is set to 5 pixels. Finally, the 5CM/5 ? metric accepts a pose estimate if the rotational and translational components differ from their ground-truth equivalents by at most 5 degrees and 5 cm, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">"Symmetric" Objects and Faulty Annotations</head><p>When it comes to the REPROJ-5PX and 5CM/5 ? metrics, they do typically not take symmetries into account. This is not a huge problem for the LINEMOD dataset, partly because the high correlation between training and test data may help resolve any potential symmetries, and partly because, as pointed out earlier, none of the objects are truly symmetrical.</p><p>For Occlusion LINEMOD however, the eggbox object is unfortunately annotated according to the supposedly equivalent 180 degrees rotated pose, in all but the first 396 frames. For this reason, above mentioned metrics make little sense. Li et al. <ref type="bibr" target="#b2">[3]</ref> do however modify these metrics in order to evaluate against the most beneficial of all proposed symmetries, which makes much more sense given the circumstances. We follow their proposal and perform the evaluation on Occlusion LINEMOD w.r.t. these symmetrically aware metrics, which we will refer to as REPROJ-S-5PX and 5CM/5 ? -S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pose Refinement Results</head><p>Here we present our main pose refinement results. For a comparison of different backbone networks and detailed per-object results, we refer the reader to Section B.1 and Section B.3 in the appendix. Illustrations of refinement iterates are also available, in Section B.2 .</p><p>State-of-the-art comparisons on the Occlusion LINEMOD dataset are given in <ref type="table" target="#tab_1">Table 1</ref>. DeepIM <ref type="bibr" target="#b2">[3]</ref> used initializations from PoseCNN <ref type="bibr" target="#b5">[6]</ref>, but as these predictions are not publicly available, we instead rely on initial pose proposals from PVNet <ref type="bibr" target="#b17">[18]</ref>. The evaluation of PVNet was carried out by us and is based on the clean-pvnet implementation along with pre-trained models 2 . Note that although the symmetry-aware REPROJ-S-5PX metric should be used on Occlusion LINEMOD (see Section 4.2.1), Oberweger et al. <ref type="bibr" target="#b12">[13]</ref> report their results based on the REPROJ-5PX metric. We also want to mention that CDPN <ref type="bibr" target="#b15">[16]</ref> perform well on Occlusion LINEMOD, but no quantitative numbers are reported.</p><p>Oberweger et al. <ref type="bibr" target="#b12">[13]</ref> PVNet <ref type="bibr" target="#b17">[18]</ref> PoseCNN <ref type="bibr" target="#b5">[6]</ref> + DeepIM <ref type="bibr" target="#b2">[3]</ref> PVNet <ref type="bibr">[</ref> We also present results on the Occlusion LINEMOD dataset where we train purely on synthetic data, see <ref type="table" target="#tab_3">Table 2</ref>. Our initial pose proposals are obtained from CDPN <ref type="bibr" target="#b15">[16]</ref>, which was the previous state-of-the-art for this set-up (cf. Benchmark for 6D Object Pose Estimation (BOP) evaluation server <ref type="bibr" target="#b22">[23]</ref>  Finally, results on the LINEMOD dataset are presented in <ref type="table">Table 3</ref> with the purpose of providing a direct comparison with DeepIM <ref type="bibr" target="#b2">[3]</ref> with identical initializations. We outperform DeepIM on all metrics using the same proposals from PoseCNN <ref type="bibr" target="#b5">[6]</ref>.</p><p>Note that although no results on LINEMOD for PoseCNN are reported in <ref type="bibr" target="#b5">[6]</ref>, predictions by PoseCNN are made available by <ref type="bibr" target="#b2">[3]</ref>. The results are also good when compared to the state-of-the-art pose estimation methods of Li et al. <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>: Comparison with the refinement method of DeepIM and ours on LINEMOD with PoseCNN as initialization. Note that <ref type="bibr" target="#b2">[3]</ref> reports results according to REPROJ-S-5PX and 5CM/5 ? -S metric instead of REPROJ-5PX and 5CM/5 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Running Time</head><p>Experiments were run on a workstation with 64 GB RAM, Intel Core i7-8700K CPU, and Nvidia GTX 1080 Ti GPU. Our pose refinement pipeline takes on average 33 seconds per frame during inference for the 100 iterations to be carried out, meaning 3 iterations / s. One way to improve on this could be by enabling analytical differentiation through differentiable rendering, although care should be taken in order to make sure that J(?) behaves smoothly enough, for instance, with a regularization scheme. Furthermore, rather than using iterative gradient-based optimization, gradient-free and sample-efficient approaches such as Bayesian optimization could be worth exploring, but is left as future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a novel rendering-based pose refinement method, which shows improved performance compared to previous refinement methods, and is robust to partial occlusions.</p><p>On the Occlusion LINEMOD benchmark, we initialize our method with pose proposals from PVNet <ref type="bibr" target="#b17">[18]</ref>, yielding state-ofthe-art results for two out of three metrics on this competitive benchmark, while performing on-par with previous methods for the third metric. Furthermore, additional experiments on Occlusion LINEMOD show that our method works well also when trained purely on synthetic data, improving on the pose estimates of CDPN <ref type="bibr" target="#b15">[16]</ref>. Finally, on the LINEMOD benchmark, previous refinement methods are outperformed for all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>A.1 Network Architecture Similar to <ref type="bibr" target="#b2">[3]</ref>, we use a pretrained FlowNetSimple optical flow network as backbone. They used the original model from Dosovitskiy et al. <ref type="bibr" target="#b23">[24]</ref>, while we use the FlowNet 2.0 version from Ilg et al. <ref type="bibr" target="#b24">[25]</ref>. We flatten the encoder output feature maps, and feed them through three fully-connected layers, constituting our main branch. In contrast to <ref type="bibr" target="#b2">[3]</ref>, we use standard ReLU rather than leaky-ReLU activation functions. Furthermore, for the hidden layers we use 1024 neurons, and apply dropout with 30 % probability. The final layer outputs one neuron, representing the average reprojection error estimate.</p><p>We also follow <ref type="bibr" target="#b2">[3]</ref> in the approach of adding an auxiliary branch for foreground / background segmentation, by adding a 1channel 3 ? 3 convolutional layer next to the optical flow prediction at level 4 (a.k.a. flow4). The optical flow prediction itself is however disregarded in order to simplify the pipeline, and we point out that the ablation study of <ref type="bibr" target="#b2">[3]</ref> showed only a minor boost from including this auxiliary task.</p><p>Finally, the FlowNet 2.0 network is fed only the observed and rendered image patches as inputs, and no segmentation is provided, as for <ref type="bibr" target="#b2">[3]</ref>. Experiments on re-training their network without segmentation input did however not result in any performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training and Loss Function</head><p>We trained on the loss function L = 0.01 ? L reproj + 0.3 ? L seg , where L reproj is the L 1 error between the true and estimated average reprojection error, and L seg is the binary cross-entropy loss of the foreground / background segmentation, averaged over all pixels. Furthermore, the target for average reprojection error was saturated at 50 px, making sure that very large perturbation samples will not introduce a disturbance into the training, letting the network focus on achieving high precision within the range of reasonable perturbations.</p><p>One model was trained for each object, for 75 epochs with 42 batches sampled in each epoch. The learning rate was initialized to 5 ? 10 ?5 , and multiplied by 0.3 every 10 epochs. The batch size was 12 and L 2 regularization was applied with weight decay 5 ? 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Pose Proposal Sampling</head><p>For training, we generate pose proposals by perturbing the ground-truth pose in three different ways: (1) With 30 % probability, a rotation around a random axis going through the object centre, whose magnitude is normally distributed with ? = 0 and ? = 45 degrees. (2) With 30 % probability, a random lateral translation, normally distributed with ? = 0 and ? = 0.1d, where d is the object diameter.</p><p>(3) With 40 % probability, a relative depth perturbation, sampled from a log-normal distribution with ? = 0 and ? = log 0.05. This procedure and settings were found to work experimentally well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Rendering Synthetic Training Data</head><p>In addition to real annotated training images, we augment the training examples by rendering synthetic observed images, illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. Like for the pose proposals, rendering is done using OpenGL. Phong shading is applied and we noticed a performance boost by taking specular effects into account. In the spirit of Domain Randomization <ref type="bibr" target="#b25">[26]</ref>, we sample variations in light source position as well as shading parameters such as ambient / diffuse / specular weights, and the whiteness / shininess parameters of the specular effects. No perturbations are applied on albedo.</p><p>Random images from Pascal VOC2012 <ref type="bibr" target="#b26">[27]</ref> were used as background and Gaussian blur was applied on the border in order to blend foreground and background and reduce overfitting to border artifacts as proposed by <ref type="bibr" target="#b27">[28]</ref>. Gaussian blur was also applied to the whole object of interest as advised by <ref type="bibr" target="#b28">[29]</ref>.</p><p>Furthermore, occluding objects of other object categories are sometimes rendered in front of the object of interest. A visible region of at least 200 pixels is however ensured, otherwise occluders are resampled. In order to prevent overfitting towards the specific objects used for occlusion, occluded regions are replaced with background with a 50 % probability.</p><p>Finally, in the cases when we trained only on synthetic data, random noise in HSV-space was applied to the observed images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Backbone Comparison</head><p>As a first experiment, we evaluated the performance when altering the backbone on Occlusion LINEMOD. In addition to using the FlowNet 2.0 backbone <ref type="bibr" target="#b24">[25]</ref>, the encoder of Zakharov et al. <ref type="bibr" target="#b3">[4]</ref>, based on ResNet-18 <ref type="bibr" target="#b29">[30]</ref> and a siamese network was re-implemented for comparison. As can be seen in <ref type="table" target="#tab_5">Table 4</ref>, the FlowNet 2.0 model outperforms the alternative, giving further evidence for the conclusion made by Li et al. <ref type="bibr" target="#b2">[3]</ref> that a feature extractor trained for optical flow is useful also for this task. <ref type="figure">Figure 4</ref> shows how our method gradually refines the pose for a few example frames of the Occlusion LINEMOD dataset, illustrated by the image patches of a few iterations. Despite the sub-optimal pose proposals from PVNet <ref type="bibr" target="#b17">[18]</ref>, the poses are accurately recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Illustration of Refinement Iterates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Detailed Pose Refinement Results</head><p>Here we present detailed (per-object) pose refinement results and corresponding comparison with other methods. Similarly, results on LINEMOD are reported in <ref type="table" target="#tab_1">Tables 11, 12</ref> and 13, for the ADD(-S)-0.1D, REPROJ-5PX and 5CM/5 ? metrics, respectively.</p><p>The symmetric objects eggbox and glue are marked with * , and for them ADD(-S)-0.1D refers to ADD-S-0.1D, and the REPROJ-S-5PX and 5CM/5 ? -S metrics also take their ambiguities through 180 degree rotations around the "up"-axis into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Notes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Negative Depth Correction of Pose Proposals</head><p>We observed that the pose proposals from PVNet <ref type="bibr" target="#b17">[18]</ref> sometimes have negative depth, and in this case we switched sign for the object center position, and rotated the object 180 degrees around the principal axis of the camera, in order to yield a feasible estimate with similar projection (the projection is identical for points on the plane which goes through the object center and is parallel to the principal plane of the camera). This correction is done both when reporting the results of <ref type="bibr" target="#b17">[18]</ref>, and when reporting the results of our refinement.</p><p>Oberweger et al. <ref type="bibr" target="#b12">[13]</ref> PVNet <ref type="bibr" target="#b17">[18]</ref> PoseCNN <ref type="bibr" target="#b5">[6]</ref> + DeepIM <ref type="bibr" target="#b2">[3]</ref> PVNet <ref type="bibr">[</ref>          <ref type="table" target="#tab_1">Table 13</ref>: Results on LINEMOD according to the 5CM/5 ? metric. Note that <ref type="bibr" target="#b2">[3]</ref> reports results according to 5CM/5 ? -S.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : 1 . 2 .</head><label>112</label><figDesc>arXiv:2005.06262v2 [cs.CV] 14 May 2020 Pose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors Estimated average reprojection error of our network for the cat object in two test frames of Occlusion LINEMOD [1]. A rotational perturbation is applied for a fixed axis in the camera coordinate frame, in the range of [?30, 30] degrees. The estimated minimum is marked in the figure. (a-b) An unoccluded example. (c-d) An example with occlusion. Synthetic rendering of the detected object under the pose proposal. Estimation of the average reprojection error of all model points, when projected into the image using the ground truth pose as well as the pose proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Step size decay schedules for the different parameters ? r , ? l and ? d during inference. The decay value is relative to the respective initial step sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The Occlusion LINEMOD dataset was produced by Brachmann et al.<ref type="bibr" target="#b0">[1]</ref> by taking one of the LINEMOD sequences and annotating the pose of the surrounding 8 objects. While the central object is typically unoccluded, the surrounding objects are often partially occluded, resulting in a challenging dataset. The central object is not part of the benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Synthetically rendered training examples of observed images. Occlusion is simulated by rendering additional objects in front of the object of interest, or alternatively the corresponding region is replaced with background, effectively making it transparent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Tables 5 ,</head><label>5</label><figDesc>6 and 7 show results on Occlusion LINEMOD for the ADD(-S)-0.1D, REPROJ-S-5PX and 5CM/5 ? -S metrics, respectively. The results of the corresponding experiments on synthetic data are reported in Tables 8, 9 and 10. Initial obs Initial rend Iteration 10 rend Final rend Final obs Figure 4: Image patches during pose refinement iterations, for a few example frames of the Occlusion LINEMOD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Section A.1 gives more details on the network architecture and Section A.2 on the loss function and hyperparameters used for training. How to sample the pose proposals during training is covered in Section A.3, while Section A.4 describes data augmentation strategies, and in particular how to generate synthetic training examples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on Occlusion LINEMOD. Note that<ref type="bibr" target="#b12">[13]</ref> report results according to the REPROJ-5PX metric instead of REPROJ-</figDesc><table><row><cell>18]</cell></row><row><cell>+ PPC (Ours)</cell></row></table><note>S-5PX.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Also note that for these experiments only a subset of 200 test frames is used, in compliance with BOP.</figDesc><table><row><cell></cell><cell>CDPN-synth [16]</cell><cell>CDPN-synth [16] + PPC-synth (Ours)</cell></row><row><cell>ADD(-S)-0.1D</cell><cell>18.76</cell><cell>23.59</cell></row><row><cell>REPROJ-S-5PX</cell><cell>32.22</cell><cell>35.99</cell></row><row><cell>5CM/5 ? -S</cell><cell>16.13</cell><cell>19.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on Occlusion LINEMOD using only synthetic training data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>16] and Peng et al. [18] on LINEMOD.</figDesc><table><row><cell></cell><cell>PoseCNN [6]</cell><cell>PoseCNN [6] + DeepIM [3]</cell><cell>PoseCNN [6] + PPC (Ours)</cell></row><row><cell>ADD(-S)-0.1D</cell><cell>62.04</cell><cell>88.33</cell><cell>88.67</cell></row><row><cell>REPROJ-5PX</cell><cell>64.52</cell><cell>97.53</cell><cell>97.60</cell></row><row><cell>5CM/5 ?</cell><cell>18.14</cell><cell>85.21</cell><cell>89.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our method on Occlusion LINEMOD for different backbones.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results on Occlusion LINEMOD according to the ADD(-S)-0.1D metric.</figDesc><table><row><cell></cell><cell cols="2">Oberweger et al. [13] PVNet [18]</cell><cell>PoseCNN [6] + DeepIM [3]</cell><cell>PVNet [18] + PPC (Ours)</cell></row><row><cell>ape</cell><cell>69.60</cell><cell>66.84</cell><cell>69.02</cell><cell>68.97</cell></row><row><cell>can</cell><cell>82.60</cell><cell>82.85</cell><cell>56.14</cell><cell>79.29</cell></row><row><cell>cat</cell><cell>65.10</cell><cell>62.34</cell><cell>50.95</cell><cell>66.47</cell></row><row><cell>driller</cell><cell>73.80</cell><cell>70.68</cell><cell>52.94</cell><cell>76.52</cell></row><row><cell>duck</cell><cell>61.40</cell><cell>59.58</cell><cell>60.54</cell><cell>66.93</cell></row><row><cell>eggbox  *</cell><cell>13.10</cell><cell>34.55</cell><cell>49.18</cell><cell>49.28</cell></row><row><cell>glue  *</cell><cell>54.90</cell><cell>47.72</cell><cell>52.92</cell><cell>48.06</cell></row><row><cell>holepuncher</cell><cell>66.40</cell><cell>70.17</cell><cell>61.16</cell><cell>75.45</cell></row><row><cell>Mean</cell><cell>60.86</cell><cell>61.84</cell><cell>56.61</cell><cell>66.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on Occlusion LINEMOD according to the REPROJ-S-5PX metric. Note that<ref type="bibr" target="#b12">[13]</ref> reports results according to</figDesc><table><row><cell>REPROJ-5PX.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PVNet [18]</cell><cell>PoseCNN [6] + DeepIM [3]</cell><cell>PVNet [18] + PPC (Ours)</cell></row><row><cell>ape</cell><cell>37.18</cell><cell>51.75</cell><cell>47.69</cell></row><row><cell>can</cell><cell>63.38</cell><cell>35.82</cell><cell>63.63</cell></row><row><cell>cat</cell><cell>19.43</cell><cell>12.75</cell><cell>33.19</cell></row><row><cell>driller</cell><cell>60.21</cell><cell>45.24</cell><cell>67.46</cell></row><row><cell>duck</cell><cell>15.31</cell><cell>22.48</cell><cell>23.01</cell></row><row><cell>eggbox  *</cell><cell>10.47</cell><cell>17.81</cell><cell>33.87</cell></row><row><cell>glue  *</cell><cell>20.93</cell><cell>42.73</cell><cell>25.80</cell></row><row><cell>holepuncher</cell><cell>40.00</cell><cell>18.84</cell><cell>37.52</cell></row><row><cell>Mean</cell><cell>33.36</cell><cell>30.93</cell><cell>41.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results on Occlusion LINEMOD according to the 5CM/5 ? -S metric. No results are reported by Oberweger et al. [13] on this metric.</figDesc><table><row><cell></cell><cell>CDPN-synth [16]</cell><cell>CDPN-synth [16] + PPC-synth (Ours)</cell></row><row><cell>ape</cell><cell>17.65</cell><cell>29.95</cell></row><row><cell>can</cell><cell>13.57</cell><cell>36.68</cell></row><row><cell>cat</cell><cell>14.29</cell><cell>16.84</cell></row><row><cell>driller</cell><cell>5.00</cell><cell>12.50</cell></row><row><cell>duck</cell><cell>20.74</cell><cell>20.21</cell></row><row><cell>eggbox  *</cell><cell>33.16</cell><cell>33.16</cell></row><row><cell>glue  *</cell><cell>26.62</cell><cell>29.87</cell></row><row><cell>holepuncher</cell><cell>24.00</cell><cell>9.50</cell></row><row><cell>Mean</cell><cell>18.76</cell><cell>23.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Synthetic results on Occlusion LINEMOD according to the ADD(-S)-0.1D metric.</figDesc><table><row><cell></cell><cell>CDPN-synth [16]</cell><cell>CDPN-synth [16] + PPC-synth (Ours)</cell></row><row><cell>ape</cell><cell>48.66</cell><cell>59.89</cell></row><row><cell>can</cell><cell>24.62</cell><cell>34.17</cell></row><row><cell>cat</cell><cell>35.20</cell><cell>40.82</cell></row><row><cell>driller</cell><cell>7.50</cell><cell>15.00</cell></row><row><cell>duck</cell><cell>51.60</cell><cell>54.79</cell></row><row><cell>eggbox  *</cell><cell>34.20</cell><cell>34.72</cell></row><row><cell>glue  *</cell><cell>14.94</cell><cell>12.99</cell></row><row><cell>holepuncher</cell><cell>48.50</cell><cell>35.50</cell></row><row><cell>Mean</cell><cell>32.22</cell><cell>35.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Synthetic results on Occlusion LINEMOD according to the REPROJ-S-5PX metric.</figDesc><table><row><cell></cell><cell>CDPN-synth [16]</cell><cell>CDPN-synth [16] + PPC-synth (Ours)</cell></row><row><cell>ape</cell><cell>26.74</cell><cell>36.90</cell></row><row><cell>can</cell><cell>17.59</cell><cell>26.13</cell></row><row><cell>cat</cell><cell>13.78</cell><cell>18.88</cell></row><row><cell>driller</cell><cell>6.50</cell><cell>11.00</cell></row><row><cell>duck</cell><cell>15.43</cell><cell>16.49</cell></row><row><cell>eggbox  *</cell><cell>30.57</cell><cell>23.83</cell></row><row><cell>glue  *</cell><cell>5.84</cell><cell>9.74</cell></row><row><cell>holepuncher</cell><cell>19.00</cell><cell>15.50</cell></row><row><cell>Mean</cell><cell>16.13</cell><cell>19.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Synthetic results on Occlusion LINEMOD according to the 5CM/5 ? -S metric.</figDesc><table><row><cell></cell><cell>PoseCNN [6]</cell><cell>PoseCNN [6] + DeepIM [3]</cell><cell>PoseCNN [6] + PPC (Ours)</cell></row><row><cell>ape</cell><cell>27.71</cell><cell>76.95</cell><cell>75.14</cell></row><row><cell>benchvise</cell><cell>68.87</cell><cell>97.48</cell><cell>94.28</cell></row><row><cell>camera</cell><cell>47.35</cell><cell>93.53</cell><cell>96.18</cell></row><row><cell>can</cell><cell>71.33</cell><cell>92.81</cell><cell>96.95</cell></row><row><cell>cat</cell><cell>56.64</cell><cell>82.14</cell><cell>89.82</cell></row><row><cell>driller</cell><cell>65.28</cell><cell>94.95</cell><cell>97.92</cell></row><row><cell>duck</cell><cell>42.86</cell><cell>77.65</cell><cell>69.39</cell></row><row><cell>eggbox  *</cell><cell>97.84</cell><cell>97.09</cell><cell>98.59</cell></row><row><cell>glue  *</cell><cell>94.88</cell><cell>99.42</cell><cell>92.95</cell></row><row><cell>holepuncher</cell><cell>44.00</cell><cell>52.81</cell><cell>68.70</cell></row><row><cell>iron</cell><cell>65.47</cell><cell>98.26</cell><cell>90.19</cell></row><row><cell>lamp</cell><cell>69.96</cell><cell>97.50</cell><cell>98.27</cell></row><row><cell>phone</cell><cell>54.39</cell><cell>87.72</cell><cell>84.32</cell></row><row><cell>Mean</cell><cell>62.04</cell><cell>88.33</cell><cell>88.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Results on LINEMOD according to the ADD(-S)-0.1D metric.</figDesc><table><row><cell></cell><cell>PoseCNN [6]</cell><cell>PoseCNN [6] + DeepIM [3]</cell><cell>PoseCNN [6] + PPC (Ours)</cell></row><row><cell>ape</cell><cell>82.67</cell><cell>98.38</cell><cell>97.71</cell></row><row><cell>benchvise</cell><cell>49.95</cell><cell>96.99</cell><cell>97.87</cell></row><row><cell>camera</cell><cell>71.67</cell><cell>98.92</cell><cell>98.63</cell></row><row><cell>can</cell><cell>69.85</cell><cell>99.70</cell><cell>97.64</cell></row><row><cell>cat</cell><cell>92.01</cell><cell>98.70</cell><cell>98.80</cell></row><row><cell>driller</cell><cell>43.45</cell><cell>96.13</cell><cell>97.13</cell></row><row><cell>duck</cell><cell>91.73</cell><cell>98.5</cell><cell>97.93</cell></row><row><cell>eggbox  *</cell><cell>41.82</cell><cell>96.15</cell><cell>98.50</cell></row><row><cell>glue  *</cell><cell>87.73</cell><cell>98.94</cell><cell>96.24</cell></row><row><cell>holepuncher</cell><cell>59.52</cell><cell>96.29</cell><cell>98.57</cell></row><row><cell>iron</cell><cell>41.68</cell><cell>97.24</cell><cell>97.24</cell></row><row><cell>lamp</cell><cell>48.27</cell><cell>94.24</cell><cell>94.15</cell></row><row><cell>phone</cell><cell>58.46</cell><cell>97.73</cell><cell>98.39</cell></row><row><cell>Mean</cell><cell>64.52</cell><cell>97.53</cell><cell>97.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Results on LINEMOD according to the REPROJ-5PX metric. Note that<ref type="bibr" target="#b2">[3]</ref> reports results according to REPROJ-S-5PX.</figDesc><table><row><cell></cell><cell>PoseCNN [6]</cell><cell>PoseCNN [6] + DeepIM [3]</cell><cell>PoseCNN [6] + PPC (Ours)</cell></row><row><cell>ape</cell><cell>6.95</cell><cell>90.38</cell><cell>96.48</cell></row><row><cell>benchvise</cell><cell>13.58</cell><cell>88.65</cell><cell>90.40</cell></row><row><cell>camera</cell><cell>20.39</cell><cell>95.78</cell><cell>91.67</cell></row><row><cell>can</cell><cell>24.39</cell><cell>92.81</cell><cell>94.39</cell></row><row><cell>cat</cell><cell>24.98</cell><cell>87.62</cell><cell>96.01</cell></row><row><cell>driller</cell><cell>18.25</cell><cell>92.86</cell><cell>96.13</cell></row><row><cell>duck</cell><cell>18.23</cell><cell>85.16</cell><cell>83.10</cell></row><row><cell>eggbox  *</cell><cell>16.53</cell><cell>63.85</cell><cell>95.49</cell></row><row><cell>glue  *</cell><cell>19.50</cell><cell>83.01</cell><cell>73.07</cell></row><row><cell>holepuncher</cell><cell>15.81</cell><cell>54.52</cell><cell>82.11</cell></row><row><cell>iron</cell><cell>12.97</cell><cell>92.65</cell><cell>92.03</cell></row><row><cell>lamp</cell><cell>24.38</cell><cell>90.88</cell><cell>92.51</cell></row><row><cell>phone</cell><cell>19.26</cell><cell>89.16</cell><cell>83.29</cell></row><row><cell>Mean</cell><cell>18.14</cell><cell>85.21</cell><cell>89.74</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the projection operator itself depends on the pose, due to the dependence of the zoomed-in image patch, and thus the effective intrinsic camera parameters, on the estimated object position.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">At times we observed negative depth estimates from PVNet, which was corrected for according to Section C.1 in the appendix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning 6D object pose estimation using 3D object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep model-based 6D pose refinement in RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepIM: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="657" to="678" />
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DPOD: 6D pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PoseNet: A convolutional network for real-time 6-DoF camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PoseCNN: A convolutional neural network for 6D object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep-6DPose: Recovering 6D object pose from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6D object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Derpanis, and Kostas Daniilidis. 6-DoF object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3D object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">iPose: instanceaware 6D pose estimation of partly occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pix2Pose: Pixel-wise coordinate regression of objects for 6D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CDPN: Coordinates-based disentangled pose network for real-time RGB-based 6-DoF object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6D object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PVNet: Pixel-wise voting network for 6DoF pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time monocular pose estimation of 3D objects using temporally consistent local color histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Henning Tjaden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>Schwanecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schomer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2012</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><forename type="middle">Glent</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Zabulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Ji?? Matas, and Carsten Rother. BOP: Benchmark for 6D object pose estimation. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On pre-trained image features and synthetic images for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
