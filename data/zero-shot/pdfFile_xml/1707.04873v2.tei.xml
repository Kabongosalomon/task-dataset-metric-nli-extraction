<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Architecture Search by Network Transformation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>j.wang@cs.ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Architecture Search by Network Transformation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The great success of deep neural networks in various challenging applications <ref type="bibr" target="#b7">(Krizhevsky, Sutskever, and Hinton 2012;</ref><ref type="bibr" target="#b0">Bahdanau, Cho, and Bengio 2014;</ref><ref type="bibr" target="#b10">Silver et al. 2016)</ref> has led to a paradigm shift from feature designing to architecture designing, which still remains a laborious task and requires human expertise. In recent years, many techniques for automating the architecture design process have been proposed <ref type="bibr" target="#b11">(Snoek, Larochelle, and Adams 2012;</ref><ref type="bibr" target="#b2">Bergstra and Bengio 2012;</ref><ref type="bibr" target="#b1">Baker et al. 2017;</ref><ref type="bibr" target="#b15">Zoph and Le 2017;</ref><ref type="bibr" target="#b9">Real et al. 2017;</ref><ref type="bibr" target="#b8">Negrinho and Gordon 2017)</ref>, and promising results of designing competitive models against humandesigned models are reported on some benchmark datasets <ref type="bibr" target="#b15">(Zoph and Le 2017;</ref><ref type="bibr" target="#b9">Real et al. 2017)</ref>. Despite the promising results as reported, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be used in practice for individual researchers, small sized companies, or university research teams. Another key drawback is that they still design and train each network from scratch during exploring the architecture space without any leverage of previously explored networks, which results in high computational resources waste.</p><p>In fact, during the architecture design process, many slightly different networks are trained for the same task. Apart from their final validation performances that are used to guide exploration, we should also have access to their architectures, weights, training curves etc., which contain abundant knowledge and can be leveraged to accelerate the architecture design process just like human experts <ref type="bibr" target="#b3">(Chen, Goodfellow, and Shlens 2015;</ref>). Furthermore, there are typically many well-designed architectures, by human or automatic architecture designing methods, that have achieved good performances at the target task. Under restricted computational resources limits, instead of totally neglecting these existing networks and exploring the architecture space from scratch (which does not guarantee to result in better performance architectures), a more economical and efficient alternative could be exploring the architecture space based on these successful networks and reusing their weights.</p><p>In this paper, we propose a new framework, called EAS, Efficient Architecture Search, where the meta-controller explores the architecture space by network transformation operations such as widening a certain layer (more units or filters), inserting a layer, adding skip-connections etc., given an existing network trained on the same task. To reuse weights, we consider the class of function-preserving transformations <ref type="bibr" target="#b3">(Chen, Goodfellow, and Shlens 2015)</ref> that allow to initialize the new network to represent the same function as the given network but use different parameterization to be further trained to improve the performance, which can significantly accelerate the training of the new network especially for large networks. Furthermore, we combine our framework with recent advances of reinforcement learning (RL) based automatic architecture designing methods <ref type="bibr" target="#b1">(Baker et al. 2017;</ref><ref type="bibr" target="#b15">Zoph and Le 2017)</ref>, and employ a RL based agent as the meta-controller.</p><p>Our experiments of exploring the architecture space of the plain convolutional neural networks (CNNs), which purely consists of convolutional, fully-connected and pooling layers without skip-connections, branching etc., on image benchmark datasets (CIFAR-10, SVHN), show that EAS with limited computational resources (5 GPUs) can design competitive architectures. The best plain model designed by EAS on CIFAR-10 with standard data augmentation achieves 4.23% test error rate, even better than many modern architectures that use skip-connections. We further apply our method to explore the DenseNet <ref type="bibr" target="#b6">(Huang et al. 2017)</ref> architecture space, and achieve 4.66% test error rate on CIFAR-10 without data augmentation and 3.44% on CIFAR-10 with standard data augmentation, surpassing the best results given by the original DenseNet while still maintaining fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work and Background</head><p>Automatic Architecture Designing There is a long standing study on automatic architecture designing. Neuroevolution algorithms which mimic the evolution processes in the nature, are one of the earliest automatic architecture designing methods <ref type="bibr" target="#b8">(Miller, Todd, and Hegde 1989;</ref><ref type="bibr" target="#b11">Stanley and Miikkulainen 2002)</ref>. Authors in <ref type="bibr" target="#b9">(Real et al. 2017</ref>) used neuro-evolution algorithms to explore a large CNN architecture space and achieved networks which can match performances of human-designed models. In parallel, automatic architecture designing has also been studied in the context of Bayesian optimization <ref type="bibr" target="#b2">(Bergstra and Bengio 2012;</ref><ref type="bibr" target="#b4">Domhan, Springenberg, and Hutter 2015;</ref><ref type="bibr" target="#b8">Mendoza et al. 2016)</ref>. Recently, reinforcement learning is introduced in automatic architecture designing and has shown strong empirical results. Authors in <ref type="bibr" target="#b1">(Baker et al. 2017</ref>) presented a Q-learning agent to sequentially pick CNN layers; authors in <ref type="bibr" target="#b15">(Zoph and Le 2017)</ref> used an auto-regressive recurrent network to generate a variable-length string that specifies the architecture of a neural network and trained the recurrent network with policy gradient.</p><p>As the above solutions rely on designing or training networks from scratch, significant computational resources have been wasted during the construction. In this paper, we aim to address the efficiency problem. Technically, we allow to reuse the existing networks trained on the same task and take network transformation actions. Both functionpreserving transformations and an alternative RL based meta-controller are used to explore the architecture space. Moreover, we notice that there are some complementary techniques, such as learning curve prediction , for improving the efficiency, which can be combined with our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Transformation and Knowledge Transfer</head><p>Generally, any modification to a given network can be viewed as a network transformation operation. In this paper, since our aim is to utilize knowledge stored in previously trained networks, we focus on identifying the kind of network transformation operations that would be able to reuse pre-existing models. The idea of reusing pre-existing models or knowledge transfer between neural networks has been studied before. Net2Net technique introduced in (Chen, Goodfellow, and Shlens 2015) describes two specific function-preserving transformations, namely Net2WiderNet and Net2DeeperNet, which respectively initialize a wider or deeper student network to represent the same functionality of the given teacher network and have proved to significantly accelerate the training of the student network especially for large networks. Similar function-preserving schemes have also been proposed in ResNet particularly for training very deep architectures <ref type="bibr" target="#b5">(He et al. 2016a)</ref>. Additionally, the network compression technique presented in <ref type="bibr" target="#b4">(Han et al. 2015)</ref> prunes less important connections (low-weight connections) in order to shrink the size of neural networks without reducing their accuracy.</p><p>In this paper, instead, we focus on utilizing such network transformations to reuse pre-existing models to efficiently and economically explore the architecture space for automatic architecture designing.</p><p>Reinforcement Learning Background Our metacontroller in this work is based on RL <ref type="bibr" target="#b13">(Sutton and Barto 1998)</ref>, techniques for training the agent to maximize the cumulative reward when interacting with an environment <ref type="bibr" target="#b2">(Cai et al. 2017)</ref>. We use the REIN-FORCE algorithm (Williams 1992) similar to <ref type="bibr" target="#b15">(Zoph and Le 2017)</ref> for updating the meta-controller, while other advanced policy gradient methods <ref type="bibr" target="#b7">(Kakade 2002;</ref><ref type="bibr" target="#b9">Schulman et al. 2015)</ref> can be applied analogously. Our action space is, however, different with that of <ref type="bibr" target="#b15">(Zoph and Le 2017)</ref> or any other RL based approach <ref type="bibr" target="#b1">(Baker et al. 2017)</ref>, as our actions are the network transformation operations like adding, deleting, widening, etc., while others are specific configurations of a newly created network layer on the top of preceding layers. Specifically, we model the automatic architecture design procedure as a sequential decision making process, where the state is the current network architecture and the action is the corresponding network transformation operation. After T steps of network transformations, the final network architecture, along with its weights transferred from the initial input network, is then trained in the real data to get the validation performance to calculate the reward signal, which is further used to update the meta-controller via policy gradient algorithms to maximize the expected validation performances of the designed networks by the meta-controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Search by Net Transformation</head><p>In this section, we first introduce the overall framework of our meta-controller, and then show how each specific network transformation decision is made under it. We later extend the function-preserving transformations to the DenseNet <ref type="bibr" target="#b6">(Huang et al. 2017</ref>) architecture space where directly applying the original Net2Net operations can be problematic since the output of a layer will be fed to all subsequent layers.</p><p>We consider learning a meta-controller to generate network transformation actions given the current network architecture, which is specified with a variable-length string <ref type="bibr" target="#b15">(Zoph and Le 2017</ref>  <ref type="figure">Figure 1</ref>: Overview of the RL based meta-controller in EAS, which consists of an encoder network for encoding the architecture and multiple separate actor networks for taking network transformation actions.</p><p>of network transformation actions while keeping the metacontroller simple, we use an encoder network to learn a lowdimensional representation of the given architecture, which is then fed into each separate actor network to generate a certain type of network transformation actions. Furthermore, to handle variable-length network architectures as input and take the whole input architecture into consideration when making decisions, the encoder network is implemented with a bidirectional recurrent network (Schuster and Paliwal 1997) with an input embedding layer. The overall framework is illustrated in <ref type="figure">Figure 1</ref>, which is an analogue of end-to-end sequence to sequence learning <ref type="bibr" target="#b12">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b0">Bahdanau, Cho, and Bengio 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actor Networks</head><p>Given the low dimensional representation of the input architecture, each actor network makes necessary decisions for taking a certain type of network transformation actions. In this work, we introduce two specific actor networks, namely Net2Wider actor and Net2Deeper actor which correspond to Net2WiderNet and Net2DeeperNet respectively.</p><p>Net2Wider Actor Net2WiderNet operation allows to replace a layer with a wider layer, meaning more units for fully-connected layers, or more filters for convolutional layers, while preserving the functionality. For example, consider a convolutional layer with kernel K l whose shape is</p><formula xml:id="formula_0">(k l w , k l h , f l i , f l o )</formula><p>where k l w and k l h denote the filter width and height, while f l i and f l o denote the number of input and output channels. To replace this layer with a wider layer that hasf l o (&gt; f l o ) output channels, we should first introduce a random remapping function G l , which is defined as</p><formula xml:id="formula_1">G l (j) = j 1 ? j ? f l o random sample from {1, ? ? ? , f l o } f l o &lt; j ?f l o .<label>(1)</label></formula><p>With the remapping function G l , we have the new kernelK l for the wider layer with shape</p><formula xml:id="formula_2">(k l w , k l h , f l i ,f l o ) K l [x, y, i, j] = K l [x, y, i, G l (j)].<label>(2)</label></formula><p>As such, the first f l o entries in the output channel dimension ofK l are directly copied from K l while the remainingf l o ?  <ref type="figure">Figure 2</ref>: Net2Wider actor, which uses a shared sigmoid classifier to simultaneously determine whether to widen each layer based on its hidden state given by the encoder network.</p><p>f l o entries are created by choosing randomly as defined in G l . Accordingly, the new output of the wider layer is? l with? l (j) = O l (G l (j)), where O l is the output of the original layer and we only show the channel dimension to make the notation simpler.</p><p>To preserve the functionality, the kernel K l+1 of the next layer should also be modified due to the replication in its input. The new kernelK l+1 with shape</p><formula xml:id="formula_3">(k l+1 w , k l+1 h ,f l+1 i = f l o , f l+1 o ) is given a? K l+1 [x, y, j, k] = K l+1 [x, y, G l (j), k] {z|G l (z) = G l (j)} .<label>(3)</label></formula><p>For further details, we refer to the original Net2Net work <ref type="bibr" target="#b3">(Chen, Goodfellow, and Shlens 2015)</ref>. In our work, to be flexible and efficient, the Net2Wider actor simultaneously determines whether each layer should be extended. Specifically, for each layer, this decision is carried out by a shared sigmoid classifier given the hidden state of the layer learned by the bidirectional encoder network. Moreover, we follow previous work and search the number of filters for convolutional layers and units for fully-connected layers in a discrete space. Therefore, if the Net2Wider actor decides to widen a layer, the number of filters or units of the layer increases to the next discrete level, e.g. from 32 to 64. The structure of Net2Wider actor is shown in <ref type="figure">Figure 2</ref>.</p><p>Net2Deeper Actor Net2DeeperNet operation allows to insert a new layer that is initialized as adding an identity mapping between two layers so as to preserve the functionality. For a new convolutional layer, the kernel is set to be identity filters while for a new fully-connected layer, the weight matrix is set to be identity matrix. Thus the new layer is set with the same number of filters or units as the layer below at first, and could further get wider when Net2WiderNet operation is performed on it. To fully preserve the functionality, Net2DeeperNet operation has a constraint on the activation function ?, i.e. ? must satisfy ?(I?(v)) = ?(v) for all vectors v. This property holds for rectified linear activation (ReLU) but fails for sigmoid and tanh activation. However, we can still reuse weights of existing networks with sigmoid  or tanh activation, which could be useful compared to random initialization. Additionally, when using batch normalization <ref type="bibr" target="#b7">(Ioffe and Szegedy 2015)</ref>, we need to set output scale and output bias of the batch normalization layer to undo the normalization, rather than initialize them as ones and zeros. Further details about the Net2DeeperNet operation is provided in the original paper <ref type="bibr" target="#b3">(Chen, Goodfellow, and Shlens 2015)</ref>. The structure of the Net2Deeper actor is shown in <ref type="figure" target="#fig_0">Figure 3</ref>, which is a recurrent network whose hidden state is initialized with the final hidden state of the encoder network. Similar to previous work <ref type="bibr" target="#b1">(Baker et al. 2017)</ref>, we allow the Net2Deeper actor to insert one new layer at each step. Specifically, we divide a CNN architecture into several blocks according to the pooling layers and Net2Deeper actor sequentially determines which block to insert the new layer, a specific index within the block and parameters of the new layer. For a new convolutional layer, the agent needs to determine the filter size and the stride while for a new fullyconnected layer, no parameter prediction is needed. In CNN architectures, any fully-connected layer should be on the top of all convolutional and pooling layers. To avoid resulting in unreasonable architectures, if the Net2Deeper actor decides to insert a new layer after a fully-connected layer or the final global average pooling layer, the new layer is restricted to be a fully-connected layer, otherwise it must be a convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function-preserving Transformation for DenseNet</head><p>The original Net2Net operations proposed in <ref type="bibr" target="#b3">(Chen, Goodfellow, and Shlens 2015)</ref> are discussed under the scenarios where the network is arranged layer-by-layer, i.e. the output of a layer is only fed to its next layer. As such, in some modern CNN architectures where the output of a layer would be fed to multiple subsequent layers, such as DenseNet <ref type="bibr" target="#b6">(Huang et al. 2017)</ref>, directly applying the original Net2Net operations can be problematic. In this section, we introduce several extensions to the original Net2Net operations to enable function-preserving transformations for DenseNet.</p><p>Different from the plain CNN, in DenseNet, the l th layer would receive the outputs of all preceding layers as input, which are concatenated on the channel dimension, denoted as [O 0 , O 1 , ? ? ? , O l?1 ], while its output O l would be fed to all subsequent layers.</p><p>Denote the kernel of the l th layer as K l with shape (k l w , k l h , f l i , f l o ). To replace the l th layer with a wider layer that hasf l o output channels while preserving the functionality, the creation of the new kernelK l in the l th layer is the same as the original Net2WiderNet operation (see Eq.</p><p>(1) and Eq. <ref type="formula" target="#formula_2">(2)</ref>). As such, the new output of the wider layer is? l with? l (j) = O l (G l (j)), where G l is the random remapping function as defined in Eq. (1). Since the output of the l th layer will be fed to all subsequent layers in DenseNet, the replication in? l will result in replication in the inputs of all layers after the l th layer. As such, instead of only modifying the kernel of the next layer as done in the original Net2WiderNet operation, we need to modify the kernels of all subsequent layers in DenseNet. For the m th layer where m &gt; l, its input becomes [O 0 , ? ? ? , O l?1 ,? l , O l+1 , ? ? ? , O m?1 ] after widening the l th layer, thus from the perspective of m th layer, the equivalent random remapping function? m can be written as?</p><formula xml:id="formula_4">m(j) = ? ? ? ? ? j 1 ? j ? f 0:l o f 0:l o +G l (j) f 0:l o &lt; j ? f 0:l o +f l o j ?f l o +f l o f 0:l o +f l o &lt; j ? f 0:m o +f l o ?f l o , (4) where f 0:l o = l?1 v=0</formula><p>f v o is the number of input channels for the l th layer, the first part corresponds to [O 0 , ? ? ? , O l?1 ], the second part corresponds to [? l ], and the last part corresponds to [O l+1 , ? ? ? , O m?1 ]. A simple example of? m is given a? Gm : {1, ? ? ? , 5,? l 6, 7, 8, 9, 10, 11} ? {1, ? ? ? , 5,? l 6, 7, 6, 6, 8, 9} where G l : {1, 2, 3, 4} ? {1, 2, 1, 1}.</p><p>Accordingly the new kernel of m th layer can be given by Eq.</p><p>(3) with G l replaced with? m .</p><p>To insert a new layer in DenseNet, suppose the new layer is inserted after the l th layer. Denote the output of the new layer as O new , and its input is</p><formula xml:id="formula_5">[O 0 , O 1 , ? ? ? , O l ].</formula><p>Therefore, for the m th (m &gt; l) layer, its new input after the insertion is </p><formula xml:id="formula_6">[O 0 , O 1 , ? ? ? , O l , O new , O l+1 , ? ? ? , O m?1 ].</formula><p>while all other values inF are set to be 0. Note that n can be chosen randomly from {1, ? ? ? , f 0:l+1 o } for each filter. After all filters in the new layer are set, we can form an equivalent random remapping function for all subsequent layers as is done in Eq. (4) and modify their kernels accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>In line with the previous work <ref type="bibr" target="#b1">(Baker et al. 2017;</ref><ref type="bibr" target="#b15">Zoph and Le 2017;</ref><ref type="bibr" target="#b9">Real et al. 2017)</ref>, we apply the proposed EAS on image benchmark datasets (CIFAR-10 and SVHN) to explore high performance CNN architectures for the image classification task 1 . Notice that the performances of the final designed models largely depend on the architecture space and the computational resources. In our experiments, we evaluate EAS in two different settings. In all cases, we use restricted computational resources (5 GPUs) compared to the previous work such as <ref type="bibr" target="#b15">(Zoph and Le 2017)</ref> that used 800 GPUs. In the first setting, we apply EAS to explore the plain CNN architecture space, which purely consists of convolutional, pooling and fully-connected layers. While in the second setting, we apply EAS to explore the DenseNet architecture space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Datasets</head><p>CIFAR-10 The CIFAR-10 dataset (Krizhevsky and Hinton 2009) consists of 50,000 training images and 10,000 test images. We use a standard data augmentation scheme that is widely used for CIFAR-10 <ref type="bibr" target="#b6">(Huang et al. 2017)</ref>, and denote the augmented dataset as C10+ while the original dataset is denoted as C10. For preprocessing, we normalized the images using the channel means and standard deviations. Following the previous work <ref type="bibr" target="#b1">(Baker et al. 2017;</ref><ref type="bibr" target="#b15">Zoph and Le 2017)</ref>, we randomly sample 5,000 images from the training set to form a validation set while using the remaining 45,000 images for training during exploring the architecture space.  <ref type="bibr" target="#b6">(Huang et al. 2017)</ref>. We follow <ref type="bibr" target="#b1">(Baker et al. 2017)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVHN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>For the meta-controller, we use a one-layer bidirectional LSTM with 50 hidden units as the encoder network <ref type="figure">(Figure 1)</ref> with an embedding size of 16, and train it with the ADAM optimizer (Kingma and Ba 2015). At each step, the meta-controller samples 10 networks by taking network transformation actions. Since the sampled networks are not trained from scratch but we reuse weights of the given network in our scenario, they are then trained for 20 epochs, a relative small number compared to 50 epochs in <ref type="bibr" target="#b15">(Zoph and Le 2017)</ref>. Besides, we use a smaller initial learning rate for this reason. Other settings for training networks on CIFAR-10 and SVHN, are similar to <ref type="bibr" target="#b6">(Huang et al. 2017;</ref><ref type="bibr" target="#b15">Zoph and Le 2017)</ref>. Specifically, we use the SGD with a Nesterov momentum <ref type="bibr" target="#b11">(Sutskever et al. 2013</ref>) of 0.9, a weight decay of 0.0001, a batch size of 64. The initial learning rate is 0.02 and is further annealed with a cosine learning rate decay (Gastaldi 2017). The accuracy in the held-out validation set is used to compute the reward signal for each sampled network. Since the gain of improving the accuracy from 90% to 91% should be much larger than from 60% to 61%, instead of directly using the validation accuracy acc v as the reward, as done in <ref type="bibr" target="#b15">(Zoph and Le 2017)</ref>, we perform a nonlinear transformation on acc v , i.e. tan(acc v ? ?/2), and use the transformed value as the reward. Additionally, we use an exponential moving average of previous rewards, with a decay of 0.95 as the baseline function to reduce the variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explore Plain CNN Architecture Space</head><p>We start applying EAS to explore the plain CNN architecture space. Following the previous automatic architecture designing methods <ref type="bibr" target="#b1">(Baker et al. 2017;</ref><ref type="bibr" target="#b15">Zoph and Le 2017)</ref>, EAS searches layer parameters in a discrete and limited space. For every convolutional layer, the filter size is chosen from {1, 3, 5} and the number of filters is chosen from <ref type="bibr">{16, 32, 64, 96, 128, 192, 256, 320, 384, 448, 512}</ref>, while the stride is fixed to be 1 <ref type="bibr" target="#b1">(Baker et al. 2017)</ref>. For every fully-connected layer, the number of units is chosen from <ref type="bibr">{64, 128, 256, 384, 512, 640, 768, 896, 1024}</ref>. Additionally, <ref type="table">Table 1</ref>: Simple start point network. C(n, f, l) denotes a convolutional layer with n filters, filter size f and stride l; P(f, l, MAX) and P(f, l, AVG) denote a max and an average pooling layer with filter size f and stride l respectively; FC(n) denotes a fullyconnected layer with n units; SM(n) denotes a softmax layer with n output units.</p><p>Model Architecture Validation Accuracy (%) C(16, 3, 1), P(2, 2, MAX), C(32, 3, 1), P(2, 2, MAX), C(64, 3, 1), P(2, 2, MAX), C(128, 3, 1), P(4, 4, AVG), FC(256), SM(10) 87.07</p><p>we use ReLU and batch normalization for each convolutional or fully-connected layer. For SVHN, we add a dropout layer after each convolutional layer (except the first layer) and use a dropout rate of 0.2 <ref type="bibr" target="#b6">(Huang et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Start with Small Network</head><p>We begin the exploration on C10+, using a small network (see <ref type="table">Table 1</ref>), which achieves 87.07% accuracy in the held-out validation set, as the start point. Different from <ref type="bibr" target="#b15">(Zoph and Le 2017;</ref><ref type="bibr" target="#b1">Baker et al. 2017</ref>), EAS is not restricted to start from empty and can flexibly use any discovered architecture as the new start point. As such, to take the advantage of such flexibility and also reduce the search space for saving the computational resources and time, we divide the whole architecture search process into two stages where we allow the meta-controller to take 5 steps of Net2Deeper action and 4 steps of Net2Wider action in the first stage. After 300 networks are sampled, we take the network which performs best currently and train it with a longer period of time (100 epochs) to be used as the start point for the second stage. Similarly, in the second stage, we also allow the meta-controller to take 5 steps of Net2Deeper action and 4 steps of Net2Wider action and stop exploration after 150 networks are sampled. The progress of the two stages architecture search is shown in <ref type="figure" target="#fig_2">Figure 4</ref>, where we can find that EAS gradually learns to pick high performance architectures at each stage. As EAS takes function-preserving transformations to explore the architecture space, we can also find that the sampled architectures consistently perform better than the start point network at each stage. Thus it is usually "safe" to explore the architecture space with EAS. We take the top networks discovered during the second stage and further train the networks with 300 epochs using the full training set. Finally, the best model achieves 95.11% test accuracy (i.e. 4.89% test error rate). Furthermore, to justify the transferability of the discovered networks, we train the top architecture (95.11% test accuracy) on SVHN from random initialization with 40 epochs using the full training set and achieves 98.17% test accuracy (i.e. 1.83% test error rate), better than both human-designed and automatically designed architectures that are in the plain CNN architecture space (see <ref type="table" target="#tab_5">Table 2</ref>).</p><p>We would like to emphasize that the required computational resources to achieve this result is much smaller than those required in <ref type="bibr" target="#b15">(Zoph and Le 2017;</ref><ref type="bibr" target="#b9">Real et al. 2017)</ref>. Specifically, it takes less than 2 days on 5 GeForce GTX 1080 GPUs with totally 450 networks trained to achieve 4.89% test error rate on C10+ starting from a small network.</p><p>Further Explore Larger Architecture Space To further search better architectures in the plain CNN architecture space, in the second experiment, we use the top architectures discovered in the first experiment, as the start points to explore a larger architecture space on C10+ and SVHN. This experiment on each dataset takes around 2 days on 5 GPUs.</p><p>The summarized results of comparing with humandesigned and automatically designed architectures that use a similar design scheme (plain CNN), are reported in <ref type="table" target="#tab_5">Table  2</ref>, where we can find that the top model designed by EAS on the plain CNN architecture space outperforms all similar models by a large margin. Specifically, comparing to humandesigned models, the test error rate drops from 7.25% to 4.23% on C10+ and from 2.35% to 1.73% on SVHN. While comparing to MetaQNN, the Q-learning based automatic architecture designing method, EAS achieves a relative test error rate reduction of 38.9% on C10+ and 16.0% on SVHN. We also notice that the best model designed by MetaQNN on C10+ only has a depth of 7, though the maximum is set to be 18 in the original paper <ref type="bibr" target="#b1">(Baker et al. 2017)</ref>. We suppose maybe they trained each designed network from scratch and used an aggressive training strategy to accelerate training, which resulted in many networks under performed, especially for deep networks. Since we reuse the weights of pre-existing networks, the deep networks are validated more accurately in EAS, and we can thus design deeper and more accurate networks than MetaQNN.</p><p>We also report the comparison with state-of-the-art architectures that use advanced techniques such as skipconnections, branching etc., on C10+ in <ref type="table" target="#tab_6">Table 3</ref>. Though it is not a fair comparison since we do not incorporate such advanced techniques into the search space in this experiment, we still find that the top model designed by EAS is highly competitive even comparing to these state-of-the-art modern architectures. Specifically, the 20-layers plain CNN with 23.4M parameters outperforms ResNet, its stochastic depth variant and its pre-activation variant. It also approaches the best result given by DenseNet. When comparing to automatic architecture designing methods that in-   corporate skip-connections into their search space, our 20layers plain model beats most of them except NAS with post-processing, that is much deeper and has more parameters than our model. Moreover, we only use 5 GPUs and train hundreds of networks while they use 800 GPUs and train tens of thousands of networks.</p><p>Comparison Between RL and Random Search Our framework is not restricted to use the RL based metacontroller. Beside RL, one can also take network transformation actions to explore the architecture space by random search, which can be effective in some cases <ref type="bibr" target="#b2">(Bergstra and Bengio 2012)</ref>. In this experiment, we compare the performances of the RL based meta-controller and the random search meta-controller in the architecture space that is used in the above experiments. Specifically, we use the network in <ref type="table">Table 1</ref> as the start point and let the meta-controller to take 5 steps of Net2Deeper action and 4 steps of Net2Wider action. The result is reported in <ref type="figure" target="#fig_3">Figure 5</ref>, which shows that the RL based meta-controller can effectively focus on the right search direction, while the random search cannot (left plot), and thus find high performance architectures more efficiently than random search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explore DenseNet Architecture Space</head><p>We also apply EAS to explore the DenseNet architecture space. We use the DenseNet-BC (L = 40, k = 40) as the start point. The growth rate, i.e. the width of the nonbottleneck layer is chosen from {40, 44, 48, 52, 56, 60, 64}, and the result is reported in <ref type="table" target="#tab_7">Table 4</ref>. We find that by applying EAS to explore the DenseNet architecture space, we achieve a test error rate of 4.66% on C10, better than the best result, i.e. 5.19% given by the original DenseNet while having 43.79% less parameters. On C10+, we achieve a test error rate of 3.44%, also outperforming the best result, i.e. 3.46% given by the original DenseNet while having 58.20% less parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we presented EAS, a new framework toward economical and efficient architecture search, where the meta-controller is implemented as a RL agent. It learns to take actions for network transformation to explore the architecture space. By starting from an existing network and reusing its weights via the class of function-preserving transformation operations, EAS is able to utilize knowledge stored in previously trained networks and take advantage of the existing successful architectures in the target task to explore the architecture space efficiently. Our experiments have demonstrated EAS's outstanding performance and efficiency compared with several strong baselines. For future work, we would like to explore more network transformation operations and apply EAS for different purposes such as searching networks that not only have high accuracy but also keep a balance between the size and the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Net2Deeper actor, which uses a recurrent network to sequentially determine where to insert the new layer and corresponding parameters for the new layer based on the final hidden state of the encoder network given the input architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>To preserve the functionality, similar to the Net2WiderNet case, O new should be the replication of some entries in [O 0 , O 1 , ? ? ? , O l ]. It is possible, since the input of the new layer is [O 0 , O 1 , ? ? ? , O l ]. Each filter in the new layer can be represented with a tensor, denoted asF with shape (k new w , k new h , f new i = f 0:l+1 o ), where k new w and k new h denote the width and height of the filter, and f new iis the number of input channels. To make the output ofF to be a replication of the n th entry in [O 0 , O 1 , ? ? ? , O l ], we can setF (using the special case that k new w = k new h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Progress of two stages architecture search on C10+ in the plain CNN architecture space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison between RL based meta-controller and random search on C10+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). To be able to generate various types</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Network Transformation</cell><cell></cell><cell>Network Transformation</cell></row><row><cell>Update the network</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multiple Actor Networks</cell><cell></cell><cell>Net2Wider Actor Network</cell><cell></cell><cell></cell><cell>Net2Deeper Actor Network</cell><cell>...</cell></row><row><cell>Encoder Network</cell><cell cols="2">Bi-LSTM</cell><cell cols="2">Bi-LSTM</cell><cell>Bi-LSTM</cell><cell>Bi-LSTM</cell></row><row><cell cols="2">Layer Embedding</cell><cell cols="2">Layer Embedding</cell><cell cols="2">Layer Embedding</cell><cell>Layer Embedding</cell><cell>Layer Embedding</cell></row><row><cell cols="2">CONV(32,3,1)</cell><cell>POOL(2,2)</cell><cell></cell><cell></cell><cell>CONV(64,5,1)</cell><cell>POOL(2,2)</cell><cell>FC(64)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The Street View House Numbers (SVHN) dataset (Netzer et al. 2011) contains 73,257 images in the original training set, 26,032 images in the test set, and 531,131 additional images in the extra training set. For preprocessing, we divide the pixel values by 255 and do not perform any data augmentation, as is done in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and use the original training set during the architecture search phase with 5,000 randomly sampled images as the validation set, while training the final discovered architectures using all the training data, including the original training set and extra training set. Experiment code and discovered top architectures along with weights: https://github.com/han-cai/EAS</figDesc><table><row><cell></cell><cell>(95.11)</cell></row><row><cell></cell><cell>300 epochs</cell></row><row><cell></cell><cell>training</cell></row><row><cell></cell><cell>(92.47)</cell></row><row><cell>100 epochs</cell><cell>(91.78)</cell></row><row><cell>training</cell><cell></cell></row><row><cell></cell><cell>(90.18)</cell></row><row><cell>(87.07)</cell><cell></cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Test error rate (%) comparison with CNNs that use convolutional, fully-connected and pooling layers alone.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">C10+ SVHN</cell></row><row><cell></cell><cell>Maxout (Goodfellow et al. 2013)</cell><cell>9.38</cell><cell>2.47</cell></row><row><cell>human</cell><cell>NIN (Lin, Chen, and Yan 2013)</cell><cell>8.81</cell><cell>2.35</cell></row><row><cell>designed</cell><cell>All-CNN (Springenberg et al. 2014)</cell><cell>7.25</cell><cell>-</cell></row><row><cell></cell><cell>VGGnet (Simonyan and Zisserman 2015)</cell><cell>7.25</cell><cell>-</cell></row><row><cell></cell><cell>MetaQNN (Baker et al. 2017) (depth=7)</cell><cell>6.92</cell><cell>-</cell></row><row><cell>auto</cell><cell>MetaQNN (Baker et al. 2017) (ensemble)</cell><cell>-</cell><cell>2.06</cell></row><row><cell>designed</cell><cell>EAS (plain CNN, depth=16)</cell><cell>4.89</cell><cell>1.83</cell></row><row><cell></cell><cell>EAS (plain CNN, depth=20)</cell><cell>4.23</cell><cell>1.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Test error rate (%) comparison with state-of-the-art architectures.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="3">Depth Params C10+</cell></row><row><cell></cell><cell>ResNet (He et al. 2016a)</cell><cell>110</cell><cell>1.7M</cell><cell>6.61</cell></row><row><cell></cell><cell>ResNet (stochastic depth) (Huang et al. 2017)</cell><cell>1202</cell><cell>10.2M</cell><cell>4.91</cell></row><row><cell></cell><cell>Wide ResNet (Zagoruyko and Komodakis 2016)</cell><cell>16</cell><cell>11.0M</cell><cell>4.81</cell></row><row><cell>human</cell><cell>Wide ResNet (Zagoruyko and Komodakis 2016)</cell><cell>28</cell><cell>36.5M</cell><cell>4.17</cell></row><row><cell>designed</cell><cell>ResNet (pre-activation) (He et al. 2016b)</cell><cell>1001</cell><cell>10.2M</cell><cell>4.62</cell></row><row><cell></cell><cell>DenseNet (L = 40, k = 12) (Huang et al. 2017)</cell><cell>40</cell><cell>1.0M</cell><cell>5.24</cell></row><row><cell></cell><cell>DenseNet-BC (L = 100, k = 12) (Huang et al. 2017)</cell><cell>100</cell><cell>0.8M</cell><cell>4.51</cell></row><row><cell></cell><cell>DenseNet-BC (L = 190, k = 40) (Huang et al. 2017)</cell><cell>190</cell><cell>25.6M</cell><cell>3.46</cell></row><row><cell></cell><cell>Large-Scale Evolution (250 GPUs)(Real et al. 2017)</cell><cell>-</cell><cell>5.4M</cell><cell>5.40</cell></row><row><cell>auto designed</cell><cell>NAS (predicting strides, 800 GPUs) (Zoph and Le 2017) NAS (max pooling, 800 GPUs) (Zoph and Le 2017) NAS (post-processing, 800 GPUs) (Zoph and Le 2017)</cell><cell>20 39 39</cell><cell>2.5M 7.1M 37.4M</cell><cell>6.01 4.47 3.65</cell></row><row><cell></cell><cell>EAS (plain CNN, 5 GPUs)</cell><cell>20</cell><cell>23.4M</cell><cell>4.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Test error rate (%) results of exploring DenseNet architecture space with EAS.</figDesc><table><row><cell>Model</cell><cell cols="4">Depth Params C10 C10+</cell></row><row><cell>DenseNet (L = 100, k = 24)</cell><cell>100</cell><cell cols="3">27.2M 5.83 3.74</cell></row><row><cell>DenseNet-BC (L = 250, k = 24)</cell><cell>250</cell><cell cols="3">15.3M 5.19 3.62</cell></row><row><cell>DenseNet-BC (L = 190, k = 40)</cell><cell>190</cell><cell>25.6M</cell><cell>-</cell><cell>3.46</cell></row><row><cell>NAS (post-processing)</cell><cell>39</cell><cell>37.4M</cell><cell>-</cell><cell>3.65</cell></row><row><cell>EAS (DenseNet on C10)</cell><cell>70</cell><cell>8.6M</cell><cell>4.66</cell><cell>-</cell></row><row><cell>EAS (DenseNet on C10+)</cell><cell>76</cell><cell>10.7M</cell><cell>-</cell><cell>3.44</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was sponsored by Huawei Innovation Research Program, NSFC (61702327) and Shanghai Sailing Program (17YF1428200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time bidding by reinforcement learning in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Malialis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Random search for hyper-parameter optimization. JMLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goodfellow</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shlens ; Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<title level="m">Net2net: Accelerating learning via knowledge transfer. ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Springenberg</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hutter ; Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<idno>Goodfellow et al. 2013</idno>
	</analytic>
	<monogr>
		<title level="m">Learning both weights and connections for efficient neural network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Shake-shake regularization. arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<title level="m">Densely connected convolutional networks. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning curve prediction with bayesian neural networks. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML</title>
		<meeting><address><addrLine>Hinton</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Learning multiple layers of features from tiny images. Imagenet classification with deep convolutional neural networks. In NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan ;</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendoza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<idno>arXiv:1704.08792. [Netzer et al. 2011</idno>
	</analytic>
	<monogr>
		<title level="m">Deeparchitect: Automatically designing and training deep architectures</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS workshop on deep learning and unsupervised feature learning</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers. ICML</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Simonyan and Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies. Evolutionary computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larochelle</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">;</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">;</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>MIT press Cambridge</publisher>
			<pubPlace>Barto; Williams</pubPlace>
		</imprint>
	</monogr>
	<note>Reinforcement learning: An introduction. Machine learning</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and Komodakis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
