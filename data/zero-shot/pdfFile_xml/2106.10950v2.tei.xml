<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Object Tracking with Mixture Density Networks for Trajectory Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gir?-I-Nieto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignasi</forename><surname>Rius</surname></persName>
							<email>irius@mediapro.tv</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marqu?s</surname></persName>
							<email>ferran.marques@upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Catalunya</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Object Tracking with Mixture Density Networks for Trajectory Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multiple object tracking faces several challenges that may be alleviated with trajectory information. Knowing the posterior locations of an object helps disambiguating and solving situations such as occlusions, re-identification, and identity switching. In this work, we show that trajectory estimation can become a key factor for tracking, and present TrajE, a trajectory estimator based on recurrent mixture density networks, as a generic module that can be added to existing object trackers. To provide several trajectory hypotheses, our method uses beam search. Also, relying on the same estimated trajectory, we propose to reconstruct a track after an occlusion occurs. We integrate TrajE into two state of the art tracking algorithms, CenterTrack <ref type="bibr" target="#b62">[63]</ref> and Tracktor <ref type="bibr" target="#b2">[3]</ref>. Their respective performances in the MOTChallenge 2017 test set are boosted 6.3 and 0.3 points in MOTA score, and 1.8 and 3.1 in IDF1, setting a new state of the art for the CenterTrack+TrajE configuration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Multiple Object Tracking (MOT) task aims to estimate tracks of multiple objects across a sequence of frames. These objects must be detected with an accurate bounding box and maintain their identity over time. Tracking all the objects over a sequence can be useful in many applications, such as autonomous driving, robotics, and sports analytics.</p><p>In object tracking, predicting the position of an object can assist in many sub-tasks, such as re-identification or track association. Motion estimators are popular among the MOT community. Some examples are the Kalman filter, optical flow, or constant velocity assumption, which are usually implemented to forecast the displacement of an object over consecutive frames. We believe that modeling the objects trajectory, in contrast with the displacement between frames, can become a key factor of a tracker's performance.</p><p>We propose TrajE, a trajectory estimator based on a recurrent mixture density network, that learns to estimate the underlying distribution of an object trajectory. By sampling from such a distribution, multiple hypotheses for the most likely position of the object can be forecasted, giving the tracker a prior on the subsequent object position. We combine this model with a beam search technique in order to explore multiple trajectory hypotheses per object. We provide a lightweight implementation of the model, designing TrajE as a single layer recurrent neural network, capable of estimating the objects trajectories.</p><p>We have trained TrajE with pedestrian trajectories in the MOT challenge dataset <ref type="bibr" target="#b34">[35]</ref>, and included it as the module of motion estimation for two existing state of the art multiple object trackers, CenterTrack <ref type="bibr" target="#b62">[63]</ref> and Tracktor <ref type="bibr" target="#b2">[3]</ref>. We conduct experiments on the effectiveness of the trajectory estimation comparing both trackers performance with and without trajectory estimation. We use Kalman filter as a trajectory estimator baseline, and compare it to TrajE. We show that trajectory estimation is useful and, by using TrajE, both tracker performances are improved over two datasets, the MOT challenge (pedestrian) dataset <ref type="bibr" target="#b34">[35]</ref>, and the UA-DETRAC (cars in traffic) dataset <ref type="bibr" target="#b51">[52]</ref>.</p><p>The contributions of this paper are (i) we show empirically that trajectory estimation can be a key factor for a better tracking performance, (ii) we build TrajE, a lightweight model for trajectory estimation based on mixture density networks that can be used as a generic motion model for many trackers, (iii) we add it to two state of the art multiple object trackers, boosting their performance by a considerable margin, and evaluate them against two different datasets on multiple object tracking, setting new state of the art results for CenterTrack + TrajE in both MOTChallenge and UA-DETRAC datasets.</p><p>Throughout this paper, we will use the following definitions: Object ID, as the identifier assigned to a given object; Track, as the list of positions assigned to an Object ID until the current time instant; Motion, as the displacement of a given object between two time instants; and Trajectory, as a concrete instantiation of the past, current, and future positions that an object can take.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multiple object tracking. This task consists on estimating the tracks of multiple objects across a video sequence. Advances in object detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17]</ref> have allowed multiple object trackers to rely on frame-by-frame detections. In consequence, most current algorithms follow the tracking-by-detection paradigm, addressing the tracking problem in two steps: (i) object detection and (ii) association of detections through time to form trajectories.</p><p>In tracking, we can differentiate between offline <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b38">39]</ref> and online <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b62">63]</ref> methods. Offline methods try to globally optimize the tracks tacking into account a whole video sequence (or a part of it), whereas online methods base their decision on previous observations and current data, and do not take into account future information. In spite of tackling the multiple object tracking problem from different angles, both paradigms have some common challenges to solve.</p><p>To improve object models, <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b24">25]</ref> combine high-level detections and low-level features (in both cases superpixels) to track the objects, <ref type="bibr" target="#b20">[21]</ref> uses head and body detections for tracking pedestrians, <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref> use CNN features on different network depths, while <ref type="bibr" target="#b9">[10]</ref> aggregates multiple features based on temporal attentions, <ref type="bibr" target="#b27">[28]</ref> builds a graph similarity model among objects, and <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49]</ref> build deep affinity networks to model the object at different time instants.</p><p>To refine tracks <ref type="bibr" target="#b33">[34]</ref> uses behavioral patterns to achieve global consistency, <ref type="bibr" target="#b52">[53]</ref> exploits different degrees of dependencies among tracks, <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">39]</ref> focus on filtering noisy detection candidates, <ref type="bibr" target="#b30">[31]</ref> cleaves and reconnects tracks with a siamese recurrent network, <ref type="bibr" target="#b61">[62]</ref> uses an iterative clustering method to generate tracklets, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref> find a global solution by optimizing a graph, <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref> use recurrent neural networks for data association, <ref type="bibr" target="#b13">[14]</ref> encodes awareness within and between target models, and <ref type="bibr" target="#b54">[55]</ref> uses relation networks in spatio-temporal domain to combine object cues.</p><p>To simplify the tracking pipeline, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54]</ref> associate de-tections in t ? 1 to t by means of a Kalman filter and the Hungarian algorithm, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b62">63]</ref> connect consecutive detections by regression, <ref type="bibr" target="#b5">[6]</ref> directly assumes good detections in all frames and uses the IoU (Intersection over Union) metric to associate the tracks, while <ref type="bibr" target="#b17">[18]</ref> computes the correlation between features from a deep network at multiple depths. In this work, we propose to use the trajectory information, and present TrajE, a trajectory estimation model that can be added to those aforementioned trackers that use motion information to boost their tracking performance. In addition, the estimated trajectory during an occlusion can be included to the tracks once they are recovered. In this case our method can be considered offline, despite no extra computations are done over previous frames. Trajectory estimation. This task focuses on modeling and reasoning on the future behavior of agents. <ref type="bibr" target="#b42">[43]</ref> classifies the different human trajectory prediction methods, based on the model: (i) Physics-based methods, where motion is predicted by dynamics equations based on physical models; (ii) Pattern-based methods, where the dynamics are learned from data; and (iii) Planning-based methods, where there is a reasoning on the agent actions. These models can use (or not) available contextual cues, such as (i) Target agent cues, which are available target agent information; (ii) Dynamic environmental cues, where the target agent is aware of other agents; and (iii) Static environmental cues, where the target agent is aware of the environment information (e.g. static obstacles, such as trees or buildings).</p><p>Lately, the trend in the field is to use pattern-based methods. They follow the Sense -Learn -Predict paradigm, and learn motion behaviors by fitting different function approximators (i.e. neural networks, hidden Markov models, or Gaussian processes) to data.</p><p>To model temporal dependencies, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b32">33]</ref> combine social information to predict trajectories, <ref type="bibr" target="#b36">[37]</ref> uses convolutional neural networks with overlapping windows, <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref> consider a pedestrian as an agent that makes decisions to predict the next position, and <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref> define a spatiotemporal graph to predict multiple futures.</p><p>Specifically, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref> use reinforcement learning to predict the objects position in the next time step, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45]</ref> use graphical models to model pedestrian interactions, <ref type="bibr" target="#b32">[33]</ref> proposes a two-step architecture for overcoming mixture density networks limitations, and <ref type="bibr" target="#b19">[20]</ref> a GAN for trajectory prediction, using an LSTM to model each trajectory and shares the information as in <ref type="bibr" target="#b0">[1]</ref>, taking into account obstacles in the scene and other objects in the modelled trajectories.</p><p>Similar to TrajE, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45]</ref>, use mixture density networks to predict trajectories. Regarding contextual cues, all these methods are aware of the dynamic obstacles in the form of other agents, and some account for static obstacles in the scene <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, are aware of the map geometry and topology <ref type="bibr" target="#b32">[33]</ref> (considering roads and sidewalks in the scene), or are able to use many diverse contextual information <ref type="bibr" target="#b44">[45]</ref>, given its modular nature. All these works use complex models, combining the different agent cues in a social-aware layer in <ref type="bibr" target="#b0">[1]</ref>, using generative-adversarial networks in <ref type="bibr" target="#b19">[20]</ref>, introducing the whole image as input to have contextual information in <ref type="bibr" target="#b32">[33]</ref>, or using variational autoencoders with recurrency in a graph formulation in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>In contrast, we define TrajE as a much lighter model: it consists on a single layer recurrent neural network, it is unaware of the other agents (dynamic cues) or environmental elements (static cues) in the scene, and the inputs of the network are directly the offsets between the centroids of the object previous positions, i.e. without any other visual or previously ordered information.</p><p>To our knowledge, we are the first to bring both the concepts of trajectory estimation using mixture density networks and the posterior trajectory sampling based on beam search to the multiple object tracking framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Trajectory estimation</head><p>Our model estimates the objects trajectories using a recurrent mixture density network model. <ref type="figure" target="#fig_0">Figure 1</ref> depicts the concept of the trajectory estimator (TrajE). Following a set of detections from t ? ? to t, our model estimates the parameters of a distribution, in this case a bivariate Gaussian distribution, that models the evolution of the trajectory for the next time step t+1. From such a distribution, we sample a set of possible states corresponding to a set of hypotheses for the object position in the next time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mixture Density Networks (MDNs)</head><p>Minimization of error functions such as sum-of-squares or cross-entropy leads a neural network model to approximate the conditional averages of the target data, conditioned on the input vector. For problems involving the prediction of continuous variables, the conditional averages provide a very limited information on the underlying structure of data.</p><p>For this reason, predicting the outputs corresponding to input vectors from the conditional probability distribution of the target data might lead to better results, as in handwriting generation <ref type="bibr" target="#b18">[19]</ref>. To estimate such a distribution we use Mixture Density Networks (MDNs), proposed in <ref type="bibr" target="#b4">[5]</ref>. These networks combine a conventional neural network with a mixture density model (e.g. a mixture of Gaussians), so that the neural network estimates the distribution parameters.</p><p>Each input vector x t is a real-valued pair (x 1,t , x 2,t ), in our case the centroid of a bounding box. The network outputs,? t , model the different parameters of a distribution P r(x t+1 |y t ). Some of these outputs model the mixture weights ? k t while the others estimate the parameters of each mixture component. As we use a mixture of Gaussians, the remaining outputs estimate the mean ? k t , variance ? k t , and correlation coefficient ? k t of the M Gaussians that generate the probability distribution. Note that the mean and standard deviation are two dimensional vectors, whereas the weight component and correlation coefficient are scalar.</p><formula xml:id="formula_0">x t ? R ? R (1) y t = {? k t , ? k t , ? k t , ? k t } M k=1<label>(2)</label></formula><p>It is important to state that the mixture components must satisfy several constraints in order to correctly form a valid probability density distribution. To achieve this, several operations are made to the direct outputs of the network y t = {? k Formally, the probability density P r(x t+1 |y t ) of the next object position x t+1 given the mixture components y t is defined as follows:</p><formula xml:id="formula_1">P r(x t+1 |y t ) = M k=1 ? k t N (x t+1 |? k t , ? k t , ? k t )<label>(6)</label></formula><p>where N (x|?, ?, ?) = 1</p><formula xml:id="formula_2">2?? 1 ? 2 1 ? ? 2 exp ?Z 2(1 ? ? 2 )<label>(7)</label></formula><p>with</p><formula xml:id="formula_3">Z = (x 1 ? ? 1 ) 2 ? 2 1 + (x 2 ? ? 2 ) 2 ? 2 2 ? 2?(x 1 ? ? 1 )(x 2 ? ? 2 ) ? 1 ? 2<label>(8)</label></formula><p>Mixture density networks are trained by maximising the log probability density of the targets under the induced distributions. Equivalently, it is common to minimize the negative log likelihood as loss function.</p><formula xml:id="formula_4">L(x) = T t=1 ? log k ? k t N (x t+1 |? k t , ? k t , ? k t )<label>(9)</label></formula><p>where T corresponds to the length of a sequence (in this case an object trajectory). Biased sampling. Directly sampling from the generated distribution leads to a huge space of possible trajectories per object. As the process is iterative (an output probability depends on all previous outputs) and the number of beams limited, the generated trajectories may not lead to an optimal solution, as seen in the experiments (Section 4.3: <ref type="figure">Fig</ref> . A way to palliate this is biasing the sampler towards more likely predictions at each time step. To do so, we modify the distribution as in <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_5">? k t = exp(? k t (1 + b)) M k =1 exp(? k t (1 + b))<label>(10)</label></formula><formula xml:id="formula_6">? k t = exp ? k t ? b<label>(11)</label></formula><p>where b ? 0, being Equation 4 and Equation 5 a specific case for b = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Trajectory Estimation with MDNs</head><p>In this work we estimate an object trajectory for the purpose of multiple object tracking. We combine a recurrent mixture density network, as in speech or handwriting synthesis <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b18">19]</ref>, that aims to model the distribution of the trajectory in time t + 1, with a beam search technique to explore several track hypotheses. For this problem, using recurrent neural networks is suitable, as the output distribution is conditioned not only on the current input, but on the history of previous inputs.</p><p>To estimate the trajectory, we use the centroid of the detected objects bounding boxes. For each centroid c j t = (c j x,t , c j y,t ), we use an instance of the trajectory estimator to estimate a mixture of Gaussians, modeling the distribution from where the centroid c j t+1 is going to be sampled. Concretely, we feed the difference between the centroids c j t?1 and c j t , ?(c j t?1 , c j t ), to the network and sample the offset in the next time step ?(c j t , c j t+1 ) from the distribution. Predicting the trajectory distribution using the offset instead of the position values can be seen as predicting the motion distribution itself. Also, as in <ref type="bibr" target="#b18">[19]</ref>, using offsets was essential to train the network. For simplicity, we refer to centroids instead of centroids offset for the rest of the paper.</p><p>Model architecture. As the number of objects in a scene can grow very large, we use a lightweight architecture that will be instanced per object to track. This architecture is a single layer Gated Recurrent Unit (GRU) <ref type="bibr" target="#b12">[13]</ref> with a hidden state of size 64 connected with 4 heads of fully connected layers. The two heads that predict the mixture weights ? and correlation coefficient ? have dimension M , while the two heads that predict the mean ? and variance ? of the coordinates have dimension 2M . In this work, we set the number of Gaussians in the mixture model to M = 5.</p><p>Beam search. Beam search is a well known technique in the Natural Language Processing (NLP) community, widely used in tasks like machine translation. When translating a sentence to another language, there are many possible combinations of words that could be outputted, but some solu-tions are better than others. These solutions may come not in the first word, but after a few generated words. Ideally, keeping all the possible combinations would be optimal in terms of translation, but unpractical in terms of computation. To deal with it, a subset of options is used and explored in order to find the best translation to the given text.</p><p>We use beam search to take into account multiple trajectory hypotheses, as sampling a single point (even the most likely one) is prone to drifting. In beam search, B stands for beam width. In machine translation the top-B predicted words are considered at every estimation step.</p><p>In our case, B encodes both the beam width and the amount of centroid positions we are going to sample from the trajectory distribution. We start with B hypotheses for the object location in the next time step t + 1. These hypotheses are propagated and, if there is no detection, again B hypotheses are generated, ending up with B 2 hypotheses. To avoid an exponential growth, we prune to keep B trajectory hypotheses, as in machine translation algorithms. Note that, for B = 1, beam search becomes the greedy search algorithm. In TrajE, this cycle is over when a) the trajectory ID is assigned to a detection, or b) the track patience counter is over, and the track is terminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multiple Object Tracking using MDNs</head><p>We integrated our trajectory estimation model, TrajE, into two existing multiple object trackers, CenterTrack <ref type="bibr" target="#b62">[63]</ref> and Tracktor <ref type="bibr" target="#b2">[3]</ref>, replacing their original motion estimation modules. In this section we cover the integration of TrajE into both trackers, track handle policies, and experiments.</p><p>Baselines. CenterTrack and Tracktor are defined as trackers-by-regression, meaning that, by "regressing" the detections in t ? 1 to detections in t, they are able to assign the newly detected object in t to the same track where the original object in t ? 1 belonged.</p><p>The main idea in CenterTrack is to regress consecutive frame centroids to associate the tracked objects from t ? 1 to the detections in t. They extend the CenterNet <ref type="bibr" target="#b16">[17]</ref> object detector to a network able to associate detections over time by means of feeding the model two consecutive images (I t?1 , I t ) and the heatmap of the detections in the previous frame (H t?1 ), biasing the detector towards previous detections. To further boost their performance, the model computes, as the motion model, the offset between detections in two consecutive time steps, as some sort of sparse optical flow between object centroids. By modeling the objects as points and estimating their motion as offsets, the tracks and detections are associated by means of L1 distance.</p><p>Tracktor also forms tracks by propagating tracked objects from t ? 1 to t, regressing them using the ROI-pooling layer present in Faster-RCNN <ref type="bibr" target="#b41">[42]</ref>, as if they were object proposals computed by the network. To improve their results, they make use of re-identification using siamese net- <ref type="figure">Figure 2</ref>. Visualization of the beam search and occlusion handling. First, B (for illustration purposes in this case B = 2) hypotheses of where the object could be in the next time step are sampled from the generated trajectory distribution. If a detection is associated to a track, the beam search corresponding to that track is reset. If no detection is associated to an active track, several hypotheses are sampled from the trajectory distribution and pruned (red crosses) in the next time step to keep B hypotheses. If the track is recovered (a detection is assigned to the track again), and the estimated trajectory is coherent with it, the computed trajectory during the lost state of the track is added to the object track. The occlusion-filling bounding boxes are generated using the centroids of the estimated trajectory, and the width and height of the new detection.</p><p>works and a motion model based on the Constant Velocity (CV) assumption or Camera Motion Compensation (CMC), depending on whether there is large camera movement in the video sequence or not.</p><p>Both methods heavily rely on the positioning of the objects to associate them with the existing tracks projected from t?1, leading to the intuition that a better object projection between time instants should directly boost the trackers performance. Also, with a reliable estimation of the trajectory, the re-identification and occlusion reconstruction problems would be reduced, as both trackers association mechanism is highly biased towards the detections on the previous frame, making it difficult to associate lost objects further in time (from t ? ? ) to newly detected objects in t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Trajectory estimation applied to tracking</head><p>Each object being tracked has an active or lost state, depending on whether it is detected and associated to an existing track, or lost due to occlusions, false negatives in the detection phase, or out of the scene.</p><p>When a track is initialized, a patience timer is assigned to it. For every frame the track is not associated to any detection (lost state), the patience value of the track is decreased by 1, until it becomes 0, when that object track will be considered as terminated. If the object is associated to a track (active state) the patience for that specific track is reset. As TrajE helps the tracker handle occlusions, we set this patience value to 100, allowing objects to reappear in the short-mid term. To add TrajE to a tracker, we define the following common track handle policies: Active tracks. An active track in t ? 1, with its B position hypotheses in t, is associated with a detection in t. The beam corresponding to the trajectory that best fits the detection is chosen, its hidden state copied to the other B ? 1 instances of the trajectory estimator, and its patience reset. The input to the trajectory estimator will be the offset between the new detection and the previous one, and there will be again B possible object mappings in t + 1. Lost tracks. If a track in t is not associated to any detection, the track will be considered as lost. From this moment until a detection is associated with this track (using re-identification) in t + ? , the track will be in the lost state. The B sampled hypotheses in t ? 1 are kept, and fed to B new instances of the trajectory estimation network in t. The resulting B 2 hypotheses are pruned to B to avoid exponential growth. Forwarding the information of lost tracks is important to re-identify the track and recover from occlusions. Also, the object track after an occlusion can be reconstructed if the new detection is coherent with the estimated trajectory. Recovered tracks. If a lost track since t ? ? is associated to a detection in t, the best beam given the likelihood of the estimated trajectory is associated to that track, the beam search exploration is reset, and a decision is taken whether the trajectory estimated during the lost state (e.g. due to an occlusion) is added to the track or not. This decision is made regarding the spatio-temporal coherence of the new detection and the estimated trajectory. To compare both, we consider a bounding box with the centroid of the estimated trajectory and the width and height of the last detection in t ? ? . If the detection associated to the track in t and the estimated trajectory of the lost object in t ? ? have an Intersection over Union (IoU) above a certain threshold (we use 0.5), we consider that TrajE generated a good trajectory estimation, and keep the trajectory as part of the path that the track has followed. Otherwise, we discard the estimated trajectory during the occlusion, and associate only the new detection to the track. Terminated tracks. If the track reaches the maximum patience in the lost state, it will be terminated and removed from the possible re-identification with new detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploration strategies</head><p>Once the trajectory distribution is computed, we propose three approaches to integrate TrajE to an object tracker based on the exploration strategy. The first one is the Best Mean (BM), which takes the most likely mean of one of the Gaussians forming the probability distribution of the trajectory. The second one is a Greedy Beam Search (GBS). It takes the local best sample (using maximum likelihood) at every time step given the estimated distribution as the motion of the tracked object. Note that the difference between GBS and BM strategies is that GBS samples from the distribution, while BM assumes that the best possible position in t + 1 is the mean of the Gaussian with highest likelihood of the mixture of Gaussians. The last strategy is a Pure Beam Search (PBS) that uses all B hypotheses to forward the track in B possible ways. If a detection is associated to the track, the best beam given the historic is chosen in order to keep a single detection per track at every time step. Note that, for B = 1, both GBS and PBS strategies become the same.</p><p>For CenterTrack, we swapped its original offset estimation directly with BM, GBS, or PBS. Note that Center-Track's offset estimation matches detections by estimating the objects offset from t to t ? 1, while TrajE estimates the trajectory of the objects from t ? 1 to t. For BM, we swapped the motion module output (offset estimation in this case) with the mean of the Gaussian with highest likelihood in the distribution generated by TrajE. In a similar way, for the GBS strategy we take the the best sample given by TrajE at every time step. For the PBS, we take the B position hypotheses per object in t, compare them to the detections of the CenterTrack in t, and solve an assignation problem to end up with their closest detection (if any). For all the different exploration strategies, the assignment of IDs to the detections follows the same strategy as in CenterTrack, which links the projected object from t ? 1 to t to its closest detection in t, in terms of L1 distance.</p><p>Tracktor relies on the regression from the Faster-RCNN's ROI pooling layer to map the objects from t ? 1 to t. For the BM and GBS strategies we follow the same strategy as in CenterTrack's case, i.e. swapping the motion estimation by the next most likely mean or sample (for BM and GBS respectively), projecting the object to its next probable position in t. For the PBS, B hypotheses are projected to the next time step (using the width and height of the projected bounding box), and B regressions per track are made. If several detections are associated to the track, the most probable trajectory associated to a detection (given maximum likelihood) is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments</head><p>Experiments and incremental studies are performed in the MOTChallenge dataset <ref type="bibr" target="#b34">[35]</ref> (pedestrians). To see whether adding TrajE to a tracker generalizes, we also evaluate it against the UA-DETRAC dataset <ref type="bibr" target="#b51">[52]</ref> (cars), using the same trajectory estimation model trained on pedestrians from the MOTChallenge training set.</p><p>The MOTChallenge dataset contains 14 challenging video sequences recording pedestrians (7 for training, 7 for testing) with both static and moving camera, recorded in the wild (unconstrained environments), and different locations. The UA-DETRAC dataset consists on 100 videos (60 for training, 40 for testing) with static camera (stable or unstable), recording the road traffic.</p><p>For the MOTChallenge, we use the object detection models provided by the two trackers (and re-identification in case of Tracktor), for the UA-DETRAC dataset we retrained those models for car detection and re-identification.</p><p>For a fair comparison, we use both trackers in their public detection mode, as MOTChallenge requires the provided (public) detections to be used. Both trackers extend their tracking algorithm with their own detector to the public detection setting by initializing the tracks only when a public detection is present.</p><p>Training. To train TrajE we used the MOT17 <ref type="bibr" target="#b34">[35]</ref> challenge data. To generate the training data, we sampled random trajectories from objects in different sequences. These trajectories are split in batches of 100 points in order to have different beginnings and ends of a trajectory. Also, we apply noise to the input sequences to make our model more robust to noisy detections. We end up with 20000 sequences for training and 2000 as validation set. We trained the model for 100 epochs with a learning rate of 0.001, multiplying it by a learning decay of 0.1 at iterations 15, 40, 80. Incremental study. <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> present a study on whether CenterTrack and Tracktor benefit from estimating the object trajectories. In this study we include popular metrics in MOT challenges: MOTA, IDF1, the recently introduced HOTA metric <ref type="bibr" target="#b29">[30]</ref>, which balances the tracking score, both in detection (DetA) and association (AssA) terms, and the identity switches (IDSW) between tracks.</p><p>The starting point are two baselines: the trackers without and with their own defined motion estimation (see the baselines in the beginning of Section 4 for details). We then swap this motion estimation for TrajE, and study the impact of the three exploration strategies, Best Mean (BM), Greedy Beam Search (GBS), and Pure Beam Search (PBS) with and without occlusion reconstruction using the estimated trajectories. To see whether the benefits of including trajectory information into object trackers generalize, we adapt the Kalman filter implemented in SORT <ref type="bibr" target="#b3">[4]</ref> for trajectory estimation, by generating trajectories for a track whose state is set to lost, while including the new detections assigned to the track whenever its state is set to active. Motion estimation. The first observation is that any provided motion information is helpful to the trackers. In Table 1, by computing the motion of the object with an offset head for CenterTrack, and <ref type="table">Table 2</ref>, by assuming a constant velocity (CV) of an object for Tracktor, both trackers improve their performance considerably.</p><p>Trajectory estimation. Following the incremental study, we observe that, by adding TrajE, both trackers consistently improve in all metrics. As TrajE has a sampling step, the mean and variance for five runs is depicted. Comparing both TrajE and the Kalman filter extended to trajectories, TrajE provides much more gain to CenterTrack in <ref type="table">Table 1</ref>, and a similar gain to Tracktor in <ref type="table">Table 2</ref>. This difference in the increment is due to the way both trackers treat the regression step. While CenterTrack computes directly the distance between the projected detection in t with the actual detection in t, Tracktor regresses the projected detection in t with the ROI-pooling layer present in the Faster-RCNN, conditioning the trajectory estimation to the object detector, leading to similar solutions if the projected detections of TrajE and the Kalman filter are close to each other. Also, by reconstructing the occlusions in the tracks by using the estimated trajectories, there is an incremental gain for all the trajectory estimation strategies. TrajE parameters. <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="figure">Figure 4</ref> study the impact of the bias and beam width (B) values for the different trajectory estimation strategies. We show the behavior of the two parameters over the different exploration strategies, BM, GBS, and PBS, and compare them with the Kalman filter adapted for trajectory estimation, and the trackers baseline (using their own motion estimators). In the trajectory estimation strategies, the occlusion reconstruction is used. Note that for B = 1, GBS and PBS strategies are the same.</p><p>The results show how, by using beam search, the trackers benefit from the several hypotheses, and how biasing the outputs of the MDN can become crucial in the trajectory estimation. Regarding the exploration strategies, PBS has better overall results than GBS. Interestingly, by using the BM strategy, which is equivalent to use a beam width of B = 1 with bias ? ?, TrajE also boosts the performance of the trackers by a considerable margin, and can be a good alternative when faster computation is mandatory. Comparison with state of the art. In <ref type="table" target="#tab_2">Table 3</ref> we compare the two trackers using TrajE with the PBS setting that lead to the best results on the training set (bias = 1, B = 5), with and without occlusion reconstruction, with respect to the state of the art of multiple object trackers in the test set of the MOTChallenge dataset. By using TrajE to predict trajectories, their performance in both MOTA and IDF1 scores is boosted by a considerable margin, and set a new state of the art results in the case of CenterTrack + TrajE, with an increase of 5.9, and 6.3 points in the MOTA score without and with occlusion reconstruction respectively.</p><p>To see if TrajE generalizes, we tested both trackers + TrajE trained with MOTChallenge data against the test set of the UA-DETRAC dataset. In <ref type="table" target="#tab_3">Table 4</ref>, we compare both CenterTrack and Tracktor with and without TrajE in the PBS configuration without and with occlusion reconstruction, with respect to other state of the art results on the UA- DETRAC dataset. In both cases, using TrajE for trajectory estimation boosts the performance of the trackers. It is important to state that UA-DETRAC dataset does not contain fully annotated frames (i.e only some of the cars inside the scene are annotated), and some of the non-annotated cars are inside "ignore zones", where the detections should be disregarded. We modified the MOT evaluation tool to take into account such zones by ignoring the detections whose bounding box centroid is inside the "ignore zones".   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have introduced TrajE, a lightweight trajectory estimator based on mixture density networks and beam search, capable of significantly increasing the performance of existing multiple object trackers. Also, with the same estimated trajectory, we propose to do a track reconstruction when the object is lost due occlusions. Our experiments adding our trajectory estimator to CenterTrack, and Tracktor provide very interesting insights on how the trajectory estimation can help in the tracking, while establishing a new state of the art in the MOTChallenge and UA-DETRAC datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>RNNFigure 1 .</head><label>1</label><figDesc>Concept of TrajE: our model to estimate trajectories. We train a Recurrent Neural Network to estimate a mixture of Gaussians that model the object trajectory. From it, several hypotheses (B) of the object position in the next time step are sampled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>- ure 3 and</head><label>3</label><figDesc>Figure 4, for bias b = 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Experiments over TrajE parameters bias and beam width B for CenterTrack using TrajE or Kalman with occlusion reconstruction. The y-axis corresponds to the metric score, the x-axis to the bias, and the columns to the different B values. Solid lines correspond to the average value over five runs, and the transparent region limits correspond to the maximum and minimum values. Experiments over TrajE parameters bias and beam width B for Tracktor using TrajE or Kalman with occlusion reconstruction. The y-axis correspond to the metric score, the x-axis to the bias, and the columns to the different B values. Solid lines correspond to the average value over five runs, and the transparent region limits correspond to the maximum and minimum values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>IDF1 ? MT % ? ML % ? FP ? FN ? Online, TrajE without occlusion reconstruction CenterTrack+TrajE 67.4(+5.9) 61.2(+1.6) 34.8 24.9 18652 161347</figDesc><table><row><cell></cell><cell></cell><cell>MOT17 [35]</cell><cell></cell><cell></cell></row><row><cell cols="2">Method MOTA ? CenterTrack [63] 61.5</cell><cell>59.6</cell><cell>26.4</cell><cell>31.9 14076 200672</cell></row><row><cell>GSM [28]</cell><cell>56.4</cell><cell>57.8</cell><cell>22.2</cell><cell>34.5 14379 230174</cell></row><row><cell cols="4">Tracktor(v2)+TrajE 56.3(+0.0) 57.8(+2.7) 21.4</cell><cell>35.8 10068 233885</cell></row><row><cell>Tracktor(v2)[3]</cell><cell>56.3</cell><cell>55.1</cell><cell>21.1</cell><cell>35.3 8866 235449</cell></row><row><cell>FAMNet [15]</cell><cell>52.0</cell><cell>48.7</cell><cell>19.1</cell><cell>33.4 14138 253616</cell></row><row><cell>STRN [55]</cell><cell>50.9</cell><cell>56.0</cell><cell>18.9</cell><cell>33.8 25295 249365</cell></row><row><cell cols="3">Offline, TrajE with occlusion reconstruction</cell><cell></cell><cell></cell></row><row><cell cols="4">CenterTrack+TrajE 67.8(+6.3) 61.4(+1.8) 36.0</cell><cell>24.5 20982 157468</cell></row><row><cell>Lif T [23]</cell><cell>60.5</cell><cell>65.6</cell><cell>27.0</cell><cell>33.6 14966 206619</cell></row><row><cell>MPNTrack [7]</cell><cell>58.8</cell><cell>61.7</cell><cell>28.8</cell><cell>33.5 17413 213594</cell></row><row><cell cols="4">Tracktor(v2)+TrajE 56.6(+0.3) 58.2(+3.1) 21.9</cell><cell>35.7 10119 231091</cell></row><row><cell>TT17 [62]</cell><cell>54.9</cell><cell>63.1</cell><cell>24.4</cell><cell>38.1 20236 233295</cell></row><row><cell>TPM [39]</cell><cell>54.2</cell><cell>52.6</cell><cell>22.8</cell><cell>37.5 13739 242730</cell></row><row><cell>JBNOT [22]</cell><cell>52.6</cell><cell>50.8</cell><cell>19.7</cell><cell>35.8 31572 232659</cell></row><row><cell>ETC [51]</cell><cell>51.9</cell><cell>58.1</cell><cell>23.1</cell><cell>35.5 36164 232783</cell></row><row><cell>eHAF[48]</cell><cell>51.8</cell><cell>54.7</cell><cell>23.4</cell><cell>37.9 33212 236772</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison integrating TrajE to CenterTrack and Tracktor against the state of the art methods on the test set of MOTChallenge 17 dataset using public detections. In bold tracker + our method (TrajE). In red the best result, in blue the second best. The tracking baselines are CenterTrack+OFF, Tracktor+CV+CMC.</figDesc><table><row><cell cols="3">Online, TrajE without occlusion reconstruction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">CenterTrack+TrajE 69.9(+0.1) 80.1(+1.6) 70.0</cell><cell cols="3">7.9 41664 146722</cell></row><row><cell>CenterTrack [63]</cell><cell>69.8</cell><cell>78.4</cell><cell>69.9</cell><cell cols="3">7.9 40994 147244</cell></row><row><cell>POI [60]</cell><cell>69.2</cell><cell>-</cell><cell>67.4</cell><cell>5.2</cell><cell>-</cell><cell>-</cell></row><row><cell>WD [61]</cell><cell>68.5</cell><cell>-</cell><cell>70.2</cell><cell>3.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">Tracktor(v2)+TrajE 67.7(+0.1) 75.5(+1.3) 60.4</cell><cell cols="3">10.0 23273 194040</cell></row><row><cell>Tracktor(v2) [3]</cell><cell>67.6</cell><cell>74.2</cell><cell>60.3</cell><cell cols="3">10.1 23699 194234</cell></row><row><cell>DeepSORT [54]</cell><cell>65.4</cell><cell>-</cell><cell>65.9</cell><cell>4.7</cell><cell>-</cell><cell>-</cell></row><row><cell>RMOT [59]</cell><cell>62.6</cell><cell>-</cell><cell>42.1</cell><cell>6.5</cell><cell>-</cell><cell>-</cell></row><row><cell>IHTLS [16]</cell><cell>62.6</cell><cell>-</cell><cell>63.4</cell><cell>3.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Offline, TrajE with occlusion reconstruction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">CenterTrack+TrajE 70.3(+0.5) 80.4(+2.0) 70.3</cell><cell cols="3">7.9 42277 143938</cell></row><row><cell cols="4">Tracktor(v2)+TrajE 68.9+(1.3) 75.7(+1.5) 63.3</cell><cell cols="3">9.0 30135 179468</cell></row></table><note>UA-DETRAC [52] Method MOTA ? IDF1 ? MT % ? ML % ? FP ? FN ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison integrating TrajE to CenterTrack and Tracktor with state-of-the art methods on the test set of UA-DETRAC dataset using CompACT<ref type="bibr" target="#b7">[8]</ref> public detections. In bold tracker + our method (TrajE). In red the best result, in blue the second best. Results from<ref type="bibr" target="#b60">[61]</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ,? k t ,? k t ,? k t } M k=1 . First, weights ? k t must satisfy M k=1 ? k t = 1(3)which can be achieved using the softmax normalization:? k t = exp(? k t ) M k =1 exp(? k t )(4)For the other components, to keep their values within a meaningful range, ? k t remains the same, the elements of ? k t must be positive, and ? k t must be a value between (?1, 1). Therefore we apply:? k t =? k t ; ? k t = exp(? k t ); ? k t = tanh(? k t )(5)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to predict human behavior in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Group and Crowd Behavior for Computer Vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="183" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="941" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Highspeed tracking-by-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Volker Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6247" to="6257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3361" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time&apos;actor-critic&apos;tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aggregate tracklet appearance features for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1613" to="1617" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="645" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with instance-aware tracker and dynamic model refreshment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6172" to="6181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The way they move: Tracking multiple targets with similar appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglayan</forename><surname>Dicle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><forename type="middle">I</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2304" to="2311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improvements to frank-wolfe optimization for multi-detector multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08314</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple people tracking using body and joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhe</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Swoboda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14550</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3542" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on computer vision workshops (ICCV workshops)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gsm: Graph similarity model for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<editor>Christian Bessiere</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="530" to="536" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main track. 2, 8</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hota: A higher order metric for evaluating multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trajectory factory: Tracklet cleaving and re-connection by deep siamese bi-gru for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Customized multi-person tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="612" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgun</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-markovian globally consistent multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrii</forename><surname>Maksai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2544" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<idno>arXiv: 1603.00831. 1</idno>
		<title level="m">MOT16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online multi-target tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural network for trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Nikhil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="452" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tpm: Multiple object tracking with tracklet-plane matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">107480</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collaborative deep reinforcement learning for multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Human motion trajectory prediction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Rudenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="300" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03093</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Better generative models for sequential data problems: Bidirectional recurrent mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="589" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Heterogeneous association graph fusion for target association in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3269" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep affinity network for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huansheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploit the connectivity: Multiobject tracking with trackletnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renshu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning non-uniform hypergraph for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8981" to="8988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatialtemporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3988" to="3998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ss-lstm: A hierarchical lstm model for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1186" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-target tracking by online learning of non-linear motion patterns and robust appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1918" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bayesian multi-object tracking using motion context from multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Hong Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust multivehicle tracking with wasserstein association metric in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinsha</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="47863" to="47876" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Long-term tracking with deep tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="6694" to="6706" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01177</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">Tracking objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
