<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Short Text Clustering with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-31">31 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pugachev</surname></persName>
							<email>leonid.pugachev@phystech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Moscow Institute of Physics and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Burtsev</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Moscow Institute of Physics and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Short Text Clustering with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-31">31 Jan 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent techniques for the task of short text clustering often rely on word embeddings as a transfer learning component. This paper shows that sentence vector representations from Transformers in conjunction with different clustering methods can be successfully applied to address the task. Furthermore, we demonstrate that the algorithm of enhancement of clustering via iterative classification can further improve initial clustering performance with different classifiers, including those based on pre-trained Transformer language models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are currently a lot of techniques developed for short text clustering (STC), including topic models and neural networks. The most recent and successful approaches leverage transfer learning through the use of pre-trained word embeddings. In this work, we show that high quality for STC on the range of datasets can be achieved with modern sentence level transfer learning techniques as well. We use deep sentence representations obtained using the Universal Sentence Encoder (USE) <ref type="bibr" target="#b1">(Cer et al., 2018;</ref><ref type="bibr">Yang et al., 2019)</ref>.</p><p>Training of deep architectures can be effective for particular clustering tasks as well. However, application of deep models to clustering directly is difficult since we do not have labels a priori. We show that fine-tuning of classifiers such as BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> and RoBERTa <ref type="bibr" target="#b7">(Liu et al., 2019)</ref> for clustering can be done with the Enhancement of Clustering by Iterative Classification (ECIC) algorithm <ref type="bibr" target="#b10">(Rakib et al., 2020)</ref>. Thus, we develop a combined approach to STC, which benefits from the usage of deep sentence representations obtained using USE and finetuning of Transformer models.</p><p>The main contributions of the work are as follows. First, we demonstrate that sentence level transfer learning for clustering which has not been a common technique so far gives good results. Second, fine-tuning of deep models for clustering is hindered because of the lack of labeled data and we propose to use the ECIC algorithm with deep models which has not been done before to tackle this problem. Third, we analyzed different combinations of components as constitutional parts of the algorithm, tested different schemes to handle weights during fine-tuning over iterations and developed a new stopping criterion for the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>One major direction in STC is based on Dirichlet multinomial mixture topic models <ref type="bibr" target="#b14">(Yin and Wang, 2014;</ref><ref type="bibr" target="#b4">Jipeng et al., 2019)</ref> including GSDPMM <ref type="bibr" target="#b15">(Yin and Wang, 2016)</ref>. Some variants of these models incorporate word embeddings <ref type="bibr" target="#b8">(Nguyen et al., 2015;</ref><ref type="bibr" target="#b5">Li et al., 2017;</ref><ref type="bibr" target="#b4">Jipeng et al., 2019)</ref>. These models assume that each document contains only one or a few topics. The models have several advantages over conventional topic modeling such as latent Dirichlet allocation, when used for short texts. First, they better cope with the sparseness of short texts, which carry limited information about word co-occurrences. Second, these models can automatically infer the number of topics. Since only one topic is presented for each document, it is straightforward to use these topic models for clustering, assuming all documents with the same topic as belonging to the same cluster.</p><p>Recent works have considered a neural approach for STC. In <ref type="bibr" target="#b11">(Xu et al., 2015</ref><ref type="bibr" target="#b12">(Xu et al., , 2017</ref>, authors propose to encode texts by pre-trained binary codes. Embeddings of words are then fed in the convolutional neural network which is trained to fit the binary codes. Finally, the obtained rep-resentations are used as features with k-means clustering algorithm. The work of <ref type="bibr" target="#b3">(Hadifar et al., 2019)</ref> uses a somewhat similar strategy called Self-Taught Approach (STA). An autoencoder is pre-trained to obtain low-dimensional features and then learn it together with clustering algorithm by iteratively updating the weights of the autoencoder and centroids of clusters. Finally, they use the resulting features with k-means clustering algorithm. Another idea is to use attentive representation learning with adversarial training for STC <ref type="bibr" target="#b16">(Zhang et al., 2019)</ref>. The work of (Rakib et al., 2020) sets the state-of-the-art results on the range of short text datasets using the ECIC algorithm which is simpler than in <ref type="bibr" target="#b3">(Hadifar et al., 2019)</ref>. They use averaged word embeddings as features for short texts and clustering algorithms such as k-means, to get the initial label assignment. The clustering performance is then improved with iterative outlier detection and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In our work, we made several important modifications to the ECIC algorithm <ref type="bibr" target="#b10">(Rakib et al., 2020)</ref> to improve their results. Namely, we included modern deep learning components such as USE, BERT and RoBERTa in the algorithm as well tested various methods to handle weights during fine-tuning over iterations such as resumption and re-initialization and developed a new stopping criterion for the algorithm. The general outline of the algorithm is shown in Algorithm ??. At the initial stage, clustering is carried out using one of the widely used clustering methods (see below). An algorithm for outlier detection is then used to split the dataset into train and test parts. Additional samples can be moved from the train to the test set based on the P number sampled randomly in the range from P 1 to P 2 . The train part is used to train the classifier. Outliers and some number of the additional samples are used as a test set and predictions for the test set are used to relabel the dataset. Steps with outlier detection, classification, and relabeling are then repeated until the stopping criterion is reached or the maximum number of iterations is exceeded. As will be shown below, this iterative procedure leads to improved clustering results in many cases.</p><p>Averaged word embeddings were used as features in <ref type="bibr" target="#b10">(Rakib et al., 2020;</ref><ref type="bibr" target="#b12">Xu et al., 2017)</ref>. One of the differences of our study is that we used  <ref type="bibr" target="#b1">(Cer et al., 2018;</ref><ref type="bibr">Yang et al., 2019)</ref> for short texts to plug them into one of the clustering algorithms: k-means, Hierarchical Agglomerative Clustering (HAC) or Spectral Clustering. We used a full similarity matrix as well as k-NN and similarity distribution based sparsification of the similarity matrix <ref type="bibr" target="#b9">(Rakib et al., 2018)</ref> with HAC. In both methods of sparsification, we set the number of non-zero elements in each row of the similarity matrix equal to the ratio of the number of samples in the dataset to the number of clusters. In addition, we tested all available linkage criteria for HAC. We tried the Isolation Forest (IF) <ref type="bibr" target="#b6">(Liu et al., 2008)</ref> and Local Outlier Factor (LOF) <ref type="bibr" target="#b0">(Breunig et al., 2000)</ref> for outlier detection. We used clustering and outlier detection algorithms implemented in the scikit-learn 2 and scipy 3 python libraries.</p><p>In contrast with (Rakib et al., 2020), we used Transformer models such as BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> and RoBERTa <ref type="bibr" target="#b7">(Liu et al., 2019)</ref> for itera-tive fine-tuning and classification. In addition, we used Multinomial Logistic Regression (MLR) as in other works.</p><p>We consider two different stopping criteria. The first stopping criterion (Rakib et al., 2020) is defined as follows ? = 1 N i |c i ? c ? i | &lt; ? where c i and c ? i are sizes of clusters determined by the current labeling L and previous labeling L ? , respectively, and i is a cluster number. The second criterion is reached immediately when ? has a minimum value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>Our study uses the same datasets as those in a number of previous studies <ref type="bibr" target="#b12">(Xu et al., 2017;</ref><ref type="bibr" target="#b3">Hadifar et al., 2019;</ref><ref type="bibr" target="#b10">Rakib et al., 2020)</ref> on STC. The statistics on the datasets are presented in Table 1. The Search Snippets dataset is composed of Google search results. The texts in the Search Snippets dataset represent sets of key words, rather than being coherent texts. The Biomedical corpus is a subset of one of the BioAsQ 4 challenge datasets. The texts in this dataset are paper titles with many special terms from biology and medicine. The Stack Overflow is a subset of the challenge on Kaggle and contains texts with question titles. AG News is a subset of the dataset that was used in <ref type="bibr" target="#b17">(Zhang and LeCun, 2015)</ref>, where 2000 samples from each of the four categories were taken randomly. The Tweet, Google News TS, T and S sets are exactly those datasets which were used in <ref type="bibr" target="#b14">(Yin and Wang, 2014)</ref>. Note that the former and the latter four datasets can be grouped by the number of clusters. The first group contains relatively low numbers of clusters, while the second has greater numbers of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>To measure the performance of our algorithm, we used such metrics as accuracy and Normalized Mutual Information (NMI). The value of NMI does not depend on the absolute values of labels. The accuracy is calculated using the Hungarian algorithm <ref type="bibr" target="#b12">(Xu et al., 2017)</ref>. It allows one to rearrange absolute label values to maximize accuracy.</p><p>Our experiments on initial clustering tested which of the USE versions and which clustering algorithm should be used to obtain the best quality in terms of both aforementioned metrics. As 4 http://bioasq.org a result, the old version of USE <ref type="bibr" target="#b1">(Cer et al., 2018)</ref> proved to be better (by a few percent) than the newer one <ref type="bibr">(Yang et al., 2019)</ref> in terms of both metrics on all 8 datasets. We tested k-means, HAC, and Spectral Clustering algorithms with these sentence embeddings. Interestingly, we found that the best clustering method was k-means for the whole group of datasets with the smaller number of clusters (see <ref type="table" target="#tab_3">Table 2</ref>). Since k-means is not a deterministic algorithm and its result depends on a particular initializatin, we averaged the results over 5 runs, each having 1000 initializations. On the contrary, HAC proved to be the best clustering method for datasets with the greater number of clusters (see <ref type="table">Table 3</ref>). Note we does not provide variance for HAC since this algorithm is determenistic. Overall, k-NN sparsification with the average linkage criterion gave the best results for the four datasets with the greater number of clusters. This differs from the results of (Rakib et al., 2020), where a sparsification based on similarity distribution and the Ward linkage criterion are described as the most effective ones.</p><p>We obtained highly competitive results for two (Stack Overflow and AG News) of the four datasets from the first group of datasets. However, we did not get comparable results on the other two datasets (Search Snippets and Biomedical corpus), which can be easily explained. The Search Snippets dataset texts are sets of key words, rather than being coherent texts. Since USE was trained on coherent texts, it cannot produce a good result. The Biomedical dataset almost completely consists of special terms. USE probably did not see many of these terms during training, which explains its poor performance on this dataset. We got the best results for all four datasets from the second group  in terms of NMI but not in terms of accuracy (see <ref type="table">Table 3</ref>).</p><p>To improve the results of initial clustering, we tested the iterative classification algorithm with MLR and with neural pre-trained classifiers, such as BERT and RoBERTa. For the neural classifier, the number of iterations T was set to be 10, the learning rate 3?10 ?5 and the number of epochs to train during each iteration 2. The use of the warm start i.e. training resumption after each iteration instead of re-initialization, and learning rate linear decaying schedule instead of the constant learning rate, did not show any considerable improvement. RoBERTa gave approximately one half percent improvement over the BERT performance. We set T to be 50 for MLR, since the algorithm worked more stable and had potential to improve for the more iterations than for neural classifiers. For the first stopping criterion we tried ? equal to 0.03 and 0.05. We found that the use of the second stopping criterion with neural classifiers gives better results than the first one. We did not use any criterion for MLR and collected the metrics at the end of 50 iterations, since both considered metrics grew monotonically for this classifier. We set P 1 to be 0.75 and P 2 to be 0.95 for both types of classifiers. We averaged our results over 3 runs in both cases. We did not find any difference in the use of IF or LOF for outlier detection with all classifiers.</p><p>The iterative classification achieved the stateof-the-art results on the Stack Overflow and AG News datasets with both types of classifiers and improved the good initial clustering result further (see <ref type="table" target="#tab_3">Table 2</ref>). The neural classifier showed a one percent better performance for the Stack Overflow in terms of accuracy than MLR. We did not get comparable results for the Biomedical and Search Snippets datasets, since the iterative classification algorithm can improve the initial clustering result by a limited number of percent and it was low efficient for these two datasets. We did not observe any improvement for the second group of datasets, since it is more difficult for the algorithm to converge to the correct solution during iterations in the case of greater number of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The sentence embeddings based algorithm for enhanced clustering by iterative classification was applied to 8 datasets with short texts. The algorithm demonstrates state of the art results for the 6 out of 8 datasets. We argue that the lack of coherent and common texts causes an inferior performance of the algorithm for the two remaining datasets.</p><p>The quality of the whole algorithm strongly depends on the initial clustering quality. Initial clustering with USE representations has already allowed us to achieve a competitive performance for a number of datasets. Therefore, due to transfer learning these representations can be readily applied to other datasets even without iterative classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Enhancement of Clustering by the Iterative ClassificationResult: Enhancement of Clustering 1 Dataset D with N texts and K clusters; 2 Apply initial clustering and labeling L; 3 Set the number of iterations T ; 4 while j ? T and the stopping criterion ? is not reached do 5 Sample P uniformly from [P 1 , P 2 ];</figDesc><table><row><cell>6</cell><cell>Apply outlier detection for each cluster</cell></row><row><cell></cell><cell>from L to remove outliers from D;</cell></row><row><cell>7</cell><cell>if Number of texts in any cluster</cell></row><row><cell></cell><cell>n ? P  *  N/K then</cell></row><row><cell>8</cell><cell>Remove texts randomly from that</cell></row><row><cell></cell><cell>cluster until n ? P  *  N/K;</cell></row><row><cell>9</cell><cell>end</cell></row><row><cell>10</cell><cell>Add the rest of D to the train set and</cell></row><row><cell></cell><cell>add all removed samples to the test</cell></row><row><cell></cell><cell>set;</cell></row><row><cell>11</cell><cell>Train a classifier on the train set and</cell></row><row><cell></cell><cell>update L based on predictions of the</cell></row><row><cell></cell><cell>classifier on the test set;</cell></row></table><note>12 Calculate the criterion ? and update j;13 end USE representations 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics on the datasets used in the study. K is the number of clusters, N is the number of samples, M is the average number of words in a document.</figDesc><table><row><cell>Dataset</cell><cell>K</cell><cell>N</cell><cell>M</cell></row><row><cell>Stack Overflow</cell><cell>20</cell><cell cols="2">20000 8.2</cell></row><row><cell>AG News</cell><cell>4</cell><cell>8000</cell><cell>22.5</cell></row><row><cell cols="2">Biomedical corpus 20</cell><cell cols="2">20000 12.9</cell></row><row><cell>Search Snippets</cell><cell>8</cell><cell cols="2">12340 17.0</cell></row><row><cell>Tweet</cell><cell>89</cell><cell>2472</cell><cell>8.4</cell></row><row><cell>Google News TS</cell><cell cols="3">152 11109 28.0</cell></row><row><cell>Google News T</cell><cell cols="3">152 11109 6.2</cell></row><row><cell>Google News S</cell><cell cols="3">152 11109 21.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with published results of accuracy and NMI scores for datasets with the smaller number of clusters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://tfhub.dev/google/collections/universal-sentenceencoder/1 2 https://scikit-learn.org/stable/index.html 3 https://www.scipy.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lof: identifying densitybased local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2000 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<idno>abs/1803.11175</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Universal sentence encoder. CoRR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A self-training approach for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hadifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP</title>
		<meeting>the 4th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Jipeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhenyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yunhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Xindong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07695</idno>
		<title level="m">Short text topic modeling techniques, applications, and performance: a survey</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing topic modeling for short texts with auxiliary word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving short text clustering by similarity matrix sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Md Rashadul Hasan Rakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Jankowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Zeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Document</title>
		<meeting>the ACM Symposium on Document</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhancement of short text clustering by iterative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Md Rashadul Hasan Rakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Zeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Jankowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Information Systems</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Short text clustering via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-taught convolutional neural networks for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2019. Multilingual universal sentence encoder for semantic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jax</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Hern?nde?</forename><surname>Abrego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<idno>abs/1907.04307</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A dirichlet multinomial mixture model-based approach for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A model-based approach for text clustering with outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 32nd International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="625" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attentive representation learning with adversarial training for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03720</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
