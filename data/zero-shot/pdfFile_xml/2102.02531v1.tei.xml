<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ABCNet: Attentive Bilateral Contextual Network for Efficient Semantic Segmentation of Fine-Resolution Remote Sensing Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Duan</surname></persName>
							<email>chenxiduan@whu.edu.cnc.duan</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ABCNet: Attentive Bilateral Contextual Network for Efficient Semantic Segmentation of Fine-Resolution Remote Sensing Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 *Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic Segmentation</term>
					<term>Attention Mechanism</term>
					<term>Convolutional Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation of remotely sensed images plays a crucial role in precision agriculture, environmental protection, and economic assessment. In recent years, substantial fineresolution remote sensing images are available for semantic segmentation. However, due to the complicated information caused by the increased spatial resolution, state-of-the-art deep learning algorithms normally utilize complex network architectures for segmentation, which usually incurs high computational complexity. Specifically, the high-caliber performance of the convolutional neural network (CNN) heavily relies on fine-grained spatial details (fine resolution) and sufficient contextual information (large receptive fields), both of which trigger high computational costs.</p><p>This crucially impedes their practicability and availability in real-world scenarios that require real-time processing. In this paper, we propose an Attentive Bilateral Contextual Network 2 (ABCNet), a convolutional neural network (CNN) with double branches, with prominently lower computational consumptions compared to the cutting-edge algorithms, while maintaining a competitive accuracy. Code is available at https://github.com/lironui/ABCNet. Index Terms-Semantic Segmentation, Attention Mechanism, Convolutional Neural Network 6</p><p>Compared with the encoder-decoder structure <ref type="figure">(Fig. 1(a)</ref>), the bilateral architecture ( <ref type="figure">Fig. 1(b)</ref>) can maintain more spatial information without retarding the speed of the model <ref type="bibr" target="#b50">(Yu et al., 2018)</ref>.</p><p>Concretely, the spatial path merely stacks three convolution layers to generate the 1/8 feature maps, while the contextual path includes two attention enhancement modules (AEM) to refine the features and capture contextual information. As features generated by two paths are disparate in the level of feature representation, we further design a feature aggregation module (FAM) to fuse these features. Our main contributions are summarized as follows: 1) We propose a novel approach for efficient semantic segmentation of fine-resolution remote sensing images. Specifically, we propose an Attentive Bilateral Contextual Network (ABCNet) with a spatial path and a contextual path.</p><p>2) We design two specific modules, attention enhancement modules (AEM) for exploring long-range contextual information and feature aggregation module (FAM) for fusing features obtained by two paths.</p><p>3) We achieve competitive results on the ISPRS Vaihingen dataset and ISPRS Potsdam dataset. More specifically, we obtain the results of 91.095% overall accuracy on the Potsdam test dataset with a speed of 72.13 FPS even on a mid-range graphics card (1660Ti).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work 1) Context information extraction</head><p>As the performance of semantic segmentation heavily hinges on the abundant context information, a great many endeavors are poured into tackling this issue. The dilated or atrous</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Profit from the rapidly expanding Earth Observation technique, a large amount of remotely sensed images with fine spatial and spectral resolutions are now available for a wide range of application scenarios such as image classification <ref type="bibr" target="#b29">(Lyons et al., 2018;</ref><ref type="bibr" target="#b31">Maggiori et al., 2016)</ref>, object detection <ref type="bibr" target="#b47">Xia et al., 2018)</ref>, and semantic segmentation <ref type="bibr" target="#b20">(Kemker et al., 2018;</ref><ref type="bibr" target="#b54">Zhang et al., 2019a)</ref>. The revisiting property of orbital acquisitions brings the consecutive monitoring of land surface, ocean, and atmosphere into the possibility . Fineresolution remote sensing images normally contain substantial detailed spatial information for land cover and land use . Semantic segmentation, which assigns each pixel in images with a definite category, has become one of the most crucial levers for ground object interpretation. Specifically, semantic segmentation from remotely sensed imagery plays a pivotal role in various scenarios including precision agriculture <ref type="bibr" target="#b13">(Griffiths et al., 2019;</ref><ref type="bibr" target="#b34">Picoli et al., 2018)</ref>, environmental protection <ref type="bibr" target="#b38">(Samie et al., 2020;</ref><ref type="bibr" target="#b48">Yin et al., 2018)</ref>, and economic assessment <ref type="bibr" target="#b54">Zhang et al., 2019a)</ref>. Looking from a panoramic view, semantic segmentation is one of the high-level tasks that paves the way for complete scene understanding. Hence, semantic segmentation is at the forefront of a comprehensive effort towards automatic Earth monitoring by international agencies. <ref type="bibr">3</ref> To identify the image content from various land cover and land use categories, tons of approaches explored the utilization of spectral and spectral-spatial features to interpret remote sensing images <ref type="bibr" target="#b12">(Gong et al., 1992;</ref><ref type="bibr" target="#b30">Ma et al., 2017;</ref><ref type="bibr" target="#b42">Tucker, 1979;</ref><ref type="bibr" target="#b59">Zhong et al., 2014;</ref><ref type="bibr" target="#b60">Zhu et al., 2017)</ref>. However, the finite ability to capture the contextual information contained in the images restricts the flexibility and adaptability of these methods <ref type="bibr" target="#b25">(Li et al., 2020c;</ref><ref type="bibr" target="#b41">Tong et al., 2020)</ref>, especially when the detailed and structural information surged by the increased spatial resolution. By contrast, bolstered by its powerful capabilities to capture nonlinear and hierarchical features automatically, deep Convolutional Neural Network (CNN) has posed a significant impact on the understanding of fine-resolution remote sensing images <ref type="bibr" target="#b23">(Li et al., 2020a;</ref><ref type="bibr" target="#b58">Zheng et al., 2020)</ref>.</p><p>For semantic segmentation, Fully Convolutional Network (FCN) <ref type="bibr" target="#b28">(Long et al., 2015)</ref> is the first proven and effective end-to-end CNN structure. Restricted by the oversimple design of the decoder, the results of FCN, although very encouraging, appear coarse. Subsequently, the more elaborate encoder-decoder structure <ref type="bibr" target="#b1">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b36">Ronneberger et al., 2015)</ref> is proposed which comprises two symmetric paths: a contracting path for extracting features and an expanding path for exact positioning to accomplish more accurate results. To guarantee the accuracy of segmentation, global contextual information and multiscale semantic features are supposed to be thoroughly utilized for semantic categories with varying sizes in images. By the spatial pyramid pooling module, the pyramid scene parsing network (PSPNet) <ref type="bibr" target="#b56">(Zhao et al., 2017)</ref> aggregates contextual information among different regions. The dual attention network (DANet) <ref type="bibr" target="#b9">(Fu et al., 2019)</ref> applies the dot-product attention mechanism to extract abundant contextual relationships. Subject to the enormous memory and computational consumptions, DANet simply attaches the dot-product attention mechanism at the lowest layer and merely captures the longrange dependencies from the smallest feature maps. DeeplabV3 <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> adopts atrous convolution to mining multiscale features, while a simple yet valid decoder module is added in DeepLabV3+  to further refine the segmentation results.</p><p>The extraction of global contextual information and the exploitation of large-scale feature maps are computationally expensive <ref type="bibr" target="#b24">Li et al., 2020b)</ref>. Therefore, a series of lightweight networks <ref type="bibr" target="#b16">(Hu et al., 2020;</ref><ref type="bibr" target="#b33">Or?i? and ?egvi?, 2021;</ref><ref type="bibr" target="#b35">Romera et al., 2017;</ref><ref type="bibr" target="#b50">Yu et al., 2018;</ref><ref type="bibr" target="#b61">Zhuang et al., 2019)</ref> are developed to accelerate the computational speed while keeps the equilibrium between accuracy and efficiency. For example, the asymmetric convolution which is used in ERFNet <ref type="bibr" target="#b35">(Romera et al., 2017)</ref> factorizes the standard 3 ? 3 convolutions into a 1 ? 3 convolution and a 3 ? 1 convolution, saving about 33% computational consumptions. By exploiting spatial correlations and cross-channel correlations respectively, BiseNet <ref type="bibr" target="#b50">(Yu et al., 2018)</ref> utilizes the depth-wise separable convolution <ref type="bibr" target="#b6">(Chollet, 2017)</ref> which further lowers the consumption of the standard convolution. Multi-scale encoder-decoder branch pairs with skip connections are studied in ShelfNet <ref type="bibr" target="#b61">(Zhuang et al., 2019)</ref> where a shared-weight strategy is harnessed in the residual block to reduces the parameter without sacrificing accuracy. For implementing the non-local context aggregation, FANet <ref type="bibr" target="#b16">(Hu et al., 2020)</ref> employs the fast attention module in efficient semantic segmentation. SwiftNet <ref type="bibr" target="#b33">(Or?i? and ?egvi?, 2021)</ref> explores the effectiveness of pyramidal fusion in compact architectures.</p><p>Due to limited capacity in extracting the global context information, there is a huge gap in accuracy between the lightweight networks and the state-of-the-art models, which is especially true when it comes to the fine-resolution remotely sensed images. As a powerful approach that can capture long-range dependencies, the dot-product attention mechanism <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> is a plausibly ideal solution to remedy this limitation. Whereas, the memory and computational consumptions of the dot-product attention mechanism increase quadratically with the spatiotemporal size of the input, which runs counter to the original intention of lightweight networks.</p><p>Encouragingly, our previous work about linear attention <ref type="bibr" target="#b23">(Li et al., 2020a)</ref> which reduces the complexity of the dot-product attention mechanism from ( 2 ) to ( ) alleviates this plight. In this paper, we aim to further improve the segmentation accuracy while simultaneously ensuring the efficiency of semantic segmentation. We approach this challenging problem by modeling the global contextual information using the linear attention mechanism. To be specific, we proposed an Attentive Bilateral Contextual Network (ABCNet) to address the efficient semantic segmentation of fine-resolution remote sensing images. Following the design philosophy of BiSeNet <ref type="bibr" target="#b50">(Yu et al., 2018)</ref>, there are two branches in the proposed ABCNet: a spatial path to retain affluent spatial details and a contextual path to capture global contextual information. 7 convolution <ref type="bibr" target="#b2">(Chen et al., 2014;</ref><ref type="bibr" target="#b51">Yu and Koltun, 2015)</ref> has been demonstrated to be an effective technology for enlarging receptive fields without shrinking spatial resolution. Also, the encoderdecoder <ref type="bibr" target="#b36">(Ronneberger et al., 2015)</ref> architecture which merges high-level and low-level features using skip connections is another valid way for extracting spatial context. Based on the encoderdecoder framework or dilation backbone, several subsequent studies focus on exploring the usage of spatial pyramid pooling (SPP) <ref type="bibr" target="#b14">(He et al., 2015)</ref>. For example, the pyramid pooling module (PPM) in PSPNet is composed of convolutions with kernels of four different sizes <ref type="bibr" target="#b56">(Zhao et al., 2017)</ref>, while DeepLab v2  equips with the atrous spatial pyramid pooling (ASPP) module which groups parallel atrous convolution layers with varying dilation rates.</p><p>However, there are still certain current limitations in SPP. The SPP with standard convolution will face a dilemma when expanding the receptive field by a large kernel size. The above operations are normally accompanied by a huge number of parameters. The SPP with small kernels (e.g. ASPP), on the other hand, lacks enough connection between adjacent features; and the gridding problem <ref type="bibr" target="#b44">(Wang et al., 2018a)</ref>, which occurs when the field is enlarged by a dilated convolutional layer. By contrast, the powerful ability to model long-range dependencies enable the dot-product attention mechanism to extract context information in the global scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Dot-Product Attention Mechanism</head><p>Let H, W, and denote the height, weight, and channels of the input, respectively. The input feature is defined as = [ 1 , ? , ] ? ? ? , where = ? . Firstly, the dot-product attention mechanism utilizes three projected matrices ? ? ? , ? ? ? , and ? ? ? to generate the corresponding query matrix Q, the key matrix K, and the value matrix V:</p><formula xml:id="formula_0">? = ? ? ? ; = ? ? ? ; = ? ? ? .</formula><p>(1) Please note that the dimensions of the Q and K are supposed to be identical and all the vectors in this section are column vectors by default. Accordingly, a normalization function ? is employed to measure the similarity between the i-th query feature ? ? and the j-th key feature ? ? as ( ) ? ? 1 . As the query feature and key feature are generated via different layers, the similarities between ( ) and ( ) are not symmetric. By calculating similarities between all pairs of pixels in the input feature maps and taking the similarities as weights, the dot-product attention mechanism generates the value at position i by aggregating the value features from all positions using weighted summation:</p><formula xml:id="formula_1">( , , ) = ( ) .<label>(2)</label></formula><p>Normally, the softmax is the frequently-used normalization function:</p><formula xml:id="formula_2">( ) = ( ),<label>(3)</label></formula><p>where indicates that the softmax is exploited along each row of the matrix .</p><p>By modeling the similarities between each pair of positions of the input, the global dependencies in the features can be thoroughly extracted by the ( ) . The dot-product attention mechanism is firstly designed for machine translation <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>, while the non-local module <ref type="bibr" target="#b46">(Wang et al., 2018b)</ref> introduces and modifies it for computer vision <ref type="figure">(Fig. 2)</ref>.</p><p>Based on the dot-product attention mechanism as well as its variants, a constellation of attentionbased networks has been proposed to tackle the semantic segmentation task. Inspired by the nonlocal module <ref type="bibr" target="#b46">(Wang et al., 2018b)</ref>, the Double Attention Networks ( 2 -Net) <ref type="bibr" target="#b5">(Chen et al., 2018b)</ref>, Dual Attention Network (DANet) <ref type="bibr" target="#b9">(Fu et al., 2019)</ref>, Point-wise Spatial Attention Network 9 (PSANet) <ref type="bibr" target="#b57">(Zhao et al., 2018)</ref>, Object Context Network (OCNet) , and Cooccurrent Feature Network (CFNet) <ref type="bibr" target="#b55">(Zhang et al., 2019b)</ref> are proposed successively for scene segmentation by exploring the long-range dependency. <ref type="figure">Fig.2</ref> The diagram of the dot-product attention modified for computer vision.</p><p>Even though the introduction of attention significantly boosts the performance on segmentation, the huge resource-demanding of dot-product critically hinders its application on large inputs. To be specific, for ? ? ? and ? ? ? , the product between and belongs to ? ? , leading to the ( 2 ) memory and computation complexity. Consequently, it is requisite to lower the high demand for computational resources of the dot-product attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Generalization and simplification of the dot-product attention mechanism</head><p>If the normalization function is set as softmax, the i-th row of the result matrix generated by the dot-product attention mechanism can be written as:</p><formula xml:id="formula_3">( , , ) = ? =1 ? =1 .<label>(4)</label></formula><p>Equation <ref type="formula" target="#formula_3">(4)</ref> can be rewritten and generalized to any normalization function as:</p><formula xml:id="formula_4">( , , ) = ? sim? , ? =1 ? sim? , ? =1 , sim? , ? ? 0.<label>(5)</label></formula><p>sim? , ? can be expanded as ( ) ( ) that measures the similarity between the and , whereupon equation <ref type="formula" target="#formula_3">(4)</ref> can be rewritten as equation <ref type="formula" target="#formula_5">(6)</ref> and be simplified as equation <ref type="formula" target="#formula_6">(7)</ref>:</p><formula xml:id="formula_5">( , , ) = ? ( ) ( ) =1 ? ( ) ( ) =1 ,<label>(6)</label></formula><formula xml:id="formula_6">( , , ) = ( ) ? ( ) =1 ( ) ? ( ) =1 .<label>(7)</label></formula><p>Particularly, if (?) = (?) = (?) , equation <ref type="formula" target="#formula_4">(5)</ref> is equivalent to equation (4). The vectorized form of equation <ref type="formula" target="#formula_6">(7)</ref> is:</p><formula xml:id="formula_7">( , , ) = ( ) ( ) ( ) ? ( ) , .<label>(8)</label></formula><p>As the softmax function is substituted for sim? , ? = ( ) ( ) , the order of the commutative operation can be altered, thereby avoiding multiplication between the reshaped key matrix K and query matrix Q. In concrete terms, the product between ( ) and V can be computed first and then multiply the result and Q, leading only ( ) time complexity and ( ) space complexity. The suitable (?) and (?) enable the above scheme to achieve the competitive performance with finite complexity <ref type="bibr" target="#b19">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b24">Li et al., 2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Linear Attention Mechanism</head><p>In our previous work <ref type="bibr" target="#b23">(Li et al., 2020a)</ref> we proposed a linear attention mechanism from another perspective that replaces the softmax function with the first-order approximation of Taylor expansion, which is shown as equation <ref type="formula" target="#formula_8">(9)</ref>:</p><formula xml:id="formula_8">? 1 + .<label>(9)</label></formula><p>To guarantee the above approximation to be nonnegative, and are normalized by 2 norm, thereby ensuring ? ?1:</p><formula xml:id="formula_9">? , ? = 1 + ? ? ? 2 ? ? ? ? 2 ?.<label>(10)</label></formula><p>Thus, equation <ref type="formula" target="#formula_4">(5)</ref> can be rewritten as equation <ref type="formula" target="#formula_10">(11)</ref> and simplified as equation <ref type="formula" target="#formula_1">(12)</ref>:</p><formula xml:id="formula_10">( , , ) = ? ? + ? ? ? ? ? ? ? ?? = ? ? + ? ? ? ? ? ? ? ?? = ,<label>(11)</label></formula><formula xml:id="formula_11">( , , ) = ? =1 + ? ? ? 2 ? ? ? ? ? 2 ? =1 + ? ? ? 2 ? ? ? ? ? 2 ? =1 .<label>(12)</label></formula><p>The equation <ref type="formula" target="#formula_1">(12)</ref> can be turned into a vectorized form:</p><formula xml:id="formula_12">( , , ) = ? , + ? ? ? 2 ? ?? ? ? 2 ? ? + ? ? ? 2 ? ? ? ? ? 2 ? , .<label>(13)</label></formula><p>Since</p><formula xml:id="formula_13">? ? ? ? 2 ? =1 and ? ? ? ? 2 ? =1</formula><p>can be calculated and reused for each query, time and memory complexity of the attention based on equation <ref type="formula" target="#formula_2">(13)</ref> is ( ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Efficient semantic segmentation</head><p>For many applications, efficiency is critical, which is especially true for real-time (?30FPS) scenarios such as autonomous driving. Therefore, recent researches have made great efforts to accelerate models for efficient semantic segmentation, which employs lightweight models or downsampling the input size. The utilization of lightweight convolutions (e.g., the asymmetric convolution and the depth-wise separable convolution) is a common strategy for designing lightweight networks <ref type="bibr" target="#b35">(Romera et al., 2017;</ref><ref type="bibr" target="#b50">Yu et al., 2018)</ref>. The downsampling of the input size is a trivial solution to speed up semantic segmentation which reduces the resolution of the input images, thereby leading to the loss of image details. To extract spatial details at original resolution, many methods further add a shallow branch, forming the two-path architecture <ref type="bibr" target="#b49">(Yu et al., 2020;</ref><ref type="bibr" target="#b50">Yu et al., 2018)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attentive Bilateral Contextual Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Spatial path</head><p>Although both of them are crucial for the high accuracy of segmentation, it is actually impossible to reconcile the affluent spatial details with the large receptive field simultaneously.</p><p>Especially, in the term of efficient semantic segmentation, the mainstream solutions focus on down-sampling the input image or speeding up the network by channel pruning. The former loses the majority of spatial details, which the latter damages spatial details. By contrast, in the proposed ABCNet, we adopt the bilateral architecture <ref type="bibr" target="#b50">(Yu et al., 2018)</ref> which is equipped with a spatial path to capture spatial details and generate low-level feature maps. Therefore, the rich channel capacity is essential for this path to encode sufficient spatial detailed information.</p><p>Meanwhile, as the spatial path merely focuses on the low-level details, the shallow structure with a small stride for this branch is enough.</p><p>Specifically, the spatial path comprises three layers as shown in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>. Each layer contains a convolution with stride = 2, followed by batch normalization <ref type="bibr" target="#b18">(Ioffe and Szegedy, 2015)</ref> and</p><p>ReLU <ref type="bibr" target="#b11">(Glorot et al., 2011)</ref>. Therefore, the output feature maps of this path are 1/8 of the original image, which encodes abundant spatial details resulting from the large spatial size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Contextual path</head><p>In parallel to the spatial path, the contextual path is designed to extract high-level global context information and provide sufficient receptive field. To enlarge the receptive field, several networks take advantage of the spatial pyramid pooling with a large kernel, leading to the huge computation demanding and memory consuming. With the consideration of the long-range context information and efficient computation simultaneously, we develop the contextual path with the linear attention mechanism <ref type="bibr" target="#b23">(Li et al., 2020a)</ref>.</p><p>Concretely, in the contextual path as shown in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>, we harness the lightweight backbone (i.e., ResNet 18) <ref type="bibr" target="#b15">(He et al., 2016)</ref> to down-sample the feature map and encode the high-level semantic information. Thereafter, we deploy two attention enhancement modules (AEM) on the tails of the backbone to fully extract the global context information. The features obtained by the last two stages are fused and fed into the feature aggregation module (FAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Feature aggregation module</head><p>The feature representation of the spatial path and the contextual path is complementary but in different domains (i.e., the spatial path generates the low-level and detailed feature, while the contextual path obtains the high-level and semantic features). Thus, the simple fusion schemes such as summation and concatenation are not appropriate manners to fuse information. In contrast, we design a feature aggregation module (FAM) to merge both types of feature representation with consideration of accuracy and efficiency.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 4(c)</ref>, with two domains of features, we first concatenate the output of spatial path and context path. Thereafter, a convolution layer with batch normalization <ref type="bibr" target="#b18">(Ioffe and Szegedy, 2015)</ref> and ReLU <ref type="bibr" target="#b11">(Glorot et al., 2011)</ref> attached to balance the scales of the features. Then, we capture the long-range dependencies of the generated features using the linear attention mechanism. The details of the design of FAM can be seen in <ref type="figure" target="#fig_2">Fig. 4(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Loss function</head><p>As can be seen from <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, besides the principal loss function to supervise the output of the whole network, we utilize two auxiliary loss functions at the context path to accelerate the convergence velocity. We select the cross-entropy loss as the principal loss:</p><formula xml:id="formula_14">( , ) = ? log( ) ? (1 ? ) log(1 ? ),<label>(14)</label></formula><p>where p is the prediction generated by the network, while y is the ground truth. The auxiliary loss functions are chosen as the focal loss:</p><formula xml:id="formula_15">1 ( , ) = 2 ( , ) = ? (1 ? ) log ? (1 ? ) log(1 ? ),<label>(15)</label></formula><p>where ? is the focusing parameter, which controls the down-weighting of the easily classified examples and is set as 2 in our experiments. Hence, the overall loss of the network is:</p><formula xml:id="formula_16">( , ) = + 1 ( , ) + 2 ( , ).<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Datasets</head><p>The effectiveness of the proposed ABCNet is verified using the ISPRS Potsdam dataset, the ISPRS Vaihingen dataset.</p><p>Potsdam: There are 38 fine-resolution images of size 6000 ? 6000 pixels with a ground sampling distance (GSD) of 5 cm in the Potsdam dataset. The dataset provides near-infrared, red, green, and blue channels as well as DSM and normalized DSM (NDSM). We utilize ID: 2_13, 2_14, 3_13, 3_14, 4_13, 4_14, 4_15, 5_13, 5_14, 5_15, 6_13, 6_14, 6_15, 7_13 for testing, ID:</p><p>2_10 for validation, and the remaining 22 images, except image named 7_10 with error annotations, for training. Please note that we only employ the red, green, and blue channels in our experiments.</p><p>Vaihingen: The Vaihingen dataset contains 33 images with an average size of 2494 ? 2064 pixels and a GSD of 5 cm. The near-infrared, red, and green channels together with DSM are provided in the dataset. We utilize <ref type="bibr">ID: 2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">12,</ref><ref type="bibr">14,</ref><ref type="bibr">16,</ref><ref type="bibr">20,</ref><ref type="bibr">22,</ref><ref type="bibr">24,</ref><ref type="bibr">27,</ref><ref type="bibr">29,</ref><ref type="bibr">31,</ref><ref type="bibr">33,</ref><ref type="bibr">35,</ref><ref type="bibr">38</ref> for testing, ID: 30 for validation, and the remaining 15 images for training. The DSM is not used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Evaluation Metrics</head><p>The performance of ABCNet is evaluated using the overall accuracy (OA), the mean Intersection over Union (mIoU), and the F1 score (F1). Based on the accumulated confusion matrix, the OA, mIoU, and F1 are computed as:</p><formula xml:id="formula_17">= ? =1 ? + + + =1 ,<label>(17)</label></formula><formula xml:id="formula_18">= 1 ? + + =1 ,<label>(18)</label></formula><formula xml:id="formula_19">1 = 2 ? ? + ,<label>(19)</label></formula><p>where , , , and represent the true positive, false positive, true negative, and false negatives, respectively, for object indexed as class k. OA is computed for all categories including the background.  <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>, ShelfNet <ref type="bibr" target="#b61">(Zhuang et al., 2019)</ref>, and SwiftNet <ref type="bibr" target="#b33">(Or?i? and ?egvi?, 2021)</ref>. The test time augmentation (TTA) in terms of rotating and flipping is applied for all comparative methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Ablation study</head><p>To verify the effectiveness of the components in the proposed ABCNet, we conduct extensive ablation experiments. atmosphere conditions, while the setting details and quantitative results are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline:</head><p>We utilize the ResNet-18 as the backbone of the contextual path and select the contextual path without the AEM (denoted as CP in <ref type="table" target="#tab_0">Table I</ref>) as the baseline. The feature maps generated by CP are directly up-sampled to the shape as the original input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation for attention enhancement module:</head><p>For capturing the global context information, we specially design an attention enhancement module (AEM) in the contextual path. As presented in <ref type="table" target="#tab_0">Table I</ref>, for two datasets, the utilization of AEM (indicated as Cp + AEM) brings more than 1.5% improvement in mIoU.</p><p>Ablation for the spatial path: As the affluent spatial information is crucial for semantic segmentation, the spatial path is designed for preserving the spatial size and extracting spatial information.  Ablation for feature aggregation module: Given the features obtained by the spatial path and the contextual path are in different domains, neither summation nor the concatenation is the optimal fusion scheme. As can be seen from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) The complexity and speed of the network</head><p>The complexity and speed are momentous factors for measuring the merit of an algorithm, which is especially true for practical application. For a thorough comparison, we implement our experiments under different settings. First, the comparison of parameters and computational complexity between different networks are reported in <ref type="table" target="#tab_3">Table ?</ref>, where 'G' indicates Gillion (i.e., the unit of floating point operations) and 'M' signifies Million (i.e., the unit of parameter number).</p><p>Meanwhile, for a fair comparison, we choose 256?256, 512?512, 1024?1024, 2048?2048, and 4096?4096 as resolutions of the input image and report the inference speed which is measured by frames per second (FPS) on a midrange notebook graphics card 1660Ti.</p><p>The proposed ABCNet simultaneously juggles both speed and accuracy. As can be seen from the last column of <ref type="table" target="#tab_3">Table ?</ref>, the mIoU on the Potsdam dataset achieved by the ABCNet is at least 1.79% higher than the comparative methods. Meanwhile, the ABCNet could maintain a 72.13</p><p>FPS speed for a 512?512 input. Besides, the elaborate design enables the ABCNet to handle the massive input (4096?4096), while more than half of the comparative methods run out of memory for a such large input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Results on the ISPRS Vaihingen dataset</head><p>The ISPRS Vaihingen is a relatively small dataset. Besides, there is a small covariate shift between training and test sets <ref type="bibr" target="#b10">(Ghassemi et al., 2019)</ref>. Therefore, the high performance can be easily achieved by specifically designed networks, especially for those fuse orthophoto (TOP) images with auxiliary DSM or NDSM. In this part, we will show that our ABCNet model using only TOP images with efficient architecture can not only also transcend lightweight networks but also achieve competitive performance with those specially designed models.</p><p>As shown in  an 85.299% F1 score, which is at least 4% higher than other methods. To further evaluate the statistical significance, we report Kappa z-test for pairwise methods based on Kappa coefficients of agreement and their variances using the following equation:</p><formula xml:id="formula_20">= ( 1 ? 1 ) ? 1 + 2 ? ,<label>(20)</label></formula><p>where k signifies the Kappa coefficient and v denotes the Kappa variance. Concretely, if the value of is greater than 1.96, the two algorithms are signally different at the 95 % confidence level. </p><formula xml:id="formula_21">- - - - - - - - - 3.16 ABCNet 0.882 1.762 - - - - - - - - - - -</formula><p>As can be seen from <ref type="table" target="#tab_6">Table ?</ref>, the accuracy of the proposedABCNet is statistically higher than other comparative methods. In addition, we visualize area 38 in <ref type="figure">Fig. 5</ref> to qualitatively demonstrate the effectiveness of our ABCNet, while the enlarged results are shown in <ref type="figure" target="#fig_5">Fig. 7 (a)</ref>.</p><p>For a comprehensive evaluation, ABCNet is also compared with other state-of-the-art methods.</p><p>As can be seen in <ref type="table" target="#tab_7">Table ?</ref>, as a lightweight network, the proposed ABCNet achieves a competitive performance even compared with those designed models with complex structures. It is worth noting that the speed of our ABCNet is two to seven times faster than those methods.  <ref type="figure" target="#fig_4">Fig. 6</ref>, and the enlarged results are exhibited in <ref type="figure" target="#fig_5">Fig. 7 (b)</ref>. As there are sufficient images in the Potsdam dataset to train the network, the performance of the ABCNet can be parity with the state-of-the-art methods with a much faster speed. The comparisons are illustrated in <ref type="table" target="#tab_10">Table ?</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we propose a novel lightweight framework for efficient semantic segmentation in the field of remote sensing, namely Attentive Bilateral Contextual Network (ABCNet), which adaptively captures abundant spatial details by spatial path and global contextual information via     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Illustration of (a) the encoder-decoder structure and (b) the bilateral architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>The (a) computation requirement and (b) memory requirement between the linear attention mechanism and dot-product attention mechanism under different input sizes. The calculation assumes = = 2 = 64. Please notice that the figure is on the log scale.The validity and efficiency of the proposed attention have been testified through extensive ablation experiments and analysis<ref type="bibr" target="#b23">(Li et al., 2020a)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>An overview of the Attentive Bilateral Contextual Network. (a) Network Architecture. (b) The Attention Enhancement Module (AEM). (c) The Feature Aggregation Module (FAM).(d) The Linear Attention Mechanism. The proposed Attentive Bilateral Contextual Network (ABCNet), as well as the components, 13 are demonstrated in Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>All of the training procedures are implemented with PyTorch on a single Tesla V100 with 32 batch size, and the optimizer is set as AdamW with a 0.0003 learning rate. For training, the raw images are cropped into 512 ? 512 patches and augmented by rotating, resizing, horizontal axis flipping, vertical axis flipping, and adding random noise. The comparative methods include the contextual information aggregation methods designed initially for natural images, such as pyramid scene parsing network (PSPNet)<ref type="bibr" target="#b56">(Zhao et al., 2017)</ref> and dual attention network (DANet)<ref type="bibr" target="#b9">(Fu et al., 2019)</ref>, the multi-scale feature aggregation models proposed for remote sensing images, like multi-stage attention ResU-Net (MAResU-Net)<ref type="bibr" target="#b23">(Li et al., 2020a)</ref> and edge-aware neural network (EaNet)<ref type="bibr" target="#b58">(Zheng et al., 2020)</ref>, and also lightweight network developed for efficient semantic segmentation including depth-wise asymmetric bottleneck network (DABNet), efficient residual factorized convNet (ERFNet)<ref type="bibr" target="#b35">(Romera et al., 2017)</ref>, bilateral segmentation network V1 (BiSeNetV1)<ref type="bibr" target="#b50">(Yu et al., 2018)</ref> and V2 (BiSeNetV2)<ref type="bibr" target="#b49">(Yu et al., 2020)</ref>, fast attention network(FANet)  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Mapping results for test images of Potsdam tile-3_13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>Enlarged visualization of results on (LEFT) the Vaihingen dataset and (RIGHT) Potsdam dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I</head><label>I</label><figDesc></figDesc><table><row><cell>demonstrated that even the simple fusion schemes such as summation</cell></row><row><cell>(represented as Cp + Sp + AEM(Sum)) and concatenation (represented as Cp + Sp + AEM(Cat))</cell></row><row><cell>boost the performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY OF EACH COMPONENT IN OUR PROPOSED ABCNET</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Mean F1</cell><cell>OA (%)</cell><cell>mIoU (%)</cell></row><row><cell></cell><cell>Cp</cell><cell>83.862</cell><cell>88.141</cell><cell>74.433</cell></row><row><cell></cell><cell>Cp + AEM</cell><cell>85.746</cell><cell>88.780</cell><cell>76.268</cell></row><row><cell>Vaihingen</cell><cell>Cp + Sp + AEM(Sum)</cell><cell>86.575</cell><cell>89.831</cell><cell>77.529</cell></row><row><cell></cell><cell>Cp + Sp + AEM(Cat)</cell><cell>87.059</cell><cell>89.715</cell><cell>78.779</cell></row><row><cell></cell><cell>Cp + Sp + AEM + FAM</cell><cell>89.497</cell><cell>90.681</cell><cell>81.833</cell></row><row><cell></cell><cell>Cp</cell><cell>89.716</cell><cell>87.912</cell><cell>84.354</cell></row><row><cell></cell><cell>Cp + AEM</cell><cell>90.600</cell><cell>89.275</cell><cell>85.864</cell></row><row><cell>Potsdam</cell><cell>Cp + Sp + AEM(Sum)</cell><cell>91.029</cell><cell>89.368</cell><cell>86.450</cell></row><row><cell></cell><cell>Cp + Sp + AEM(Cat)</cell><cell>91.233</cell><cell>89.819</cell><cell>86.912</cell></row><row><cell></cell><cell>Cp + Sp + AEM + FAM</cell><cell>92.498</cell><cell>91.095</cell><cell>88.561</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table I, the significant gap of performance explains the validity of the feature aggregation module (signified as Cp + Sp + AEM + FAM).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE ?</head><label>?</label><figDesc>THE COMPLEXITY AND SPEED OF THE PROPOSED ABCNET AND COMPARATIVE METHODS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="9">Backbone Complexity(G) Parameters(M) 256?256 512?512 1024?1024 2048?2048 4096?4096 mIoU</cell></row><row><cell>DABNet (Li et al., 2019)</cell><cell>-</cell><cell>5.22</cell><cell>0.75</cell><cell>90.67</cell><cell>87.74</cell><cell>27.41</cell><cell>7.44</cell><cell>*</cell><cell>82.144</cell></row><row><cell>ERFNet (Romera et al., 2017)</cell><cell>-</cell><cell>14.75</cell><cell>2.06</cell><cell>90.51</cell><cell>59.04</cell><cell>17.59</cell><cell>4.87</cell><cell>1.25</cell><cell>79.152</cell></row><row><cell>BiSeNetV1 (Yu et al., 2018)</cell><cell>ResNet18</cell><cell>15.25</cell><cell>13.61</cell><cell>143.50</cell><cell>87.63</cell><cell>25.89</cell><cell>7.23</cell><cell>1.84</cell><cell>84.537</cell></row><row><cell>PSPNet (Zhao et al., 2017)</cell><cell>ResNet18</cell><cell>12.55</cell><cell>24.03</cell><cell>151.12</cell><cell>105.03</cell><cell>34.83</cell><cell>10.16</cell><cell>2.66</cell><cell>77.971</cell></row><row><cell>BiSeNetV2 (Yu et al., 2020)</cell><cell>-</cell><cell>13.91</cell><cell>12.30</cell><cell>124.49</cell><cell>82.84</cell><cell>25.64</cell><cell>7.07</cell><cell>*</cell><cell>85.167</cell></row><row><cell>DANet (Fu et al., 2019)</cell><cell>ResNet18</cell><cell>9.90</cell><cell>12.68</cell><cell>181.66</cell><cell>124.18</cell><cell>40.80</cell><cell>11.42</cell><cell>*</cell><cell>82.546</cell></row><row><cell>FANet (Hu et al., 2020)</cell><cell>ResNet18</cell><cell>21.66</cell><cell>13.81</cell><cell>112.59</cell><cell>67.97</cell><cell>20.41</cell><cell>5.57</cell><cell>*</cell><cell>86.722</cell></row><row><cell>ShelfNet (Zhuang et al., 2019)</cell><cell>ResNet18</cell><cell>12.36</cell><cell>14.58</cell><cell>123.59</cell><cell>90.41</cell><cell>30.93</cell><cell>9.06</cell><cell>2.40</cell><cell>86.770</cell></row><row><cell cols="2">SwiftNet (Or?i? and ?egvi?, 2021) ResNet18</cell><cell>13.08</cell><cell>11.80</cell><cell>157.63</cell><cell>97.62</cell><cell>30.79</cell><cell>8.65</cell><cell>*</cell><cell>86.285</cell></row><row><cell>MAResU-Net (Li et al., 2020a)</cell><cell>ResNet18</cell><cell>25.43</cell><cell>16.17</cell><cell>70.12</cell><cell>37.55</cell><cell>13.35</cell><cell>3.51</cell><cell>*</cell><cell>85.928</cell></row><row><cell>EaNet (Zheng et al., 2020)</cell><cell>ResNet18</cell><cell>18.75</cell><cell>34.23</cell><cell>73.98</cell><cell>55.95</cell><cell>17.94</cell><cell>5.53</cell><cell>1.54</cell><cell>85.763</cell></row><row><cell>ABCNet</cell><cell>ResNet18</cell><cell>18.72</cell><cell>14.06</cell><cell>113.09</cell><cell>72.13</cell><cell>22.73</cell><cell>6.23</cell><cell>1.60</cell><cell>88.561</cell></row><row><cell>* means the network is out of memory.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE ? ,</head><label>?</label><figDesc>the numeric scores for the ISPRS Vaihingen test dataset demonstrated that our ABCNet delivers robust performance, and exceeded other lightweight networks in the mean F1, OA, and mIoU by a considerable margin. Significantly, the ''car'' class in Vaihingen</figDesc><table /><note>dataset is difficult to handle as it is a relatively small object. Nonetheless, our ABCNet acquires</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE ?</head><label>?</label><figDesc>QUANTITATIVE COMPARISON RESULTS ON THE VAIHINGEN TEST SET.</figDesc><table><row><cell>Method</cell><cell cols="4">Backbone Imp. surf. Building Low veg.</cell><cell>Tree</cell><cell>Car</cell><cell cols="3">Mean F1 OA (%) mIoU (%)</cell></row><row><cell>DABNet (Li et al., 2019)</cell><cell>-</cell><cell>87.775</cell><cell>88.808</cell><cell>74.319</cell><cell cols="2">84.905 60.247</cell><cell>79.211</cell><cell>84.278</cell><cell>67.373</cell></row><row><cell>ERFNet (Romera et al., 2017)</cell><cell>-</cell><cell>88.451</cell><cell>90.239</cell><cell>76.394</cell><cell cols="2">85.751 53.649</cell><cell>78.897</cell><cell>85.751</cell><cell>67.698</cell></row><row><cell>BiSeNetV1 (Yu et al., 2018)</cell><cell>ResNet18</cell><cell>89.115</cell><cell>91.304</cell><cell>80.867</cell><cell cols="2">86.911 73.122</cell><cell>84.264</cell><cell>87.084</cell><cell>74.094</cell></row><row><cell>PSPNet (Zhao et al., 2017)</cell><cell>ResNet18</cell><cell>89.005</cell><cell>93.161</cell><cell>81.483</cell><cell cols="2">87.657 43.926</cell><cell>79.046</cell><cell>87.651</cell><cell>68.861</cell></row><row><cell>BiSeNetV2 (Yu et al., 2020)</cell><cell>-</cell><cell>89.884</cell><cell>91.911</cell><cell>82.020</cell><cell cols="2">88.271 71.417</cell><cell>84.701</cell><cell>87.972</cell><cell>75.005</cell></row><row><cell>DANet (Fu et al., 2019)</cell><cell>ResNet18</cell><cell>89.983</cell><cell>93.879</cell><cell>82.218</cell><cell cols="2">87.301 44.540</cell><cell>79.584</cell><cell>88.150</cell><cell>69.596</cell></row><row><cell>FANet (Hu et al., 2020)</cell><cell>ResNet18</cell><cell>90.652</cell><cell>93.782</cell><cell>82.595</cell><cell cols="2">88.555 71.602</cell><cell>85.437</cell><cell>88.872</cell><cell>75.884</cell></row><row><cell>EaNet (Zheng et al., 2020)</cell><cell>ResNet18</cell><cell>91.675</cell><cell>94.522</cell><cell>83.095</cell><cell cols="2">89.243 79.984</cell><cell>87.704</cell><cell>89.688</cell><cell>79.223</cell></row><row><cell>ShelfNet (Zhuang et al., 2019)</cell><cell>ResNet18</cell><cell>91.825</cell><cell>94.562</cell><cell>83.776</cell><cell cols="2">89.270 77.906</cell><cell>87.468</cell><cell>89.806</cell><cell>78.943</cell></row><row><cell>MAResU-Net (Li et al., 2020a)</cell><cell>ResNet18</cell><cell>91.971</cell><cell>95.044</cell><cell>83.735</cell><cell cols="2">89.349 78.283</cell><cell>87.676</cell><cell>90.047</cell><cell>80.749</cell></row><row><cell cols="2">SwiftNet (Or?i? and ?egvi?, 2021) ResNet18</cell><cell>92.222</cell><cell>94.843</cell><cell>84.138</cell><cell cols="2">89.309 81.234</cell><cell>88.349</cell><cell>90.199</cell><cell>80.034</cell></row><row><cell>ABCNet</cell><cell>ResNet18</cell><cell>92.726</cell><cell>95.239</cell><cell>84.541</cell><cell cols="2">89.680 85.299</cell><cell>89.497</cell><cell>90.681</cell><cell>81.833</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE ?</head><label>?</label><figDesc>KAPPAZ-TEST COMPARING THE PERFORMANCE OF DIFFERENT METHODS ON THE VAIHINGEN DATASET.</figDesc><table><row><cell>Method</cell><cell>Kappa</cell><cell>KV</cell><cell cols="11">ERFNet PSPNet BiSeNetV1 DANet BiSeNetV2 FANet EaNet ShelfNet MAResU-Net SwiftNet ABCNet</cell></row><row><cell>DABNet</cell><cell>0.798</cell><cell>2.808</cell><cell>6.04</cell><cell>19.84</cell><cell>21.73</cell><cell>22.53</cell><cell>26.86</cell><cell>27.89</cell><cell>33.03</cell><cell>34.38</cell><cell>35.68</cell><cell>35.94</cell><cell>39.06</cell></row><row><cell>ERFNet</cell><cell>0.812</cell><cell>2.643</cell><cell>-</cell><cell>13.80</cell><cell>15.70</cell><cell>16.50</cell><cell>20.84</cell><cell>21.86</cell><cell>27.02</cell><cell>28.37</cell><cell>29.67</cell><cell>29.93</cell><cell>33.06</cell></row><row><cell>BiSeNetV1</cell><cell>0.843</cell><cell>2.272</cell><cell>-</cell><cell>-</cell><cell>1.89</cell><cell>2.70</cell><cell>7.04</cell><cell>8.08</cell><cell>13.24</cell><cell>14.60</cell><cell>15.91</cell><cell>16.17</cell><cell>19.32</cell></row><row><cell>PSPNet</cell><cell>0.847</cell><cell>2.218</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.80</cell><cell>5.15</cell><cell>6.19</cell><cell>11.35</cell><cell>12.72</cell><cell>14.03</cell><cell>14.29</cell><cell>17.44</cell></row><row><cell>BiSeNetV2</cell><cell>0.849</cell><cell>2.198</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.35</cell><cell>5.38</cell><cell>10.55</cell><cell>11.91</cell><cell>13.22</cell><cell>13.48</cell><cell>16.63</cell></row><row><cell>DANet</cell><cell>0.858</cell><cell>2.081</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.04</cell><cell>6.21</cell><cell>7.57</cell><cell>8.88</cell><cell>9.14</cell><cell>12.30</cell></row><row><cell>FANet</cell><cell>0.860</cell><cell>2.057</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.17</cell><cell>6.53</cell><cell>7.84</cell><cell>8.10</cell><cell>11.26</cell></row><row><cell>EaNet</cell><cell>0.870</cell><cell>1.918</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.36</cell><cell>2.68</cell><cell>2.94</cell><cell>6.10</cell></row><row><cell>ShelfNet</cell><cell>0.873</cell><cell>1.883</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.31</cell><cell>1.57</cell><cell>4.73</cell></row><row><cell>MAResU-Net</cell><cell>0.875</cell><cell>1.850</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.26</cell><cell>3.42</cell></row><row><cell>SwiftNet</cell><cell>0.876</cell><cell>1.843</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE ?</head><label>?</label><figDesc>QUANTITATIVE COMPARISON RESULTS ON THE VAIHINGEN TEST SET WITH STATE-OF-THE-ART METHODS.We carry out experiments on the ISPRS Potsdam dataset to further evaluate the performance of ABCNet. Numerical comparisons with other lightweight methods are shown inTable ?, whilethe Kappa-z test is illustrated inTable ?. Remarkably, ABCNet achieves 91.095% in overall accuracy and 88.561% in mIoU, and the Kappa-z test strongly illuminates the superiority contrasted with other lightweight networks. The visualization of area 3_13 is displayed in</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Imp. surf. Building Low veg.</cell><cell>Tree</cell><cell>Car</cell><cell cols="4">Mean F1 OA (%) mIoU (%) Speed</cell></row><row><cell>DeepLabV3+ (Chen et al., 2018a)</cell><cell>ResNet101</cell><cell>92.38</cell><cell>95.17</cell><cell>84.29</cell><cell cols="2">89.52 86.47</cell><cell>89.57</cell><cell>90.56</cell><cell>81.47</cell><cell>13.27</cell></row><row><cell>PSPNet (Zhao et al., 2017)</cell><cell>ResNet101</cell><cell>92.79</cell><cell>95.46</cell><cell>84.51</cell><cell cols="2">89.94 88.61</cell><cell>90.26</cell><cell>90.85</cell><cell>82.58</cell><cell>22.03</cell></row><row><cell>DANet (Fu et al., 2019)</cell><cell>ResNet101</cell><cell>91.63</cell><cell>95.02</cell><cell>83.25</cell><cell cols="2">88.87 87.16</cell><cell>89.19</cell><cell>90.44</cell><cell>81.32</cell><cell>21.97</cell></row><row><cell>EaNet (Zheng et al., 2020)</cell><cell>ResNet101</cell><cell>93.40</cell><cell>96.20</cell><cell>85.60</cell><cell cols="2">90.50 88.30</cell><cell>90.80</cell><cell>91.20</cell><cell>-</cell><cell>9.97</cell></row><row><cell>DDCM-Net (Liu et al., 2020)</cell><cell>ResNet50</cell><cell>92.70</cell><cell>95.30</cell><cell>83.30</cell><cell cols="2">89.40 88.30</cell><cell>89.80</cell><cell>90.40</cell><cell>-</cell><cell>37.28</cell></row><row><cell>HUSTW5 (Sun et al., 2019)</cell><cell>ResegNets</cell><cell>93.30</cell><cell>96.10</cell><cell>86.40</cell><cell cols="2">90.80 74.60</cell><cell>88.20</cell><cell>91.60</cell><cell>-</cell><cell>-</cell></row><row><cell>CASIA2 (Liu et al., 2018)</cell><cell>ResNet101</cell><cell>93.20</cell><cell>96.00</cell><cell>84.70</cell><cell cols="2">89.90 86.70</cell><cell>90.10</cell><cell>91.10</cell><cell>-</cell><cell>-</cell></row><row><cell>V-FuseNet# (Audebert et al., 2018)</cell><cell>FuseNet</cell><cell>91.00</cell><cell>94.40</cell><cell>84.50</cell><cell cols="2">89.90 86.30</cell><cell>89.20</cell><cell>90.00</cell><cell>-</cell><cell>-</cell></row><row><cell>DLR_9# (Marmanis et al., 2018)</cell><cell>-</cell><cell>92.40</cell><cell>95.20</cell><cell>83.90</cell><cell cols="2">89.90 81.20</cell><cell>88.50</cell><cell>90.30</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="7">Fig.5 Mapping results for test images of Vaihingen tile-38.</cell><cell></cell><cell></cell></row><row><cell>ABCNet</cell><cell>ResNet18</cell><cell>92.73</cell><cell>95.24</cell><cell>84.54</cell><cell cols="2">89.68 85.30</cell><cell>89.50</cell><cell>90.68</cell><cell>81.83</cell><cell>72.13</cell></row><row><cell cols="4">-means the results are not repoted in the original paper. 7) Results on the ISPRS Potsdam dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3"># means the DSM or NDSM are used in the network.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE ? 88.687 95.921 92.498 91.095 88.561 the</head><label>?</label><figDesc>QUANTITATIVE COMPARISON RESULTS ON THE POTSDAM TEST SET. contextual path. In particular, we design an attention enhancement module to model longrange dependencies from extracted feature maps. Additionally, to address the feature fusion issue and improve the effectiveness, a feature aggregation module is presented to adequately merge the detailed features captured by the spatial path and semantic features generated by the contextual path. Extensive experiments on ISPRS Vaihingen and Potsdam datasets demonstrate the effectiveness and efficiency of the proposed ABCNet.</figDesc><table><row><cell>Method</cell><cell cols="4">Backbone Imp. surf. Building Low veg.</cell><cell>Tree</cell><cell>Car</cell><cell cols="3">Mean F1 OA (%) mIoU (%)</cell></row><row><cell>ERFNet (Romera et al., 2017)</cell><cell>-</cell><cell>88.675</cell><cell>92.991</cell><cell>81.100</cell><cell cols="2">75.843 90.534</cell><cell>85.829</cell><cell>84.492</cell><cell>79.152</cell></row><row><cell>DABNet (Li et al., 2019)</cell><cell>-</cell><cell>89.939</cell><cell>93.188</cell><cell>83.596</cell><cell cols="2">82.257 92.578</cell><cell>88.312</cell><cell>86.664</cell><cell>82.144</cell></row><row><cell>PSPNet (Zhao et al., 2017)</cell><cell>ResNet18</cell><cell>89.116</cell><cell>94.501</cell><cell>84.041</cell><cell cols="2">85.766 76.622</cell><cell>86.009</cell><cell>87.216</cell><cell>77.971</cell></row><row><cell>BiSeNetV1 (Yu et al., 2018)</cell><cell>ResNet18</cell><cell>90.241</cell><cell>94.554</cell><cell>85.527</cell><cell cols="2">86.195 92.684</cell><cell>89.840</cell><cell>88.163</cell><cell>84.537</cell></row><row><cell>BiSeNetV2 (Yu et al., 2020)</cell><cell>-</cell><cell>91.280</cell><cell>94.316</cell><cell>85.048</cell><cell cols="2">85.192 94.112</cell><cell>89.990</cell><cell>88.174</cell><cell>85.167</cell></row><row><cell>EaNet (Zheng et al., 2020)</cell><cell>ResNet18</cell><cell>92.008</cell><cell>95.692</cell><cell>84.308</cell><cell cols="2">85.719 95.112</cell><cell>90.568</cell><cell>88.703</cell><cell>85.763</cell></row><row><cell>MAResU-Net (Li et al., 2020a)</cell><cell>ResNet18</cell><cell>91.414</cell><cell>95.572</cell><cell>85.823</cell><cell cols="2">86.608 93.306</cell><cell>90.545</cell><cell>89.043</cell><cell>85.928</cell></row><row><cell>DANet (Fu et al., 2019)</cell><cell>ResNet18</cell><cell>91.003</cell><cell>95.567</cell><cell>86.089</cell><cell cols="2">87.579 84.301</cell><cell>88.908</cell><cell>89.129</cell><cell>82.546</cell></row><row><cell cols="2">SwiftNet (Or?i? and ?egvi?, 2021) ResNet18</cell><cell>91.834</cell><cell>95.943</cell><cell>85.721</cell><cell cols="2">86.837 94.456</cell><cell>90.958</cell><cell>89.329</cell><cell>86.285</cell></row><row><cell>FANet (Hu et al., 2020)</cell><cell>ResNet18</cell><cell>91.985</cell><cell>96.101</cell><cell>86.045</cell><cell cols="2">87.833 94.533</cell><cell>91.299</cell><cell>89.822</cell><cell>86.722</cell></row><row><cell>ShelfNet (Zhuang et al., 2019)</cell><cell>ResNet18</cell><cell>92.530</cell><cell>95.750</cell><cell>86.595</cell><cell cols="2">87.070 94.585</cell><cell>91.306</cell><cell>89.920</cell><cell>86.770</cell></row><row><cell>ABCNet</cell><cell>ResNet18</cell><cell>93.270</cell><cell>96.798</cell><cell>87.814</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE ?</head><label>?</label><figDesc>KAPPAZ-TEST COMPARING THE PERFORMANCE OF DIFFERENT METHODS ON THE POTSDAM DATASET.</figDesc><table><row><cell>Method</cell><cell>Kappa</cell><cell>KV</cell><cell cols="11">DABNet PSPNet BiSeNetV1 BiSeNetV2 EaNet DANet MAResU-Net SwiftNet FANet ShelfNet ABCNet</cell></row><row><cell>ERFNet</cell><cell>0.837</cell><cell>4.344</cell><cell>9.06</cell><cell>11.25</cell><cell>17.17</cell><cell>17.51</cell><cell>19.64</cell><cell>20.18</cell><cell>21.01</cell><cell>22.27</cell><cell>23.84</cell><cell>24.32</cell><cell>29.66</cell></row><row><cell>DABNet</cell><cell>0.863</cell><cell>3.712</cell><cell>-</cell><cell>2.19</cell><cell>8.14</cell><cell>8.50</cell><cell>10.64</cell><cell>11.19</cell><cell>12.02</cell><cell>13.29</cell><cell>14.88</cell><cell>15.37</cell><cell>20.77</cell></row><row><cell>PSPNet</cell><cell>0.869</cell><cell>3.563</cell><cell>-</cell><cell>-</cell><cell>5.96</cell><cell>6.33</cell><cell>8.46</cell><cell>9.01</cell><cell>9.85</cell><cell>11.12</cell><cell>12.71</cell><cell>13.21</cell><cell>18.62</cell></row><row><cell>BiSeNetV1</cell><cell>0.884</cell><cell>3.187</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.37</cell><cell>2.50</cell><cell>3.06</cell><cell>3.90</cell><cell>5.16</cell><cell>6.76</cell><cell>7.26</cell><cell>12.69</cell></row><row><cell>BiSeNetV2</cell><cell>0.885</cell><cell>3.182</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.13</cell><cell>2.68</cell><cell>3.52</cell><cell>4.78</cell><cell>6.38</cell><cell>6.88</cell><cell>12.30</cell></row><row><cell>EaNet [7]</cell><cell>0.890</cell><cell>3.032</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.56</cell><cell>1.40</cell><cell>2.66</cell><cell>4.26</cell><cell>4.77</cell><cell>10.20</cell></row><row><cell>DANet</cell><cell>0.892</cell><cell>3.006</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.84</cell><cell>2.10</cell><cell>3.70</cell><cell>4.21</cell><cell>9.64</cell></row><row><cell>MAResU-Net</cell><cell>0.894</cell><cell>2.959</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.26</cell><cell>2.86</cell><cell>3.37</cell><cell>8.80</cell></row><row><cell>SwiftNet</cell><cell>0.897</cell><cell>2.870</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.60</cell><cell>2.11</cell><cell>7.54</cell></row><row><cell>FANet</cell><cell>0.901</cell><cell>2.780</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.51</cell><cell>5.94</cell></row><row><cell>ShelfNet</cell><cell>0.902</cell><cell>2.757</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.43</cell></row><row><cell>ABCNet</cell><cell>0.914</cell><cell>2.425</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE ?</head><label>?</label><figDesc>QUANTITATIVE COMPARISON RESULTS ON THE POTSDAM TEST SET WITH STATE-OF-THE-ART METHODS. results are not repoted in the original paper. # means the DSM or NDSM are used in the network.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Imp. surf. Building Low veg.</cell><cell>Tree</cell><cell>Car</cell><cell cols="4">Mean F1 OA (%) mIoU (%) Speed</cell></row><row><cell>DeepLabV3+ (Chen et al., 2018a)</cell><cell>ResNet101</cell><cell>92.95</cell><cell>95.88</cell><cell>87.62</cell><cell cols="2">88.15 96.02</cell><cell>92.12</cell><cell>90.88</cell><cell>84.32</cell><cell>13.27</cell></row><row><cell>PSPNet (Zhao et al., 2017)</cell><cell>ResNet101</cell><cell>93.36</cell><cell>96.97</cell><cell>87.75</cell><cell cols="2">88.50 95.42</cell><cell>94.40</cell><cell>91.08</cell><cell>84.88</cell><cell>22.03</cell></row><row><cell>DDCM-Net (Liu et al., 2020)</cell><cell>ResNet50</cell><cell>92.90</cell><cell>96.90</cell><cell>87.70</cell><cell cols="2">89.40 94.90</cell><cell>92.30</cell><cell>90.80</cell><cell>-</cell><cell>37.28</cell></row><row><cell>CCNet (Huang et al., 2020)</cell><cell>ResNet101</cell><cell>93.58</cell><cell>96.77</cell><cell>86.87</cell><cell cols="2">88.59 96.24</cell><cell>92.41</cell><cell>91.47</cell><cell>85.65</cell><cell>5.56</cell></row><row><cell>AMA_1</cell><cell>-</cell><cell>93.40</cell><cell>96.80</cell><cell>87.70</cell><cell cols="2">88.80 96.00</cell><cell>92.54</cell><cell>91.20</cell><cell>-</cell><cell>-</cell></row><row><cell>SWJ_2</cell><cell>ResNet101</cell><cell>94.40</cell><cell>97.40</cell><cell>87.80</cell><cell cols="2">87.60 94.70</cell><cell>92.38</cell><cell>91.70</cell><cell>-</cell><cell>-</cell></row><row><cell>HUSTW4 (Sun et al., 2019)</cell><cell>ResegNets</cell><cell>93.60</cell><cell>97.60</cell><cell>88.50</cell><cell cols="2">88.80 94.60</cell><cell>92.62</cell><cell>91.60</cell><cell>-</cell><cell>-</cell></row><row><cell>V-FuseNet# (Audebert et al., 2018)</cell><cell>FuseNet</cell><cell>92.70</cell><cell>96.30</cell><cell>87.30</cell><cell cols="2">88.50 95.40</cell><cell>92.04</cell><cell>90.60</cell><cell></cell></row><row><cell>DST_5# (Sherrah, 2016)</cell><cell>FCN</cell><cell>92.50</cell><cell>96.40</cell><cell>86.70</cell><cell cols="2">88.00 94.70</cell><cell>91.66</cell><cell>90.30</cell><cell></cell></row><row><cell>ABCNet</cell><cell>ResNet18</cell><cell>93.27</cell><cell>96.80</cell><cell>87.81</cell><cell cols="2">88.69 95.92</cell><cell>92.50</cell><cell>91.10</cell><cell>88.56</cell><cell>72.13</cell></row><row><cell>-means the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lef?vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A^ 2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-Head Linear Attention Generative Adversarial Network for Thin Cloud Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10898</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness and Sparsity Regularized Tensor Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3446</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 12</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning and adapting robust features for satellite image segmentation on heterogeneous data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fiandrotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="6517" to="6529" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A comparison of spatial feature extraction algorithms for land-use classification with SPOT HRV data. Remote sensing of environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Marceau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Howarth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="137" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Intra-annual reflectance composites from Sentinel-2 and Landsat for national-scale crop and land cover mapping. Remote sensing of environment 220</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hostert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="263" to="270" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CCNet: Criss-Cross Attention for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16236</idno>
		<title level="m">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning. ISPRS journal of photogrammetry and remote sensing 145</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvaggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="60" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11357</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotation-insensitive and context-augmented object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14302</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-Attention-Network for Semantic Segmentation of High-Resolution Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02130</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Classification of Hyperspectral Image Based on Double-Branch Dual-Attention Mechanism Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">582</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 12</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense dilated convolutions&apos; merging network for land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="6309" to="6320" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network. ISPRS journal of photogrammetry and remote sensing 145</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="78" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comparison of resampling methods for remote sensing classification and accuracy assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Phinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page" from="145" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of supervised object-based land-cover image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="277" to="293" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for large-scale remote-sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="645" to="657" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient semantic segmentation with pyramidal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107611</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Big earth observation time series analysis for monitoring Brazilian agriculture. ISPRS journal of photogrammetry and remote sensing 145</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C A</forename><surname>Picoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sanches</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sim?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Esquerdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Begotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Examining the impacts of future land use/land cover changes on climate in Punjab province, Pakistan: implications for environmental sustainability and economic growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Samie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Azeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental Science and Pollution Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="25415" to="25433" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Problems of encoder-decoder frameworks for high-resolution remote sensing image segmentation: Structural stereotype and insufficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="297" to="304" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Land-cover classification with high-resolution remote sensing images using transferable deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">111322</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing of Environment 237</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Red and photographic infrared linear combinations for monitoring vegetation. Remote sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="127" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Land use and land cover change in Inner Mongolia-understanding the effects of China&apos;s re-vegetation programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pflugmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hostert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Scale Sequence Joint Deep Learning (SS-JDL) for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">111593</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing of Environment 237</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Joint Deep Learning for land cover and land use classification. Remote sensing of environment 221</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="173" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Parsing very high resolution urban scene images by learning deep ConvNets with edge-aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A hybrid object-oriented conditional random field classification framework for high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="7023" to="7037" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Shelfnet for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
