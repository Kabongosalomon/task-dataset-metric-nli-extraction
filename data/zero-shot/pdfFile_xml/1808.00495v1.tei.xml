<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Classification of 3D Point Clouds with Multiscale Spherical Neighborhoods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Gall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Classification of 3D Point Clouds with Multiscale Spherical Neighborhoods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a new definition of multiscale neighborhoods in 3D point clouds. This definition, based on spherical neighborhoods and proportional subsampling, allows the computation of features with a consistent geometrical meaning, which is not the case when using k-nearest neighbors. With an appropriate learning strategy, the proposed features can be used in a random forest to classify 3D points. In this semantic classification task, we show that our multiscale features outperform state-of-the-art features using the same experimental conditions. Furthermore, their classification power competes with more elaborate classification approaches including Deep Learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past years, the interest in 3D scanning technologies has constantly grown in the computer vision community. The benefits of combining 3D and semantic information are fundamental for robotic applications or autonomous driving. To assign the right label to every point in a 3D scene, semantic classification algorithms need to understand the geometry of the scene. Among the ways to achieve such an understanding, two paradigms stand out. In the first instance, the point cloud is segmented and then a label is given to each segment <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. The weakness of this first strategy is that it depends on a prior segmentation which does not use semantic information. In the second instance, each point is considered individually and is given a semantic label or class probabilities (see <ref type="figure" target="#fig_0">Figure 1</ref>) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. Without any prior segmentation, classifying 3D points only relies on the appearance of points neighborhoods. Thus, we need a set of expressive features to describe the geometry in a point neighborhood. Demantk? et al. proposed a description based on local covariance <ref type="bibr" target="#b5">[6]</ref>. To complement this local shape description, Weinmann et al. added measures of verticality and height distribution <ref type="bibr" target="#b27">[28]</ref>. We could find more complex descriptors in the literature like Spin Images <ref type="bibr" target="#b10">[11]</ref> or Fast Point Feature Histograms <ref type="bibr" target="#b20">[21]</ref>. However, we chose to use a multiscale approach with simple features, which has been proven to be more expressive <ref type="bibr" target="#b8">[9]</ref>.</p><p>Standard machine learning techniques are used to classify 3D points described by geometric features. According to Weinmann et al.'s extensive work <ref type="bibr" target="#b26">[27]</ref>, Random Forest is the most suitable classifier. In that case, spatial relations between points are ignored. Features can instead be used as unary potentials in Markov Random Fields <ref type="bibr" target="#b13">[14]</ref> to ensure spatial coherence in the classification. These two techniques can also be combined by using class probabilities given by a standard classifier as unary potentials <ref type="bibr" target="#b23">[24]</ref>. In that case, the random fields can be seen as a subsequent semantic segmentation, and the first point-wise classification problem remains. We focus on this first classification, because the better its results are, the better further processing will perform. 3D neural networks have recently been used for 3D Semantic Segmentation. Huang et al. <ref type="bibr" target="#b9">[10]</ref> used a 3D version of fully convolutional neural networks (FCNN) to label point clouds from voxel-wise predictions. An original Multilayer Perceptron (MLP) architecture, named Pointnet, has been presented by Qi et al. <ref type="bibr" target="#b16">[17]</ref>, and is able to extract both global and local features from a 3D point cloud. Its extension, Pointnet++ <ref type="bibr" target="#b17">[18]</ref>, achieves even better results by the aggregation of local features in a hierarchical manner. 3D neural networks can also be combined with graph-based segmentation methods to design more elaborate 3D semantic segmentation algorithms. Tchapmi et al. <ref type="bibr" target="#b24">[25]</ref> used a CRF on 3D FCNN predictions to enforce global consistency and provide fine-grained semantics. On the other hand, Landrieu and Simonovsky <ref type="bibr" target="#b11">[12]</ref> prefer to segment the cloud in a Superpoint Graph first, and then use Pointnet architecture and graph convolutions to classify each Superpoint. Even though hand-crafted features rarely perform at the level of Deep Learning architectures, our multiscale features compete with most of these methods.</p><p>Although we propose a slightly different set of features than Hackel et al. <ref type="bibr" target="#b8">[9]</ref>, the originality of our method lies in the points neighborhood selection. In the case of 3D points classification, the two most commonly used neighborhood definitions are the spherical neighborhood <ref type="bibr" target="#b3">[4]</ref> and the knearest neighbors (KNN) <ref type="bibr" target="#b27">[28]</ref>. For a given point P , the spherical neighborhood comprises the points situated less than a fixed radius from P , and the k-nearest neighbors comprises a fixed number of closest points to P . We can also add a third definition, which is mostly used for airborne lidar data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>, the cylindrical neighborhood which comprises the points situated less than a fixed radius from P , on a 2D projection of the cloud (frequently on the horizontal plane). Whatever definition is chosen, the scale of the neighborhood has to be determined. Using a fixed scale across the scene is inadequate because most scenes contain objects of various sizes. Weinmann et al. explored a way to adapt the scale to each point of the cloud <ref type="bibr" target="#b26">[27]</ref>. However, using a multiscale approach has proven to be more effective, whether it is used with KNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, with spherical/cylindrical neighborhoods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> or with a combination of all neighborhood types <ref type="bibr" target="#b1">[2]</ref>. The major drawback of the multiscale neighborhoods is their computational time, but Hackel et al. <ref type="bibr" target="#b8">[9]</ref> suggested a simple and efficient solution to implement them, based on iterative subsamplings of the cloud. However, their definition of multiscale neighborhoods, using KNN, lacks geometrical meaning. Section 2. describes our definition of multiscale spherical neighborhoods, which keeps the features undistorted while ensuring sufficient density at each scale.</p><p>We chose to evaluate our multiscale spherical neighborhoods definition on a semantic classification basis. The features we use and our learning strategy are described in Section 3. We conduct several experiments detailed in Section 4 on various datasets. First, we validate that our multiscale features outperform state of the art features in the same experimental conditions on two small outdoor datasets. Then we compare our classification results to more elaborate semantic segmentation methods on three bigger datasets. The parameters' influence is eventually highlighted in the last paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multiscale Spherical Neighborhoods</head><p>Our new definition of multiscale neighborhoods is inspired by <ref type="bibr" target="#b8">[9]</ref> with spherical neighborhoods instead of KNN. This section highlights the differences between both definitions. Let C ? R 3 be a point cloud, the spherical neighborhood of point p 0 ? R 3 in C with radius r ? R is defined by: Unlike KNN, this neighborhood corresponds to a fixed part of the space as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. This property is the key to give a more consistent geometrical meaning to the features. But, in that fixed part, the number of points can vary according to the cloud density. As Hackel et al. <ref type="bibr" target="#b8">[9]</ref> explained, radius search should be the correct procedure from a purely conceptual viewpoint but it is impractical if the point density exhibits strong variations. Two phenomena appear in particular:</p><formula xml:id="formula_0">S r (p 0 , C) = p ? C p ? p 0 r (1)</formula><p>? Having too many points when the neighborhood scale is too big or the density too high</p><p>? Having too few points when the neighborhood scale is too small or the density too low</p><p>The first phenomenon has computational consequences as getting a large number neighbors in a larger set of points takes a lot of time. The computational cost of multiscale features does not come from the fact that we have to compute the features S times, where S is the number of scales. The real limiting factor is the number of points contained in the biggest scales. Furthermore, all those points are not required to compute relevant features. Our features capture a global shape in the neighborhood and do not need fine details. The solution proposed by Hackel et al. <ref type="bibr" target="#b8">[9]</ref> to subsample the cloud proportionally to the scale of the neighborhood can be adapted to the spherical definition. This solution better suits the spherical neighborhoods than the KNN. With a uniform density, the number of points in the neighborhood becomes a feature itself, describing the neighborhood occupancy rate. We chose to subsample the cloud with a grid, by keeping the barycenter of the points comprised in each cell. Let l ? R be the size of the grid cells, for any radius of a neighborhood r ? R, we can control the maximum number of points in our neighborhood with the parameter ? = r l . If ? is too low, we will not have enough points and the features will not be discriminant, but the higher its value is, the longer computations are.</p><p>The impact of the second phenomenon, caused by low densities, should be limited by the use of multiple scales. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, if the density is too low, the points will remain the same after subsampling between two consecutive scales (neighborhood C). With spherical neighborhoods, the small scale might not contain enough points for a good description, but the large scale will deliver the information. In the same case, the KNN behave differently, giving exactly the same information at both scales. Regardless of the neighborhoods, there is no information to get from the data at the smaller scale. However, the KNN give a false description of the smaller scale without any measure of its reliability, whereas spherical neighbors give the number of points, which is an indication of the robustness of the description.</p><p>The scales are defined by three parameters: the radius of the smallest neighborhood r 0 , the number of scales S, and the ratio between the radius of consecutive neighborhoods ?. We can then define the neighborhood at each scale s ? {0, ..., S ? 1} around point p 0 ? R 3 as:</p><formula xml:id="formula_1">N s (p 0 ) = S rs (p 0 , C s )<label>(2)</label></formula><p>with r s = r 0 * ? s being the radius at scale s and C s being the cloud subsampled with a grid size of r s /?. Despite its similarity with the definition proposed by Hackel et al. <ref type="bibr" target="#b8">[9]</ref>, our multiscale neighborhood definition stands out with its geometrical meaning. With spherical neighborhoods instead of KNN, the features always describe a part of the space of the same size at each scale. Moreover, the number of points in the neighborhood is now a feature itself adding even more value to this definition. More than a theoretical good behaviour, this leads to better feature performances, as shown in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Point-wise Semantic Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geometric and Color Features</head><p>For benchmarking purposes, we divide our features in two sets described in <ref type="table">Table 1</ref>. The first set does not use any additional information like intensity, color, or multispectral <ref type="table">Table 1</ref>. Features used for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Definitions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sum of eigenvalues ?i</head><p>Omnivariance ?i <ref type="bibr">1 3</ref> Eigenentropy ? ?i ln(?i)</p><formula xml:id="formula_2">Linearity (?1 ? ?2)/?1</formula><p>Planarity</p><formula xml:id="formula_3">(?2 ? ?3)/?1 Sphericity ?3/?1 Change of curvature ?3/(?1 + ?2 + ?3) Verticality (x2) ? 2 ? angle(ei, ez) i?(0,2) Absolute moment (x6) 1 |N | p ? p0, ei k i?(0,1,2) Vertical moment (x2) 1 |N | p ? p0, ez k Number of points |N | Average color (x3) 1 |N | c Color variance (x3) 1 |N | ? 1 (c ?c) 2</formula><p>measure, to keep previous work conditions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> in our first experiment (Section 4.1). In the other experiments, additional color features are used when available. We use covariance based features that simply derive from the eigenvalues ? 1 ? 2 ? 3 ? R and corresponding eigenvectors e 1 , e 2 , e 3 ? R 3 of the neighborhood covariance matrix defined by:</p><formula xml:id="formula_4">cov(N ) = 1 |N | p?N (p ?p)(p ?p) T<label>(3)</label></formula><p>Wherep is the centroid of the neighborhood N . From the eigenvalues, we can compute several features: sum of eigenvalues, omnivariance, eigenentropy, linearity, planarity, sphericity, anisotropy, and change of curvature. However, among those commonly used features, we eliminate anisotropy defined by (? 1 ? ? 3 )/? 1 as it is strictly equivalent to sphericity. We can notice that, thanks to the nature of our neighborhoods, we do not need to normalize the eigenvalues as in previous works. Their values do not vary with the original point cloud density which means the features that do not involve ratios, e. g. sum of eigenvalues, omnivariance, and eigenentropy, make more sense. Our feature set is completed by verticality that we redefined as | ? 2 ? angle(e i , e z )|. Unlike Hackel et al. <ref type="bibr" target="#b8">[9]</ref>, we keep the verticality for the first and the last eigenvectors. The first one encodes the verticality of linear objects, and the last one the verticality of the normal vector of planar objects. We also use first and second order moments around all three eigenvectors, but in absolute value as the eigenvectors have random orientations. Following our assumption that vertical direction plays an important role, additional vertical moments are computed around the vertical vector e z in relative value as the upward direction is always the same. Eventually, as explained in Section 2, the number of points in a neighborhood completes our first set of features, which contains 18 values at each scale. In Section 4.2, we use colors as previous works did, because some objects like closed doors or windows are indistinguishable in 3D. We chose simple features, the mean and the variance of each color channel, bringing the total number of features per scale to 24.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Strategy</head><p>There are two setbacks when classifying a point cloud. First, its size is generally huge, and then, the classes are heavily unbalanced. To fix those problems, one can take a subset of the training data, small enough to allow reasonable training times, and balance the classes in that subset. The scope of the results also depends on the test set. With small datasets, the rest of the points are used as the test set, even if they represent the same scene. The results from such experiments would be questionable as a measure of the classification performances, however, they still may be used to compare the descriptive power of different features. The recent appearance of bigger point cloud datasets allowed the separation of the training set and the test set. With such point clouds, it is possible to get a relevant measure of how well the classification generalizes to unseen data.</p><p>In Section 4.1, we compare our multiscale features to state of the art features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> in the same experimental conditions. The same number of points is randomly picked in each class to train a classifier, and this classifier is tested on the rest of the cloud. We go further than previous works by computing our results several times with different training sets. We cannot ensure that the comparison is valid without checking the distribution of results on a large number of trials. The quality of our multiscale features can be assessed more reliably in these conditions.</p><p>On bigger datasets, we use a different learning strategy. We iteratively add points to the training set with a trial and error procedure. A classifier is trained on a set of points T from the training clouds U, then the classifier is tested on U, and we randomly add some of the misclassified points to T . After some iterations, the classifier is used on the test clouds. The experiments in Section 4.2 use this learning strategy, which only consists in a smart choice of the training points. We can't use this strategy on small datasets, because the test scene is the same as the training scene and our classification would show overfitted results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">State of the art features comparison</head><p>The goal of our first experiment is to assess the performances of our multiscale features against other state-of-the- art features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, thus, we keep the same experimental conditions as Weinmann et al. <ref type="bibr" target="#b26">[27]</ref> and Hackel et al. <ref type="bibr" target="#b8">[9]</ref>. We use the Paris-Rue-Madame dataset <ref type="bibr" target="#b22">[23]</ref>, a 160-meter street scan containing 20 million points and the Paris-Rue-Cassette dataset <ref type="bibr" target="#b25">[26]</ref>, a 200-meter street scan containing 12 million points. To focus the comparison on the features, we use a random forest classifier trained on 1000 random points per class for each dataset as previous works did. We chose the parameters S = 8, r 0 = 0.1m, ? = 2 and ? = 5. The first three parameters were chosen so that the scales of our neighborhoods cover a range from the smallest object size to the order of magnitude of a facade and the last parameter ? was chosen empirically (see Section 4.3). With an average personal computer setup (32 GB RAM, Intel Core i7-3770; 3.4 GHz), our feature extraction took about 319 seconds on Rue Cassette, which is the same order of magnitude as the method in <ref type="bibr" target="#b8">[9]</ref> (191s) and way faster than the method in <ref type="bibr" target="#b26">[27]</ref> (23000s). For consistency, we chose to use the classes "Intersection over Union" metric in all our experiments. Thus, we convert the class F 1 scores given in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> with the equation :</p><formula xml:id="formula_5">F 1 = 2TP 2TP + FP + FN (4) IoU = TP TP + FP + FN = F 1 2 ? F 1 (5)</formula><p>where TP, FP, and FN respectively denote true positives, false positives, and false negatives for each class. As stated in Section 3.2, we reproduce our results 500 times to ensure the validity of the comparison despite the random factor in the choice of the training set. In <ref type="table" target="#tab_0">Table 2</ref>, we report the average class IoU to compare with previous results and the standard deviations to prove the consistency of the classification. The performances of our multiscale features exceed previous results by 24 mean IoU points on Rue Madame and 11 mean IoU points on Rue Cassette. We can also note that our results do not vary much, the standard deviations being limited to a few percents even for the hardest classes with fewer points.</p><p>As a conclusion, the low standard deviation validates the random selection of the training set and legitimates the comparison of the different sets of features. Our multiscale features thus proved to be superior to state of the art features. The difference between Hackel et al.'s multiscale features <ref type="bibr" target="#b8">[9]</ref> and ours may seem like an implementation detail, with radius neighborhoods instead of KNN. However, the results prove that the type of local neighborhood definition has a great impact on the robustness of the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on large scale data</head><p>Our second experiment shows how our classification method generalizes to unseen data. As shown in <ref type="table" target="#tab_1">Table 4</ref>, we chose three large scale datasets from different environments and acquisition methods. With these datasets, we use the smart choice of training points described in Section 3.2 and a random forest classifier. This simple classification algorithm is designed to focus on the point-wise descriptive power of our features, we called it RF_MSSF for "Random Forest with Multi-Scale Spherical Features." Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) <ref type="bibr" target="#b0">[1]</ref> was acquired by 3D cameras and covers six large-scale indoor areas from three different buildings for a total of 6000 m 2 and 273 million points. To keep the same experimental conditions as Tchapmi et al. <ref type="bibr" target="#b24">[25]</ref>, we use the fifth area as the test set and train on the rest of the data. Original annotation comprises 12 semantic elements which pertain to the categories of structural building elements (ceiling, floor, wall, beam, column, window, and door) and commonly found furniture (table, chair, sofa, bookcase, and board). A clutter class exists as well for all other elements. This last class has no semantic meaning like the "unclassified" points in the other datasets and will not be considered during training and testing. As the object scales in this dataset are smaller than the object scales in a street, we adapt the parameters to S = 8, r 0 = 0.05m, ? = 2, and ? = 5. The classifier is trained on 50000 sample points chosen with the procedure described in Section 3.2. <ref type="table" target="#tab_2">Table 3</ref> shows that our classification method outperforms the <ref type="bibr" target="#b0">1</ref> As Pointnet was evaluated in a k-fold strategy in the original paper, we obtained the results on this particular split from the authors.  deep learning architectures of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> but is unable to compete with the cutting edge algorithm of Landrieu and Simonovsky <ref type="bibr" target="#b11">[12]</ref>.</p><p>Semantic3D <ref type="bibr" target="#b7">[8]</ref> is an online benchmark comprising several fixed lidar scans of different outdoor places. This is currently the dataset with the highest number of points (more than 4 billion), and the greatest covered area (around 110000 m 2 ). We kept the parameters used in previous outdoor experiment: S = 8, r 0 = 0.1m, ? = 2, and ? = 5. Table 5 provides our results on the reduced-8 challenge. Our classification method ranked second at the time of the submission. Once again, it beats several deep learning architectures and is only outperformed by the same algorithm <ref type="bibr" target="#b11">[12]</ref>. We can notice that our results exceed Hackel et al. results <ref type="bibr" target="#b8">[9]</ref> by a large margin, consolidating the conclusion in Section 4.1.</p><p>Paris-Lille-3D <ref type="bibr" target="#b19">[20]</ref> is a recent dataset that was acquired with a Mobile Laser Scanning system in two cities in France: Lille and Paris. Overall, the scans contain more than 140 million points on 2 km of streets, covering a 55000 m 2 area, which is much bigger than other mobile mapping datasets like Rue Madame and Rue Cassette. This dataset, fully annotated by hand, comprises 50 classes unequally distributed in three scenes Lille1, Lille2, and Paris. Following the authors' guideline, we designed 10 coarser classes defining meaningful groups: Unclassified, Ground, Building, Signage, Bollard, Trash cans, Barriers, Pedestrians, Cars, and Vegetation. We provide an "XML" file in supplementary materials, which maps original classes to our coarse classes. Among our ten classes, the first one Unclassified will be ignored during training and test. We choose to train our classifier on the two scenes Lille1 and Lille2 and to use Paris as the test fold. This dataset does not include colors, so we only use our first set of features and choose the parameters used in the other outdoor environments: S = 8, r 0 = 0.1m, ? = 2, and ? = 5. Our results are shown in <ref type="table">Table 6</ref>. Although this dataset is recent and does not have any other baseline result for now, we find it very interesting  because of its cross-city split. We see that our classifier can transfer knowledge from one city to another and is particularly efficient on buildings. This is remarkable given that Lille and Paris architectural styles are very different. <ref type="figure" target="#fig_2">Figures 3, 4, and 5</ref> show some examples of classified scenes. First, we can notice that the classification has no object coherence as some unstructured patches appear, for example on the columns in <ref type="figure" target="#fig_2">Figure 3</ref> or on the facades in <ref type="figure">Figure 5</ref>. This highlights the particularity of our method to focus on points independently, not using any segmentation scheme. Another very interesting pattern appears on the second scene in <ref type="figure">Figure 5</ref>: when a car is close to a tree, it is misclassified and we can actually see the influence area of the tree on the car. We can assume that the classifier relies more on the large scales to distinguish those two particular classes.</p><p>Overall, our classification algorithm ranks among the best approaches, beating nearly every other elaborate method apart from Superpoint Graphs <ref type="bibr" target="#b11">[12]</ref> on these datasets. However, this has to be considered in light of the fact that we do not use any segmentation or regularization process and only focus on the descriptive power of our fea- <ref type="figure">Figure 6</ref>. Influence of the parameter ? on classification performances and computation speed on Paris-Lille-3D dataset tures. We proved that our features beat state-of-the-art features in terms of classification performances, and that they could, alone, compete with complex classification schemes, including deep learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Density parameter influence</head><p>We eventually evaluate the influence of the parameter ? in our classification method. As a reminder, this parameter controls the number of subsampled points that a neighborhood can contain. A high value means better features but slower computations. In this experiment, we chose to use Paris-Lille-3D for two reasons. First, we want to focus on the 3D descriptors and, thus, do not need color information. Then, the results generalize well because they are cross-city, tested on Paris after being trained on Lille. With the parameters previously used on this dataset, we compute average IoU scores across all classes for different values of ?. <ref type="figure">Figure 6</ref> shows the evolution of the results along with the features computation speed for every split of the dataset. We can note that average IoU scores rise quickly up to ? = 3 and do not increase a lot for higher values of ?. Depending on the application, one can choose to optimize the results or the processing speed with this parameter. Although our performances could be slightly increased with a higher ? value, we chose to keep ? = 5 in our work because it is a trade-off between performance and computation speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a 3D point cloud semantic classification approach articulated around new multiscale features. The use of spherical neighborhoods instead of KNN increases the discriminating power of our features, leading to better performances than state-of-the-art features in the same experimental conditions. We also showed that the performances of our algorithm are consistent on three datasets acquired with different technologies in different environ-ments. Eventually, we proved that our approach outperforms recent and complex classification schemes, including deep learning methods, on large scale datasets. Deep learning is becoming the standard for several classification tasks, but there is room for improvements with handcrafted methods. Furthermore, the ideas that come up from such methods, like our new multiscale neighborhood definition, could benefit other frameworks including deep learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our 3D semantic classification framework: a set of features is computed for every point with our new multiscale spherical neighborhood definition. These features are then used to classify each point independently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Behavior of multiscale neighborhoods defined with KNN or with spherical neighborhoods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Examples of classified scenes in S3DIS dataset (left) with groundtruth (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Examples of classified scenes in Semantic3D.net dataset (left) with groundtruth (right) Examples of classified scenes in Paris-Lille-3D dataset (left) with groundtruth (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Average IoU (with standard deviation) on Rue Madame (top) and Rue Cassette (bottom) datasets. Results for<ref type="bibr" target="#b8">[9]</ref> and<ref type="bibr" target="#b26">[27]</ref> are converted from corresponding articles.</figDesc><table><row><cell>Class</cell><cell>Ours</cell><cell>[9]</cell><cell>[27]</cell></row><row><cell>Facade</cell><cell>98.22% (?0.11)</cell><cell>97.06%</cell><cell>91.81%</cell></row><row><cell>Ground</cell><cell>96.62% (?0.18)</cell><cell>96.29%</cell><cell>84.88%</cell></row><row><cell>Cars</cell><cell>95.37% (?0.43)</cell><cell>89.09%</cell><cell>55.48%</cell></row><row><cell>Motorcycles</cell><cell>61.55% (?2.22)</cell><cell>47.44%</cell><cell>9.44%</cell></row><row><cell>Traffic Signs</cell><cell>67.43% (?8.67)</cell><cell>33.96%</cell><cell>4.90%</cell></row><row><cell>Pedestrians</cell><cell>77.86% (?3.32)</cell><cell>24.13%</cell><cell>1.63%</cell></row><row><cell>Mean</cell><cell>82.84%</cell><cell>58.89%</cell><cell>31.68%</cell></row><row><cell>Facade</cell><cell>97.27% (?0.20)</cell><cell>93.89%</cell><cell>86.65%</cell></row><row><cell>Ground</cell><cell>97.77% (?0.20)</cell><cell>96.99%</cell><cell>95.75%</cell></row><row><cell>Cars</cell><cell>84.94% (?1.54)</cell><cell>80.88%</cell><cell>47.31%</cell></row><row><cell>Motorcycles</cell><cell>58.99% (?3.27)</cell><cell>51.33%</cell><cell>17.12%</cell></row><row><cell>Traffic Signs</cell><cell>12.71% (?3.81)</cell><cell>18.58%</cell><cell>14.29%</cell></row><row><cell>Pedestrians</cell><cell>35.31% (?3.80)</cell><cell>24.69%</cell><cell>9.06%</cell></row><row><cell>Vegetation</cell><cell>71.48% (?1.94)</cell><cell>51.40%</cell><cell>24.63%</cell></row><row><cell>Mean</cell><cell>65.50%</cell><cell>54.08%</cell><cell>35.30%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Dataset characteristics</cell><cell></cell></row><row><cell>Name</cell><cell>S3DIS</cell><cell cols="2">Semantic3D Paris-Lille-3D</cell></row><row><cell>Environment</cell><cell>Indoor</cell><cell>Outdoor</cell><cell>Outdoor</cell></row><row><cell>Acquisition</cell><cell>Cameras</cell><cell>Fixed lidar</cell><cell>Mobile lidar</cell></row><row><cell>Colors</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Points</cell><cell>273M</cell><cell>4009M</cell><cell>140M</cell></row><row><cell cols="2">Covered area 6000 m 2</cell><cell>110000 m 2</cell><cell>55000 m 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>IoU per class on S3DIS dataset</figDesc><table><row><cell>Method</cell><cell>ceiling</cell><cell>floor</cell><cell>wall</cell><cell cols="3">beam column window</cell><cell>door</cell><cell>chair</cell><cell>table</cell><cell>bookcase</cell><cell>sofa</cell><cell>board</cell><cell>mean</cell></row><row><cell>PointNet [17] 1</cell><cell>88.8</cell><cell>97.3</cell><cell>69.8</cell><cell>0.1</cell><cell>3.9</cell><cell>46.3</cell><cell>10.8</cell><cell>52.6</cell><cell>58.9</cell><cell>40.3</cell><cell>5.9</cell><cell>26.4</cell><cell>41.7</cell></row><row><cell>SEGCloud [25]</cell><cell>90.1</cell><cell>96.1</cell><cell>69.9</cell><cell>0.0</cell><cell>18.4</cell><cell>38.4</cell><cell>23.1</cell><cell>75.9</cell><cell>70.4</cell><cell>58.4</cell><cell>40.9</cell><cell>13.0</cell><cell>49.5</cell></row><row><cell>SPGraph [12]</cell><cell>89.4</cell><cell>96.9</cell><cell>78.1</cell><cell>0.0</cell><cell>42.8</cell><cell>48.9</cell><cell cols="3">61.6 84.7 75.4</cell><cell>69.8</cell><cell>52.6</cell><cell>2.1</cell><cell>58.5</cell></row><row><cell>RF_MSSF (ours)</cell><cell>95.9</cell><cell>96.4</cell><cell>67.6</cell><cell>0.0</cell><cell>11.9</cell><cell>48.3</cell><cell>28.8</cell><cell>64.4</cell><cell>68.9</cell><cell>58.6</cell><cell>33.9</cell><cell>22.3</cell><cell>49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>IoU per class on Semantic3D dataset (Fold 5)</figDesc><table><row><cell>Method</cell><cell cols="3">man-made natural terrain terrain</cell><cell cols="2">high vegetation vegetation low</cell><cell>buildings</cell><cell>hard scape</cell><cell cols="2">scanning artefacts</cell><cell>cars</cell><cell>mean</cell></row><row><cell cols="2">TMLC-MSR [9]</cell><cell>89.8</cell><cell>74.5</cell><cell>53.7</cell><cell>26.8</cell><cell>88.8</cell><cell>18.9</cell><cell></cell><cell>36.4</cell><cell>44.7</cell><cell>54.2</cell></row><row><cell cols="2">DeePr3SS [13]</cell><cell>85.6</cell><cell>83.2</cell><cell>74.2</cell><cell>32.4</cell><cell>89.7</cell><cell>18.5</cell><cell></cell><cell>25.1</cell><cell>59.2</cell><cell>58.5</cell></row><row><cell>SnapNet [3]</cell><cell></cell><cell>82.0</cell><cell>77.3</cell><cell>79.7</cell><cell>22.9</cell><cell>91.1</cell><cell>18.4</cell><cell></cell><cell>37.3</cell><cell>64.4</cell><cell>59.1</cell></row><row><cell cols="2">SEGCloud [25]</cell><cell>83.9</cell><cell>66.0</cell><cell>86.0</cell><cell>40.5</cell><cell>91.1</cell><cell>30.9</cell><cell></cell><cell>27.5</cell><cell>64.3</cell><cell>61.3</cell></row><row><cell>SPGraph [12]</cell><cell></cell><cell>97.4</cell><cell>92.6</cell><cell>87.9</cell><cell>44.0</cell><cell>93.2</cell><cell>31.0</cell><cell></cell><cell>63.5</cell><cell>76.2</cell><cell>73.2</cell></row><row><cell cols="2">RF_MSSF (ours)</cell><cell>87.6</cell><cell>80.3</cell><cell>81.8</cell><cell>36.4</cell><cell>92.2</cell><cell>24.1</cell><cell></cell><cell>42.6</cell><cell>56.6</cell><cell>62.7</cell></row><row><cell></cell><cell></cell><cell cols="6">Table 6. IoU per class on Paris-Lille-3D dataset (Fold: Paris)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="8">ground building signage bollard trash cans barriers pedestrians</cell><cell>cars</cell><cell cols="2">vegetation</cell><cell>mean</cell></row><row><cell>RF_MSSF (ours)</cell><cell>99.1</cell><cell>90.5</cell><cell>66.4</cell><cell>62.6</cell><cell>5.8</cell><cell>52.1</cell><cell>5.7</cell><cell></cell><cell>86.2</cell><cell>84.7</cell><cell>61.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of largescale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification of airborne laser scanning data using geometric multi-scale features and different neighbourhood types. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blomley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d terrestrial lidar data classification of complex natural scenes using a multi-scale dimensionality criterion: Applications in geomorphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Airborne lidar feature selection for urban classification using random forests. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note>Part 3</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality based scale selection in 3d lidar point clouds. The International Archives of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demantke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Part 5</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape-based recognition of 3d point clouds in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2154" to="2161" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03847</idno>
		<title level="m">Semantic3d. net: A new Large-scale Point Cloud Classification Benchmark</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
			<pubPlace>Prague, Czech Republic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Pattern Recognition (ICPR)</title>
		<meeting>of the International Conf. on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09869</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Onboard contextual classification of 3-d point clouds with learned high-order markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>ICRA&apos;09</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Contextual classification of lidar data and building object detection in urban areas. ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Soergel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-scale feature extraction on point-sampled surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="281" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and Robust Segmentation and Classification for Change Detection in Urban Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS 2016-XXIII ISPRS Congress</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Paris-lille-3d: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00032</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation, 2009. ICRA&apos;09. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detection, segmentation and classification of 3d urban objects using mathematical morphology and supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Paris-rue-Madame database: a 3d mobile laser scanner dataset for benchmarking urban detection, segmentation and classification methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Pattern Recognition, Applications and Methods ICPRAM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonassociative markov networks for 3d point cloud classification. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Velizhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XXXVIII, Part 3A. Citeseer</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Terramobilita/iqmulus urban point cloud analysis benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Br?dif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paparoditis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="126" to="133" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="286" to="304" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature relevance assessment for the semantic interpretation of 3d point cloud data. ISPRS Annals of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
