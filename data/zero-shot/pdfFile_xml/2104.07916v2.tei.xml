<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting Deep Classifiers with Polynomial Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
							<email>grigorios.chrysos@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markos</forename><surname>Georgopoulos</surname></persName>
							<email>m.georgopoulos@imperial.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Augmenting Deep Classifiers with Polynomial Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* First two authors have contributed equally.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Polynomial neural networks</term>
					<term>tensor decompositions</term>
					<term>poly- nomial expansions</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have been the driving force behind the success in classification tasks, e.g., object and audio recognition. Impressive results and generalization have been achieved by a variety of recently proposed architectures, the majority of which are seemingly disconnected. In this work, we cast the study of deep classifiers under a unifying framework. In particular, we express state-of-the-art architectures (e.g., residual and non-local networks) in the form of different degree polynomials of the input. Our framework provides insights on the inductive biases of each model and enables natural extensions building upon their polynomial nature. The efficacy of the proposed models is evaluated on standard image and audio classification benchmarks. The expressivity of the proposed models is highlighted both in terms of increased model performance as well as model compression. Lastly, the extensions allowed by this taxonomy showcase benefits in the presence of limited data and long-tailed data distributions. We expect this taxonomy to provide links between existing domainspecific architectures. The source code is available at https://github. com/grigorisg9gr/polynomials-for-augmenting-NNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The unprecedented performance of AlexNet <ref type="bibr" target="#b36">[37]</ref> in ImageNet classification <ref type="bibr" target="#b51">[52]</ref> led to the resurgence of research in the field of neural networks. Since then, an extensive corpus of papers has been devoted to improving classification performance by modifying the architecture of the neural network. However, only a handful (of seemingly disconnected) architectures, such as ResNet <ref type="bibr" target="#b23">[24]</ref> or nonlocal neural networks <ref type="bibr" target="#b61">[62]</ref>, have demonstrated impressive generalization across different tasks (e.g., <ref type="bibr" target="#b71">[72]</ref>), domains (e.g., <ref type="bibr" target="#b64">[65]</ref>) and modalities (e.g., <ref type="bibr" target="#b32">[33]</ref>). This arXiv:2104.07916v2 [cs.CV] 11 Aug 2022 <ref type="figure">Fig. 1</ref>: Parameter interactions for different degrees of polynomials. For visualization purpose we assume a second-degree expansion, where we have folded the learnable parameters into a tensorW and the input is vectorized. The equation on the top is the second-degree polynomial in a tensor-format, while thez is a padded version of z. Notice that first-degree polynomials <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55]</ref> have zeros in a large part of the tensorW, while similarly second-degree polynomials <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> have the matrices connected to first-order interactions zero. On the contrary, ??nets along with the proposed PDC capture both interactions. Importantly, we illustrate how PDC learns a more expressive model without the enforced sharing of the ??net. The notation of mode-m product along with the derivation is conducted in sec. B (supplementary). phenomenon can be attributed to the challenging nature of devising a network and the lack of understanding regarding the assumptions that come with its design, i.e., its inductive bias.</p><p>Demystifying the success of deep neural architectures is of paramount importance. In particular, significant effort has been devoted to the study of neural architectures, e.g., depth versus width of the neural network <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b48">49]</ref> and the effect of residual connections on the training of the network <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. In this work, we offer a principled approach to study state-of-the-art classifiers as polynomial expansions. We show that polynomials have been a recurring theme in numerous classifiers and interpret their design choices under a unifying framework.</p><p>The proposed framework provides a taxonomy for a collection of deep classifiers, e.g., a non-local neural network is a third-degree polynomial and ResNet is a first-degree polynomial. Thus, we provide an intuitive way to study and extend existing networks as visualized in <ref type="figure">Fig. 1</ref>, as well as interpret their gap in performance. Lastly, we design extensions on existing methods and show that we can improve their classification accuracy or achieve parameter reduction. Concretely, our contributions are the following: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Fundamentals on polynomial expansions</head><p>In this section, we provide the notation and an intuitive explanation on how a polynomial expansion emerges on various types of variables. Notation: Below, we symbolize matrices (vectors) with bold capital (lower) letters, e.g. X(x). A variable that can be either a matrix or a vector is denoted by?. Tensors are considered as the multidimensional equivalent of matrices. Tensors are symbolized by calligraphic boldface letters e.g., X . The symbol ? ? 1 denotes a vector of ones and I is a third-order super-diagonal unit tensor. Due to constrained space, the detailed notation is on sec. A, while in <ref type="table" target="#tab_0">Table 1</ref> the core symbols are summarized. Polynomial expansions: Below, polynomials express a relationship between an input variable (e.g., a scalar z) and (learnable) coefficients; this relationship only involves the two core operations of addition and multiplication. When the input variable is in vector form, e.g., z ? R ? with ? ? N, then the polynomial captures the relationships between the different elements of the input vector. The input variable can also be a higher-dimensional structure, e.g., a matrix. This is frequently the case in computer vision, where one dimension can express spatial dimensions, while the other can express the features (channels). The polynomial can either capture the interactions across every element of the matrix with every other element, or it can have higher-order interactions between specific elements, e.g., the interactions of a row with each column. Relationship between polynomial expansions and tensors: Multivariate polynomial expansions are intertwined with tensors. Specifically, the polynomial Blocks (up to third-degree) from different architectures. The layers (i.e., blue boxes) denote any linear operation, e.g., a convolution or a fully-connected layer, depending on the architecture. From left to right, the degree of the polynomial is increasing. Our framework enables also to complete the missing terms of the polynomial (i.e., PDC-NL versus NL).</p><p>expansions of interest involve vector, matrix or tensor form as the input. Let us express the output y ? R as a N th degree polynomial expansion of a ddimensional input z ? R d :</p><formula xml:id="formula_0">y = ? + d i=1 w [1] i z i + d i=1 d j=1 w [2] i,j z i z j + . . . + d i=1 ? ? ? d j=1 N sums w [N ] i,??? ,k z i ? ? ? z k ,<label>(1)</label></formula><p>where ? ? R is the constant term, and the w terms are the scaling parameters for every degree. Notice that w <ref type="bibr">[n]</ref> for n &gt; 2 depends on n indices, thus it can be expressed as a tensor. Collecting those tensors,</p><formula xml:id="formula_1">{W [n] ? R d ? ? ? ? ? d n times } N n=1</formula><p>represent learnable parameters in <ref type="bibr" target="#b0">(1)</ref>. Due to the constrained space this relationship is further quantified in sec. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Polynomials and deep classifiers</head><p>The proposed framework unifies recent deep classifiers through the lens of polynomials. We formalize the framework of polynomials below. Formally, let functions ? <ref type="bibr">[d]</ref> i (?) define a linear (or multi-linear) function over?. The input variabl? Z can either be a vector or a matrix, while d declares the degree and i the index of the function. Then, a polynomial of degree?N is expressed as:</p><formula xml:id="formula_2">Y =? + ? [1] 1 (?) + ? [2] 1 (?)? [2] 2 (?) + . . . + ? [N ] 1 (?) . . . ? [N ] N ?1 ? [N ] N (?) N terms (2)</formula><p>where? is the constant term. Evidently, each additional degree introduces new parameters that can grow up exponentially with respect to the degree. However, we posit that this introduces a needed inductive bias on the method. In addition, if reduced parameters are required, this can be achieved by using low-rank assumptions or by sharing parameters across different degrees.</p><p>Using the formulation in (2), we exhibit how well-established methods can be formulated as polynomials. We present a taxonomy of classifiers based on the degree of the polynomial. In particular, we present first, second, third and higher-degree polynomial expansions. For modeling purposes, we focus on the core block of each architecture, while ignoring any activation functions. First-degree polynomials include the majority of the feed-forward networks, such as AlexNet <ref type="bibr" target="#b36">[37]</ref>, VGG <ref type="bibr" target="#b54">[55]</ref>. Specifically, the networks that include stacked linear operations (fully-connected or convolutional layers) but do not include any matrix or elementwise multiplication of the representations fit in this category. Such networks can be expressed in the form? = C? +?, where the weight matrix C is a learnable parameter. A special case is ResNet <ref type="bibr" target="#b23">[24]</ref>. The idea is to introduce shortcut connections that enable residual learning. Notation-wise this is a re-parametrization of the weight matrix C as? r = (I + C)? +? where I is an identity matrix. Thus, ?</p><formula xml:id="formula_3">[1] 1 (?) = (I + C)?.</formula><p>Second-degree polynomials model self-interactions, i.e., they can selectively maximize the related inputs through second-order interactions. Often the interactions emerge in a particular dimension, e.g., correlations of the channels only, based on the particular application.</p><p>One special case of the second-degree polynomial is the Squeeze-and-excitation networks (SENet) <ref type="bibr" target="#b26">[27]</ref>. The motivation lies in improving the channel-wise interactions, since there is already a strong inductive bias for the spatial interactions through convolutional layers. Notation-wise, the squeeze-and-excitation block is expressed as:</p><formula xml:id="formula_4">Y s = (ZC 1 ) * r(p(ZC 1 )C 2 ),<label>(3)</label></formula><p>where * denotes the Hadamard product, p is the global pooling function, r is a function that replicates the channels in the spatial dimensions and C 1 , C 2 are weight matrices with learnable parameters. For simplicity, we assume the input is a matrix instead of a tensor, i.e., Z ? R hw?c with h the height, w the width and c the channels of the image. Then, Y s expresses a second-degree term with ?</p><formula xml:id="formula_5">[2] 1 (Z) = ZC 1 and ? [2] 2 (Z) = 1 hw I ? 3 (C T 2 C T 1 Z T ? ? 1 )</formula><p>where I is a third-order super-diagonal unit tensor and ? ? 1 is a vector of ones. The Squeeze-and-excitation block has been extended in the literature. The selective kernels networks <ref type="bibr" target="#b37">[38]</ref> introduce a variant, where Z is replaced with the transformed ZU 1 + ZU 2 . The learnable parameters U 1 , U 2 include different receptive fields. However, the degree of the polynomial remains the same, i.e., second-degree.</p><p>The Factorized Bilinear Model <ref type="bibr" target="#b38">[39]</ref> aim to extend the linear transformations of a convolutional layer by modeling the pairwise feature interactions. In particular, every output y f b ? R is expressed as the following expansion of the input</p><formula xml:id="formula_6">z ? R d : y f b = c 1 + c T 2 z + z T C T 3 C 3 z,<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">c 1 ? R, c 2 ? R d , C 3 ? R k?d are learnable parameters with k ? N.</formula><p>Arguably, a more general form of second-degree polynomial expansion is proposed in SORT <ref type="bibr" target="#b62">[63]</ref>. The idea is to combine different branches with multiplicative interactions. SORT is expressed as:</p><formula xml:id="formula_8">Y t = C 1? + C 2? + g((C 1? ) * (C 2? )),<label>(5)</label></formula><p>where the C 1 , C 2 are learnable parameters and g an elementwise, differentiable function. When C 1 is identity, they g is the elementwise square root function.</p><p>Third-degree polynomials can encode long-range dependencies that are not captured by local operators, such as convolutions. A popular framework in this category is the non-local neural networks (NL) <ref type="bibr" target="#b61">[62]</ref>. The non-local block can be expressed as:</p><formula xml:id="formula_9">Y n = (ZC 1 C 2 Z T )ZC 3 ,<label>(6)</label></formula><p>where the matrices C i for i = 1, 2, 3 are learnable parameters and Z ? R hw?c is the input. The third-degree term is then ?</p><formula xml:id="formula_10">[3] 1 (Z) = ZC 1 , ? [3] 2 (Z) = C 2 Z T and ? [3] 3 (Z) = ZC 3 .</formula><p>Recently, disentangled non-local networks (DNL) <ref type="bibr" target="#b67">[68]</ref> extends the formulation by including a second-degree term and the generated output is:</p><formula xml:id="formula_11">Y dn = ((ZC 1 ? ? q )(C 2 Z ? ? k ) T + Zc 4 ? ? 1 T )ZC 3 ,<label>(7)</label></formula><p>where c 4 is a weight vector, ? q , ? k are the mean vectors of the keys and queries representations and ? ? 1 ? R hw?1 a vector of ones. This translates to a new second-degree term with ?</p><formula xml:id="formula_12">[2] 1 (Z) = Zc 4 ? ? 1 T and ? [2] 2 (Z) = ZC 3 .</formula><p>Higher-degree polynomials can (virtually) approximate any smooth functions. The Weierstrass theorem <ref type="bibr" target="#b55">[56]</ref> and its extension <ref type="bibr" target="#b44">[45]</ref> (pg 19) guarantee that any smooth function can be approximated by a higher-degree polynomial.</p><p>A recently proposed framework that leverages high-degree polynomials to approximate functions is ?-net <ref type="bibr" target="#b8">[9]</ref>. Each ??net block is a polynomial expansion with a pre-determined degree. The learnable coefficients of the polynomial are represented with higher-order tensors. One of the drawbacks of this method is that the order of the tensor increases linearly with the degree, hence the parameters explode exponentially. To mitigate this issue, a coupled tensor decomposition that allows for sharing among the coefficients is utilized. Thus, the number of parameters is reduced significantly. Although the method is formulated as a complete polynomial, the proposed sharing of the coefficients suppresses its expressive power in favour of model compression. The model used for the classification can be expressed with the following recursive relationship:</p><formula xml:id="formula_13">x n = A T [n] z * S T [n] x n?1 + B T [n] b [n] + x n?1<label>(8)</label></formula><p>for an N th ?degree expansion order with n = 2, . . . , N . The weight matrices</p><formula xml:id="formula_14">A [n] , S [n] , B</formula><p>[n] and the weight vector b are learnable parameters, while <ref type="bibr" target="#b0">[1]</ref> . The recursive equation can be used to express an arbitrary degree of expansion, while the weight matrices are shared across different degree terms. For instance, A <ref type="bibr" target="#b1">[2]</ref> is shared by both first and second-degree terms when N = 2. From blocks to architecture: The core blocks of different architectures, and their polynomial counterparts, are analyzed above. The final network (in each case) is obtained by concatenating the respective blocks in a cascade. That is, the output of the first block is used as the input for the next block and so on. Each block expresses a polynomial expansion, thus, the final architecture expresses a product of polynomials.</p><formula xml:id="formula_15">x 1 = A T [1] z * B T [1] b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Novel architectures based on the taxonomy</head><p>The taxonomy offers a new perspective on how to modify existing architectures in a principled way. We showcase how new architectures arise in a natural way by modifying the popular Non-local neural network and the recent ??nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Higher-degree ResNet blocks</head><p>As shown in the previous section, the ResNet block is a first-degree polynomial. We can extend the polynomial expansion degree in order to enable higher-order correlations. A general N th ?degree polynomial is expressed as:</p><formula xml:id="formula_16">y = N n=1 W [n] n+1 j=2 ? j z + ?<label>(9)</label></formula><p>where z ? R ? , ? m denotes the mode-m vector product,</p><formula xml:id="formula_17">W [n] ? R o? n m=1 ?m? N n=1</formula><p>are the tensor parameters. To reduce the learnable parameters, we assume a lowrank CP decomposition <ref type="bibr" target="#b33">[34]</ref> on each tensor. By applying Lemma 1, we obtain:</p><formula xml:id="formula_18">y = ? + C T 1,[1] z + C T 1,[2] z * C T 2,[2] z + . . . + C T 1,[N ] z * . . . * C T N,[N ] z N Hadamard products<label>(10)</label></formula><p>where all C i are learnable parameters. Our proposed model in <ref type="formula" target="#formula_0">(10)</ref> is modular and can be designed for arbitrary polynomial degree. A schematic of (10) is depicted in <ref type="figure" target="#fig_2">Fig 3.</ref> The proposed model differentiates itself from <ref type="formula" target="#formula_13">(8)</ref> by not assuming any parameter sharing across the different degree terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Polynomial non-local blocks of different degrees</head><p>In this section, we demonstrate how the proposed taxonomy can be used to design a new architecture based on non-local blocks (NL). NL includes a third-degree term, while disentangled non-local network (DNL) includes both a third-degree and a second-degree term. We begin by including an additional first-degree term with learnable weights. The formed PDC-N L <ref type="bibr" target="#b2">[3]</ref> block is:</p><formula xml:id="formula_19">Y [3] ours = (ZC 1 C 2 Z T )ZC 3 + ZC 4 ZC 5 + ZC 6<label>(11)</label></formula><p>where C i are learnable parameters. In practice, a softmax activation function is added for the second and third degree factors, similar to the baseline. Besides the first-degree term, the proposed model differentiates itself from DNL by removing the sharing between the factors of third and second degree (i.e., C 3 and C 5 ) as well as utilizing a full factor matrix instead of a vector for the latter (i.e., C 4 ). In our implementation the matrices C i matrices (for i ? = 4) compress the channels of the input by a factor of 4 for all models. Building on <ref type="formula" target="#formula_0">(11)</ref>, we propose to expand the PDC-N L <ref type="bibr" target="#b2">[3]</ref> to a fourth degree polynomial expansion. We multiply the right hand side of (11) with a linear function with respect to the input Z. That is, the PDC-N L <ref type="bibr" target="#b3">[4]</ref> block is:</p><formula xml:id="formula_20">Y [4] ours = Y [3] ours + Y [3] ours * r(p(ZC 7 )C 8 ),<label>(12)</label></formula><p>where the term r(p(ZC 7 )C 8 ) is similar to squeeze-and-excitation right hand side term. The new term captures the channel-wise correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental evaluation</head><p>In this section, we study how the degree of the polynomial expansion affects the expressive power of the model. Our goal is to illustrate how the taxonomy enables improvements in strong-performing models with minimal and intuitive modifications. This approach should also shed light on the inductive bias of each model, as well as the reasons behind the gap in performance and model compression. Unless mentioned otherwise, the degree of the polynomial for each block is considered the second-degree expansion. The proposed variants are referred to as PDC. Experiments with higher-degree polynomials, which are denoted as PDC <ref type="bibr">[k]</ref> for k th degree, are also conducted.</p><p>Training details: For fair comparison all the aforementioned experiments have the same optimization-related hyper-parameters, e.g., the layer initialization. The training is run for 120 epochs with batch size 128 and the SGD optimizer is used. The initial learning rate is 0.1, while the learning rate is multiplied with a factor of 0.1 in epochs 40, 60, 80, 100. Classification accuracy is utilized as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image classification with residual blocks</head><p>The standard benchmarks of CIFAR10 <ref type="bibr" target="#b35">[36]</ref>, CIFAR100 <ref type="bibr" target="#b34">[35]</ref> and ImageNet <ref type="bibr" target="#b51">[52]</ref> are utilized to evaluate our framework on image classification. To highlight the Notice how we can trivially add new blocks in PDC to reach the pre-determined degree of the complete polynomial. The N th -degree term includes N + 1 new layers (i.e., blue boxes), which might make it impractical to expand over the fourth-degree. In practice, higher-degree polynomials make the model more expressive and hence allow for reduced number of channels.</p><p>impact of the degree of the polynomial, we experiment with first (ResNet), second (SENet and ??net ? ) as well as higher-degree polynomials. PDC relies on eq. 10; that is, we do not assume shared factor matrices across the different degree terms. The first experiment is conducted on CIFAR100. The ResNet18 and the respective SENet are the baselines, while the ?-net-ResNet is the main comparison method. To exhibit the efficacy of PDC, we implement various versions: (a) PDC-channels is the variant that retains the same channels as the original ResNet18, (b) PDC-param has approximate the same parameters as the corresponding baselines, (c) PDC-comp has the least parameters that can achieve a performance similar to the ResNet18 (which is achieved by reducing the channels), (d) the PDC which is the variant that includes both reduced parameters and increased performance with respect to ResNet18, (e) the higher-degree variants of PDC <ref type="bibr" target="#b2">[3]</ref> and PDC <ref type="bibr" target="#b3">[4]</ref> that modify the PDC such that the degree of each respective block is three and four respectively (see <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>The accuracy of each method is reported in <ref type="table" target="#tab_1">Table 2</ref>. SENet improves upon the baseline of ResNet18, which verifies the benefit of second-degree correlations in the channels. This is further demonstrated with ?-net-ResNet that captures second-degree information on all dimensions and can achieve the same performance with reduced parameters. Our method can further reduce the parameters by 60% over ResNet18 and achieve the same accuracy. This can be attributed ? The default ??net block is designed as second-degree polynomial. Higher-degree blocks are denoted with the symbol ? <ref type="bibr">[k]</ref> .</p><p>to the no-sharing scheme that enables more flexibility in the learned decompositions. The PDC-param further improves the accuracy by increasing the parameters, while the PDC-channels achieves the best accuracy by maintaining the same channels per residual block as ResNet18. The experiment is repeated with ResNet34 as the baseline. The accuracies in <ref type="table" target="#tab_8">Table 9</ref> demonstrate similar pattern, i.e., the parameters can be substantially reduced without sacrificing the accuracy. The results exhibit that the expressive power of PDC can allow it to be used both for compression and for improving the performance by maintaining the same number of parameters.  <ref type="bibr" target="#b2">[3]</ref> 16.8 0.766 PDC <ref type="bibr" target="#b3">[4]</ref> 28.0 0.771 A similar experiment is conducted on CIFAR10. The same networks as above are used as baselines, i.e., ResNet18 and SENet. The methods ?-net-ResNet and the PDC variants are also implemented as above. The results in <ref type="table" target="#tab_2">Table 3</ref> verify that both the ?-net-ResNet and PDC-comp can compress substantially the number of parameters required while retaining the same accuracy. Importantly, our method achieves a parameter reduction of 28% over ?-net-ResNet.</p><p>The last experiment is conducted on ImageNet <ref type="bibr" target="#b12">[13]</ref>, which contains 1.28M training images and 50K validation images from 1000 classes. We follow the standard settings to train deep networks on the training set and report the single-crop top-1 and the top-5 errors on the validation set. Our pre-processing and augmentation strategy follows the settings of the baseline (i.e., ResNet18). All models are trained for 100 epochs on 8 GPUs with 32 images per GPU (effective batch size of 256) with synchronous SGD of momentum 0.9. The learning rate is initialized to 0.1, and decays by a factor of 10 at the 30 th , 60 th , and 90 th epochs. The results in <ref type="table" target="#tab_3">Table 4</ref> reflect the patterns that emerged above. The proposed PDC can reduce the number of parameters required while retaining the same accuracy. The variant of PDC that has similar number of parameters to the baseline ResNet18 can achieve an increase in the accuracy. More specifically, we set the stem channel number to 52 for PDC-ResNet18-cmp and 60 for PDC18, respectively. In addition, we also decrease the channel to 1/4 before the Hadamard product in the proposed PDC block to save the computation cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image classification with non-local blocks</head><p>In this section, an experimental comparison and validation is conducted with non-local blocks. The benchmark of CIFAR100 is selected, while ResNet18 is referred as the baseline. The original non-local network (NL) and the disentangled non-local (DNL) are the main compared methods from the literature. As a reminder we perform two extensions in DNL: i) we add a first-degree term (PDC-N L <ref type="bibr" target="#b2">[3]</ref> ), ii) we add a fourth degree term (PDC-N L <ref type="bibr" target="#b3">[4]</ref> ). <ref type="table" target="#tab_4">Table 5</ref> contains the accuracy of each method. Notice that DNL improves upon the baseline NL, while PDC-N L <ref type="bibr" target="#b2">[3]</ref> improves upon both DNL and NL. Interestingly, the fourth-degree variant, i.e., PDC-N L <ref type="bibr" target="#b3">[4]</ref> outperforms all the compared methods by a considerable margin without increasing the number of parameters significantly. The results verify our intuition that the different polynomial terms enable additional flexibility to the model.</p><p>Besides the experiments on CIFAR100, we also test the proposed Non-local blocks in ImageNet. The training setup remains the same as in sec. 5.1. As shown in <ref type="table" target="#tab_5">Table 6</ref>, the proposed Non-local blocks with different polynomial degrees significantly improve the top-1 accuracy on ImageNet while the total parameter numbers are lower than the baseline ResNet18. More specifically, we set the bottleneck ratio of 4 for the proposed PDC-N L <ref type="bibr" target="#b2">[3]</ref> and PDC-N L <ref type="bibr" target="#b3">[4]</ref> , and we apply non-local blocks in multiple layers (c3+c4+c5) to better capture long-range dependency with only a slight increase in the computation cost. PDC-N L <ref type="bibr" target="#b3">[4]</ref> significantly outperforms NL by 1.03%, showing the advantages of the polynomial information fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Audio classification</head><p>Besides image classification, we conduct a series of experiments on audio classification to test the generalization of the polynomial expansion in different types of signals. The popular dataset of Speech Commands <ref type="bibr" target="#b63">[64]</ref> is selected as our benchmark. The dataset consists of 60, 000 audio files containing a single word each.   <ref type="table" target="#tab_6">Table 7</ref>. All the compared methods have accuracy over 0.97, while the polynomial expansions of ?-net-ResNet and PDC are able to reduce the number of parameters required to achieve the same accuracy, due to their expressiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Image classification on long-tailed distributions</head><p>We scrutinize the performance of the proposed models on long-tailed image recognition. We convert CIFAR10, which has 5, 000 number of samples per class, to a long-tailed version, called CIFAR10-LT. The imbalance factor (IF) is defined as the ratio of the largest class to the smallest class. The imbalance factor varies from 10 ? 200, following similar benchmarks tailored to long-tailed distributions <ref type="bibr" target="#b11">[12]</ref>. We note that the models are as defined above; the only change is the data distribution. The accuracy of each method is reported in <ref type="table" target="#tab_7">Table 8</ref>.</p><p>The results exhibit the benefits of the proposed models, which outperform the baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Classification benchmarks <ref type="bibr" target="#b51">[52]</ref> have a profound impact in the progress observed in machine learning. Such benchmarks been a significant testbed for new architectures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref> and for introducing novel tasks, such as adversarial perturbations <ref type="bibr" target="#b58">[59]</ref>. The architectures originally designed for classification have been applied to diverse tasks, such as image generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b71">72]</ref>. ResNet <ref type="bibr" target="#b23">[24]</ref> has been among the most influential works of the last few years which can be attributed both in its simplicity and its stellar performance. ResNet has been studied in both its theoretical capacity <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b69">70]</ref> and its empirical performance <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b70">71]</ref>. A number of works focus on modifying the shortcut connection and the concatenation operation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>The Squeeze-and-Excitation network (SENet) <ref type="bibr" target="#b26">[27]</ref> has been extended by <ref type="bibr" target="#b49">[50]</ref> to capture second-degree correlations in both the spatial and the channel dimensions. <ref type="bibr" target="#b25">[26]</ref> extend SENet by replacing the pooling operation with alternative operators that aggregate contextual information. <ref type="bibr" target="#b66">[67]</ref> inspired by the SENet propose a gated convolution module. <ref type="bibr" target="#b50">[51]</ref> improve SENet by introducing long-range dependencies. SENet has also been used as a drop-in module to improve the performance of residual blocks <ref type="bibr" target="#b14">[15]</ref>.</p><p>Non-local neural networks have been used extensively for capturing longrange interactions in both image-related tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b72">73]</ref> and video-related tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>. Non-local networks are also related with self-attention <ref type="bibr" target="#b59">[60]</ref> that is widely used in both vision <ref type="bibr" target="#b45">[46]</ref> and natural language processing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44]</ref>. <ref type="bibr" target="#b4">[5]</ref> study when the long-range interactions emerge in non-local neural networks. They also frame a simplified non-local block, which in our terminology is a second-degree polynomial. Naturally, they observe that this resembles the SENet and merge their simplified non-local block and SENet block.</p><p>A promising line of research is that of polynomial activation functions. For instance, the element-wise quadratic activation function f 2 applied to a linear operation C? is (C?) 2 . Both the theoretical <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42]</ref> and the empirical results <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47]</ref> support that polynomial activation functions can be beneficial. Our work is orthogonal to the polynomial activation functions, as they express a polynomial expansion of the representation, while we model a polynomial expansion of the input.</p><p>A well-established line of research is that of considering second or higherorder moments for various tasks. For instance, second-order statistics have been used for normalization methods <ref type="bibr" target="#b28">[29]</ref>, learning latent variable models <ref type="bibr" target="#b0">[1]</ref>. However, our work focuses on classification methods that can be expresses as polynomials and not on the method of moments.</p><p>Tensors and tensor decompositions are related to our work <ref type="bibr" target="#b53">[54]</ref>. Tensor decompositions such as the CP or the Tucker decompositions <ref type="bibr" target="#b33">[34]</ref> are frequently used for model compression in computer vision. Tensor decompositions have also been used for modeling the components of deep neural networks. <ref type="bibr" target="#b10">[11]</ref> interpret a whole convolutional network as a tensor decomposition, while the recent Einconv <ref type="bibr" target="#b20">[21]</ref> focus on a single convolutional layer and model them with tensor decompositions. In our work the focus is not in the tensor decomposition used, but on the polynomial expansion that provides insights on the correlations that are captured by each model.</p><p>A line of research that is related to ours is that of multiplicative data fusion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b68">69]</ref>. Even though multiplicative interactions can be considered as second-degree polynomials, data fusion of the aforementioned works is not our focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we study popular classification networks under the unifying perspective of polynomials. Notably, the popular ResNet, SENet and non-local networks are expressed as first, second and third degree polynomials respectively. The common framework provides insights on the inductive biases of each model and enables natural extensions building upon their polynomial nature. We conduct an extensive evaluation on image and audio classification benchmarks. We show how intuitive extensions to existing networks, e.g., converting the thirddegree non-local network into a fourth degree, can improve the performance. Such natural extensions can be used for designing new architectures based on the proposed taxonomy. Importantly, our experimental evaluation highlights the dual utility of the polynomial framework: the networks can be used either for model compression or increased model performance. We expect this to be a significant feature when designing architectures for edge devices. Our experimentation in the presence of limited data and long-tailed data distributions highlights the benefits of the proposed taxonomy and provides a link to real-world applications, where massive data annotation is challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed notation</head><p>Products: The Hadamard product of A, B ? R I?N is defined as A * B and is equal to a (i,j) b (i,j) for the (i, j) element. The Khatri-Rao product of matrices A ? R I?N and B ? R J?N is denoted by A?B and yields a matrix of dimensions (IJ) ? N . The Khatri-Rao product for a set of matrices</p><formula xml:id="formula_21">{A [m] ? R Im?N } M m=1 is abbreviated by A [1] ? A [2] ? ? ? ? ? A [M ] . = M m=1 A [m]</formula><p>. Tensors: Each element of an M th order tensor X is addressed by M indices, i.e., (X ) i1,i2,...,i M . = x i1,i2,...,i M . An M th -order tensor X is defined over the tensor space R I1?I2?????I M , where I m ? Z for m = 1, 2, . . . , M . The mode-m unfolding of a tensor X ? R I1?I2?????I M maps X to a matrix X (m) ? R Im??m with? m = M k=1 k? =m I k such that the tensor element x i1,i2,...,i M is mapped to the</p><formula xml:id="formula_22">matrix element x im,j where j = 1 + M k=1 k? =m (i k ? 1)J k with J k = k?1 n=1 n? =m I n .</formula><p>The mode-m vector product of X with a vector c ? R Im , denoted by X ? m c ? R I1?I2?????Im?1?Im+1?????I M , results in a tensor of order M ? 1:</p><formula xml:id="formula_23">(X ? m c) i1,...,im?1,im+1,...,i M = Im im=1 x i1,i2,...,i M u im .<label>(13)</label></formula><p>The CP decomposition <ref type="bibr" target="#b33">[34]</ref> factorizes a tensor into a sum of component rank-one tensors. The rank-R CP decomposition of an M th -order tensor X is written as: <ref type="bibr" target="#b0">[1]</ref> , C <ref type="bibr" target="#b1">[2]</ref> , . . . ,</p><formula xml:id="formula_24">X . = [[C</formula><formula xml:id="formula_25">C [M ] ]] = R r=1 c (1) r ? c (2) r ? ? ? ? ? c (M ) r ,<label>(14)</label></formula><p>where ? is the vector outer product. The factor matrices C</p><formula xml:id="formula_26">[m] = [c (m) 1 , c (m) 2 , ? ? ? , c (m) R ] ? R Im?R M</formula><p>m=1 collect the vectors from the rank-one components. By considering the mode-1 unfolding of X , the CP decomposition can be written in matrix form as:</p><formula xml:id="formula_27">X (1) . = C [1] 2 m=M C [m] T<label>(15)</label></formula><p>The following lemma is useful in our method:</p><formula xml:id="formula_28">Lemma 1 ( [10]). For a set of N matrices {A [?] ? R I? ?K } N ?=1 and {B [?] ? R I? ?L } N ?=1</formula><p>, the following equality holds:</p><formula xml:id="formula_29">( N ?=1 A [?] ) T ? ( N ?=1 B [?] ) = (A T [1] ? B [1] ) * . . . * (A T [N ] ? B [N ] )<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Polynomials as a single tensor product</head><p>As mentioned in the main paper polynomials and tensors are closely related.</p><p>To illustrate the differences between the proposed variant of sec. 4.1 and the proposed taxonomy we can formulate them as a single tensor product. We assume a second-degree polynomial expansion of (1). The tensors are then up to thirdorder, which enables a visualization (as in the <ref type="figure">Fig.1</ref>). The initial equation is:</p><formula xml:id="formula_30">y = ? + W [1] T z + W [2] ? 2 z ? 3 z<label>(17)</label></formula><p>The ? th output of (17) can be written in element-wise form as:</p><formula xml:id="formula_31">y ? = ? ? + ? k=1 w [1] ?,k z k + ? k,m=1 w [2] ?,k,m z k z m<label>(18)</label></formula><p>We can collect all the parameters of (17) under a single tensor by padding the input z ? R ? . Specifically, if we consider the padded versionz = [z 1 , . . . , z ? , 1] T , then <ref type="bibr" target="#b16">(17)</ref> can be written in the format y =W ? 2z ? 3z as we demonstrate below.</p><p>The ? th output ofW ? 2z ? 3z is: </p><formula xml:id="formula_32">y ? =</formula><p>If we set:</p><formula xml:id="formula_34">? ? ? ? ? ? ? =w ?,?+1,?+1 w [1]</formula><p>?,k =w ?,?+1,k +w ?,k,?+1 for k = 1, . . . , ? w <ref type="bibr" target="#b1">[2]</ref> ?,k,m =w ?,k,m for k, m = 1, . . . , ?</p><p>then <ref type="bibr" target="#b18">(19)</ref> becomes the polynomial expansion of <ref type="bibr" target="#b17">(18)</ref>. This enables us to express different degree polynomial expansions with a third-order tensor. The first-degree methods, e.g., ResNet <ref type="bibr" target="#b23">[24]</ref>, have w <ref type="bibr" target="#b1">[2]</ref> ?,k,m = 0, while SENet <ref type="bibr" target="#b26">[27]</ref> assumes w <ref type="bibr" target="#b0">[1]</ref> ?,k = 0. The ?-net family assumes low-rank decomposition with shared factors, i.e., the low-rank decompositions of W [n] N n=1 share factor matrices. On the contrary, our proposed PDC does not assume a sharing pattern, thus it can express independently the terms W <ref type="bibr" target="#b0">[1]</ref> , W <ref type="bibr" target="#b1">[2]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs</head><p>Claim. The Squeeze-and-excitation block of (3) is a special form of seconddegree polynomial term.</p><p>Proof. The global pooling function on a matrix C can be expressed as 1</p><formula xml:id="formula_36">hw ? ? 1 T C.</formula><p>The r function that replicates the channels acts on a vector c and results in the expression ? ? 1 c T . The identity X * ab T = diag(a)Xdiag(b) can be used to convert the Hadamard product of (3) into a matrix multiplication <ref type="bibr" target="#b56">[57]</ref>. Then, (3) becomes:</p><formula xml:id="formula_37">Y s = (ZC 1 ) * ? ? 1 1 hw ? ? 1 T ZC 1 C 2 T = (ZC 1 ) 1 hw diag(C T 2 C T 1 Z T ? ? 1 ) = (ZC 1 ) 1 hw I ? 3 (C T 2 C T 1 Z T ? ? 1 )<label>(21)</label></formula><p>where as a reminder I is a third-order super-diagonal unit tensor. The last equation is a second-degree term with ? </p><formula xml:id="formula_38">[2] 1 (Z) = ZC 1 and ? [2] 2 (Z) = 1 hw I ? 3 (C T 2 C T 1 Z T ? ? 1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Auxiliary experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Classification without activation functions</head><p>Typical feed-forward neural networks, such as CNNs, require activation functions to learn complex functions <ref type="bibr" target="#b24">[25]</ref>. However, the proposed view of polynomial expansion enables capturing higher-order correlations even in the absence of activation functions. That is, the expressivity of higher-degree polynomials can be assessed without activation functions. We conduct a series of experiments on all three datasets with higher-degree polynomials. Our core experiments study the higher-degree polynomials of ?-nets <ref type="bibr" target="#b8">[9]</ref>, versus the proposed model of <ref type="bibr" target="#b9">(10)</ref>. We also implement the ResNet without activation functions to assess how firstdegree polynomials perform. For the first experiment, we utilize ResNet18 as the backbone and test the baselines on CIFAR100. Three variations of P i?net are considered as the compared methods: one with second-degree, one with third-degree and one with fourth-degree residual blocks. The same polynomial expansions are used for the proposed PDC. The accuracy of each method is reported in <ref type="table" target="#tab_0">Table 10</ref>. All the variants of ?-net-ResNet and PDC exhibit a high accuracy based solely on the high-degree polynomial expansion. However, ?-net-ResNet saturates when the residual block is a third or fourth degree polynomial, while the PDC does not suffer from the same issue. On the contrary, the performance of the PDC variant with third and forth degree residual block outperforms the second-degree residual block. <ref type="table" target="#tab_0">Table 10</ref>: Image classification on CIFAR100 without activation functions. Both ?-net-ResNet and PDC use high-degree polynomial expansion to achieve high accuracy even in the absence of activation functions. The proposed PDC achieves both increased performance and improves its performance when each residual block has third or fourth degree polynomial instead of second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p># param (?10 6 ) Accuracy ResNet18 11.2 0.168 ?-net-ResNet 11.9 0.667 ?-net-ResNet <ref type="bibr" target="#b2">[3]</ref> 11.2 0.648 ?-net-ResNet <ref type="bibr" target="#b3">[4]</ref> 11.2 0.626 PDC 5.46 0.689 PDC <ref type="bibr" target="#b2">[3]</ref> 11.2 0.703 PDC <ref type="bibr" target="#b3">[4]</ref> 18.8 0.699</p><p>The models are also evaluated on CIFAR10 with ResNet18 and three variants of ??nets as the backbone. Three variants of PDC with different expansion degrees are designed. The results are tabulated on <ref type="table" target="#tab_0">Table 11</ref>. Each variant of ?net-ResNet and PDC surpasses the 0.87 accuracy and outperform the ResNet18 by a wide margin. In contrast to ?-net-ResNet, the performance of PDC does not decrease when the degree of the residual block increases, i.e., from second to fourth-degree. Overall, PDC outperforms ??net.</p><p>The last experiment is conducted on the Speech Commands dataset. The baseline of ResNet18 is selected, while the ?-net-ResNet is the compared method. The results in <ref type="table" target="#tab_0">Table 12</ref> depict the same motif: the two polynomial expansions are very expressive. Impressively, in this dataset the result without activation functions is only 0.007 decreased when compared to the respective results with activation functions. This highlights that simple datasets might not always demand activation functions to achieve high-accuracy. <ref type="table" target="#tab_0">Table 11</ref>: Image classification on CIFAR10 without activation functions. The results illustrate the expressiveness of the proposed model even in the absence of activation functions. Notice that PDC <ref type="bibr" target="#b2">[3]</ref> improves upon PDC with second-degree blocks. On the contrary, this does not happen to the compared ?-net-ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p># param (?10 6 ) Accuracy ResNet18 11.2 0.391 ?-net-ResNet 11.9 0.907 ?-net-ResNet <ref type="bibr" target="#b2">[3]</ref> 11.2 0.891 ?-net-ResNet <ref type="bibr" target="#b3">[4]</ref> 11.2 0.877 PDC 5.4 0.909 PDC <ref type="bibr" target="#b2">[3]</ref> 11.2 0.918 PDC <ref type="bibr" target="#b3">[4]</ref> 18.8 0.918  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Object detection and segmentation</head><p>We adopt MS COCO 2017 <ref type="bibr" target="#b40">[41]</ref> as the primary benchmark for the experiments of object detection and segmentation. We use the train split (118k images) for training and report the performance on the val split (5k images). We employ standard evaluation metrics for COCO dataset, where multiple IoU thresholds from 0.5 to 0.95 are applied. The detection results are evaluated with mAP.</p><p>We use the final model weights from ImageNet-1K pre-training as network initializations and fine-tune Mask R-CNN <ref type="bibr" target="#b21">[22]</ref> and Cascade Mask R-CNN [4] on the COCO dataset. Following default settings in MMDetection, we use the 1? schedule (i.e.,12 epochs). <ref type="table" target="#tab_0">Table 13</ref> shows object detection and instance segmentation results comparing ResNet18 and the proposed PDC-ResNet18. As we can see from the results, the proposed PDC-ResNet18 achieves an obvious better performance than the baseline ResNet18 in terms of the box and mask AP, confirming the effectiveness of the proposed polynomial learning scheme. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>-</head><label></label><figDesc>We express a collection of state-of-the-art neural architectures as polynomials. Our unifying framework sheds light on the inductive bias of each architecture. We experimentally verify the performance of different methods of the taxonomy on four standard benchmarks.-Our framework allows us to propose intuitive modifications on existing architectures. The proposed new architectures consistently improve upon their corresponding baselines, both in terms of accuracy as well as model compression. -We evaluate the performance under various changes in the training distribution, i.e., limiting the number of samples per class or creating a long-tailed distribution. The proposed models improve upon the baselines in both cases. -We release the code as open source to enable the reproduction of our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Blocks (up to third-degree) from different architectures. The layers (i.e., blue boxes) denote any linear operation, e.g., a convolution or a fully-connected layer, depending on the architecture. From left to right, the degree of the polynomial is increasing. Our framework enables also to complete the missing terms of the polynomial (i.e., PDC-NL versus NL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the modular nature of the propose polynomial expansion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>,m z k z m second-degree term</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Top-1 validation error on ImageNet with proposed PDC and NL methods throughout the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Symbols</figDesc><table><row><cell cols="2">Symbol Dimension(s)</cell><cell>Definition</cell></row><row><cell>N</cell><cell>N</cell><cell>Degree of polynomial expansion.</cell></row><row><cell>k</cell><cell>N</cell><cell>Rank of the decompositions.</cell></row><row><cell>z</cell><cell>R d</cell><cell>Vector-form input to the expansion.</cell></row><row><cell>?,  *</cell><cell>-</cell><cell>Khatri-Rao product, Hadamard product.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Image classification on CI-FAR100 with variants of ResNet18. The symbol 'p' denotes parameters.</figDesc><table><row><cell>Model</cell><cell cols="2">#p ?10 6 Accuracy</cell></row><row><cell>ResNet18</cell><cell>11.2</cell><cell>0.756</cell></row><row><cell>SENet</cell><cell>11.6</cell><cell>0.760</cell></row><row><cell>?-net-ResNet</cell><cell>6.1</cell><cell>0.760</cell></row><row><cell>PDC-comp</cell><cell>4.3</cell><cell>0.760</cell></row><row><cell>PDC-channels</cell><cell>19.2</cell><cell>0.773</cell></row><row><cell>PDC-param</cell><cell>11.4</cell><cell>0.770</cell></row><row><cell>PDC</cell><cell>8.0</cell><cell>0.765</cell></row><row><cell>PDC</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">#p ?10 6 Accuracy</cell></row><row><cell>ResNet18</cell><cell>11.2</cell><cell>0.945</cell></row><row><cell>SENet</cell><cell>11.5</cell><cell>0.946</cell></row><row><cell>?-net-ResNet</cell><cell>6.0</cell><cell>0.945</cell></row><row><cell>PDC</cell><cell>8.0</cell><cell>0.946</cell></row><row><cell>PDC-comp</cell><cell>4.3</cell><cell>0.945</cell></row><row><cell>ResNet34</cell><cell>21.3</cell><cell>0.948</cell></row><row><cell>?-net-ResNet</cell><cell>13.0</cell><cell>0.949</cell></row><row><cell>PDC</cell><cell>10.5</cell><cell>0.948</cell></row></table><note>Image classification on CI- FAR10. The symbol 'p' denotes pa- rameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Image classification on ImageNet with variants of ResNet18.</figDesc><table><row><cell>Model</cell><cell cols="4">Top-1 Accuracy Top-5 Accuracy Flops #p ?10 6</cell></row><row><cell>ResNet18</cell><cell>0.698</cell><cell>0.891</cell><cell cols="2">1.82G 11.69M</cell></row><row><cell>SE-ResNet18</cell><cell>0.706</cell><cell>0.896</cell><cell cols="2">1.82G 11.78 M</cell></row><row><cell>?-net-ResNet</cell><cell>0.707</cell><cell>0.895</cell><cell cols="2">1.8207G 11.96M</cell></row><row><cell>PDC-cmp</cell><cell>0.698</cell><cell>0.893</cell><cell>1.30G</cell><cell>7.51M</cell></row><row><cell>PDC</cell><cell>0.710</cell><cell>0.899</cell><cell cols="2">1.67G 10.69M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification on CIFAR100 with non-local blocks.</figDesc><table><row><cell cols="3">Model #p ?10 6 Accuracy</cell></row><row><cell>ResNet18</cell><cell>11.2</cell><cell>0.756</cell></row><row><cell>NL</cell><cell>11.57</cell><cell>0.769</cell></row><row><cell>DNL</cell><cell>11.57</cell><cell>0.771</cell></row><row><cell cols="2">PDC-N L [3] 11.87</cell><cell>0.773</cell></row><row><cell cols="2">PDC-N L [4] 12.00</cell><cell>0.779</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Classification on ImageNet with non-local blocks.</figDesc><table><row><cell cols="3">Model #p ?10 6 Accuracy</cell></row><row><cell>ResNet18</cell><cell>11.69</cell><cell>0.698</cell></row><row><cell>NL</cell><cell>12.02</cell><cell>0.702</cell></row><row><cell>PDC</cell><cell>10.69</cell><cell>0.710</cell></row><row><cell cols="2">PDC-N L [3] 11.35</cell><cell>0.712</cell></row><row><cell cols="2">PDC-N L [4] 11.51</cell><cell>0.716</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Speech classification with ResNet variants. Four residual blocks are used in ResNet7 instead of eight of ResNet18. Nevertheless, the respective PDC7 can reduce even further the parameters to achieve the same performance.</figDesc><table><row><cell>Model</cell><cell cols="2"># param (?10 6 ) Accuracy</cell></row><row><cell>ResNet7</cell><cell>4.9</cell><cell>0.974</cell></row><row><cell>SENet</cell><cell>5.1</cell><cell>0.974</cell></row><row><cell>PDC</cell><cell>3.9</cell><cell>0.975</cell></row><row><cell>ResNet18</cell><cell>11.2</cell><cell>0.977</cell></row><row><cell>SENet</cell><cell>11.5</cell><cell>0.977</cell></row><row><cell>?-net-ResNet</cell><cell>6.0</cell><cell>0.977</cell></row><row><cell>PDC</cell><cell>8.0</cell><cell>0.978</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Accuracy on image classification on CIFAR10-LT. Each column corresponds to a different imbalance factor (IF).</figDesc><table><row><cell>Model</cell><cell cols="2">IF 200</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell></row><row><cell cols="2">ResNet18</cell><cell cols="5">0.645 0.696 0.784 0.844 0.877</cell></row><row><cell>SENet</cell><cell></cell><cell cols="5">0.636 0.713 0.784 0.844 0.878</cell></row><row><cell cols="7">?-net-ResNet 0.653 0.718 0.783 0.845 0.879</cell></row><row><cell cols="7">PDC-comp 0.653 0.727 0.786 0.848 0.882</cell></row><row><cell cols="7">PDC-param 0.665 0.726 0.792 0.851 0.886</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Image classification on CIFAR100 with variants of ResNet34. Image classification with limited data A number of experiments is performed by progressively reducing the number of training samples per class. The number of samples is reduced uniformly from the original 5, 000 down to 50 per class, i.e., a 100? reduction, in CIFAR10. The architectures ofTable 3(similar to ResNet18) are used unchanged; only the number of training samples is progressively reduced. The resultingFig. 4visualizes the performance as we decrease the training samples. The accuracy of ResNet18 decreases fast for limited training samples. SENet deteriorates at a slower pace, steadily increasing the difference from ResNet18 (note that both share similar number of parameters). ?-net-ResNet improves upon SENet and performs better even under limited data. However, the proposed PDC-comp outperforms all the compared methods for 50 training samples per class. The difference in the accuracy between PDC and ?-net-ResNet increases as we reduce the number of training samples. Indicatively, with 50 samples per class, ResNet18 attains accuracy of 0.347, SENet scores 0.355, ?-net-ResNet scores 0.397 and PDC-comp scores 0.426, which is a 22% increase over the ResNet18 baseline. Image classification with limited data. The x-axis declares the number of training samples per class (log-axis). As the number of samples is reduced (i.e., moving from right to the left), the performance gap between ?-net-ResNet and ResNet18 increases. Similarly, PDC-comp performs better than ?-net-ResNet, especially in the limited data regimes on the left.</figDesc><table><row><cell>Model</cell><cell cols="2"># param (?10 6 ) Accuracy</cell></row><row><cell>ResNet34</cell><cell>21.3</cell><cell>0.769</cell></row><row><cell>?-net-ResNet</cell><cell>14.7</cell><cell>0.769</cell></row><row><cell>PDC-channels</cell><cell>36.3</cell><cell>0.774</cell></row><row><cell>PDC</cell><cell>10.5</cell><cell>0.770</cell></row><row><cell>D.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Audio classification without activation functions.</figDesc><table><row><cell>Model</cell><cell cols="2"># param (?10 6 ) Accuracy</cell></row><row><cell>ResNet18</cell><cell>11.2</cell><cell>0.464</cell></row><row><cell>?-net-ResNet</cell><cell>11.9</cell><cell>0.971</cell></row><row><cell>PDC</cell><cell>5.4</cell><cell>0.972</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 :</head><label>13</label><figDesc>COCO object detection and segmentation results using Mask-RCNN and Cascade Mask-RCNN. The backbone models are pre-trained ResNet18 and PDC-ResNet18 models on ImageNet-1K. We employ MMDetection with 1? schedule. backbone AP box AP box 50 AP box 75 AP mask AP mask 50</figDesc><table><row><cell>AP mask 75</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor decompositions for learning latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2773" to="2832" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeezeexcitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops (ICCV&apos;W)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A?2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional generation using polynomial expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<title level="m">?nets: Deep polynomial neural networks. In: Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06571</idno>
		<title level="m">Polygan: High-order polynomial generators</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional rectifier networks as generalized tensor decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention, please! a critical review of neural attention models in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02181</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Res2net: A new multi-scale backbone architecture. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilinear latent conditioning for generating unseen attribute combinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mitigating demographic bias in facial datasets with style-based multi-attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oldfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal function approximation by deep neural nets with bounded width and relu activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">992</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identity matters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Einconv: Exploring unexplored tensor network decompositions for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5353" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9401" to="9411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decorrelated batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiplicative interactions and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the expressive power of deep polynomial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kileel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/~kriz/cifar.html" />
		<title level="m">Cifar-100</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html55" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Factorized bilinear models for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2079" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural architecture search for lightweight non-local networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10297" to="10306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the computational efficiency of training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="855" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generating accurate pseudo-labels in semi-supervised learning and avoiding overconfident predictions via hermite polynomial activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lokhande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tasneeyapant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11435" to="11443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A tensorized transformer for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2232" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nikol&amp;apos;skii</surname></persName>
		</author>
		<title level="m">Analysis III: Spaces of Differentiable Functions. Encyclopaedia of Mathematical Sciences</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The power of deeper networks for expressing natural functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Concurrent spatial and channel &apos;squeeze &amp; excitation&apos;in fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="421" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Linear context transform block</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="5553" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Are resnets provably better than linear predictors? In: Advances in neural information processing systems (NeurIPS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tensor decomposition for signal processing and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3551" to="3582" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The generalized weierstrass approximation theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics Magazine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Hadamard products and multivariate statistical analysis. Linear algebra and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Styan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="217" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Mixed link networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>IJCAI)</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sort: Second-order response transform for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1359" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Toward interpretable music tagging with selfattention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04972</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Gated channel transformation for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11794" to="11803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with coattention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaeemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rahnavard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07477</idno>
		<title level="m">Norm-preservation: Why residual networks can become extremely deep? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
