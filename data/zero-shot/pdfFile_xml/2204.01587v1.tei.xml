<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation Taeyoung Son * NALBI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Sohyun</forename><forename type="middle">Lee</forename><surname>Gsai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Suha Kwak GSAI</orgName>
								<address>
									<postBox>POSTECH</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung@nalbi</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Suha Kwak GSAI</orgName>
								<address>
									<postBox>POSTECH</postBox>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation Taeyoung Son * NALBI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust visual recognition under adverse weather conditions is of great importance in real-world applications. In this context, we propose a new method for learning semantic segmentation models robust against fog. Its key idea is to consider the fog condition of an image as its style and close the gap between images with different fog conditions in neural style spaces of a segmentation model. In particular, since the neural style of an image is in general affected by other factors as well as fog, we introduce a fog-pass filter module that learns to extract a fog-relevant factor from the style. Optimizing the fog-pass filter and the segmentation model alternately gradually closes the style gap between different fog conditions and allows to learn fog-invariant features in consequence. Our method substantially outperforms previous work on three real foggy image datasets. Moreover, it improves performance on both foggy and clear weather images, while existing methods often degrade performance on clear scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We have witnessed great advances in semantic segmentation for the last decade. However, most of existing models and datasets focus merely on improving accuracy under controlled environments, without considering image degradation caused by adverse weather conditions (e.g., fog, rain, and snow), over-and under-exposure, motion blur, sensor noise, etc. The robustness of semantic segmentation models against these factors is of great importance in safetycritical applications and recently has gained increasing attention <ref type="bibr">[3, 6, 8, 51-53, 57, 68]</ref>.</p><p>Motivated by this, we study semantic segmentation of foggy scenes, whose goal and results are illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The task is challenging since fog often damages visibility of images seriously, leading to substantial performance degradation. Attaching a fog removal network to the front of * This work was done while Taeyoung Son was in POSTECH. an existing model is not always useful for mitigating this issue <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b52">53]</ref> as well as being heavy in computation and memory. The other reason for the difficulty is the absence of fully annotated data for the task. Collecting a large set of foggy scenes is not straightforward since they can be captured under only a specific condition, and it is hard to label them due to their limited visibility. Existing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> tackle these issues through synthetic foggy image datasets, which are obtained by applying realistic fog effects to fully annotated clear weather images and are used for supervised learning of semantic segmentation. Furthermore, they introduce curriculum learning approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref> that gradually adapt a model from light synthetic fog to dense real fog using unlabeled real foggy images additionally. Although these methods have achieved impressive robustness, there remains room for further improvement in that their training strategies are limited to ordinary supervised learning. In addition, the curriculum adaptation demands external modules to control the fog density of real foggy images in training, and tends to make the final model biased to foggy scenes; it thus requires extra computation and additional hyper-parameters in training, and often degrades performance on clear images.</p><p>To resolve the above issues, we propose a new method that learns Fog-Invariant features for FOggy scene segmentation, dubbed FIFO. Its overall pipeline is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. FIFO considers the fog condition of an image as its style, ideally independent of its content, and aims to learn a segmentation model insensitive to fog style variation of input image. To this end, we first define three different domains of training images, i.e., clear weather (CW), synthetic fog (SF), and real fog (RF), where images of the first two domains are labeled while those of the last one are not. FIFO then encourages the segmentation network to close the style discrepancy between different fog domains in feature spaces so that it learns fog-invariant features.</p><p>Then the success of FIFO depends heavily on the quality of the fog style representation. Unfortunately, existing style representation schemes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b60">61]</ref> are not desirable for our task since they are manually designed to capture the holistic style of an image that is affected also by factors other than fog (e.g., when and where the image was taken) and even the content of the image <ref type="bibr" target="#b5">[6]</ref>; the direct use of these neural styles thus introduce side-effects like content alteration and result in suboptimal solutions consequently.</p><p>To address this issue, we present fog-pass filters, learnable modules that take an ordinary neural style-the Gram matrix of a feature map <ref type="bibr" target="#b17">[18]</ref> as input and extract only a fogrelevant information from the style precisely in the form of embedding vectors, called fog factors. In particular, they learn to draw fog factors of the same domain together and hold those of different domains apart so that they discriminate fog conditions of input images through their fog factors. The segmentation model is in turn encouraged to reduce the gap between fog factors of images from different domains during training. The alternating optimization of the fog-pass filter and the segmentation network gradually closes the fog style gap between different domains and eventually leads to fog-invariant features of the segmentation network. Note that the fog-pass filters are auxiliary modules for training only, thus not required in testing.</p><p>FIFO has advantages over the previous work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> in terms of both simplicity in training and efficacy in testing. Unlike the previous work, FIFO does not need to control fog density levels of synthetic and real foggy images in training, thus allowing end-to-end learning of a segmentation model with fewer hyper-parameters. More importantly, for the same reason, it demands no extra module for estimating and manipulating fog density of real foggy images in training. Regarding the effectiveness, FIFO clearly outperforms all existing records and improves performance on both foggy and clear weather domains, while existing methods often degrade performance on clear scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Foggy Scene Segmentation. Previous work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> has developed fog simulators that are applied to clear images with full annotations to obtain labeled synthetic foggy images. Since supervised learning on the synthetic data limits performance due to the visual gap between synthetic and real foggy images, recent methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref> further employ curriculum learning to gradually adapt a model from light synthetic fog to dense real fog. However, the curriculum adaptation often degrades performance on clear weather images and demands extra modules to control density levels of real foggy images during training. Image Dehazing. Fog damages visibility of image, and accordingly, degrades visual recognition performance substantially. Numerous dehazing algorithms have been proposed so far to restore latent clean image from foggy input <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b68">69]</ref>. However, they are usually too heavy in computation to be attached to the front of recognition models. Further, recent studies <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b52">53]</ref> suggest that most dehazing models do not help improve recognition performance on foggy scenes. Hence, instead of having a separate dehazing module, our work learns a segmentation model whose features are invariant to fog condition. Robustness. Robustness of recognition models against adverse conditions has been actively investigated due to its importance in real-world applications <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b67">68]</ref>, and a variety of methods have been proposed to improve robustness <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b65">66]</ref>. FIFO shares a similar idea with RobustNet <ref type="bibr" target="#b5">[6]</ref>, which regards adverse condition of input as its style and removes the effect of photometric transform of input from a neural style representation of a model so that the model becomes invariant to the transform. Compared to RobustNet, FIFO more explicitly quantifies the effect of adverse condition through a learnable module (i.e., fog-pass filter), so is able to more precisely manipulate the adverse effects during training. Style Transfer. Neural style transfer has been studied to comprehend the style of an image apart from its content <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60]</ref>. In particular, a seminal work <ref type="bibr" target="#b17">[18]</ref> studied the Gram matrix of a feature map as a neural style representation and showed that the style of an image can be transferred to another by approximating its Gram matrix; the efficacy of Gram matrix has been proven further in later studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>. Also, Li et al. <ref type="bibr" target="#b38">[39]</ref> proved that matching Gram matrices is useful for domain adaptation since it is equivalent to minimizing maximum mean discrepancy between domains. However, we found that Gram matrix is not appropriate as-is for quantifying the effect of fog in our task since it is affected other style factors or even content <ref type="bibr" target="#b5">[6]</ref> as well as fog. We thus introduce the fog-pass filter to precisely capture only a fog-relevant factor from the Gram matrix of a feature map. Unsupervised Domain Adaptation (UDA). Our work is also relevant to UDA since both adapt models to an unlabeled target domain. UDA methods for semantic segmentation can be categorized by the level at which adaptation is performed: Input-level <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>, featurelevel <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65]</ref>, and output-level <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>. FIFO is related in particular to the feature-level adaptation that learns domain-invariant features. Most of existing methods in this category <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65]</ref> train a discriminator together with a segmentation model so that the discriminator maximizes a discrepancy between source and target domains while the segmentation model learns to minimize the discrepancy. FIFO shares a similar idea with these methods, but as will be demonstrated, closing the gap between fog factors in FIFO is more effective than fooling a fog domain classifier in fog-invariant feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Configuration of Training Data</head><p>Training images for FIFO are categorized into three different domains according to their fog types: clear weather (CW), synthetic fog (SF), and real fog (RF). For CW images, we adopt the Cityscapes dataset <ref type="bibr" target="#b6">[7]</ref>, which is fully annotated for supervised learning of semantic segmentation. Meanwhile, as SF images, we utilize the Foggy Cityscapes-DBF dataset <ref type="bibr" target="#b51">[52]</ref>, which is constructed by simulating realistic fog effects on images of the Cityscapes dataset, thus also fully annotated. Finally, RF images are taken from the Foggy Zurich dataset <ref type="bibr" target="#b51">[52]</ref>, which is a collection of unlabeled foggy scenes captured in the real world.</p><p>Note that the way FIFO uses the two foggy image datasets is different from that of existing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref>. First, in the Foggy Cityscapes-DBF dataset, FIFO fixes the density level of synthetic fog by a single value (i.e., the attenuation coefficient ? = 0.005) and utilizes the entire dataset. On the other hand, the previous work adopts only a refined, high-quality subset of the dataset and varies the fog level during training for the curriculum learning. Second, FIFO utilizes the Foggy Zurich dataset as a whole, whereas the previous work divides it into multiple sets of different density levels using extra modules that estimate the fog density of the images. These differences allow the pipeline of FIFO to be more straightforward and concise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>The training procedure for the fog-pass filters and the segmentation network is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref> and their structures are presented in <ref type="figure" target="#fig_2">Fig. 3</ref>. The segmentation network is first pretrained on the Cityscapes dataset <ref type="bibr" target="#b6">[7]</ref>, and fog-pass filters initialized randomly are attached to different feature maps of the network. Thereafter, on the dataset of the three fog domains introduced in Sec. 3, the two parts of FIFO are trained alternately per mini-batch, except for the first 5K iterations where the fog-pass filters are solely trained to avoid cold-start. Note that the fog-pass filters are used only in training for fog-invariant feature learning of the segmentation network. In other words, FIFO imposes no additional inference-time complexity since it employs only the segmentation network for testing.</p><p>To construct a mini-batch, we randomly sample the same number of images from CW and RF domains, and choose SF counterparts of the sampled CW images. Given such a mini-batch, the fog-pass filters are learned to draw fog factors of the same fog domain together and hold those of different domains apart so that they discriminate input according to its fog domain. On the other hand, the segmentation network is optimized for closing the distance between fog factors of different domains as well as for minimizing the ordinary segmentation loss. This alternating optimization closes the fog style gap between different domains precisely, leading to fog-invariant features.</p><p>The remaining part of this section first gives details of training the fog-pass filtering modules and the segmentation network, then empirically verifies the key ideas of FIFO. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fog-pass Filtering Modules</head><p>Instead of a raw feature map of the segmentation network, a fog-pass filtering module takes a holistic style representation of the feature map as input in order to focus more on the style of image by filtering out most of its content information. In this context, the style representation can be considered as a hardwired layer <ref type="bibr" target="#b30">[31]</ref> that encodes our prior knowledge. We in particular adopt the Gram matrix of the feature map <ref type="bibr" target="#b17">[18]</ref> as the style representation as it provides richer style information than other methods, e.g., channel-wise feature statistics <ref type="bibr" target="#b60">[61]</ref>. The Gram matrix, denoted by G ? R c?c , captures correlations between c channels of its input feature map. The (i, j) element of G indicates the correlation between i th and j th feature channels and is computed by G i,j = a ? i a j , where a i is the vector form of the i th channel of the input feature map. Specifically, since the Gram matrix is symmetric, the vector form of only the upper triangular part of the matrix is used as input to the fog-pass filtering module.</p><p>Let I a and I b be a pair of images from the mini-batch and F l denote the fog-pass filter attached to the l th layer of the segmentation network. Then the fog factors of the two images are computed by f a,l = F l (u a,l ) and f b,l = F l (u b,l ), respectively, where u a,l and u b,l denote the vectorized upper triangular parts of the Gram matrices computed from their l th intermediate feature maps. The role of the fog-pass filter is to inform the segmentation network how I a and I b are different in terms of fog condition through f a,l and f b,l . For this purpose, the fog-pass filter learns a space of fog factors where those of the same fog domain are grouped closely together and those of different domains are far from each other. Given the set of every image pair P in the mini-batch, the loss function for F l is designed as follows:</p><formula xml:id="formula_0">L F l = (a,b)?P 1 ? I(a, b) m ? d f a,l , f b,l 2 + +I(a, b) d f a,l , f b,l ? m 2 + ,<label>(1)</label></formula><p>where d(?) is the cosine distance, m is a margin, and I(a, b) denotes the indicator function that returns 1 if I a and I b are of the same fog domain and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Segmentation Network</head><p>The segmentation network is trained using three different objectives, which are designed for semantic segmentation, fog-invariant feature learning, and consistent prediction regardless of fog condition of input, respectively. We elaborate on each of the loss functions below. Segmentation Loss. For learning semantic segmentation, we apply the pixel-wise cross-entropy loss to individual images. To be specific, the loss is given by</p><formula xml:id="formula_1">L seg (P, Y) = ? 1 n i j Y i,j log P i,j ,<label>(2)</label></formula><p>where P i,j ? R and Y i,j ? {0, 1} denote the predicted score and groundtruth label of class j at pixel i, respectively, while n is the number of pixels. Fog Style Matching Loss. Given a pair of images from different fog domains, the segmentation network learns foginvariant features that close the distance between their fog factors. To this end, the second loss matches the two fog factors given by the frozen fog-pass filters. Let f a,l i and f b,l i be the fog factors of the images computed by the fog-pass filter F l . Then the loss is given by</p><formula xml:id="formula_2">L l fsm (f a,l , f b,l ) = 1 4d 2 l n 2 l d l i=1 f a,l i ? f b,l i 2 ,<label>(3)</label></formula><p>where d l and n l denote the dimension of their fog factors and the spatial size of the l th feature map, respectively. Prediction Consistency Loss. A CW image and its SF counterpart have exactly the same semantic layout. By forcing predictions for these images being identical, we can  align the CW and SF domains more aggressively in the learned representation. Hence, only for CW and SF images of the same origin, we encourage the model to predict the same segmentation map. Let P CW i ? R c and P SF i ? R c denote their class probability vectors predicted by the segmentation model for pixel i, where c is the number of classes. The third loss is designed to force the consistency between P CW i and P SF i for all pixels, and is given by</p><formula xml:id="formula_3">L con (P CW , P SF ) = i KLdiv(P CW i , P SF i ),<label>(4)</label></formula><p>where KLdiv(?, ?) is the Kullback-Leibler divergence. This loss shares the same goal with the fog style matching loss in Eq. <ref type="formula" target="#formula_2">(3)</ref>, but more strongly forces fog-invariance in the prediction level via a small number of CW-SF pairs. Also, it is complementary to the segmentation loss in Eq. (2) since the probability distributions in Eq. (4) provide information beyond the class labels used by the segmentation loss. Training Strategy. Given a mini-batch, the same number of image pairs are sampled from each of the three different domain pairs, i.e., CW-SF, CW-RF, and SF-RF. Note that images of each CW-SF pair are of the same semantic layout so that the prediction consistency loss is applied to them. For CW-SF pairs, the segmentation network is trained by minimizing</p><formula xml:id="formula_4">L CW-SF S = L seg (P CW , Y CW ) + L seg (P SF , Y SF ) + ? fsm l L l fsm (f CW,l , f SF,l ) + ? con L con (P CW , P SF ),<label>(5)</label></formula><p>where ? fsm and ? con are balancing hyper-parameters and Y CW = Y SF . On the other hand, for the other pairs of input domains including RF, the loss consists of the segmentation and fog style matching terms only, and is given by</p><formula xml:id="formula_5">L D-RF S = L seg (P D , Y D ) + ? fsm l L l fsm (f D,l , f RF,l ),<label>(6)</label></formula><p>where D ? {CW, SF}. Note that L seg is not applied to the prediction for real foggy image P RF due to the absence of its segmentation label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Empirical Verification</head><p>Impact of Fog-pass Filtering Modules. To justify the use of the fog-pass filters, we compare Gram matrices and their fog factors computed by a fog-pass filter in how well they disentangle the fog condition and the other aspects of an image. To this end, we examine distributions of Gram matrices and fog factors of CW, SF, and RF. To be specific, training images of the Cityscapes dataset <ref type="bibr" target="#b6">[7]</ref>, their synthetic foggy counterparts given by the fog simulator <ref type="bibr" target="#b7">[8]</ref> with the attenuation coefficient ? = 0.005, and those of the Foggy Zurich dataset <ref type="bibr" target="#b51">[52]</ref> are adopted as CW, SF, and RF images, respectively; their Gram matrices are computed from ResBlock1 outputs of RefineNet-lw <ref type="bibr" target="#b46">[47]</ref> pre-trained on the Cityscapes, and the corresponding fog factors are computed from the Gram matrices through the fog-pass filtering module. <ref type="figure" target="#fig_3">Fig. 4</ref>(a) presents t-SNE visualization <ref type="bibr" target="#b61">[62]</ref> of the distributions, in which Gram matrices of CW and SF largely overlap each other while fog factors are well separated according to their fog domains. This result suggests that Gram matrices are affected substantially by image content while fog factors represent only fog-relevant information as desired. The same trend is observed in <ref type="figure" target="#fig_3">Fig. 4</ref>(b) that quantitatively evaluates the quality of k-means clusters <ref type="bibr" target="#b20">[21]</ref> of the Gram matrices and fog factors via adjusted Rand index <ref type="bibr" target="#b29">[30]</ref>. Fog-invariance Learned by FIFO. To investigate the impact of FIFO on fog-invariance learning, we first demonstrate that FIFO effectively reduces the gap between fog domains in the space of fog factors. To this end, each domain is represented as the set of fog factors of its images, and the gap between a pair of domains is measured by the average Hausdorff distance <ref type="bibr" target="#b8">[9]</ref> between such sets of the domains before and after training with FIFO. <ref type="figure" target="#fig_3">Fig. 4(c)</ref> shows that FIFO closes the fog-style gap in all three domain pairs. The impact is also verified qualitatively through images re- network trained by FIFO. <ref type="bibr" target="#b0">1</ref> As a reconstruction model, we adopt RefineNet-lw with two additional upsampling layers; its decoder is first trained to reconstruct images while freezing its encoder pretrained on the Cityscapes dataset, then the encoder is replaced with that of the segmentation network trained by FIFO. Also, for comparisons, we reconstruct images using a baseline (i.e., training only on the Cityscapes) and a variant of FIFO with no fog-pass filter (i.e., training by directly reducing the gap between Gram matrices) in the same manner. <ref type="figure" target="#fig_4">Fig. 5</ref> presents examples of the reconstructed images, which demonstrate that FIFO allows to sharpen images effectively, well emphasize object boundaries in particular, and make fewer artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Network Architecture. We adopt RefineNet-lw <ref type="bibr" target="#b46">[47]</ref> with ResNet-101 <ref type="bibr" target="#b22">[23]</ref> backbone as our segmentation network. Two fog-pass filtering modules are then respectively attached to the outputs of Conv1 and ResBlock1 layers of the segmentation network. As illustrated in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, the two modules are implemented by multi-layer perceptrons with leaky ReLU activation functions <ref type="bibr" target="#b44">[45]</ref>.</p><p>Optimization and Hyper-parameters. The segmentation network is trained by SGD with a momentum of 0.9 and the initial learning rate of 6e?4 for the encoder and 6e?3 for the decoder; both learning rates are decreased by polynomial decay with a power of 0.5. The two fog-pass filtering modules are trained by Adamax <ref type="bibr" target="#b34">[35]</ref> with initial learning rates of 5e?4 (Conv1) and 1e?3 (ResBlock1), respectively; the dimensionality of fog factors is set to 64. Each mini-batch is constructed by sampling 4 images from each fog domain, thus its size is 12. During training, images are resized, cropped to 600 ? 600, and flipped horizontally at random. Finally, the hyper-parameters ? fsm , ? con , and m are set to 5e?8, 1e?4, and 0.1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets for Evaluation</head><p>FIFO is evaluated and compared with previous work on three real foggy datasets: Foggy Zurich (FZ) test v2 <ref type="bibr" target="#b51">[52]</ref>, Foggy Driving (FD) <ref type="bibr" target="#b52">[53]</ref>, and Foggy Driving Dense (FDD) <ref type="bibr" target="#b51">[52]</ref>; FDD is a subset of FD. Images of these datasets depict various fog densities and are fully annotated. Also, they share the same class set with the Cityscapes dataset as described in <ref type="bibr" target="#b7">[8]</ref>. We further apply FIFO and previous work to an unseen clear weather dataset, Cityscapes lindau 40 introduced in <ref type="bibr" target="#b7">[8]</ref>, to evaluate their performance on clear weather scenes as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative Analysis</head><p>Quantitative results of FIFO and previous arts are summarized in <ref type="table">Table 1</ref>. As shown in the table, FIFO largely outperforms CMAda3+ <ref type="bibr" target="#b7">[8]</ref>, the current best performing model based on RefineNet backbone <ref type="bibr" target="#b39">[40]</ref>, on all the three foggy image datasets. These results indicate that our method of closing the fog style gap is superior to the curriculum adaptation on real images with various fog densities.</p><p>We also validate the performance of the models on the clear weather dataset. In this experiment, the accuracy of CMAda3+ drops substantially; we suspect this is a side effect of the curriculum adaptation, which may lead to overfitting to foggy scenes due to catastrophic forgetting. On the other hand, FIFO enhances performance on clear weather, probably because of the data augmentation effects of using the three domains altogether during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Results</head><p>FIFO is qualitatively compared with two other methods. One is RefineNet-lw trained only on the Cityscapes dataset, which we call baseline. The other is a reduced version of FIFO using no fog-pass filtering module; this method learns the segmentation network while closing the gap between Gram matrices from different domains. Qualitative examples of their predictions are presented in <ref type="figure" target="#fig_5">Fig. 6</ref>. The baseline yields poor results in most cases. The reduced version of FIFO outperforms the baseline, especially for car, road, and vegetation classes, but FIFO using the fog-pass filtering modules clearly demonstrates the best segmentation results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with UDA</head><p>The task of FIFO is ostensibly identical to that of unsupervised domain adaptation (UDA) since both of them adapt models to an unlabeled target domain. Hence, one may wonder how well UDA models work for semantic foggy scene segmentation under the setting of FIFO. In this context, FIFO is compared with multiple UDA methods based on various levels of adaptation: FDA <ref type="bibr" target="#b66">[67]</ref> for input-level adaptation, AdSegNet <ref type="bibr" target="#b58">[59]</ref> and AdvEnt <ref type="bibr" target="#b63">[64]</ref> for output-level adaptation, and DANN <ref type="bibr" target="#b16">[17]</ref> for feature-level adaptation. We also evaluate a variant of DANN whose domain classifier takes as input a Gram matrix, instead of a raw feature map, like FIFO; this variant is denoted by DANN-Gram. The UDA models are trained using the same datasets, i.e., CW, SF, and RF. The input-and output-level adaptation models are pretrained on CW and further trained using SF while adapting to RF. DANN and DANN-Gram are trained using CW, SF, and RF at once like FIFO: Their discriminators are optimized to maximize the discrepancy of fog domains like our fog-pass filters while their segmentation networks are learned to minimize the discrepancy.</p><p>The performance of these UDA methods is reported in <ref type="table">Table 1</ref>. As shown in the table, the UDA models are all inferior to FIFO and CMAda3+, which suggests that, even though it is apparently similar to UDA for semantic segmentation, semantic foggy scene segmentation is of a different nature and has its own challenges. In the typical UDA setting, each source and target domain has its own style, by which it can be defined. However, the style of a foggy scene is not determined only by its fog condition, rather is the result of the chemical combination of fog and other style factors of the scene. The UDA methods that consider SF and RF as domains with unique styles are thus not well suited to the task. Moreover, fog substantially damages visibility, and enlarges intra-domain variations since the effect of fog varies significantly according to the 3D configuration of the scene <ref type="bibr" target="#b7">[8]</ref>. Due to these differences and challenges, foggy scene segmentation demands dedicated solutions like FIFO.   <ref type="table">Table 4</ref>. Analysis on the impact of synthetic fog densities. ? = 0 denotes a model trained with CW and RF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CW-SF CW-RF SF-RF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Study</head><p>We conduct extensive experiments while varying domain pairs of FIFO to investigate their effects. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results. We found that using more pairs in general boosts performance, which suggests that all the three pairs contribute to performance on real foggy images.</p><p>We also investigate contributions of the fog style matching loss L fsm , the prediction consistency loss L con , and the fog-pass filtering modules to the performance. <ref type="table" target="#tab_3">Table 3</ref> compares FIFO with its variants with and without L fsm , L con , and the fog-pass filters in terms of segmentation quality on real foggy images. Note that FIFO w/o L fsm is trained by the segmentation and prediction consistency losses only, dropping the fog style matching loss, while FIFO w/ Gram uses Gram matrices instead of fog factors when matching fog styles. The results in the tables suggest that all the losses and the fog-pass filtering contribute to the performance on all the three real foggy datasets, but the impact of the fog style matching is substantially larger than the others. Also, the gap between FIFO and FIFO w/ Gram demonstrates the superiority of fog factors over Gram matrices, which justifies the use of the fog-pass filters.</p><p>In addition, we demonstrate the effect of our chosen value of ?, which is the attenuation coefficient used for generating synthetic fog <ref type="bibr" target="#b52">[53]</ref>. Note that our method exploits a single value of ? to resolve the issue of the curriculum adaptation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref>. <ref type="table">Table 4</ref> summarizes the performance of variants of FIFO trained using different values of ?; the optimal value for ? is 0.005.</p><p>Finally, we examine the impact of the layers to which  the fog style matching loss L f sm is applied. Specifically, L f sm is applied to the output of the first convolutional layer (C1), the first residual block (R1), or both of them (C1+R1). As summarized in <ref type="table">Table 5</ref>, the performance improves as more feature maps are affected by the loss. Overall, C1+R1, which is our final model, shows the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Generalization to Other Weather Conditions</head><p>We investigate the generalization ability of FIFO on other weather conditions, rain and frost. To this end, RefineNet-lw trained by FIFO in Sec. 4 is evaluated asis on rainy <ref type="bibr" target="#b27">[28]</ref> and frosty <ref type="bibr" target="#b23">[24]</ref> versions of the Cityscapes dataset. On the rainy Cityscapes dataset, our model is compared with existing methods reported in <ref type="bibr" target="#b13">[14]</ref> that aim at improving robustness using CW images only. As summarized in <ref type="table">Table 6</ref>, our model largely outperforms the previous work based on a stronger segmentation network (i.e., DeepLab v3+ <ref type="bibr" target="#b3">[4]</ref>, indicated by ? in the table). <ref type="figure" target="#fig_6">Fig. 7</ref> demonstrates that FIFO generalizes to the frosty images also. More results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented FIFO, a new approach to learning fog-invariant features for foggy scene segmentation. It precisely quantifies the fog style of an image through the fogpass filtering modules and learns a segmentation network for closing the gap between images of different fog conditions in the fog style space. FIFO outperforms previous arts without sacrificing performance on clear weather images. Moreover, unlike the current best-performing method, it enables end-to-end learning and demands no extra module nor human intervention for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm of FIFO</head><p>We present the training procedure of FIFO in Algorithm 1. for {l ?? 1 to L} do 4:</p><formula xml:id="formula_6">L F l ?? L F l ({f l i } m i=1 ) 5:</formula><p>Update the fog-pass filtering module for {l ?? 1 to L} do 10: L S ?? l L l fsm + L con + L seg 21:</p><formula xml:id="formula_7">{f a,l j } m j=1 , ?? {F l (u a,l j )} m j=1 11: {f b,l j } m j=1 , ?? {F l (u b,l j )} m j=1 12: L l fsm ?? {L l fsm (f a,l j , f b,l j )} m<label>j=1 13</label></formula><p>Update the segmentation network S 22: end for Consequently, the total objective of FIFO is following:</p><formula xml:id="formula_8">l min F l L l F l + min S ( l L l fsm + L con + L seg ),<label>(7)</label></formula><p>where l is the layer index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization to Other Weather Conditions</head><p>We investigate the generalization ability of FIFO on the other weather conditions, rainy <ref type="bibr" target="#b27">[28]</ref> and frosty <ref type="bibr" target="#b23">[24]</ref> versions of the Cityscapes <ref type="bibr" target="#b6">[7]</ref> dataset, according to the severity of the corruptions. <ref type="figure" target="#fig_0">Figure A1</ref> presents the performance of baseline <ref type="bibr" target="#b46">[47]</ref>, an ordinary segmentation model trained on clear weather images, and FIFO on varying the severity of frosty and rainy corruptions. FIFO tends to be robust to each corruption than the baseline, even when the corruption gets severe. <ref type="table" target="#tab_2">Table A1 and Table A2</ref> show detailed quantitative results of baseline and FIFO on frosty and rainy corruptions, respectively. Additional qualitative results are presented in <ref type="figure" target="#fig_6">Figure A7</ref>.</p><p>We also evaluate FIFO on ACDC, the real-world adverse conditions dataset for semantic driving scene understanding. For fair comparisons on ACDC <ref type="bibr" target="#b53">[54]</ref>, FIFO is trained on the Cityscapes, Foggy Cityscapes-DBF, and Foggy Zurich datasets, following the unsupervised learning setting of the benchmark. As summarized in <ref type="table">Table.</ref> A3, FIFO outperforms the existing foggy scene segmentation methods reported in <ref type="bibr" target="#b53">[54]</ref> for all four conditions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Independence Analysis of Fog Factors</head><p>In this section, we quantitatively evaluate the independence of the fog factors from the image content compared to that of the Gram matrices from the content. To this end, we design a content-pass filtering module that is optimized to extract content-relevant information, which we call content factors. Training Content-pass Filtering Module. Let I a and I b be a pair of images from the mini-batch, and C l denote the content-pass filtering module attached to the l th layer of the segmentation network. Let u a,l and u b,l be the vectorized upper triangular parts of the Gram matrices computed from the l th feature maps of I a and I b . Then the content factors of the two images are computed by c a,l = C l (u a,l ) and c b,l = C l (u b,l ). In contrast to the fog-pass filtering module, this module is optimized to learn an embedding space of content factors where the pairs having the same content, i.e., CW-SF are grouped closely and else pairs are far from each other. Given the set of every image pair P in the mini-batch, the loss function for C l is designed accordingly as follows:</p><formula xml:id="formula_9">L C l = (a,b)?P 1 ? I(a, b) m ? d f a,l , f b,l 2 + +I(a, b) d f a,l , f b,l ? m 2 + ,<label>(8)</label></formula><p>where d(?) is the cosine distance, m is a margin, and I(a, b) denotes the indicator function that returns 1 if the pair of I a and I b is a CW-SF pair and 0 otherwise, respectively. Independence Analysis of Fog Factors. We design the independence score to quantitatively evaluate and compare the independence of fog factors and that of Gram matrices from content factors. We first measure the score of the independence of fog factors from content factors. To this end, we select one image I i , then choose k images {I n } whose fog factors are most similar to the fog factor f i of the selected image I i . Then, we also choose k images {I m } whose content factors are most similar to the content factor c i of the selected image I i . After that, we compute the proportion of the number of overlapped images |{I n } ? {I m }| between {I n } and {I m }. Then, we repeat the process for all N images and calculate the average proportion as the independence score.</p><p>Let I, f , and c be an image, a fog factor, and a content factor, then, the independence score is calculated as follows:</p><formula xml:id="formula_10">IndependenceScore(F, C) = 1 ? 1 N N i=1 1 k {I n |f n ? F, d(f i , f n ) ? d(f i , f k )} ?{I m |c m ? C, d(c i , c m ) ? d(c i , c k )} ,<label>(9)</label></formula><p>where d(?) and k are a cosine distance and a number of selecting similar factors set to 200, f k and c k are the k th most similar fog factor from f i and the k th most similar content factor from c i , where F and C denote the set of fog factors and content factors, respectively. We then replace the fog factors with Gram matrices, then repeat the same process for calculating the independence score of Gram matrices from the content factors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDEPENDENCE SCORE</head><p>Gram matrices fog factors <ref type="figure" target="#fig_1">Figure A2</ref>. Independence score of fog factors and Gram matrices from content factors. <ref type="figure" target="#fig_1">Figure A2</ref> presents the independent score of fog factors and Gram matrices from content factors. Note that the experiment settings and dataset configurations are all the same as in the main paper. <ref type="figure" target="#fig_1">Figure A2</ref> proves that fog factors are more independent to content factors compared to Gram matrices, as desired. It indicates that the fog-pass filtering module extracts only fog-relevant information apart from the image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of Fog Style Matching Loss</head><p>This section conducts extensive experiments to investigate the effect of the fog style matching loss L f sm . In FIFO, the fog style matching loss L f sm is carried out by bidirectionally matching each fog condition (i.e., CW, SF, and RF), so we denote it as a 'Bidirectional' setting in this section. We conduct additional experiments about variants of the fog style matching loss L f sm in the 'Unidirectional' setting (from Fog to Clear, from Clear to Fog). For 'Fog to Clear' settings, fog styles of real foggy images are unidirectionally matched to those of clear weather images, which is regarded as feature-level dehazing on real foggy images. This is implemented simply by detaching the gradient flows from the fog style matching loss L f sm to clear weather images. For 'Clear to Fog' settings, fog styles of clear weather images are unidirectionally matched to real foggy images similar to feature-level fog synthesis on clear weather images. This is also implemented by detaching the gradient from the fog style matching loss L f sm to real foggy images. <ref type="table" target="#tab_7">Table A4</ref> summarizes the results. We found that the bidirectional fog style matching outperforms its unidirectional counterpart when the same domain pairs are involved; this result justifies the fog style matching loss in FIFO. In addition, unidirectional (Fog to Clear) models have superior performance on the clear weather dataset <ref type="bibr" target="#b7">[8]</ref> compared to others due to the effect of focusing on clear weather conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Impact of Fog Factors</head><p>We present additional comparison results for the quality of k-means clustering <ref type="bibr" target="#b20">[21]</ref> of the Gram matrices and that of the corresponding fog factors in other measures, normalized mutual information <ref type="bibr" target="#b10">[11]</ref>, and adjusted mutual information <ref type="bibr" target="#b62">[63]</ref>. All of the measures prove the impact of fog factors in that they are more clustered than Gram matrices according to each fog condition as shown in <ref type="figure" target="#fig_2">Figure A3</ref> and <ref type="table" target="#tab_8">Table A5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Empirical Verification Using Evaluation Dataset</head><p>We present additional results of the empirical verification using evaluation splits of the datasets, i.e., Cityscapes (500 images) as CW, Foggy Cityscapes-DBF (500 images) as SF, and Foggy Zurich-test v2 (40 images) and Foggy Driving (101 images) as RF. <ref type="figure" target="#fig_3">Fig. A4</ref> shows that the tendency of the results is consistent with that of <ref type="figure" target="#fig_3">Fig. 4</ref> of the main paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SF-RF CW-RF CW-SF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVERAGE HAUSDORFF DISTANCE</head><p>Before After </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Generalization on Deep Features</head><p>This section empirically investigates the generalization of fog-invariant learning of our method on deep features. It has been reported in domain adaptation and generalization literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b69">70]</ref> that domain alignment at bottom layers closes the domain gap of deeper layer features. As shown in <ref type="figure" target="#fig_4">Fig. A5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SF-RF CW-RF CW-SF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVERAGE HAUSDORFF DISTANCE (RES BLOCK 4)</head><p>Before After <ref type="figure" target="#fig_4">Figure A5</ref>. Distances between sets of deep features from different domains before and after training with FIFO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Comparison with UDA</head><p>In this section, we discuss the reason for failure when UDA methods are applied to the foggy scene segmentation in <ref type="table">Table 1</ref> of the main paper. Analysis on the failure of FDA. We suspect this is because the style representation of Fourier domain adaptation (FDA) is not suitable for handling foggy scenes: FDA considers the low-frequency spectrum of an image as its style, but in the case of a foggy image, both its style and content lie in its low-frequency spectrum. We verify this via qualitative results of the spectral style transfer, an intermediate step in FDA. <ref type="figure" target="#fig_5">Fig. A6</ref> presents examples of the style transfer from CW to RF and from RF to CW. The former causes severe artifacts on RF as the content CW as well as its style is transferred. On the other hand, the latter applies fog effects to CW, but the result is not realistic. Superiority of FIFO over Domain Adversarial Learning. First, FIFO minimizes the entire objective function as presented in Section A while domain adversarial learning optimizes the min-max loss. Hence, FIFO does not suffer from the instability issue of adversarial learning in training. Second, FIFO can model and exploit within-domain fog style variations better than the domain adversarial learning (e.g., DANN). This is crucial since images of the same fog condition have different fog styles in general. FIFO achieves this property by the losses in Eq. (1) and Eq. (3) of the main paper; the former motivated by metric learning enables the fog-pass filter to learn within-domain fog style variations, and the latter enables the segmentation model to keep such variations while closing style gaps only between different fog domains. Accordingly, thanks to the superiority of FIFO over domain adversarial learning, FIFO clearly outperforms DANN in <ref type="table">Table 1</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CW RF</head><p>CW ? RF RF ? CW <ref type="figure" target="#fig_5">Figure A6</ref>. Outputs of spectral style transfer in FDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Comparison with Variants of CMAda</head><p>This section presents the comparison of FIFO to the variants of CMAda reported in <ref type="bibr" target="#b7">[8]</ref>, which suggests the current best performing model, CMAda3+. <ref type="table">Table A6</ref> demonstrates the superiority of FIFO over the variants of CMAda. In Table A6, CMAda models denoted '+' are conducted the additional procedure of fog densification for making the fog density of real foggy training images similar to target fog density of the test real foggy images. FIFO outperforms all variants of CMAda regardless of their number of stages and densification procedure.  <ref type="table">Table A6</ref>. Comparison of FIFO to variants of CMAda. '+' denotes models applied the additional procedure of fog densification for real foggy training datasets. The numbers attached to CMAda means the number of stages for curriculum learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Additional Qualitative Results</head><p>This section presents additional qualitative results omitted in the main sections due to the space limit. More segmentation results of FIFO are illustrated in <ref type="figure" target="#fig_14">Figure A8</ref>. We compare the results between FIFO, a variant of FIFO, by directly reducing the gap between Gram matrices and baseline. Overall, FIFO offers higher quality segmentation results than the baseline regardless of fog density and datasets. Specifically, FIFO seems best performing on parts where dense fog is laid while other models fail, which indicates FIFO working as desired. <ref type="figure" target="#fig_15">Figure A9</ref> exihibits additional qualitative results on image reconstruction. Likewise, the image quality where dense fog is laid is improved, which implies FIFO extract fog-invariant features. In addition, clear weather images, as well as foggy images, become more clear when the features are trained by FIFO.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A summary of our results. Predictions of FIFO are accurate for both clear weather and real foggy images while the baseline, an ordinary segmentation model trained on clear weather images, fails to handle foggy images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall pipeline of FIFO. For each iteration of training, the fog-pass filtering module and the segmentation network are updated alternately. (top) Given Gram matrices of feature maps of the segmentation network as input, the fog-pass filtering module learns to extract fog factors so that fog conditions of images are discriminated by their fog factors. (bottom) The segmentation network is trained by reducing the gap between fog factors of images with different fog conditions as well as by the segmentation loss. Note that the fog-pass filters are auxiliary modules used only in training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>A schematic of FIFO. (a) The fog-pass filtering modules. Each of them takes as input the upper triangular part of a Gram matrix and returns a fog factor. The loss pulls or pushes a pair of fog factors according to the equivalence of their fog conditions. (b) Training of the segmentation network with the frozen fog-pass filters. Given a pair of images with different fog conditions as input, the network is trained by closing the gap between their fog factors and that between their segmentation predictions as well as by the ordinary segmentation loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Empirical analysis on the impact of FIFO. (a) 2D visualization of distributions of Gram matrices and their fog factors. (b) Comparison between the quality of k-means clustering of the Gram matrices and that of the corresponding fog factors in adjusted Rand index. (c) The fog-style gap between different domains before and after training with FIFO, where the gap is measured by the average Hausdorff distance between two sets of fog factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIFOFigure 5 .</head><label>5</label><figDesc>constructed from intermediate features of the segmentation Baseline Images reconstructed by the baseline, a variant of FIFO closing the gap between Gram matrices, and FIFO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results on the real foggy datasets. (a) Input images. (b) Baseline. (c) FIFO without the fog-pass filtering. (d) FIFO. (e) Groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results under rain and frost conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 :</head><label>1</label><figDesc>Training FIFO Input: Pretrained fog-pass filtering module for the l th layer: F l (?), Segmentation network: S(?), Number of layers: L, Batch size per domain: m, Segmentation prediction: P , Segmentation label: Y , Input image set {I CW , I SF , I RF }: x, Subset of two elements from domain set {CW,SF,RF}: {a, b} and Segmentation label set {Y CW , Y RF }: y. Output: Optimized segmentation network S(?). 1: for {1, . . . , # of training iterations} do 2: Sample mini-batch {x i } m i=1 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>F l 6 : end for 7 : 8 :</head><label>678</label><figDesc>Sample mini-batch {x j } m j=1 and {y j } m j=1Sample the pair {I a , I b } ? x j 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A1 .</head><label>A1</label><figDesc>Performance (mIoU) versus the corruption severity. Ours (FIFO) and baseline are evaluated on Frosty Cityscapes and Rainy Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A3 .</head><label>A3</label><figDesc>Comparison between fog factors and Gram matrices for the quality of k-means clustering by normalized mutual information and adjusted mutual information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A4 .</head><label>A4</label><figDesc>Results of the empirical verification using the evaluation datasets. (a) t-SNE visualization of distributions of Gram matrices and their fog factors. (b) Comparison between the quality of k-means clustering of the fog factors and Gram matrices in adjusted Rand index. (c) The fog-style gap between different domains before and after training with FIFO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A7 .</head><label>A7</label><figDesc>Additional qualitative results on frost and rain weather corruptions. (a) Weather corrupted input images. (b) Baseline. (c) FIFO. (d) Groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure A8 .</head><label>A8</label><figDesc>Additional qualitative results on the real foggy datasets. (a) Real foggy images. (b) Baseline. (c) Reduced version of FIFO closing the gap between gram matrices. (d) FIFO. (e) Groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure A9 .</head><label>A9</label><figDesc>Additional qualitative results on image reconstruction. (a) Real foggy images. (b) Baseline. (c) Reduced version of FIFO closing the gap between gram matrices. (d) FIFO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Clear Weather Synthetic Fog Real Fog (FZ v2)</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ADJUSTED RAND INDEX</cell><cell cols="4">AVERAGE HAUSDORFF DISTANCE</cell></row><row><cell></cell><cell>0.6 0.7 0.8 0.9 1</cell><cell cols="2">1000 iter 3000 iter 5000 iter</cell><cell>1.40e+02</cell><cell>5.35e+01</cell><cell>1.62e+02</cell><cell>7.92e+01</cell><cell>7.85e-01 Before 2.24e-01 After</cell></row><row><cell></cell><cell></cell><cell>Gram matrices</cell><cell>Fog factors</cell><cell cols="4">SF-RF CW-RF CW-SF</cell></row><row><cell>Gram matrices</cell><cell>Fog factors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Analysis on the impact of domain pairs. CW, SF and RF denote clear weather, synthetic fog, and real fog, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>FZ</cell><cell>FDD</cell><cell>FD</cell><cell>CW</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell cols="2">43.7 38.6 46.1 67.6</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell cols="2">37.7 40.3 47.2 66.0</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell cols="2">39.3 42.8 49.7 61.6</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell cols="2">49.7 46.0 49.9 65.8</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell cols="2">46.0 47.6 50.0 62.3</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell cols="2">47.4 38.2 47.0 64.3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">48.4 48.9 50.7 64.8</cell></row><row><cell>Method</cell><cell cols="2">FZ FDD FD</cell><cell></cell></row><row><cell>Baseline</cell><cell cols="2">28.5 35.9 43.6</cell><cell></cell></row><row><cell cols="3">FIFO w/o L fsm 31.7 38.5 45.1</cell><cell></cell></row><row><cell cols="3">FIFO w/o L con 41.6 45.4 48.9</cell><cell></cell></row><row><cell cols="3">FIFO w/ Gram 41.3 43.8 49.1</cell><cell></cell></row><row><cell>FIFO</cell><cell cols="2">48.4 48.9 50.7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Analysis on the impact of the fog style matching loss, the prediction consistency loss, and the fog-pass filtering modules.</figDesc><table><row><cell>?</cell><cell>FZ FDD FD</cell></row><row><cell>0</cell><cell>37.7 40.3 47.2</cell></row><row><cell cols="2">0.0025 45.5 40.4 45.0</cell></row><row><cell>0.01</cell><cell>42.4 45.4 50.0</cell></row><row><cell>0.02</cell><cell>42.9 42.5 48.6</cell></row><row><cell>0.005</cell><cell>48.4 48.9 50.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Ours) 48.4 48.9 50.7 Analysis on the layers to which the fog style matching loss is applied. C1 and R1 indicate the output of the first convolution layer and that of the first residual block, respectively. Quantitative results on Rainy Cityscapes (RC).</figDesc><table><row><cell cols="4">FZ 45.3 41.0 48.3 FDD FD 45.1 39.1 47.0 C1+R1 (Method C1 R1  ? Baseline [25]  ? Cutmix [16]  ? LP-BNN [15]  ? Superpixel-mix [14] 61.9 RC 59.0 61.9 60.7</cell></row><row><cell></cell><cell cols="2">Baseline [47]</cell><cell>57.6</cell></row><row><cell></cell><cell>FIFO</cell><cell></cell><cell>67.6</cell></row><row><cell>Input Image</cell><cell>FIFO</cell><cell>Groundtruth</cell></row><row><cell>Frost Rain</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A3</head><label>A3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Corruption Severity</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Frost</cell><cell cols="5">Baseline 45.53 23.59 14.97 13.60 10.66 FIFO 46.85 30.64 22.66 20.88 17.47</cell></row><row><cell cols="6">Table A1. Quantitative results on Frosty Cityscapes according to</cell></row><row><cell cols="3">the severity of corruptions.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Corruption Severity</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>all</cell></row><row><cell cols="2">Rain</cell><cell cols="4">Baseline 64.03 60.51 54.62 57.60 FIFO 69.01 68.03 65.92 67.62</cell></row><row><cell cols="6">Table A2. Quantitative results on Rainy Cityscapes according to</cell></row><row><cell cols="3">the severity of corruptions.</cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell cols="4">Fog Rain Snow Night Avg.</cell></row><row><cell cols="3">RefineNet 46.4 52.6</cell><cell>43.3</cell><cell>29.0</cell><cell>43.7</cell></row><row><cell cols="3">SFSU [2] 45.6 51.6</cell><cell>41.4</cell><cell>29.5</cell><cell>42.9</cell></row><row><cell cols="2">CMAda</cell><cell>51.2 53.4</cell><cell>47.6</cell><cell>32.0</cell><cell>47.1</cell></row><row><cell cols="2">FIFO</cell><cell>54.1 58.8</cell><cell>51.8</cell><cell>32.5</cell><cell>49.4</cell></row></table><note>. Quantitative results on the ACDC dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A4 .</head><label>A4</label><figDesc>SF CW &amp; RF SF &amp; RF mIoU (%) mIoU (%) mIoU (%) mIoU (%) CW ? SF CW ? RF SF ? RF CW ? SF CW ? RF SF ? RFAnalysis on the impact of the fog style matching loss. CW, SF, and RF denote clear weather, synthetic fog, and real fog, respectively.</figDesc><table><row><cell>Method</cell><cell>Image Pair</cell><cell></cell><cell>FZ</cell><cell>FDD</cell><cell>FD</cell><cell>C-Lindau</cell></row><row><cell>1 pair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Unidirectional (Fog to Clear)</cell><cell>CW ? RF</cell><cell></cell><cell>38.5</cell><cell>36.3</cell><cell>45.6</cell><cell>67.1</cell></row><row><cell>Unidirectional (Clear to Fog)</cell><cell>CW ? RF</cell><cell></cell><cell>36.6</cell><cell>36.9</cell><cell>46.2</cell><cell>64.7</cell></row><row><cell>Bidirectional</cell><cell>CW ? RF</cell><cell></cell><cell>37.7</cell><cell>40.3</cell><cell>47.2</cell><cell>66.0</cell></row><row><cell>2 pairs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Unidirectional (Fog to Clear) CW ? SF</cell><cell>SF ? RF</cell><cell>43.3</cell><cell>39.2</cell><cell>48.8</cell><cell>68</cell></row><row><cell cols="2">Unidirectional (Clear to Fog) CW ? SF</cell><cell>SF ? RF</cell><cell>43.4</cell><cell>39.3</cell><cell>48.4</cell><cell>63.3</cell></row><row><cell>Bidirectional</cell><cell>CW ? SF</cell><cell>SF ? RF</cell><cell>46.0</cell><cell>47.6</cell><cell>50.0</cell><cell>62.3</cell></row><row><cell>3 pairs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Unidirectional (Fog to Clear) 44.4</cell><cell>42.6</cell><cell>47.1</cell><cell>68.8</cell></row><row><cell cols="4">Unidirectional (Clear to Fog) 44.1</cell><cell>36.5</cell><cell>46</cell><cell>64.5</cell></row><row><cell>Bidirectional (FIFO)</cell><cell cols="2">CW ? SF CW ? RF SF ? RF</cell><cell>48.4</cell><cell>48.9</cell><cell>50.7</cell><cell>64.8</cell></row></table><note>CW &amp;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A5 .</head><label>A5</label><figDesc>Quantitative results of the quality of k-means clusters of the Gram matrices and fog factors in normalized mutual information, adjusted mutual information, and adjusted Rand index<ref type="bibr" target="#b29">[30]</ref>.</figDesc><table><row><cell>(a)</cell><cell>CW</cell><cell>SF</cell><cell cols="2">RF (FZ v2)</cell></row><row><cell></cell><cell cols="2">Gram matrices</cell><cell></cell><cell cols="2">Fog factors</cell></row><row><cell></cell><cell cols="2">ADJUSTED RAND INDEX</cell><cell></cell><cell></cell></row><row><cell>0.5 0.6 0.7 0.8 0.9</cell><cell></cell><cell></cell><cell>5.11e+02</cell><cell>2.29e+02</cell><cell>5.94e+02</cell><cell>2.89e+02</cell></row><row><cell>0.4</cell><cell cols="3">1000 iter 3000 iter 5000 iter</cell><cell></cell><cell>7.29</cell><cell>6.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Comparison 1000 iter 3000 iter 5000 iter</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Normalized Mutual Information</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gram matrix 0.6602</cell><cell>0.6602</cell><cell>0.6602</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fog factor</cell><cell>0.8453</cell><cell>0.9313</cell><cell>0.9387</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Adjusted Mutual Information</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gram matrix 0.6601</cell><cell>0.6601</cell><cell>0.6601</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fog factor</cell><cell>0.8452</cell><cell>0.9313</cell><cell>0.9387</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Adjusted Rand Index</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gram matrix 0.6304</cell><cell>0.6304</cell><cell>0.6304</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fog factor</cell><cell>0.8683</cell><cell>0.9533</cell><cell>0.9596</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, we empirically verify our case: The average Hausdorff distances between ResBlock4 features from different domains decrease noticeably by FIFO.</figDesc><table><row><cell>2.11</cell><cell>2.06</cell><cell></cell></row><row><cell>1.30</cell><cell>1.37</cell><cell>0.92</cell></row><row><cell></cell><cell></cell><cell>0.19</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We conduct this experiment only for qualitative analysis on the effect of FIFO. Note that FIFO directly learns fog-invariant features while bypassing explicit fog removal of input image, thus does not reconstruct images at all in both training and testing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Acknowledgement: This work was supported by Samsung Research Funding &amp; Incubation Center of Samsung Electronics under Project Number SRFC-IT1801-05.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating long-range consistency in cnn-based texture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeing through fog without seeing fog: Deep multimodal sensor fusion in unseen adverse weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bijelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Mannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pms-net: Robust haze removal based on patch map for single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Jiun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sy-Yen</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwon</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding. International Journal of Computer Vision (IJCV)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A modified hausdorff distance for object matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-P</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Pattern Recognition (ICPR)</title>
		<meeting>International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Normalized mutual information feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Est?vez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">A</forename><surname>Tesmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust semantic segmentation with superpixel-mix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nacim</forename><surname>Belkhir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><forename type="middle">Lan</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Encoding the latent posterior of bayesian neural networks for uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Aldea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?verine</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Bloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02818</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Controlling perceptual factors in neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shechtman</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Journal of the royal statistical society. series c (applied statistics)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manchek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Algorithm as 136: A k-means clustering algorithm</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth-attentional features for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongchao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arezou</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallace</forename><surname>Lira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenggen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mahdavi-Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04606</idno>
		<title level="m">Raidar: A rich annotated image dataset of rainy street scenes</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixel-level cycle association: A new perspective for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Universal style transfer via feature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01036</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Griddehazenet: Attention-based multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongrui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep photo style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Categorylevel adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lightweight refinenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Does haze removal help cnn-based image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanting</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Domain bridge for unpaired image-to-image translation and unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Zaccaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Cerri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horia</forename><surname>Porav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina-Nicoleta</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bruls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04742,2020.3</idno>
		<title level="m">Rainy screens: Collecting rainy datasets, indoors</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Informative dropout for robust representation learning: A shape-bias perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Urie: Universal image enhancement for visual recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juwon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namyup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Etnet: Error transition network for arbitrary style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Studying very low resolution recognition using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Wilddashcreating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Fernandez</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Domain generalization with mixstyle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno>2021. 12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
