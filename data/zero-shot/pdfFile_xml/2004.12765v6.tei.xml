<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computational Humor Using BERT Sentence Embedding in Parallel Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issa</forename><surname>Annamoradnejad</surname></persName>
							<email>i.moradnejad@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gohar</forename><surname>Zoghi</surname></persName>
							<email>zoghi.g@goums.ac.ir</email>
						</author>
						<title level="a" type="main">Computational Humor Using BERT Sentence Embedding in Parallel Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automation of humor detection and rating has interesting use cases in modern technologies, such as humanoid robots, chatbots, and virtual assistants. In this paper, we propose a novel approach for detecting and rating humor in short texts based on a popular linguistic theory of humor. The proposed technical method initiates by separating sentences of the given text and utilizing the BERT model to generate embeddings for each one. The embeddings are fed to separate lines of hidden layers in a neural network (one line for each sentence) to extract latent features. At last, the parallel lines are concatenated to determine the congruity and other relationships between the sentences and predict the target value. We accompany the paper with a novel dataset for humor detection consisting of 200,000 formal short texts. In addition to evaluating our work on the novel dataset, we participated in a live machine learning competition focused on rating humor in Spanish tweets. The proposed model obtained F1 scores of 0.982 and 0.869 in the humor detection experiments which outperform general and state-of-the-art models. The evaluation performed on two contrasting settings confirm the strength and robustness of the model and suggests two important factors in achieving high accuracy in the current task: 1) usage of sentence embeddings and 2) utilizing the linguistic structure of humor in designing the proposed model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Interstellar (2014 movie), a future earth is depicted where robots easily understand and use humor in their connections with their owners and humans can set the level of humor in their personal robots 2 . While we may have a long road toward the astral travels, we are very close in reaching high-quality systems injected with adjustable humor.</p><p>Humor, as a potential cause of laughter, is an important part of human communication, which not only makes people feel comfortable but also creates a cozier environment <ref type="bibr" target="#b0">[1]</ref>. Automatic humor detection in texts has interesting use cases in building human-centered artificial intelligence systems such as humanoid robots, chatbots, and virtual assistants. An appealing use case is to identify whether an input command should be taken seriously or not, which is a critical step to understanding the real motives of users, returning appropriate answers, and enhancing the overall experience of users with the AI system. A more advanced outcome would be the injection of humor into computer-generated responses, thus making the human-computer interaction more engaging and interesting <ref type="bibr" target="#b1">[2]</ref>. This is an outcome that is achievable by setting the level of humor in possible answers to the desired level, similar to the mentioned movie.</p><p>Humor can be attained through several linguistic or semantic mechanisms, such as wordplay, exaggeration, misunderstanding, and stereotype. Researchers proposed several theories to explain humor functionality as a trait, one of which is called "incongruity theory", where laughter is the result of realizing incongruity in the narrative. A general version states that a common joke consists of a few sentences that conclude with a punchline. The punchline is responsible for bringing contradiction into the story, thus making the whole text laughable. In other words, any sentence can be non-humorous in itself, but when we try to comprehend all sentences together in one context or in a single line of story, the text becomes humorous.</p><p>By focusing on this widely used structure of jokes, we believe and show that it is required to view and encode each sentence separately and capture the underlying relation between sentences in a proper way. As a result, our proposed model for the task of humor detection is based on creating parallel paths of neural network hidden layers, in addition to encoding a given text as a whole.</p><p>In short, the proposed approach initiates by separating the text into its sentences. Then, it utilizes the BERT model in order to encode each sentence and the whole text into embeddings. Next, the embeddings will be fed into parallel hidden layers of a neural network to extract latent features regarding each sentence. The last three layers combine the output of all previous lines of hidden layers to determine the relationship between the sentences to predict the final output. In theory, these final layers are responsible for determining the congruity or detecting the transformation of the reader's viewpoint after reading the punchline.</p><p>In addition to proposing a model for humor detection, we curated a large dataset for the binary task of humor detection. Previous attempts combined formal non-humorous texts with informal humorous short texts, which due to the incompatible statistics of the parts (text length, words count, etc.) makes it more likely to detect humor with simple analytical models and without understanding the underlying latent lingual features and structures. To address this problem, we applied multiple analytical pre-processing steps to trim outlier texts which resulted in a dataset that is statistically similar based on the target class.</p><p>To test the robustness and stability of the proposed model, we evaluate its performance in two different settings:</p><p>1. Evaluation on the novel dataset (short formal English texts): The model is evaluated for the binary task of humor detection on the novel dataset, and 2. Evaluation in a live machine-learning competition (Spanish informal texts): The model competes against strong real teams to detect and rate humor in informal Spanish texts (variant lengths).</p><p>We summarize our contributions as follows:</p><p>? We propose an automated approach for humor detection in texts that is based on a general theory of humor. We introduce the model architecture and components in detail. ? We introduce a new dataset for the task of humor detection, entitled the "ColBERT dataset", which contains 200k short texts (100k positive and 100k negative). We reduced or completely removed issues prevalent in the existing datasets to build a proper dataset for the task. ? We evaluate the performance of our proposed model on the novel dataset in comparison with five strong baselines. ? We further evaluate its accuracy and robustness in a data science competition for Spanish texts.</p><p>The structure of this article is as follows: Section 2 reviews past works on the task of humor detection with a focus on transfer learning methods. Section 3 describes the data collection and preparation techniques and introduces the new dataset. Section 4 elaborates on the methodology, and section 5 presents our experimental results. Section 6 is the concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head><p>With advances in NLP, researchers applied and evaluated state-of-the-art methods for the task of humor detection. This includes using statistical and N-gram analysis <ref type="bibr" target="#b2">[3]</ref>, Regression Trees <ref type="bibr" target="#b3">[4]</ref>, Word2Vec combined with K-NN Human Centric Features <ref type="bibr" target="#b4">[5]</ref>, and Convolutional Neural Networks <ref type="bibr" target="#b5">[6]</ref> [7].  Weller and Seppi <ref type="bibr" target="#b6">[7]</ref> focused on the task of humor detection by using a Transformer architecture. The work approached the task by learning on ratings taken from the popular Reddit r/Jokes thread (13884 negative and 2025 positives). Kramer <ref type="bibr" target="#b7">[8]</ref> suggest that our sense of humour is acutely aware of our flaws and tackle the problem in words of error detection.</p><p>There are emerging tasks related to humor detection. Ref <ref type="bibr" target="#b8">[9]</ref> focused on predicting humor by using audio information, hence reached 0.750 AUC by using only audio data. A good number of research is focused on the detecting humor in non-English texts, such as on Spanish <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, Chinese <ref type="bibr" target="#b8">[9]</ref>, and English-Hindi <ref type="bibr" target="#b12">[13]</ref>.</p><p>With the popularity of transfer learning, some researchers focused on using pre-trained models for several tasks of text classification. Among them, BERT <ref type="bibr" target="#b13">[14]</ref> utilizes a multi-layer bidirectional transformer encoder consisting of several encoders stacked together, which can learn deep bi-directional representations. Similar to previous transfer learning methods, it is pre-trained on unlabeled data to be later fine-tuned for a variety of tasks. It initially came with two model sizes (BERT BASE and BERT LARGE ) and obtained eleven new state-of-the-art results. Since then, it was pre-trained and fine-tuned for several tasks and languages, and several BERT-based architectures and model sizes have been introduced (such as Multilingual BERT, RoBERTa <ref type="bibr" target="#b14">[15]</ref>, ALBERT <ref type="bibr" target="#b15">[16]</ref> and VideoBERT <ref type="bibr" target="#b16">[17]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Existing humor detection datasets use a combination of formal texts and informal jokes with incompatible statistics (text length, words count, etc.), making it more likely to detect humor with simple analytical models and without understanding the underlying latent connections. Moreover, they are relatively small for the tasks of text classification, making them prone to over-fit models. These problems encouraged us to create a new dataset exclusively for the task of humor detection, where simple feature-based models will not be able to predict without an insight into the linguistic features.</p><p>In this section, we will introduce data collection method, data sources, filtering methods, and some general statistics on the new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>We carefully analyzed existing datasets (exclusively on news stories, news headlines, Wikipedia pages, tweets, proverbs, and jokes) with regard to table size, character length, word count, and formality of language. <ref type="table" target="#tab_0">Table 1</ref> present an overview of the existing humor detection datasets (binary task) and highlight their size and data sources. There are other datasets focused on closely related tasks, e.g. the tasks of punchline detection and success (whether or not a punchline triggers laughter) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, or on using speak audio and video to detect humor <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True</head><p>On set with Paul Mitchell: from our network False Starting a cover band called a book so no one can judge us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True</head><p>To curate the new dataset, we chose two data sources with formal texts (one with humor texts and one without) that were syntactically similar in our initial analysis.</p><p>1. News category dataset <ref type="bibr" target="#b21">[22]</ref>, published under CC0 Public Domain, consist of 200k Huffington Post news headlines from 2012-2018 and contains headlines, corresponding URLs, categories and full stories. The stories are scattered in several news categories, including politics, wellness, entertainment and parenting. 2. Jokes dataset contains 231,657 jokes/humor short texts, crawled from Reddit communities <ref type="bibr" target="#b2">3</ref> .</p><p>The dataset is compiled as a single csv file with no additional information about each text (such as the source, date, etc) and is available at Kaggle. Ref <ref type="bibr" target="#b5">[6]</ref> combined this dataset with the WMT162 English news crawl, but did not publicly publish the dataset. Ref <ref type="bibr" target="#b6">[7]</ref> also combined this dataset with extracted sentences from the WMT162 news crawl and made it publicly available.</p><p>Next, we performed a few preprocessing steps to create a dataset that is syntactically the same for both target class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing and Filtering</head><p>This part contains a few preprocessing steps to create the new dataset.</p><p>The initial step was to drop duplicate texts, as we identified duplicate rows in both datasets. Dropping duplicate rows removed 1369 rows from the jokes dataset and 1558 rows from the news dataset.</p><p>Then, to make the lexical statistics similar, we calculated the average and standard deviation of number of characters and words for each group. Then, we started selecting texts in pairs in a way that each text with some statistic will have a similar text from the other group. As a result, we only kept texts with character length between 30 and 100, and word length between 10 and 18. Resulting data parts have very similar distribution with regard to these statistics.</p><p>In addition, we noticed that headlines in the news dataset use Title Case 4 formatting, which was not the case with the jokes dataset. Thus, we decided to apply Sentence Case 5 formatting to all news headlines by keeping the first character of the sentences in capital and lower-casing the rest. This simple modification helps to prevent simple classifiers from reaching perfect accuracy.</p><p>Finally, we randomly selected 100k rows from both datasets and merged them together to create an evenly distributed dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Statistics</head><p>Dataset 6 contains 200k labeled short texts equally distributed between humor and non-humor. It is much larger than the previous datasets <ref type="table" target="#tab_0">(Table 1</ref>) and it includes texts with similar textual features. <ref type="table" target="#tab_2">Table 3</ref> contains a few random examples from the dataset and <ref type="table" target="#tab_1">Table 2</ref> displays general textual statistics of the dataset. The correlation between character count and the target is insignificant.</p><p>The sentiment polarity and subjectivity are calculated for all texts using TextBlob python library. The correlation analysis resulted in coefficients of -0.09 and +0.02 for polarity and subjectivity, respectively, which suggest no notable connection between the target value and sentiment features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method</head><p>In this section, we will explore our proposed method for the task of humor detection. From a technical viewpoint, we are proposing a supervised binary classifier that takes a string as input and determines if the given text is humorous or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Humor Structure</head><p>First, we take a look at the general structure of a joke to understand the underlying linguistic features that makes a text laughable.</p><p>There has been a long line of works in linguistics of humor that classify jokes into various categories based on their structure or content. Many suggested that humor arises from the sudden transformation of an expectation into nothing <ref type="bibr" target="#b22">[23]</ref>. In this way, punchline, as the last part of a joke, destroys the perceiver's previous expectations and brings humor to its incongruity. Based on this, some popular theories suggest that the structure of a joke involve two or three stages of storytelling that conclude with a punchline <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Raskin <ref type="bibr" target="#b25">[26]</ref> presented Semantic Script Theory of Humor (SSTH), a detailed formal semantic theory of humor. The SSTH has the necessary condition that a text has to have two distinct related scripts that are opposite in nature, such as real/unreal, possible/impossible. For example, let us review a typical joke:</p><p>"Is the doctor at home?" the patient asked in his bronchial whisper. "No," the doctor's young and pretty wife whispered in reply. "Come right in." <ref type="bibr" target="#b25">[26]</ref> This is compatible with the two-staged theory which ends with a punchline. The punchline is related to previous sentences but is included as opposition to previous lines in order to transform the reader's expectation of the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Architecture</head><p>Based on the presented short introduction to the structure of humor, if one reads sentences of a joke separately, they are most likely to be found as normal and non-humorous texts. On the other hand, if we try to comprehend all sentences together in one context or in one line of story, the text becomes humorous. Our proposed method utilizes this linguistic characteristic of humor in order to view or encode sentences separately and extract mid-level features using hidden layers. <ref type="figure" target="#fig_0">Figure 1</ref> displays the architecture of the proposed method. It contains separate paths of hidden layers specially designed to extract latent features from each sentence. Furthermore, there is a separate path to extract latent features of the whole text. Hence, our proposed neural network structure includes a single path to view the text as a whole and several other paths to view each sentence separately. It is comprised of a few general steps:</p><p>1. First, to assess each sentence separately and extract numerical features, we separate sentences and tokenize them individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To prepare these textual parts as proper numerical inputs for the neural network, we encode them using BERT sentence embedding. This step is performed individually on each sentence (left side in <ref type="figure" target="#fig_0">Figure 1</ref>) and also on the whole text (right side in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>3. Now that we have BERT sentence embedding for each sentence, we feed them into parallel hidden layers of neural network to extract mid-level features for each sentence (related to context, type of sentence, etc). The output of this part for each sentence is a vector of size 20.</p><p>4. While our main idea is to detect existing relationships between sentences (specifically the punchline's relationship with the rest), it is also required to examine word-level connections in the whole text that may have meaningful impacts in determining congruity of the text. For example, existence of synonyms and antonyms in text could be meaningful. We feed BERT sentence embedding for the whole text into hidden layers of neural network (right side in <ref type="figure" target="#fig_0">Figure 1</ref>). The output of this part is a vector of size 60.</p><p>5. Finally, three sequential layers of neural network conclude our model. These final layers combine the output of all previous paths of hidden layers in order to predict the final output. In theory, these final layers should determine the congruity of sentences and detect the transformation of reader's viewpoint after reading the punchline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Notes</head><p>Since our approach builds on using BERT sentence embedding in a neural network, first, we obtain token representation using BERT tokenizer with the maximum sequence length of 100 (the maximum sequence length of BERT is 512). Then, we generate BERT sentence embedding by feeding tokens as input into the BERT model (vector size=768).</p><p>The model will pass BERT embedding vectors of the given text and its sentences as inputs to a neural network with eight layers. For each sentence, We have a separate parallel line of three hidden layers which are concatenated in the fourth layer and continue in a sequential manner to predict the single target value. We use huggingface and keras.tensorflow packages for the BERT model and neural network implementations, respectively.</p><p>It is important to note that we used the BERT model to generate sentence embedding. Therefore, training is performed on the neural network and not on the BERT model. BERT comes with two pre-trained general types (the BERT BASE and the BERT LARGE ), both of which are pre-trained from unlabeled data extracted from BooksCorpus <ref type="bibr" target="#b26">[27]</ref> with 800M words and English Wikipedia with 2,500M words <ref type="bibr" target="#b13">[14]</ref>. In our proposed method, we use the smaller sized (BERT BASE ) with 12 layers, 768-hidden states, 12-heads, and 110M parameters, which are pre-trained on lower-cased English text (uncased).</p><p>To achieve clean data, we performed a few textual preprocessing actions on all input texts. They are performed as part of the method and the novel dataset is not impacted:</p><p>? Expanding contractions: We replaced all contractions with the expanded version of the expressions. For example, "is not" instead of "isn't".</p><p>? Cleaning punctuation marks: We separated the punctuation marks 7 from words to achieve cleaner sentences. For example, the sentence "This is' (fun)." is converted to "This is ' ( fun ) ." ? Cleaning special characters: We replaced some special characters with an alias. For example, "alpha" instead of "?".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Discussion</head><p>In this section, we evaluate the performance and robustness of the proposed method on two datasets. First, we compare the performance of the proposed model with a few baselines on the novel ColBERT dataset of formal English texts. Then, we report the performance evaluation on a dataset composed of informal tweets in the Spanish language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on the ColBERT dataset</head><p>The performance of the ColBERT model on the novel dataset is compared with five general baselines. In short, the dataset contains short formal humorous texts in English language. For the purposes of this section, the data is split into 80% (160k) train and 20% (40k) test parts.</p><p>We chose five strong baselines for comparison. The baseline models are:</p><p>1. Decision Tree: A methodology that is commonly used as a data mining method for establishing classification systems based on multiple covariates or for developing prediction algorithms for a target variable. The method uses the train dataset to generate a branch-like segments that construct an inverted tree with a root node, internal nodes, and leaf nodes <ref type="bibr" target="#b27">[28]</ref>. For our evaluation, we used CountVectorizer to generate numerical word representations. 2. SVM: A supervised model that achieved robust results for many classification and regression tasks. For this baseline, we applied TfidfVectorizer to generate numerical word representations with some optimization on hyper-parameters. 3. Multinomial na?ve Bayes: The model is suited when we deal with discrete integer features, such as word counts in a text. Here, we used CountVectorizer to generate numerical word representations. 4. XGBoost: XGBoost is the latest step in the evolution of tree-based algorithms that include decision trees, boosting, random forests, boosting and gradient boosting. It is an optimized distributed gradient boosting that provides fast and accurate results, which achieves accurate results in less time <ref type="bibr" target="#b28">[29]</ref>. We applied XGBoost on numerical word representations generated by CountVectorizer which resulted in better accuracy than TfidfVectorizer. 5. XLNet: A generalized language model that aims to mitigate the issues related to BERT model and previous autoregressive language models. For the task of text classification (and some other NLP tasks), XLNet outperforms BERT on several benchmark datasets <ref type="bibr" target="#b29">[30]</ref>. We used xlnet-large-cased that has 24 layers and 340M parameters.</p><p>We trained these baselines on using the cross-validation approach (K=5). Thus, in every fold, we used 128K for training of the model and the remaining 32K for evaluation. The following results are based on the final evaluation of the trained models on the test part of the dataset (remaining 40K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Results</head><p>The results of our experiments on the ColBERT dataset are displayed in <ref type="table" target="#tab_3">Table 4</ref>. They found the proposed model's accuracy and F1 score to be 98.2%, thus outperforming all selected baselines with a large margin. This is a 7% jump from the recent state-of-the-art XLNet model (with 340M parameters) and 17% higher than the gradient boosting classifier. Traditional models of Decision Tree, SVM and Multinomial na?ve Bayes gained less than 90% accuracy in their, still an acceptable performance for a general model. XGBoost, a strong implementation of gradient boosting, achieved 81% F1-score based on the selected word representations. XLNet Large , which required less optimization, was the Regarding time performance, the proposed model requires 2 hours (in average) to perform one epoch of training on 128k rows of the dataset on a computer with NVIDIA TESLA P100 GPUs. This is comparably less than the XLNet model, but longer than the rest of the selected baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Discussion</head><p>The results suggest two important factors in achieving high accuracy in the current task. First, methods that rely on pre-trained language models to produce sentence embedding outperform traditional methods of the word representation. XLNet and the proposed model both use their own embeddings and achieve much better results than other baselines, and the traditional methods of word representations such as TF-IDF could not break a limit even with the use of the latest classification boosting models (such as XGBoost). Second, our model with 110M parameters and 8 layers was able to outperform XLNet with 340M parameters and 24 layers, which could be a result of utilizing the linguistic structure of humor in designing the proposed model.</p><p>In addition, by reviewing the wrong predictions by our classifier, it is clear that the mislabeled items are generally close to (or even a part of) the items of the other class. For example, our model mislabeled the following two sentences as not humor:</p><p>? "A recent study by UN has found Dexter to be the no 1 cause for ocean pollution"</p><p>? "One out of five dentists has the courage to speak their own mind"</p><p>On the contrast, the following news articles are mislabeled as humor:</p><p>? "If we treated men like we do women, would they cry more at work?" (News story: 8 )</p><p>? "How do we keep alias generation off facebook? permanent mittens." (News story: 9 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on informal Spanish tweets</head><p>In the second part of the evaluation, we test the robustness of the proposed method by applying it to a new context. For this step, we participated in a recent shared task to detect and rate humor in Spanish tweets (HAHA 2021 10 ). In this way, we compete against real teams of machine learning engineers on a previously unseen dataset. The competition was organized as a part of the IberLEF 2021 forum and attracted seventeen active teams that competed via the CodaLab platform. The new setting is different from the previous evaluation method: 1. The new task is to detect and rate humor in Spanish, which we have no linguistic knowledge of.</p><p>2. The texts are informal tweets, differing in size and structure from the formal texts of the previous benchmark.</p><p>To predict results, we did not change the model structure, hyper-parameters, or any of the preprocessing functions. However, in order to extract sentence embedding for Spanish texts, we changed the selected BERT model from the English BERT-base-uncased to a recent Spanish equivalent (BETO-uncased <ref type="bibr" target="#b30">[31]</ref>). This was a required and logical step to achieve meaningful embeddings.</p><p>The organizers provided a corpus of crowd-annotated tweets using a voting scheme with six options <ref type="bibr" target="#b31">[32]</ref>: the tweet is not humorous, or the tweet is humorous and a score is given between one (not funny) to five (excellent). Data contains 36,000 tweets separated into training (24,000 tweets), development (6,000 tweets), and testing (6,000 tweets).</p><p>Based on the official results reported by the organizers <ref type="bibr" target="#b32">[33]</ref>, the proposed model performed strongly and achieved the 2 nd place for rating humor with 0.6246 root mean square error (RMSE). In the binary task of humor detection, our model achieved the 3 rd place with 0.8696 F1 score. The results clearly indicate the robustness and stability of the proposed method in detecting humor in any given text. Full results and discussions for this evaluation are presented at <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Limitations and Future Work</head><p>Multimodal humor detection: Some recent works focused on detecting humor in voice, videos, and pictures (e.g. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>). While the focus of this work was on textual content, we believe that it is possible to apply the underlying idea of the proposed method to other types of media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labeling accuracy:</head><p>We used an automated way to curate the accompanying dataset and classify them into two categories. As we saw in the evaluations, some news headlines can be considered humorous texts. While we evaluated our proposed method on a second dataset, the ColBERT dataset could be improved by human annotation to be better suited for the evaluation of future works.</p><p>Other humor theories: The proposed method is based on one popular theory of humor. There are other theories that discuss the cause of laughter in humans (See <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>). It would be interesting to create models based on alternative theories and compare the results of the execution, as a way to prove or quantify their accuracy.</p><p>Punchline detection and continuous texts: The current work determines the existence of humor in a given text as a whole. While this has its own applications in the classification of short text posts and user commands, in many situations the text is continuous and there are no clear cut boundaries between the sentences. In those situations, it would become essential to pinpoint the parts of speech that contain humor or separate sentences based on context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>For ages, human beings have been fantasizing about humanoid robots indistinguishable from humans. In making that a reality, humor cannot be missed as a major human feature, which for its subjectivity, ambiguity, and semantic intricacies has been a difficult problem for researchers to tackle. This work contributes to this human fantasy and is paving the way for creating high-quality artificial intelligence systems (such as chatbots, virtual assistants, and even robots) injected with adjustable humor.</p><p>Our technical approach is based on injecting BERT sentence embedding into a neural network model that processes sentences separately in parallel lines of hidden layers. This conforms to a widely accepted theory of humor. Our method obtained F1 scores of 0.982 and 0.869 on two different settings and outperforms state-of-the-art models. Furthermore, we presented a novel dataset consisting of 200k formal short texts for the task of humor detection.</p><p>Based on our results, we identified two important factors in achieving high accuracy in the current task: 1) usage of sentence embeddings, and 2) utilizing the linguistic structure of humor in designing the proposed model. Results showed that our hypothesis on the structure of humor is valid and can be utilized to create very accurate systems of humor detection. Future work can adapt the proposed method for developing systems for other tasks of computational humor, or test the proposed parallel neural network in a wider range of text classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Components of the proposed method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets for the binary task of humor classification</figDesc><table><row><cell></cell><cell>Parts</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">#Positive #Negative</cell></row><row><cell cols="2">16000 One-Liners [18] 16,000</cell><cell>16,002</cell></row><row><cell>Pun of the Day [5]</cell><cell>2,423</cell><cell>2,403</cell></row><row><cell>PTT Jokes [6]</cell><cell>1,425</cell><cell>2,551</cell></row><row><cell>English-Hindi [13]</cell><cell>1,755</cell><cell>1,698</cell></row><row><cell>ColBERT</cell><cell>100,000</cell><cell>100,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="5">: General statistics of the ColBERT dataset (100k positive, 100k negative)</cell><cell></cell></row><row><cell></cell><cell cols="3">#chars #words #unique</cell><cell>#punctuation</cell><cell>#duplicate</cell><cell>sentiment</cell><cell>sentiment sub-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>words</cell><cell></cell><cell>words</cell><cell>polarity</cell><cell>jectivity</cell></row><row><cell>mean</cell><cell cols="3">71.561 12.811 12.371</cell><cell>2.378</cell><cell>0.440</cell><cell>0.051</cell><cell>0.317</cell></row><row><cell>std</cell><cell cols="2">12.305 2.307</cell><cell>2.134</cell><cell>1.941</cell><cell>0.794</cell><cell>0.288</cell><cell>0.327</cell></row><row><cell>min</cell><cell>36</cell><cell>10</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>-1.000</cell><cell>0.000</cell></row><row><cell cols="2">median 71</cell><cell>12</cell><cell>12</cell><cell>2</cell><cell>0</cell><cell>0.000</cell><cell>0.268</cell></row><row><cell>max</cell><cell>99</cell><cell>22</cell><cell>22</cell><cell>37</cell><cell>13</cell><cell>1.000</cell><cell>1.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>A few examples from the novel dataset TextIs humor? Why your purse is giving you back pain... and 11 ways to fix it False Why was the fruit/vegetable hybrid upset? he was a meloncauliflower.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance evaluation on the ColBERT Dataset</figDesc><table><row><cell>Method</cell><cell>Configuration</cell><cell cols="4">Accuracy Precision Recall F1</cell></row><row><cell>Decision Tree</cell><cell></cell><cell>0.786</cell><cell>0.769</cell><cell>0.821</cell><cell>0.794</cell></row><row><cell>SVM</cell><cell>sigmoid, gamma=1.0</cell><cell>0.872</cell><cell>0.869</cell><cell>0.880</cell><cell>0.874</cell></row><row><cell cols="2">Multinomial NB alpha=0.2</cell><cell>0.876</cell><cell>0.863</cell><cell>0.902</cell><cell>0.882</cell></row><row><cell>XGBoost</cell><cell></cell><cell>0.720</cell><cell>0.753</cell><cell>0.777</cell><cell>0.813</cell></row><row><cell>XLNet</cell><cell>XLNet-Large-Cased</cell><cell>0.916</cell><cell>0.872</cell><cell>0.973</cell><cell>0.920</cell></row><row><cell>Proposed</cell><cell></cell><cell>0.982</cell><cell>0.990</cell><cell>0.974</cell><cell>0.982</cell></row><row><cell cols="6">strongest among the baselines, reaching close to 92% accuracy, 4 percent higher than Multinomial</cell></row><row><cell>NB.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Mostly from /r/jokes and /r/cleanjokes subreddits.<ref type="bibr" target="#b3">4</ref> All words are capitalized, except non-initial articles like "a, the, and", etc.<ref type="bibr" target="#b4">5</ref> Capitalization as in a standard English sentence, e.g., "Witchcraft is real.".<ref type="bibr" target="#b5">6</ref> The dataset is available at: https://github.com/Moradnejad/ColBERT-Using-BERT-Sentence-Embeddingfor-Humor-Detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The punctuation marks are: period, comma, question mark, hyphen, dash, parentheses, apostrophe, ellipsis, quotation mark, colon, semicolon, exclamation point.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.huffpost.com/entry/if-we-treated-men-like-women-would-they-cry-more-at_b_5904e9ace4b084f59b49f99b 9 https://www.huffpost.com/entry/facebook-and-kids_b_1579207 10 https://www.fing.edu.uy/inco/grupos/pln/haha/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is this a joke? detecting humor in spanish tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cubero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moncecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ibero-American Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making social robots more attractive: the effects of voice pitch, humor and empathy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>See</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of social robotics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="191" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computationally recognizing wordplay in jokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Mazlack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Humor: Prosody analysis and automatic recognition for f* r* i* e* n* d* s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purandare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Humor recognition and humor anchor extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Humor recognition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-W</forename><surname>Soo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="113" to="117" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seppi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00252</idno>
		<title level="m">Humor detection: A transformer gets the last laugh</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Kramer, I laugh because it&apos;s absurd: Humor as error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Phil Archive</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting humor by learning from time-aligned comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="496" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of haha at iberlef 2019: Humor analysis based on human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Etcheverry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ros?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Languages Evaluation Forum</title>
		<meeting>the Iberian Languages Evaluation Forum<address><addrLine>Bilbao, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings, CEUR-WS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Humor analysis based on human annotation challenge at iberlef 2019: First-place solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ismailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Languages Evaluation Forum</title>
		<meeting>the Iberian Languages Evaluation Forum<address><addrLine>Bilbao, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings, CEUR-WS</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aspie96 at haha (iberlef 2019): Humor detection in spanish tweets with characterlevel convolutional rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Giudice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Iberian Languages Evaluation Forum</title>
		<meeting>the Iberian Languages Evaluation Forum<address><addrLine>Bilbao, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings, CEUR-WS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Humor detection in english-hindi code-mixed social media content: Corpus and baseline system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05513</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<meeting><address><addrLine>Roberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making computers laugh: Investigations in automatic humor recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting audience&apos;s laughter during presentations using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 12th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06618</idno>
		<title level="m">Ur-funny: A multimodal language dataset for understanding humor</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning of audio and language features for humor prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="496" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Misra</surname></persName>
		</author>
		<idno type="DOI">10.13140/RG.2.2.20331.18729</idno>
	</analytic>
	<monogr>
		<title level="j">News category dataset</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kritik der urteilskraft</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="1913" />
			<publisher>Meiner</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The appreciation of humour: an experimental and theoretical study 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Eysenck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology. General Section</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="309" />
			<date type="published" when="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A two-stage model for the appreciation of jokes and cartoons: An informationprocessing analysis, The psychology of humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Suls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical perspectives and empirical issues</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="100" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raskin</surname></persName>
		</author>
		<title level="m">Semantic mechanisms of humor</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Decision tree methods: applications for classification and prediction, Shanghai archives of psychiatry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">130</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xlnet</forename></persName>
		</author>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spanish pre-trained bert model and evaluation data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ca?ete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>P?rez</surname></persName>
		</author>
		<idno>PML4DC at ICLR 2020</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Haha 2019 dataset: A corpus for humor analysis in spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ros?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5106" to="5112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?ngora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ros?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Meaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="257" to="268" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Colbert at haha 2021: Parallel neural networks for rating humor in spanish tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Annamoradnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zoghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IberLEF@ SEPLN</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="860" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Punchline detection using context-aware hierarchical multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimodal Interaction</title>
		<meeting>the 2020 International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="675" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal humor dataset: Predicting laughter tracks for sitcoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lunayach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="576" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mumor: A multimodal dataset for humor detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCF International Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="619" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Attardo</surname></persName>
		</author>
		<title level="m">Linguistic theories of humor</title>
		<imprint>
			<publisher>Walter de Gruyter</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Definitions, theories, and measurement of humor, in: Humor at work in teams, leadership, negotiations, learning and health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scheel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="9" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
