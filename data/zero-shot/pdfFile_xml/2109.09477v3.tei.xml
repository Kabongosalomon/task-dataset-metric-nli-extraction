<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation via Semantic Knowledge Transfer and Self-Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjoon</forename><surname>Yoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaeeun</forename><surname>Rhee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">KAIST 4</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naver</forename><surname>Clova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naver</forename><forename type="middle">Ai</forename><surname>Lab</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation via Semantic Knowledge Transfer and Self-Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised instance segmentation (WSIS) has been considered as a more challenging task than weaklysupervised semantic segmentation (WSSS). Compared to WSSS, WSIS requires instance-wise localization, which is difficult to extract from image-level labels. To tackle the problem, most WSIS approaches use off-the-shelf proposal techniques that require pre-training with instance or object level labels, deviating the fundamental definition of the fully-image-level supervised setting. In this paper, we propose a novel approach including two innovative components. First, we propose a semantic knowledge transfer to obtain pseudo instance labels by transferring the knowledge of WSSS to WSIS while eliminating the need for the offthe-shelf proposals. Second, we propose a self-refinement method to refine the pseudo instance labels in a selfsupervised scheme and to use the refined labels for training in an online manner. Here, we discover an erroneous phenomenon, semantic drift, that occurred by the missing instances in pseudo instance labels categorized as background class. This semantic drift occurs confusion between background and instance in training and consequently degrades the segmentation performance. We term this problem as semantic drift problem and show that our proposed self-refinement method eliminates the semantic drift problem. The extensive experiments on PASCAL VOC 2012 and MS COCO demonstrate the effectiveness of our approach, and we achieve a considerable performance without off-theshelf proposal techniques. The code is available at https: //github.com/clovaai/BESTIE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent line of weakly-supervised semantic segmentation (WSSS) approaches <ref type="bibr" target="#b11">[21,</ref><ref type="bibr" target="#b17">26,</ref><ref type="bibr" target="#b18">27,</ref><ref type="bibr" target="#b35">43]</ref> have achieved impressive performance enhancement, with commonly using class activation maps (CAMs) <ref type="bibr" target="#b36">[44]</ref> to obtain class-wise localization maps from image-level labels. However, weakly- Previous Method <ref type="figure">Figure 1</ref>. Two limitations of previous WSIS methods: (1) dependency on off-the-shelf proposal technique that requires pretraining with high-level labels including object or instance information.</p><p>(2) semantic drift problem caused by the missing instances in the pseudo label guided as background class. Our proposed BESTIE aims to solve the two problems simultaneously.</p><p>supervised instance segmentation (WSIS) using imagelevel labels is still an open task because the CAM does not provide instance-wise localization maps.</p><p>To extract the instance-wise information, most WSIS methods use off-the-shelf proposal techniques. PRM <ref type="bibr" target="#b37">[45]</ref> takes suitable instance masks from segment proposals generated by MCG <ref type="bibr" target="#b28">[36]</ref> and generates pseudo instance labels. Also, LIID <ref type="bibr" target="#b22">[31]</ref> utilizes a pre-trained salient instance segmentor <ref type="bibr" target="#b2">[12]</ref> which produces class-agnostic instance-level masks. We note that MCG and salient instance segmentor used in each method require pre-training with object boundary labels and class-agnostic instance masks, respectively.</p><p>However, the proposal-guided WSIS methods have two limitations as shown in <ref type="figure">Figure 1</ref>. First, their dependency on the off-the-shelf proposal techniques is considerably high, and it makes the methods difficult to apply to other specific domains such as medical images since the proposal techniques mostly target general objects. Furthermore, in a strict sense, the use of the proposal techniques trained by object or instance level information deviates from the definition of fully-image-level supervised segmentation. Second, these methods cannot cope with the performance degradation caused by noisy pseudo-labels containing missing instances (i.e., false-negatives). As shown in <ref type="figure">Figure 1</ref>, the left two missing cows are guided to the background class and the right cow is guided to the cow class, although all cows have semantically similar visual cues. We call this problem as semantic drift problem. This semantic drift between background and instance confuses the network and deteriorates the stable training convergence.</p><p>In this paper, we propose a new WSIS method, BESTIE: BEyond Semantic segmentation To InstancE Segmentation. BESTIE deprecates the use of off-the-shelf techniques to strictly follow a fully-image-level supervised setting. Also, BESTIE alleviates the semantic drift problem. To solve the two problems, BESTIE proposes two innovative components, semantic knowledge transfer and self-refinement.</p><p>Specifically, in semantic knowledge transfer, we transfer the knowledge of WSSS, which is relatively profoundly studied, to WSIS to generate the rough pseudo instance labels. To obtain the instance cues from image-level labels, we propose peak attention module (PAM) that makes the CAM highlight the sparse representative region of objects. We note that the proposed components only use image-level labels, including the WSSS, and this eliminates the need for the off-the-shelf proposal technique. Furthermore, to address the semantic drift problem, we introduce instanceaware guidance that dynamically assigns the guidance region only to the labeled instance region. This strategy allows more stable training of the network and progressively captures instance-level information of the missing instances. Along with this strategy, to further refine the pseudo labels, we propose the self-supervised instance label refinement method that converts false-negatives in the pseudo labels to true-positives by a self-supervised manner and reflects them to the training in an online manner. This method, shortly named self-refinement, improves the quality of the pseudo labels as training progresses.</p><p>The extensive experiments on PASCAL VOC 2012 <ref type="bibr" target="#b1">[11]</ref> and MS COCO 2017 <ref type="bibr" target="#b20">[29]</ref> show the effectiveness of the proposed design. Even without the off-the-shelf proposal techniques, our method achieves a state-of-the-art performance of 51.0% mAP 50 on VOC 2012 and 28.0% AP 50 on COCO dataset. Furthermore, we model the point-supervised instance segmentation by replacing the instance cues with point labels and can further boost the performance with an economical annotation cost.</p><p>Our contribution can be summarized as follows:</p><p>? We propose a novel WSIS method only using imagelevel labels, strictly following the fully-image-level supervised setting without the help of the proposal techniques pre-trained by object or instance level labels.</p><p>? We design the semantic knowledge transfer strategy to obtain pseudo instance labels. This transfers the knowledge of WSSS and instance cues, extracted from the proposed PAM, to WSIS while eliminating the use of off-the-shelf proposal techniques.</p><p>? We propose a self-refinement method to refine the pseudo instance labels in a self-supervised manner and reflect them back to the training in an online manner.</p><p>Here, we introduce the instance-aware guidance strategy to resolve the semantic drift problem newly discovered in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly-Supervised Semantic Segmentation</head><p>Most weakly-supervised semantic segmentation (WSSS) studies handling image-level labels use CAMs <ref type="bibr" target="#b36">[44]</ref> to localize class-wise object regions. However, CAMs mainly focus on the sparse and discriminative object regions. To address this issue, recent WSSS methods have proposed many approaches to expand activation regions. AE-PSL <ref type="bibr" target="#b34">[42]</ref> removes discriminative object regions by shifting attention to adjacent non-discriminative regions. DRS <ref type="bibr" target="#b11">[21]</ref> proposes a module suppressing discriminative regions to expand activation regions. However, these approaches are dependent on the off-the-shelf guidance, i.e., saliency map. To eliminate the use of the saliency map, saliency map-free methods has been also proposed: RRM <ref type="bibr" target="#b35">[43]</ref> proposes an endto-end network jointly producing both CAMs and segmentation output to generate pseudo labels from only reliable pixels. PMM <ref type="bibr" target="#b18">[27]</ref> proposes the proportional pseudo-mask generation by variation smoothing. Also, some approaches have shown the expansion possibility of the WSSS to various domains such as medical [6] and satellite <ref type="bibr" target="#b24">[33]</ref> images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Instance Segmentation</head><p>Unlike semantic segmentation categorizing the pixel region in class-level, instance segmentation requires an instance-level mask. The most widely used approach is a box-based two-stage method, e.g., Mask R-CNN <ref type="bibr" target="#b5">[15]</ref>, that predicts bounding boxes and then extracts the instance mask for each bounding box; this approach has reigned on the throne with state-of-the-art performance. Recently, for the simple instance segmentation process, box-free one-stage instance segmentation methods <ref type="bibr">[7,</ref><ref type="bibr" target="#b23">32]</ref> have been proposed. They represent each instance using 2-dimensional (2D) offset vectors; the pixels covering each instance are represented as 2D offset vectors directed to the center of each instance. The center point of each instance is extracted from the center heatmap <ref type="bibr">[7]</ref> or by clustering 2D offset vectors <ref type="bibr" target="#b23">[32]</ref> and the instance mask is obtained from instance grouping with the center point and 2D offset vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Weakly-Supervised Instance Segmentation</head><p>The major difficulty in solving weakly-supervised instance segmentation (WSIS) occurs in the process of obtaining instance-level information from image-level labels. To solve the problem, PRM <ref type="bibr" target="#b37">[45]</ref>   <ref type="figure">Figure 2</ref>. The overview of our framework including two-innovated steps: semantic knowledge transfer and self-refinement. In semantic knowledge transfer, we obtain pseudo instance labels from the knowledge of WSSS and instance cues. Here, we obtain instance cues using peak attention module (PAM). In the self-refinement, the network refines the pseudo labels in a self-supervised scheme and reflects them to the training in an online manner. For the stable training alleviating the semantic drift problem, we apply instance-aware guidance strategy. We note that the entire process only uses the image-level labels.</p><p>using the proposed peak back-propagation and then selects appropriate segment proposals generated by MCG <ref type="bibr" target="#b28">[36]</ref>. Arun et al.</p><p>[3] defines the uncertainty in the pseudo label generation with the help of the segment proposals and iteratively train the network using the pseudo labels in an offline manner. Fan et al. <ref type="bibr" target="#b3">[13]</ref> and LIID <ref type="bibr" target="#b22">[31]</ref>) use the salient instance segmentor <ref type="bibr" target="#b2">[12]</ref> that produces class-agnostic instance-level masks when they generate pseudo labels. In addition, box-based two-stage approaches <ref type="bibr" target="#b4">[14,</ref><ref type="bibr" target="#b9">19]</ref> use a selective search <ref type="bibr" target="#b32">[40]</ref> method to generate box proposals. However, the off-the-shelf proposal techniques require pretraining with high-level supervision: class-agnostic object boundary for MCG and class-agnostic instance mask for the salient instance segmentor. Also, since these proposal techniques target general objects, it disturbs their utilization in other domains such as a medical image. IRN [1] proposes a proposal-free method focusing on class-equivalence relations between a pair of pixels and represents instancelevel information using their displacement field. However, IRN has trouble in obtaining accurate instance-level information because the inter-pixel relations used in IRN are based on inter-class, not inter-instance. To the best of our knowledge, existing methods have not considered the semantic drift problem caused by missing instances in pseudo labels, one fundamental challenge that should be addressed for WSIS. In this paper, we discover and tackle the semantic drift problem explicitly for the first time and achieve an improved result in a fully-image-level supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As shown in the left part of <ref type="figure">Figure 2</ref>, we first obtain pseudo instance labels using the knowledge of WSSS and instance cues, and this process is called semantic knowledge transfer. Here, we extract instance cues from image-level labels using the proposed peak attention module (PAM) module. Then, we apply the self-supervised pseudo label refinement abbreviated as self-refinement strategy that refines the pseudo instance labels in a self-supervised scheme and reflects them to the training in an online manner as described in the right part of <ref type="figure">Figure 2</ref>. In order to resolve the semantic drift problem and ensure stable training, we introduce the instance-aware guidance strategy. We note that our framework only uses the image-level labels as our guidance source including the WSSS part to deprecate offthe-shelf proposal techniques in the entire process. We also provide the Pytorch-style pseudo-code for each proposed component in our supplementary material, showing that our method is quite easy to be implemented and simple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preliminary: Instance Representation</head><p>Motivated by Panoptic-DeepLab [7], we represent an instance as a center point and corresponding 2D offset vectors. The 2D offset vectors direct the center point of each instance. By adopting this representation method, we con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAM PAM Image</head><p>Instance cue from CAM Instance cue from PAM struct an instance segmentation network following the architecture of Panoptic-DeepLab, which consists of three output branches: semantic segmentation map, center map, and offset map. The semantic segmentation map determines the foreground region. For the post-processing, we extract center points of each instance from the center map; pixel locations with the same value before and after the maxpooling of the center map are regarded as center points. Then, we allocate the ID of each instance in a pixel-level, and this procedure is called as instance grouping; when the extracted n-th center point is denoted as (x n , y n ) and the offset map is denoted as O(i, j) at the pixel location (i, j), the instance ID k i,j at the pixel (i, j) becomes</p><formula xml:id="formula_0">k i,j = argmin k ||(x k , y k ) ? ((i, j) + O(i, j))||. (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Knowledge Transfer</head><p>The proposed semantic knowledge transfer transfers the knowledge of WSSS to WSIS. For the transfer, we rethink the two conditions of semantic and instance segmentation. The first condition is that instance segmentation should separate overlapping instances of the same class, unlike semantic segmentation. The second condition is that semantic segmentation and instance segmentation are equivalent to each other when instances of the same class do not overlap. Based on these conditions, we generate pseudo instance labels utilizing the knowledge of WSSS. From the WSSS output and instance presence cues, we check whether instances overlap. Then, we select a non-overlapping instance mask as a pseudo instance mask, described in the left part of <ref type="figure">Figure 2</ref>. Specifically, by performing the connected component labeling (CCL) algorithm <ref type="bibr" target="#b7">[17]</ref> on the WSSS output of each class, we obtain instance mask candidates and  <ref type="figure">Figure 4</ref>. PAM architecture. From the selector, criteria of peak regions are selected. For a better explanation, three of them are illustrated in red points. Then, the controller determines how much to strengthen the attention on peak regions with control values, and each value is illustrated as the length of a blue line. Using criteria points and control values, the boundary of peak regions is set, and the stimulator strengthens the attention on peak regions by deactivating noisy regions whose values are lower than the boundary.</p><p>check how many instance cues are included in each instance mask candidate. Following the second condition, the instance mask candidate with only one instance cue is selected as the pseudo instance mask. For the proper semantic knowledge transfer, we require the accurate instance cue extraction method only using image-level labels. The previous work, PRM <ref type="bibr" target="#b37">[45]</ref>, extracts the instance cue from CAMs <ref type="bibr" target="#b36">[44]</ref>. However, CAMs have a limitation in obtaining the accurate instance cue because several instance cues might be extracted in a single instance due to noisy activation regions as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. It disturbs the generation of pseudo instance labels since it violates the second condition.</p><p>To address this limitation, we propose a peak attention module (PAM) to extract one appropriate instance cue per instance, motivated by DRS <ref type="bibr" target="#b11">[21]</ref>. DRS suppresses discriminative object regions, spreading attention to adjacent nondiscriminative regions in a self-supervised manner. Contrary to the DRS, our PAM aims to strengthen the attention on peak regions, while weakening the attention on noisy activation regions. PAM consists of three parts, as illustrated in <ref type="figure">Figure 4</ref>: selector, controller, and peak stimulator. We denote the intermediate feature map as X ? R H?W ?K , where H, W , and K are the height, width, and the number of channels of X, respectively. The selector selects criteria points of peak regions using the global max pooling of X, and the criteria points are denoted as S p ? R 1?1?K . The controller determines how much strengthen the attention on peak regions and is denoted as G p ? [0, 1] 1?1?K . We strengthen the attention on peak regions by deactivating the attention on noisy regions. In particular, ? p = S p ? G p plays a role of the boundary of peak regions where ? is an element-wise multiplication. The regions in X higher than ? p are regarded as peak regions, otherwise regarded as noisy regions. We deactivate the noisy regions by setting the value to zero, focusing on peak regions. For the controller, we adopt the non-learnable setting of DRS, that is, all elements  <ref type="figure" target="#fig_4">Figure 5</ref>. Comparison of offset and center maps. As training iteration progresses, the network generates the higher-quality refined label. of G p are set to a constant value ?; the ? is set to 0.7, and we find out that changes in the ? between 0.3 and 0.7 do not significantly affect the performance of WSIS. The PAM is plugged into the classifier, and we produce activation maps that localize the sparse representative region of each object as shown in <ref type="figure" target="#fig_1">Figure 3</ref> and then extract local maximum points (i.e., instance cues). Note that our PAM does not require additional training parameters, and the classifier with PAM is optimized with a binary cross-entropy objective function by adaptively focusing on peak regions while increasing classification ability.</p><p>Combined with the knowledge of WSSS and instance cues extracted from PAM, we obtain pseudo instance masks and convert these masks into the pseudo center and offset maps following our instance representation method, as illustrated in <ref type="figure">Figure 2</ref>. For the center map, the centroid point of each pseudo instance mask is encoded in a 2D Gaussian kernel with a standard deviation of 6 pixels. For the offset map, all pixels in the pseudo instance mask contain 2D offset vectors directed to the corresponding center point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Instance-aware Guidance</head><p>When training with the pseudo instance labels obtained by the semantic knowledge transfer, we should handle the semantic drift problem. Since the missing instances in the pseudo labels are guided as a background class, semantic drift between background and instance deteriorates the stable training convergence. To alleviate this problem, we introduce an instance-aware guidance by taking the advantage of our instance representation method.</p><p>In our instance representation in Section 3.2, the offset and center maps represent instance-level information within the foreground region determined by the semantic segmentation map. It means that the background region of the offset and center maps can be regarded as ignored regions. Correspondingly, we dynamically assign the guidance region for the offset and center maps to only the region for the labeled instances; this strategy is called the instance-aware guidance and helps alleviate the semantic drift problem because the region of the offset and center maps for the missing instances (e.g., the white region of the pseudo offset map in <ref type="figure">Figure 2</ref>) is not reflected in the objective function. Consequently, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the network can stably cap-ture the instance-level information of the missing instances as the training progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Self-Supervised Pseudo Label Refinement</head><p>Even we can alleviate the semantic drift problem, the number of true-positives in the pseudo labels is still not enough for training the network. For example, we can only have 30% true-positives in the pseudo labels for VOC 2012. Here, we propose a self-supervised pseudo label refinement strategy, abbreviated as self-refinement, that refines the pseudo labels by converting false-negatives to true-positives in a self-supervised manner and reflects the refined labels to the training in an online manner. The overall process of the self-refinement is illustrated on the right side of <ref type="figure">Figure  2</ref>. First, by training with the pseudo instance labels using instance-aware guidance strategy, the network stably develops the generalization ability and gradually captures the instance-level information of the missing instances (i.e., false-negatives). Next, following the Eq. (1), we perform the instance grouping using the network outputs. Then, we generate refined offset and center maps from the instance mask created by instance grouping. Last, the refined maps are used as guidance for the network.</p><p>For better refinement, we extract the center point by clustering the 2D offset vectors in the output offset map. We call this process a center clustering and explain the detailed algorithm in the supplementary material. Even if the output center map misses some center points, we complement the refined center map using the clustered center points.</p><p>Using both pseudo labels and refined labels, we train the network. We denote the network output semantic segmentation, offset map, and center map as S(?), O(?), and C(?), respectively. For the instance-aware guidance of the offset and center maps, we collect sets of pixels for labeled instance regions from pseudo and refined labels, and each set is denoted as P pseudo and P ref ined . To utilize the refined labels as soft labels, we design a weight mask W(i, j):</p><formula xml:id="formula_1">W n (i, j) = C(x n , y n ) (i, j) ? P n pseudo , 0 otherwise,<label>(2)</label></formula><p>where the center point of n-th instance in the refined labels are denoted as (x n , y n ), and C(x n , y n ) means the con-fidence score of the n-th instance. The W is used as the weight of the objective function for the refined labels. The objective function of the center map is defined as:</p><formula xml:id="formula_2">L center = 1 |P pseudo | (i,j)?P pseudo (C(i, j) ??(i, j)) 2 + 1 |P ref ined | (i,j)?P ref ined W(i, j) ? (C(i, j) ?C(i, j)) 2 ,</formula><p>(3) where the pseudo and refined center maps are?(i, j) and C(i, j), respectively. Also, the objective function of the offset map is defined as:</p><formula xml:id="formula_3">L of f set = 1 |P pseudo | (i,j)?P pseudo |O(i, j) ??(i, j)|+ 1 |P ref ined | (i,j)?P ref ined W(i, j) ? |O(i, j) ??(i, j)|,<label>(4)</label></formula><p>where the pseudo and refined offset maps are?(i, j) and O(i, j), respectively. Lastly, the objective function of the segmentation map is defined as:</p><formula xml:id="formula_4">L sem = ? 1 |P sem | (i,j)?Psem logS(i, j),<label>(5)</label></formula><p>where S is the output semantic map and P sem is the set of all pixels in S. The network is jointly trained with the above objective functions, and the final objective function is:</p><formula xml:id="formula_5">L = ? center L center + ? of f set L of f set + ? sem L sem ,<label>(6)</label></formula><p>where ? is a weight parameter, and set ? center = 200, ? of f set = 0.01, and ? sem = 1 as used in <ref type="bibr">[7]</ref>. Through our self-refinement strategy, the pseudo labels can convert into high-quality refined labels. The refined labels are generated from the network in an online manner at every mini-batch. Also, since most of the processes are performed by GPU operation, the latency of the self-refinement is negligibly small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metrics</head><p>We demonstrate the effectiveness of the proposed approach on Pascal VOC 2012 <ref type="bibr" target="#b1">[11]</ref> and COCO <ref type="bibr" target="#b20">[29]</ref> datasets. For VOC 2012 dataset, following the common practice in previous works <ref type="bibr">[1,</ref><ref type="bibr" target="#b22">31]</ref>, we use the augmented dataset that contains 10,582 training and 1,449 validation images with 20 object categories. COCO dataset consists of 115K training, 5K validation, and 20K testing images with 80 object categories. We evaluate the performance using the mean average precision (mAP) with intersection-over-union (IoU) thresholds of 0.25, 0.5, 0.7, and 0.75 for VOC 2012 and averaged AP over IoU thresholds from 0.5 to 0.95 for COCO. <ref type="table">Table 1</ref>. Quantitative comparison of state-of-the-art WSIS methods on VOC 2012 val-set. ? indicates applying MRCNN refinement. We denote the supervision sources as: F (full mask), I (image-level label), P (point), C (object count). The off-the-shelf proposal techniques are denoted as follows: M (segment proposal <ref type="bibr" target="#b28">[36]</ref>), R (region proposal <ref type="bibr" target="#b32">[40]</ref>), SI (salient instance segmentor <ref type="bibr" target="#b2">[12]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For the semantic knowledge transfer, we extract instance cues from the classifier equipped with our PAM. We describe the detailed architecture of the classifier and analysis in the supplementary material. For the fully-image-level supervised setting, we adopt PMM <ref type="bibr" target="#b18">[27]</ref> as our WSSS method because it does not use the saliency maps.</p><p>For the instance segmentation network, we follow the network structure of Panoptic-DeepLab [7] with a modification. We change the center map from class-agnostic to class-wise for more accurate instance grouping. We adopt HRNet48 <ref type="bibr" target="#b31">[39]</ref> as our backbone network. The input size for training is 416?416, and we keep the original resolution for evaluation. We train the network for 70 epochs with 32 batch size using Adam optimizer <ref type="bibr" target="#b14">[23]</ref> with 5?10 ?5 learning rate and polynomial learning rate scheduling <ref type="bibr" target="#b21">[30]</ref>. Some approaches [1, 31] employ an additional training step on <ref type="table">Table 3</ref>. Effect of the proposed methods: PAM, IAG (instance-aware guidance), refine (selfrefinement), and cluster (center clustering). # iters mAP 50 0 41.8 1 41.9 2 41.9 <ref type="figure">Figure 6</ref>. Evolution of the number of true-positives on VOC 2012 train-set. <ref type="figure">Figure 7</ref>. Evolution of the mAP50 on VOC 2012 val-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PAM IAG refine cluster mAP</head><formula xml:id="formula_6">50 ? ? ? ? 12.9 ? ? ? ? 29.3 ? ? ? ? 39.2 ? ? ? ? 41.5 ? ? ? ? 41.8</formula><p>Mask R-CNN <ref type="bibr" target="#b5">[15]</ref>; we denote this step as MRCNN refinement and train the Mask R-CNN following the official training recipe using pseudo labels generated by our network. We used PyTorch 1.7 framework <ref type="bibr" target="#b26">[35]</ref> with CUDA 10.1, CuDNN 7 and 8 NVIDIA V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Point-Supervised Instance Segmentation</head><p>In our framework, point label can be used as weak supervision. According to <ref type="bibr">[4,</ref><ref type="bibr">5]</ref>, annotation costs are as follow: image-level (20.0 sec/img), object count (22.2 sec/img), point (23.3 sec/img), bounding box (38.1 sec/img), full mask (239.7 sec/img). The point label is an economical label that is 16% more expensive than the image-level label. For our point-supervised setting, we replace instance cues from PAM with point labels for the semantic knowledge transfer and replace the pseudo and refined center maps with the ground-truth center map for the self-refinement. Using the point supervision, we can obtain 10% more truepositives in pseudo labels for VOC 2012 due to the accurate instance cue, boosting the performance as in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">State-of-the-arts Comparison</head><p>We compare our BESTIE with existing state-of-the-art WSIS methods in <ref type="table" target="#tab_3">Table 1 for VOC 2012 and Table 2</ref> for COCO dataset. Even without the off-the-shelf proposals, BESTIE outperforms existing methods, especially in the AP 50 metric. Although LIID <ref type="bibr" target="#b22">[31]</ref> achieved a 1.6% AP higher than ours on COCO, they utilized two proposal tech-niques that require pre-training with high-level labels, violating the fully-image-level supervised setting. Compared to the fully-image-level supervised method, IRN [1], we outperform the method (51.0% v.s. 46.7%) because their displacement field, which is similar to our offset map, does not consider the semantic drift problem. Also, IRN often fails to segment overlapping instances as in <ref type="figure" target="#fig_3">Figure 8</ref> because their inter-pixel relations are derived from class-wise information, not instance-wise information. Given point supervision, we further increased the performance gap with other methods at a reasonable cost and achieve a new state-of-theart performance on VOC and COCO datasets compared to the previous best point-supervised method, Wise-Net <ref type="bibr" target="#b16">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study and Analysis</head><p>For analysis, we skip the Mask R-CNN refinement and follow the mentioned implementation details. We count the number of true-positives on VOC 2012 train set containing 1,464 images and 3,507 instances in <ref type="figure">Figure 6</ref> and measure the mAP 50 on VOC 2012 validation set in <ref type="figure">Figure 7</ref>. <ref type="figure" target="#fig_1">Figure 3</ref>, we have trouble in obtaining accurate instance cues from conventional CAM due to the noisy activation regions. However, with our PAM, we can extract appropriate instance cues, which tremendously helps the proper semantic knowledge transfer and obtain three times more true-positive training samples than the CAM as shown in <ref type="figure">Figure 6</ref>, giving a 16.4% improvement as in the first and second rows of <ref type="table">Table 3</ref>.  Effect of Instance-aware Guidance: In this section, we abbreviate instance-aware guidance as IAG. For analysis, we train the network without IAG; it means that the whole region (including the background region) of the offset and center maps are reflected in the objective function. As in the first and third rows of <ref type="table">Table 3</ref>, without IAG, the performance drops by 9.9% because it suffers from the semantic drift problem as mentioned in the method section. Also, as shown in <ref type="figure">Figure 7</ref>, the model without IAG seems to be stuck in a local minimum, whereas the model with IAG seems to avoid the local minimum, enhancing the performance as training progresses. This result convinces us that IAG is effective to alleviate the semantic drift problem. Effect of Self-Refinement: Here, we compare the result of the network trained using only the pseudo labels without the self-refinement. As shown in <ref type="figure">Figure 6</ref>, the network with the self-refinement can have more true-positives as training progresses. Since the refined labels from the self-refinement are guided to the training, the network can further capture instance-level features and improve the performance by 2.3% as in the third and fourth rows of <ref type="table">Table 3</ref>. Effect of Center Clustering: As in the fourth and last rows of <ref type="table">Table 3</ref>, a 0.3% improvement when using the center clustering demonstrates that the center clustering can complement the generation of the refined labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of PAM: As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of WSSS method:</head><p>The results in <ref type="table" target="#tab_4">Table 4</ref> shows how the WSSS result affects the WSIS. Originally, we adopt PMM <ref type="bibr" target="#b18">[27]</ref> for our WSSS method, which shows 70.0% mIoU on VOC 2012 validation set. Adopting the WSSS methods of 7.3% and 3.7% lower mIoU (SingleStage [2] and RRM <ref type="bibr" target="#b35">[43]</ref>) drops mAP 50 by 2.1% and 0.7%. The results show that the performance of WSIS is relatively robust to that of WSSS. Additionally, we train with ground-truth semantic segmentation labels and obtain a performance gain of 7.6% mAP 50 ; this result leaves us the opportunity that the advancement of the WSSS method can improve the performance of our approach. Does iterative training help? Some weakly-supervised methods [3, 20, 41] maximize their performance by the iterative training strategy; they generate pseudo labels when training is finished and re-train the network using the pseudo labels in an offline manner. This strategy gives a progressive improvement but requires a huge training complexity. However, this strategy does not give us a noticeable improvement as in <ref type="table">Table 5</ref>, and we show that our single-step online self-refinement is quite efficient for label refinement.</p><p>Qualitative Results: We provide some qualitative results in <ref type="figure" target="#fig_3">Figure 8</ref>. Although the pseudo labels contain a few instance labels, BESTIE can accurately represent instance-level information, achieving high-quality instance masks.</p><p>Limitation and Future direction: Despite the significant performance enhancement from the proposed BESTIE, it remains more room to be improved. In our method, the number of true-positives in the pseudo labels is limited by overlapping objects in the image (see <ref type="figure" target="#fig_3">Figure 8</ref>). The number of overlapping objects is different from dataset to dataset, i.e., less for VOC dataset but many for COCO dataset, and this affects the performance in some sort. Although our method achieved promising results in the VOC dataset with small 30% of true-positives in the pseudo labels, one future direction will be the suggestion of more effective truepositive acquiring rules for the various condition of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel approach for WSIS by addressing the pain-points of previous methods: dependency on the off-the-shelf proposals where extra instance or object level guidance is indispensable and the semantic drift problem. In our semantic knowledge transfer, we transferred the knowledge of WSSS combined with instance cues to WSIS and obtained pseudo instance labels. Here, we proposed the PAM module to accurately extract the instance cues. In our self-refinement, we refined the pseudo labels in a self-supervised scheme and employed them in training. For the stable learning with resolving the semantic drift problem, we introduced the instance-aware guidance strategy. Extensive experiments demonstrated the effectiveness of our approach, and our approach outperforms the previous methods with only using image-level labels and without any off-the-shelf proposals. Lastly, we conclude that this research does not contain potential negative societal impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>The authors thank NAVER Smart Machine Learning (NSML) <ref type="bibr" target="#b12">[22]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Detail of Center Clustering Algorithm</head><p>For the complementary knowledge between each network output, we employ the center clustering algorithm to extract center points from the offset map when generating the refined label. Here, we describe a detailed algorithm for the center clustering with a <ref type="figure">Figure 9</ref>. First, from the offset map, we create a magnitude map where each pixel represents the magnitude of the 2D vector. In this magnitude map, the pixel near the center of each instance is close to zero. Second, we apply a threshold to the magnitude map. We set the threshold to 2.5. Last, we extract the center point of each mask candidate obtained from the connected component labeling (CCL) algorithm. Here, we observe that the optimal area of the mask candidate is determined according to the threshold. For example, when the threshold is 2.5, the desired area of the mask candidate is near 21. For reliability-check utilizing the above observation, we additionally check whether the area of the mask candidate is between 21-? and 21+?; we empirically set the ? to 3. Due to the reliability-check process, we can prevent extracting false center points from the unstable offset map in the early training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Offset map Magnitude map Threshold &amp; CCL <ref type="figure">Figure 9</ref>. Illustration of the center clustering algorithm. The blue and yellow pixels in the magnitude map indicate that pixel values are close to zero and far from zero, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Additional Ablation Study</head><p>Here, we provide some additional ablation studies. First is the class-wise center map. As mentioned in the experiment section, we modify the original Panoptic-DeepLab network; we change the class-agnostic center map to the class-wise center map. This modification yields a 1.0% mAP 50 improvement due to more accurate instance grouping; compared to the full supervision, our noisy offset vectors in the offset map are sometimes grouped with incorrect center points. To prevent incorrect instance grouping, we restrict the centers of other classes not to be grouped by adopting the class-wise center map.</p><p>The second is an additional analysis for the proposed methods. In <ref type="table">Table 3</ref> in the main paper, we provided the analysis for the proposed methods. Here, we show an additional study for the model without IAG but with self-refine. As in <ref type="table" target="#tab_7">Table 6</ref>, the self-refine without IAG drops the performance because the model without IAG suffers from the semantic drift problem. And the drift iteratively degrades the quality of the refined label, hurting the model. The third is the effect of hyperparameter ? that is a threshold for the PAM module. When the ? becomes large, more noisy regions are deactivated. However, due to the IAG and self-refine, mAP 50 result of BESTIE is robust to the ? as in <ref type="table">Table 7</ref>.</p><p>The fourth is the effect of the backbone network in BESTIE. As mentioned in the experiment section, we adopt HRNet-48 <ref type="bibr" target="#b31">[39]</ref> as our backbone network. Here, we study the effect of the backbone network by replacing another backbone network, i.e., ResNet-50 <ref type="bibr" target="#b6">[16]</ref>. As an experimental result, the HRNet-48 backbone yields about 1% mAP 50 higher performance than the ResNet-50 (41.8% mAP 50 for HRNet-48 and 40.9% mAP 50 ResNet-50 on VOC 2012 validation set). The reason is that the receptive field of the HRNet-48 is much larger than the ResNet-50 and the HRNet-48 is a well-designed network for the key-point representation.</p><p>The last is a threshold for extracting instance cues from PAM. Namely, we extract instance cues by obtaining local maximum points from the PAM. Here, we adopt the instance cues whose value is larger than the threshold, which is set to 0.5. When we change the threshold to 0.3 and 0.7, the number of true-positives in pseudo labels changes slightly, but the mAP 50 variation is quite small to ?0.1%. This is because our self-refinement method can progressively refine the pseudo labels and increase the number of true-positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Details of Peak Attention Module (PAM) Implementation Details</head><p>As described in the main paper, we extract instance cues from the classifier with our PAM. Here, we explain the implementation details of the PAM. We employ VGG-16 <ref type="bibr" target="#b30">[38]</ref> classifier and plugin our PAM into the last three convolutional layers of the classifier. The architecture of the classifier with PAM is illustrated in <ref type="figure">Figure 10</ref>. For training the classifier with PAM, we use the binary cross-entropy loss function and the stochastic gradient descent (SGD) optimizer with a weight decay of 0.0005 and a momentum of 0.9. The initial learning rate is set to 0.001 and is decreased  <ref type="figure">Figure 10</ref>. The detailed architecture of our classification network. BCE loss means the binary cross entropy loss function. The PAM is equipped in last three convolutional layers and trained with a self-supervised scheme.</p><p>by a factor of 10 at epoch 5 and 10. For data augmentation, images are randomly cropped to 321?321, and random horizontal flipping and random color jittering are applied. We use a batch size of 5 and train the classifier for 15 epochs. In the following section, we analyze how the PAM module affects each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of PAM on each layer of Classifier</head><p>In this section, we analyze the effect of the PAM on each layer of the classifier. With the classifier described in <ref type="figure">Figure 10</ref> as our baseline, we plug-in or plug-out the module. For the quantitative comparison as in <ref type="table" target="#tab_9">Table 8</ref>, we evaluate the mean average precision (mAP) of our instance segmentation network without the Mask R-CNN refinement step. Since our PAM strengthens the attention on peak regions by deactivating noisy regions, we necessary to accurately distinguish between peak and noise regions. In lower-level layers (i.e., layer-1, layer-2, and layer-3), the classifier captures the local features such as edges, and the definition of the peak region is unclear, so the effect of the PAM is minor. In contrast, in higher-level layers (i.e., layer-4, layer-5, and layer-6), especially in the last layer, the classifier captures the global features, and the distinction between peak regions and noisy regions is more clear; our PAM plays a meaningful role in the last layer. From the results in Table 8, we note that the PAM equipped in only the last layer yields the best performance (41.8% mAP 50 ) but the PAM equipped in the last three layers significantly degrades the performance (34.8% mAP 50 ). We conclude that it is most effective to use the PAM only in the last layer where the definition of peak regions and noisy regions is the most obvious, and excessive modulization of PAM might deactivate the important features, degrading the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Qualitative Results of Pseudo Label</head><p>In <ref type="figure">Figure 11</ref>, we provide more qualitative results of activation maps and pseudo labels. As in the orange area of <ref type="figure">Figure 11</ref>, the conventional CAMs have a limitation in generating high-quality pseudo labels due to the noisy activation region. However, as in the green area of <ref type="figure">Figure 11</ref>, our PAM produces sparse CAMs that help to extract one instance cue per instance. Therefore, from the semantic knowledge transfer, we can obtain more reliable pseudo labels, and the pseudo labels contain more true positive training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Qualitative Results of Proposed Method</head><p>In <ref type="figure">Figure 12</ref>, we provide more qualitative results of our pseudo labels and network outputs. The pseudo label provides some reliable true-positive samples but contains lots of false-negatives (i.e., missing instances). Due to the proposed self-refinement with the instance-aware guidance, the network can produce high-quality instance masks including missing instances in pseudo labels. In addition, we compare our instance mask with that of IRN [1], which is the proposal-free method. The comparison results clearly show that our approach can properly segment multiple instances with a high-precision instance mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Failure Cases for PAM</head><p>We provide some failure cases of PAM in <ref type="figure" target="#fig_1">Figure 13</ref>, and these examples demonstrate the superiority of the pointsupervised setting because inaccurate instance cues are replaced by ground-truth points. PAM has trouble in accurately localizing overlapping instances, which leads to the incorrect pseudo label (first row in <ref type="figure" target="#fig_1">Figure 13</ref>). In addition, missing instance cue or noisy localization increases missing instances in the pseudo label (second and third rows in <ref type="figure" target="#fig_1">Figure 13)</ref>. Last, the WSSS method is trained with only imagelevel labels, some insufficient semantic segmentation maps yield inaccurate pseudo labels (last row in <ref type="figure" target="#fig_1">Figure 13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Failure Cases for Proposed Method</head><p>We provide some failure cases of BESTIE in <ref type="figure">Figure 14</ref>. First, when center points of instances are close to each other, we often fail to obtain the proper instance masks (first row in <ref type="figure">Figure 14)</ref>; however, the keypoint-based method has suffered this issue even in a fully-supervised setting. In addition, noisy center and offset maps lead to false instance masks (second and third rows in <ref type="figure">Figure 14)</ref>. Last, when the semantic segmentation map provides a noisy foreground region, we often fail to obtain the precise instance mask.</p><p>Appendix: Pytorch-style Pseudo-code.</p><p>To describe the details of each proposed methods, we provide pytorch-style pseudo-code algorithm. Note that our BESTIE is simple and easy to be implemented.   <ref type="figure">Figure 11</ref>. Qualitative results of pseudo labels generated from conventional CAMs (orange region) and pseudo labels generated from our PAM (green region). The PAM can more accurately extract one peak point per instance than the CAM.  <ref type="figure">Figure 12</ref>. Qualitative results of our pseudo labels and outputs of BESTIE on VOC 2012 dataset. We note that we only use the image-level labels without the off-the-shelf proposal techniques. Compared with IRN [1], which is the proposal-free method, our BESTIE can segment multiple instances more accurately and precisely. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The activation maps from CAM and PAM. PAM helps extract more accurate instance cues than CAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of our BESTIE trained with image-level supervision on VOC 2012 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Listing 5 .</head><label>5</label><figDesc>Refined Label Generation 1 def objective_function_with_IAG(pred, gt, eps=1e-4): 2 gamma_offset, gamma_center, gamma_semantic = 0.01, 200.0, 1.0 3 guidance_region = (gt['offset'] != 0).float() # labeled instance region 4 5 offset_map_loss = F.l1_loss(pred['offset'], gt['offset'], reduction=' none') * guidance_region 6 offset_map_loss = offset_map_loss.sum() / (guidance_region.sum() + eps) 7 8 center_map_loss = F.mse_loss(pred['center'],gt['center'], reduction=' none') * guidance_region 9 center_map_loss = center_map_loss.sum() / (guidance_region.sum() + eps) 10 11 semantic_map_loss = F.cross_entropy(pred['semantic'], gt['semantic']) 12 13 return (offset_map_loss * gamma_offset) + (center_map_loss * gamma_center) + (semantic_map_loss * gamma_semantic) Listing 6. Objective Function with IAG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Failure cases of the PAM. Failure cases of BESTIE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Semantic Knowledge Transfer PAM</head><label></label><figDesc>produces a peak response map</figDesc><table><row><cell></cell><cell></cell><cell>Instance cue</cell><cell>Self-Supervised Pseudo Label Refinement</cell><cell>Forward</cell><cell>Guidance</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Center clustering</cell></row><row><cell></cell><cell></cell><cell>Pseudo Offset Map</cell><cell>Output Offset Map</cell><cell>Refined Offset Map</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Instance Grouping</cell></row><row><cell cols="2">Pseudo Instance Label</cell><cell>Pseudo Center Map</cell><cell>Output Center Map</cell><cell>Refined Center Map</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Foreground mask</cell></row><row><cell>Reject</cell><cell>Accept</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Pseudo Semantic Map</cell><cell>Output Semantic Map</cell><cell>Input</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Network</cell></row><row><cell cols="2">Weakly-Supervised</cell><cell></cell><cell></cell></row><row><cell>Semantic</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Segmentation</cell><cell></cell><cell>3 output branches</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Method</cell><cell>Sup</cell><cell>Extra</cell><cell cols="4">mAP 25 mAP 50 mAP 70 mAP 75</cell></row><row><cell>Mask R-CNN [15]</cell><cell>F</cell><cell>-</cell><cell>76.7</cell><cell>67.9</cell><cell>52.5</cell><cell>44.9</cell></row><row><cell>PRM [45]</cell><cell>I</cell><cell>M</cell><cell>44.3</cell><cell>26.8</cell><cell>-</cell><cell>9.0</cell></row><row><cell>IAM [46]</cell><cell>I</cell><cell>M</cell><cell>45.9</cell><cell>28.3</cell><cell>-</cell><cell>11.9</cell></row><row><cell>Label-PEnet [14]</cell><cell>I</cell><cell>R</cell><cell>49.2</cell><cell>30.2</cell><cell>-</cell><cell>12.9</cell></row><row><cell>CL [19]</cell><cell>I</cell><cell>M, R</cell><cell>56.6</cell><cell>38.1</cell><cell>-</cell><cell>12.3</cell></row><row><cell>BESTIE (ours)</cell><cell>I</cell><cell>-</cell><cell>53.5</cell><cell>41.8</cell><cell>28.3</cell><cell>24.2</cell></row><row><cell>OCIS [9]</cell><cell>C</cell><cell>M</cell><cell>48.5</cell><cell>30.2</cell><cell>-</cell><cell>14.4</cell></row><row><cell>WISE-Net [24]</cell><cell>P</cell><cell>M</cell><cell>53.5</cell><cell>43.0</cell><cell>-</cell><cell>25.9</cell></row><row><cell>BESTIE (ours)</cell><cell>P</cell><cell>-</cell><cell>58.6</cell><cell>46.7</cell><cell>33.1</cell><cell>26.3</cell></row><row><cell>WISE  ? [25]</cell><cell>I</cell><cell>M</cell><cell>49.2</cell><cell>41.7</cell><cell>-</cell><cell>23.7</cell></row><row><cell>IRN  ? [1]</cell><cell>I</cell><cell>-</cell><cell>-</cell><cell>46.7</cell><cell>23.5</cell><cell>-</cell></row><row><cell>LIID  ? [31]</cell><cell>I</cell><cell>M, S I</cell><cell>-</cell><cell>48.4</cell><cell>-</cell><cell>24.9</cell></row><row><cell>Arun et al.  ? [3]</cell><cell>I</cell><cell>M</cell><cell>59.7</cell><cell>50.9</cell><cell>30.2</cell><cell>28.5</cell></row><row><cell>BESTIE (ours)  ?</cell><cell>I</cell><cell>-</cell><cell>61.2</cell><cell>51.0</cell><cell>31.9</cell><cell>26.6</cell></row><row><cell>BESTIE (ours)  ?</cell><cell>P</cell><cell>-</cell><cell>66.4</cell><cell>56.1</cell><cell>36.5</cell><cell>30.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Sup</cell><cell>Extra</cell><cell cols="3">AP AP 50 AP 75</cell></row><row><cell></cell><cell cols="2">COCO val2017</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [15]</cell><cell>F</cell><cell>-</cell><cell cols="2">35.4 57.3</cell><cell>37.5</cell></row><row><cell>WS-JDS [37]</cell><cell>I</cell><cell>M</cell><cell>6.1</cell><cell>11.7</cell><cell>5.5</cell></row><row><cell>WISE-Net [24]</cell><cell>P</cell><cell>M</cell><cell>7.8</cell><cell>18.2</cell><cell>8.8</cell></row><row><cell>BESTIE (ours)</cell><cell>I</cell><cell>-</cell><cell cols="2">14.3 28.0</cell><cell>13.2</cell></row><row><cell>BESTIE (ours)</cell><cell>P</cell><cell>-</cell><cell cols="2">17.7 34.0</cell><cell>16.4</cell></row><row><cell></cell><cell cols="2">COCO test-dev</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [15]</cell><cell>F</cell><cell>-</cell><cell cols="2">35.7 58.0</cell><cell>37.8</cell></row><row><cell>Fan et al. [13]</cell><cell>I</cell><cell>S I</cell><cell cols="2">13.7 25.5</cell><cell>13.5</cell></row><row><cell>LIID [31]</cell><cell>I</cell><cell cols="3">M, S I 16.0 27.1</cell><cell>16.5</cell></row><row><cell>BESTIE (ours)</cell><cell>I</cell><cell>-</cell><cell cols="2">14.4 28.0</cell><cell>13.5</cell></row><row><cell>BESTIE (ours)</cell><cell>P</cell><cell>-</cell><cell cols="2">17.8 34.1</cell><cell>16.7</cell></row></table><note>Quantitative comparison of state-of-the-art WSIS meth- ods on MS COCO 2017 dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Analysis of the effect of WSSS result on our WSIS performance. We measure the mAP50 instance segmentation performance according to WSSS methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Table 5. Effect of itera-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tive training strategy on</cell></row><row><cell></cell><cell></cell><cell></cell><cell>our method. iters=0 means</cell></row><row><cell></cell><cell></cell><cell></cell><cell>without the iterative train-</cell></row><row><cell cols="3">Semantic Segmentation Instance Segmentation</cell><cell>ing strategy.</cell></row><row><cell cols="2">WSSS method mIoU</cell><cell>mAP 50</cell></row><row><cell>SingleStage [2]</cell><cell>62.7</cell><cell>39.7</cell></row><row><cell>RRM [43]</cell><cell>66.3</cell><cell>41.1</cell></row><row><cell>PMM [27]</cell><cell>70.0</cell><cell>41.8</cell></row><row><cell>ground truth</cell><cell>-</cell><cell>49.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>team for the GPU support. This work was partly supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2021-0-02068, Artificial Intelligence Innovation Hub) and Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT &amp; Future Planning (NRF-2021R1A2C2008946).</figDesc><table><row><cell>References</cell></row><row><cell>[1] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly super-</cell></row><row><cell>vised learning of instance segmentation with inter-pixel rela-</cell></row><row><cell>tions. In Proceedings of the IEEE Conference on Computer</cell></row><row><cell>Vision and Pattern Recognition, pages 2209-2218, 2019. 3,</cell></row><row><cell>6, 7, 12, 15</cell></row><row><cell>[2] Nikita Araslanov and Stefan Roth. Single-stage seman-</cell></row><row><cell>tic segmentation from image labels. In Proceedings of</cell></row><row><cell>the IEEE/CVF Conference on Computer Vision and Pattern</cell></row><row><cell>Recognition, pages 4253-4262, 2020. 7, 8</cell></row><row><cell>[3] Aditya Arun, CV Jawahar, and M Pawan Kumar. Weakly su-</cell></row><row><cell>pervised instance segmentation by learning annotation con-</cell></row><row><cell>sistent instances. In European Conference on Computer Vi-</cell></row><row><cell>sion, pages 254-270. Springer, 2020. 3, 6, 8</cell></row><row><cell>[4] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li</cell></row><row><cell>Fei-Fei. What's the point: Semantic segmentation with point</cell></row><row><cell>supervision. In European conference on computer vision,</cell></row><row><cell>pages 549-565. Springer, 2016. 7</cell></row><row><cell>[5] M?riam Bellver Bueno, Amaia Salvador Aguilera, Jordi Tor-</cell></row><row><cell>res Vi?als, and Xavier Gir? Nieto. Budget-aware semi-</cell></row><row><cell>supervised semantic and instance segmentation. In The IEEE</cell></row><row><cell>Conference on Computer Vision and Pattern Recognition</cell></row><row><cell>(CVPR) Workshops, 2019, pages 93-102, 2019. 7</cell></row><row><cell>[6] Lyndon Chan, Mahdi S Hosseini, Corwyn Rowsell, Kon-</cell></row><row><cell>stantinos N Plataniotis, and Savvas Damaskinos. Histoseg-</cell></row><row><cell>net: Semantic segmentation of histological tissue type in</cell></row><row><cell>whole slide images. In Proceedings of the IEEE/CVF In-</cell></row><row><cell>ternational Conference on Computer Vision, pages 10662-</cell></row><row><cell>10671, 2019. 2</cell></row><row><cell>[7] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,</cell></row><row><cell>Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.</cell></row><row><cell>Panoptic-deeplab: A simple, strong, and fast baseline for</cell></row><row><cell>bottom-up panoptic segmentation. In Proceedings of the</cell></row><row><cell>IEEE/CVF Conference on Computer Vision and Pattern</cell></row><row><cell>Recognition, pages 12475-12485, 2020. 2, 3, 6</cell></row><row><cell>[8] Junsuk Choe, Seungho Lee, and Hyunjung Shim. Attention-</cell></row><row><cell>based dropout layer for weakly supervised single object lo-</cell></row><row><cell>calization and semantic segmentation. IEEE Transactions on</cell></row><row><cell>Pattern Analysis and Machine Intelligence, 2020. 9</cell></row><row><cell>[9] Hisham Cholakkal, Guolei Sun, Fahad Shahbaz Khan, and</cell></row><row><cell>Ling Shao. Object counting and instance segmentation with</cell></row><row><cell>image-level supervision. In Proceedings of the IEEE con-</cell></row><row><cell>ference on computer vision and pattern recognition, pages</cell></row><row><cell>12397-12405, 2019. 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Additional analysis for the proposed methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Table 7. Effect of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">hyperparameter ?.</cell></row><row><cell cols="3">PAM IAG refine mAP 50 ? 29.3 ? ? 39.2 ? ? ? 41.8</cell><cell>? 0.3 0.5</cell><cell>mAP 50 41.76 41.80</cell></row><row><cell>?</cell><cell>?</cell><cell>27.8</cell><cell>0.7</cell><cell>41.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Effect of the PAM on each layer of the classifier. ? means the PAM is equipped.</figDesc><table><row><cell></cell><cell>PAM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">layer4 layer5 layer6 mAP 25 mAP 50 mAP 75</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40.2</cell><cell>34.7</cell><cell>19.6</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>53.5</cell><cell>41.8</cell><cell>24.2</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>49.6</cell><cell>39.5</cell><cell>23.9</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>43.7</cell><cell>34.8</cell><cell>21.2</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>53.6</cell><cell>40.0</cell><cell>23.8</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>48.3</cell><cell>37.6</cell><cell>22.1</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>49.9</cell><cell>39.6</cell><cell>23.4</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>49.3</cell><cell>38.4</cell><cell>22.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>(np.zeros_like(mask_idx[0]), mask_idx[0], mask_idx[1]) 21 offset_x_index = (np.ones_like(mask_idx[0]), mask_idx[0], mask_idx[1]) 22 23 pseudo_offset_map[offset_y_index] = cy -y_coord[mask_idx] 24 pseudo_offset_map[offset_x_index] = cx -x_coord[mask_idx]</figDesc><table><row><cell></cell><cell></cell><cell>19</cell><cell></cell></row><row><cell></cell><cell></cell><cell>20</cell><cell>offset_y_index = Listing 4. Pseudo-Label Generation</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>def refined_label_gen(center_map, offset_map, seg_map, thresh):</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>instance_masks = instance_grouping(center_map, offset_map, seg_map,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>thresh)</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>for mask in instance_masks:</cell></row><row><cell></cell><cell></cell><cell>5</cell><cell>center_point = centroid(mask)</cell></row><row><cell></cell><cell></cell><cell>6</cell><cell>center_map_generation(center_point)</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>offset_map_generation(center_point, mask)</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell></cell></row><row><cell></cell><cell></cell><cell>9</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>def instance_grouping(center_map, offset_map, seg_map, thresh):</cell></row><row><cell></cell><cell></cell><cell>11</cell><cell>ctr = extract_peak_points(center_map, thresh)</cell></row><row><cell></cell><cell></cell><cell>12</cell><cell>ctr = center_clustering(ctr, offset_map)</cell></row><row><cell></cell><cell></cell><cell>13</cell><cell># center_map : [1, C, H, W], ctr : [N, 2], offset_map : [2, H, W]</cell></row><row><cell></cell><cell></cell><cell>14</cell><cell></cell></row><row><cell></cell><cell></cell><cell>15</cell><cell>H, W = offset_map.size()[1:]</cell></row><row><cell></cell><cell></cell><cell>16</cell><cell>y_coord = torch.arange(H).repeat(1, W, 1).transpose(1, 2)</cell></row><row><cell></cell><cell></cell><cell>17</cell><cell>x_coord = torch.arange(W).repeat(1, H, 1)</cell></row><row><cell></cell><cell></cell><cell>18</cell><cell>coord = torch.cat((y_coord, x_coord), dim=0)</cell></row><row><cell></cell><cell></cell><cell>19</cell><cell></cell></row><row><cell></cell><cell></cell><cell>20</cell><cell>ctr_loc = coord + offsets</cell></row><row><cell>1</cell><cell>for n in range(train_iterations):</cell><cell>21</cell><cell>ctr_loc = ctr_loc.reshape((2, H*W)).transpose(1, 0) # [H*W, 2]</cell></row><row><cell>2</cell><cell># load data: x (input tensor), seg_map (weakly-supervised semantic map)</cell><cell>22</cell><cell></cell></row><row><cell>3</cell><cell>x, seg_map = loader.next()</cell><cell>23</cell><cell>dist = torch.norm(ctr.unsqueeze(1)-ctr_loc.unsqueeze(0), dim=-1) # [N,</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>H*W]</cell></row><row><cell>5</cell><cell># pseudo-label generation</cell><cell>24</cell><cell></cell></row><row><cell>6</cell><cell>PAM = classifier(x)</cell><cell>25</cell><cell># finds center with minimum distance at each location</cell></row><row><cell>7</cell><cell>peak_points = extract_peak_points(PAM)</cell><cell>26</cell><cell>instance_mask = torch.argmin(dist, dim=0).reshape((1, H, W)) + 1</cell></row><row><cell>8</cell><cell>pseudo_label = pseudo_label_gen(seg_map, peak_points)</cell><cell>27</cell><cell></cell></row><row><cell>9</cell><cell></cell><cell>28</cell><cell>return instance_mask</cell></row><row><cell>10</cell><cell># network forwarding</cell><cell></cell><cell></cell></row><row><cell>11</cell><cell>outputs = BESTIE(x)</cell><cell></cell><cell></cell></row><row><cell>12</cell><cell></cell><cell></cell><cell></cell></row><row><cell>13</cell><cell># refined-label generation</cell><cell></cell><cell></cell></row><row><cell>14</cell><cell>refined_label = refined_label_gen(outputs)</cell><cell></cell><cell></cell></row><row><cell>15</cell><cell></cell><cell></cell><cell></cell></row><row><cell>16</cell><cell># backward &amp; optimize</cell><cell></cell><cell></cell></row><row><cell>17</cell><cell>loss_pseudo = objective_function_with_IAG(outputs, pseudo_label)</cell><cell></cell><cell></cell></row><row><cell>18</cell><cell>loss_refined = objective_function_with_IAG(outputs, refined_label)</cell><cell></cell><cell></cell></row><row><cell>19</cell><cell>loss = loss_pseudo + loss_refined</cell><cell></cell><cell></cell></row><row><cell>20</cell><cell>loss.backward()</cell><cell></cell><cell></cell></row><row><cell>21</cell><cell>optimizer.step()</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Listing 1. Overview of BESTIE</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>def PAM(x, control_values):</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>x = F.relu(x) # [B, K, H, W]</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>criteria_points = F.adaptive_max_pool2d(x, 1) # [B, K, 1, 1]</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>criteria_points = criteria_points.expand_as(x) # [B, K, H, W]</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>noisy_region = (x &lt; criteria_points * control_values)</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>x[noisy_region] = 0 # deactivate noisy region</cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>return x</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Listing 2. PAM Module</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>def extract_peak_points(heatmap, kernel, threshold, K=40):</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>B, C, H, W = heatmap.size()</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>heatmap_max = F.max_pool2d(heatmap, (kernel, kernel),</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>stride=1, padding=(kernel -1) // 2)</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>keep = (heatmap_max == heatmap).float()</cell><cell></cell><cell></cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>local_max = heatmap * keep</cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>peak_points = torch.nonzero(local_max &gt; threshold)</cell><cell></cell><cell></cell></row><row><cell>10</cell><cell>return peak_points</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Listing 3. Point Extraction</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>def pseudo_label_gen(seg_map, peak_points):</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell># seg_map : [H, W] , peak points : [N, 2]</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>mask_candidates = connected_component_labeling(seg_map)</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>for mask in mask_candidates:</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>if one_peak_point_in_mask(mask, peak_points):</cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>center_point = centroid(mask)</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>center_map_generation(center_point)</cell><cell></cell><cell></cell></row><row><cell>9</cell><cell>offset_map_generation(center_point, mask)</cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>11</cell><cell></cell><cell></cell><cell></cell></row><row><cell>12</cell><cell>def offset_map_generation(center_point, mask): # mask: bindary mask</cell><cell></cell><cell></cell></row><row><cell>13</cell><cell>cy, cx = center_point</cell><cell></cell><cell></cell></row><row><cell>14</cell><cell>mask_idx = np.where(mask &gt; 0)</cell><cell></cell><cell></cell></row><row><cell>15</cell><cell></cell><cell></cell><cell></cell></row><row><cell>16</cell><cell>coord = np.ones_like(mask, dtype=np.float32)</cell><cell></cell><cell></cell></row><row><cell>17</cell><cell>y_coord = np.cumsum(coord, axis=0)-1</cell><cell></cell><cell></cell></row><row><cell>18</cell><cell>x_coord = np.cumsum(coord, axis=1)-1</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">S4net: Single stage salient-instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6103" to="6112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Associating inter-image salient instances for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Label-penet: Sequential label propagation and enhancement networks for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast connected-component labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kesheng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1977" to="1987" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghun</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7322" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation by deep community learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaedong</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative region suppression for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1754" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<pubPlace>Heungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proposal-based instance segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2126" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Where are the masks: Instance segmentation with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Railroad is not a train: Saliency as pseudo-pixel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5495" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pseudo-mask matters in weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6964" to="6973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leveraging instance-, imageand dataset-level information for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Huan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Song</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8837" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation of satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Nivaggioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hicham</forename><surname>Randrianarivo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Joint urban remote sensing event (JURSE)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<imprint>
			<pubPlace>Zeming</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="128" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="697" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12765" to="12772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation using class peak response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3791" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning instance activation maps for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3116" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
