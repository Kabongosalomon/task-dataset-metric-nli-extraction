<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimVP: Simpler yet Better Video Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
							<email>*gaozhangyang@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tan</surname></persName>
							<email>tancheng@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
							<email>wulirong@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>stan.zq.li@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimVP: Simpler yet Better Video Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From CNN, RNN, to ViT, we have witnessed remarkable advancements in video prediction, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. We admire these progresses but are confused about the necessity: is there a simple method that can perform comparably well? This paper proposes SimVP, a simple video prediction model that is completely built upon CNN and trained by MSE loss in an end-to-end fashion. Without introducing any additional tricks and complicated strategies, we can achieve state-of-the-art performance on five benchmark datasets. Through extended experiments, we demonstrate that SimVP has strong generalization and extensibility on real-world datasets. The significant reduction of training cost makes it easier to scale to complex scenarios. We believe SimVP can serve as a solid baseline to stimulate the further development of video prediction. The code is available at Github. RNN RNN RNN RNN RNN RNN RNN RNN RNN CNN RNN CNN CNN RNN CNN CNN RNN CNN CNN ViT ViT (a) Stacked RNN (b) CNN-RNN-CNN (c) CNN-ViT-CNN (d) CNN-CNN-CNN ViT CNN CNN CNN CNN CNN CNN CNN CNN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A wise person can foresee the future, and so should an intelligent vision model do. Due to spatio-temporal information implying the inner laws of the chaotic world, video prediction has recently attracted lots of attention in climate change <ref type="bibr" target="#b71">[71]</ref>, human motion forecasting <ref type="bibr" target="#b2">[3]</ref>, traffic flow prediction <ref type="bibr" target="#b69">[69]</ref> and representation learning <ref type="bibr" target="#b55">[55]</ref>. Struggling with the inherent complexity and randomness of video, lots of interesting works have appeared in the past years. These methods achieve impressive performance gain by introducing novel neural operators like various RNNs <ref type="bibr" target="#b65">[65]</ref><ref type="bibr" target="#b66">[66]</ref><ref type="bibr" target="#b67">[67]</ref><ref type="bibr" target="#b68">[68]</ref><ref type="bibr" target="#b69">[69]</ref><ref type="bibr" target="#b71">71]</ref> or transformers <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b70">70]</ref>, delicate architectures like autoregressive <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b71">71]</ref> or normalizing flow <ref type="bibr" target="#b73">[73]</ref>, and applying distinct training strategies such as adversarial training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b64">64]</ref>. However, there is relatively little understanding of their necessity for good performance since many methods use different metrics and datasets. Moreover, the increasing model complexity further aggravates * Equal contribution this dilemma. A question arises: can we develop a simpler model to provide better understanding and performance?</p><p>Deep video prediction has made incredible progress in the last few years. We divide primary methods into four categories in <ref type="figure">Figure.</ref>   <ref type="table" target="#tab_6">Table.</ref> 1, from which we observe that RNN models have been favored since 2014. In this context, lots of novel RNNs are proposed. Con-vLSTM <ref type="bibr" target="#b71">[71]</ref> extends fully connected LSTMs to have convolutional structures for capturing spatio-temporal correlations. PredRNN <ref type="bibr" target="#b67">[67]</ref> suggests simultaneously extracting and memorizing spatial and temporal representations. MIM-LSTM <ref type="bibr" target="#b69">[69]</ref> applies a self-renewed memory module to model both non-stationary and stationary properties. E3D-LSTM <ref type="bibr" target="#b66">[66]</ref> integrates 3D convolutions into RNNs. Phy-Cell <ref type="bibr" target="#b17">[18]</ref> learns the partial differential equations dynamics in the latent space.</p><p>Recently, vision transformers (ViT) have gained tremendous popularity. AViT <ref type="bibr" target="#b70">[70]</ref> merges ViT into the autoregressive framework, where the overall video is divided into volumes, and self-attention is performed within each block in-RNN-RNN-RNN CNN-RNN-CNN CNN-ViT-CNN CNN-CNN-CNN 2014-2015 <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b71">71]</ref> [ <ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">46]</ref> - <ref type="bibr" target="#b39">[40]</ref> 2016-2017 <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b67">67]</ref> [ <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b62">62]</ref> - <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">59]</ref> 2018-2019 <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b74">74]</ref> [6, <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b73">73]</ref> [70] <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b72">72</ref>] 2020-2021 <ref type="bibr" target="#b68">[68]</ref> [ <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref> [48] <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">54]</ref>  dependently. Latent AViT <ref type="bibr" target="#b48">[48]</ref> uses VQ-VAE <ref type="bibr" target="#b43">[44]</ref> to compress the input images and apply AViT in the latent space to predict future frames. In contrast, purely CNN-based models are not as favored as the approaches mentioned above, and fancy techniques are usually required to improve the novelty and performance, e.g., adversarial training <ref type="bibr" target="#b28">[29]</ref>, teacher-student distilling <ref type="bibr" target="#b6">[7]</ref>, and optical flow <ref type="bibr" target="#b15">[16]</ref>. We admire their significant advancements but expect to exploit how far a simple model can go. In other words, we have made much progress against the baseline results, but have the baseline results been underestimated?</p><p>We aim to provide a simpler yet better video prediction model, namely SimVP. This model is fully based on CNN and trained by the MSE loss end-to-end. Without introducing any additional tricks and complex strategies, SimVP can achieve state-of-the-art performance on five benchmark datasets. The simplicity makes it easy to understand and use as a common baseline. The better performance provides a solid foundation for further improvements. We hope this study will shed light on future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Problem statement Video prediction aims to infer the future frames using the previous ones. Given a video sequence X t,T = {x i } t t?T +1 at time t with the past T frames, our goal is to predict the future sequence Y t,T ? = {x i } t+T ? t at time t that contains the next T ? frames, where x i ? R C,H,W is an image with channels C, height H, and width W . Formally, the predicting model is a mapping F ? : X t,T ? Y t,T ? with learnable parameters ?, optimized by:</p><formula xml:id="formula_0">? * = arg min ? L(F ? (X t,T ), Y t,T ? )<label>(1)</label></formula><p>where L can be various loss functions, and we simply employ MSE loss in our setting. <ref type="figure">Figure.</ref> 1 (a), this kind of method stacks RNN to make predictions. They usually design novel RNN modules (local) and overall architectures (global). Recurrent Grammar Cells <ref type="bibr" target="#b40">[41]</ref> stacks multiple gated autoencoders in a recurrent pyramid structure. Con-vLSTM <ref type="bibr" target="#b71">[71]</ref> extends fully connected LSTMs to have convolutional computing structures to capture spatio-temporal correlations. PredRNN <ref type="bibr" target="#b67">[67]</ref> suggests simultaneously extracting and memorizing spatial and temporal representations. PredRNN++ <ref type="bibr" target="#b65">[65]</ref> proposes gradient highway unit to alleviate the gradient propagation difficulties for capturing long-term dependency. MIM-LSTM <ref type="bibr" target="#b69">[69]</ref> uses a selfrenewed memory module to model both the non-stationary and stationary properties of the video. dGRU <ref type="bibr" target="#b42">[43]</ref> shares state cells between encoder and decoder to reduce the computational and memory costs. Due to the excellent flexibility and accuracy, these methods play fundamental roles in video prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-RNN-RNN As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-RNN-CNN This framework projects video frames</head><p>to the latent space and employs RNN to predict the future latent states, seeing <ref type="figure" target="#fig_1">Figure. 1</ref>   <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b70">70]</ref> using ViT are still limited. More related works may emerge in the future.</p><p>CNN-CNN-CNN The CNN-based framework is not as popular as the previous three because it is so simple that complex modules and training strategies are usually required. DVF <ref type="bibr" target="#b34">[35]</ref> suggests learning the voxel flow by CNN autoencoder to reconstruct a frame by borrowing voxels from nearby frames. PredCNN <ref type="bibr" target="#b72">[72]</ref> combines cascade multiplicative units (CMU) with CNN to capture inter-frame dependencies. DPG <ref type="bibr" target="#b15">[16]</ref> disentangles motion and background via a flow predictor and a context generator. <ref type="bibr" target="#b6">[7]</ref> encodes RGB frames from the past and decodes the future semantic segmentation by using CNN and teacher-student distilling. <ref type="bibr" target="#b54">[54]</ref> uses a hierarchical neural model to make predictions at different spatial resolutions and train the model with adversarial and perceptual loss functions. While these approaches have made progress, we are curious about what happens if the complexity is reduced. Is there a solution that is are much simpler but can exceed or match the perfor-mance of state-of-the-art methods?</p><p>Motivation We have witnessed many terrific methods that have achieved outstanding performance. However, as the models become more complex, understanding their performance gain is an inevitable challenge, and scaling them into large datasets is intractable. This work does not propose new modules. Instead, we aim to build a simple network based on existing CNNs and see how far the simple model can go in video prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SimVP</head><p>Our model, dubbed SimVP, consists of an encoder, a translator and a decoder built on CNN, seeing <ref type="figure">Figure.</ref> 2. The encoder is used to extract spatial features, the translator learns temporal evolution, and the decoder integrates spatio-temporal information to predict future frames.</p><p>Encoder The encoder stacks N s ConvNormReLU blocks (Conv2d+LayerNorm+LeakyReLU) to extract spatial features, i.e., convoluting C channels on (H, W ). The hidden feature is:</p><formula xml:id="formula_1">z i = ?(LayerNorm(Conv2d(z i?1 ))), 1 ? i ? N s (2)</formula><p>where the input z i?1 and output z i shapes are (T, C, H, W ) and (T,?,?,? ), respectively.</p><p>Translator The translator employs N t Inception modules to learn temporal evolution, i.e., convoluting T ?C channels on (H, W ). The Inception module consists of a bottleneck Conv2d with 1?1 kernel followed by parallel GroupConv2d operators. The hidden feature is:</p><formula xml:id="formula_2">z j = Inception(z j?1 ), N s &lt; j ? N s + N t<label>(3)</label></formula><p>where the inputs z j?1 and output z j shapes are (T ? C, H, W ) and (T ??, H, W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>The decoder utilizes N s unConvNormReLU blocks (ConvTranspose2d+GroupNorm+LeakyReLU) to reconstruct the ground truth frames, which convolutes C channels on (H, W ). The hidden feature is:</p><formula xml:id="formula_3">z k = ?(GroupNorm(unConv2d(z k?1 ))), N s + N t &lt; k ? 2N s + N t<label>(4)</label></formula><p>where the shapes of input z k?1 and output z k are (T,?,?,? ) and (T, C, H, W ), respectively. We use Con-vTranspose2d <ref type="bibr" target="#b12">[13]</ref> to serve as the unConv2d operator.</p><p>Summary SimVP does not use advanced modules such as RNN, LSTM and Transformer, nor introduce complex training strategies such as adversarial training and curriculumn learning. All the things we need are CNN, shortcuts and vanilla MSE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Metrics We employ MSE, MAE, Structural Similarity Index Measure (SSIM), and Peak Signal to Noise Ratio (PSNR) to evaluate the quality of predictions, following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b73">73]</ref>. We also report the running time per epoch and the memory footprint per sample on a single NVIDIA-V100 to provide a comprehensive view for future research. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We conduct experiments on five datasets for evaluation. The statistics are summarized in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">How far can SimVP go?</head><p>We pursue the simple but effective model. The simplicity has been described in Section. 3. The effectiveness will be verified through exploratory experiments in this section. Challenge We aim to provide a comprehensive and rigorous view of video prediction methods. However, three challenges stand in our way:</p><p>? Various methods may adopt disparate metrics.</p><p>? These methods apply experiments on different datasets with distinct protocols. ? They use different code frameworks and unique tricks, making it difficult to compare fairly.</p><p>Solution To overcome aforemesioned challenges, we choose the common used dataset (Moving MNIST) and metrics (MSE and SSIM) to evaluate recent important researches. We directly report the best metrics according to the original papers, avoiding the risk of performance degradation caused by our reproduction. Any method that does not use the same dataset, metrics, or protocol will be neglected here. For convenience, we provide the publication status and Github links of these method in <ref type="table" target="#tab_6">Table.</ref> 3.</p><p>Results and Discovery As shown in <ref type="table" target="#tab_6">Table.</ref> 3, SimVP achieves state-of-the-art MSE and SSIM on Moving MNIST. We observe that SimVP, PhyDNet, and CrevNet significantly outperform previous methods, with MSE reduction up to 42%. However, SimVP is much simpler than PhyDNet and CrevNet, without using RNN, LSTM, or complicated modules, which are considered as the important reason for performance improvement. Through these explorations, we are excited to find that it is promising to achieve better performance with a extremely simple model. Perhaps previous works pay too much attention to the model complexity and novelty, and it's time to go back to basics because a simpler model makes things clearer.</p><p>Simplicity leads to efficiency. Another benefit that comes from simplicity is good computational efficiency. In Table. 4, we compare GPU memory (per sample), FLOPs (per image) and training time of SOTA methods on Moving MNIST. As CNN has good computational optimization and avoids iterative calculation, the training process of SimVP is much faster than others, which means that SimVP can be used and extended more easily. With the rapid development of novel temporal modules based on RNN, Transformer, and CNN, researchers may be dazzled and confused about which one to choose for video modeling. In SimVP, the Translator module is responsible for learning temporal evolution. We replace the CNN-based Translator with the most representative RNNs and Transformers to reveal that which time module is more suitable for video modeling under the SimVP framework.</p><p>Translator selection The CNN-based Translator serves as the baseline, since it is quite simple yet effective. We hope to find new modules that is promising to outperform the Translator to inspire subsequent researches. For RNN, we choose currently state-of-the-art PhyDNet <ref type="bibr" target="#b17">[18]</ref> and CrevNet <ref type="bibr" target="#b73">[73]</ref>. As suggested in PhyDNet, we use the PhyCell, a novel time module considering physical dynamics, for temporal modeling. As to CrevNet, we use normalizing flow autoencoder + ST-LSTM <ref type="bibr" target="#b67">[67]</ref> as the translator. For Transformers, we chose recent influential work such as Video Swin Transformer <ref type="bibr" target="#b33">[34]</ref> and Latent Video Transformer <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b70">70]</ref>. To make these modules workable under the SimVP framework, we may modify a few implementation details without changing the core algorithm, such as removing the autoregressive generation scheme and reimplementing position encoding.</p><p>Setting We compare five translators on Moving MNIST and Human, keeping the encoder and decoder the same. We adjust the hyperparameters of these translators to make them work with similar GPU memory footprints. By default, we use batch size 16, epoch 100, and Adam optimizer. The number of encoder and decoder layers is 4. We choose the largest learning rate from {1e ?2 , 1e ?3 , 1e ?4 } under the premise of stable training.  Results and Discoveries In <ref type="figure">Figure.</ref> 3, we show the training dynamics using various translators on Moving MNIST and Human3.6, respectively. CrevNet looks like a good solution because the inherent simplicity of Moving MNIST, while our SimVP is suboptimal. However, this does not always hold true when it comes to real-world datasets. For example, on Human dataset ( <ref type="figure" target="#fig_3">Figure. 3(b)</ref>), SimVP significantly outperforms other methods. As to the training stability, CNN performs better than RNN ( <ref type="figure">Figure.</ref> 7, appendix), since it will not fluctuate violently under large learning rate. In short, our discoveries are:</p><p>1. CNN and RNN achieves state-of-the-art performance under limited computation costs. 2. RNN converges faster than others in the long run <ref type="figure">(Figure.</ref> 3) if the model capacity is sufficient. 3. CNN training is more robust and does not fluctuate dramatically at large learning rates ( <ref type="figure">Figure. 7</ref>). 4. Transformer has no advantage in our SimVP framework under the similar resource consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Does SimVP achieve SOTA on general cases?</head><p>From previous analysis, we believe that SimVP has potential to outperform recent state-of-the-art methods. In this section, we provide more experimental evidence to confirm this claim. Specifically, we aim to answer three questions:</p><p>? Q1: Can SimVP achieve SOTA results on other common benchmarks? Setting of Q1 Reporting the widely used metrics on standard benchmark datasets is the key to advancing the research progress. We evaluate SimVP on three common used benchmarks, i.e., Moving MNIST, TrafficBJ and Hu-man3.6. Moving MNIST <ref type="bibr" target="#b55">[55]</ref> consists of two digits independently moving within the 64 ? 64 grid and bounced off the boundary. By assigning different initial locations and velocities to each digit, we can get an infinite number of sequences, and predict the future 10 frames from previous 10 frames. TrafficBJ contains the trajectory data in Beijing collected from taxicab GPS with two channels, i.e. inflow or outflow defined in <ref type="bibr" target="#b75">[75]</ref>. Following <ref type="bibr" target="#b69">[69]</ref>, we transform the data into [0, 1] via max-min normalization. Since the original data is between -1 and 1, the reported MSE and MAE are 1/4 and 1/2 of the original ones, consistent with previous literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b69">69]</ref>. Models are trained to predict 4 next frames by observing prioring 4 frames. Human3.6 <ref type="bibr" target="#b23">[24]</ref> is a complex human pose dataset with 3.6 million samples, recording different activities. Similar to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b69">69]</ref>, only videos with "walking" scenario are used, and 4 future frames are generated by feeding previous 4 RGB frames. Following <ref type="bibr" target="#b17">[18]</ref>, we report the MSE, MAE and SSIM of SimVP in  Setting of Q2 Generalizing the knowledge across different datasets, especially under the unsupervised setting, is the core research point of machine learning. To investigate the generalization ability of SimVP, we train the model for 50 epochs on KITTI and evaluate it on Caltech Pedestrian. KITTI <ref type="bibr" target="#b16">[17]</ref> is one of the most popular datasets for mobile robotics and autonomous driving. It includes hours of traffic scenarios recorded with high-resolution RGB images. CalTech Pedestrian <ref type="bibr" target="#b10">[11]</ref> is a driving dataset focused on detecting pedestrians. It is conformed of approximately 10 hours of 640 ? 480 30 FPS video taken from a vehicle driving through regular traffic in an urban environment. Models are trained on KITTI dataset to predict the next frame after 10-frame warm-up and are evaluated on Caltech Pedestrian. Compared with the previous experiments, carmounted camera videos dataset and the distinct training-evaluating data present another level of difficulty for video prediction as it describes various nonlinear threedimensional dynamics of multiple moving objects including backgrounds. Following <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b73">73]</ref>, we center-crop Sixteen baselines are selected for comparision, including Be-yondMSE <ref type="bibr" target="#b39">[40]</ref>, MCnet <ref type="bibr" target="#b62">[62]</ref>, DVF <ref type="bibr" target="#b34">[35]</ref>, Dual-GAN <ref type="bibr" target="#b31">[32]</ref>, CtrlGen <ref type="bibr" target="#b18">[19]</ref>,PredNet <ref type="bibr" target="#b36">[37]</ref>, ContextVP <ref type="bibr" target="#b4">[5]</ref>, GAN-VGG, G-VGG, G-MAE, GAN-MAE <ref type="bibr" target="#b54">[54]</ref>, SDC-Net <ref type="bibr" target="#b50">[50]</ref>, rCycle-Gan <ref type="bibr" target="#b28">[29]</ref>, DPG <ref type="bibr" target="#b15">[16]</ref>, CrevNet <ref type="bibr" target="#b73">[73]</ref> and STMFANet <ref type="bibr" target="#b25">[26]</ref>.  Answer of Q2 SimVP generalize well cross different datasets, seeing <ref type="figure" target="#fig_5">Figure. 5</ref>. Although there is room to improve the clarity of the generated objects, the evaluated metrics have exceeded previous methods. As shown in <ref type="table" target="#tab_6">Table.</ref> 6, the MSE is reduced by 3.1%, the SSIM is improved by 1.4%, and the PSNR is improved by 13.0%. The training process can be finished within 4h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting of Q3</head><p>A possible limitation of CNN-based methods is that it may be diffifcult to scale to prediction with flexible length. We handle this problem by imitating RNN, that is taking previous predictions as recent inputs to recursively produce long-term predictions. Following <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b66">66]</ref>, we compare the PSNR and SSIM of SimVP with other baselines on KTH, seeing  <ref type="bibr" target="#b62">[62]</ref>, ConvLSTM <ref type="bibr" target="#b71">[71]</ref>, SAVP, SAVP-VAE <ref type="bibr" target="#b29">[30]</ref>, VPN <ref type="bibr" target="#b27">[28]</ref>, DFN <ref type="bibr" target="#b24">[25]</ref>, fRNN <ref type="bibr" target="#b42">[43]</ref>, Znet <ref type="bibr" target="#b74">[74]</ref>, SV2P <ref type="bibr" target="#b2">[3]</ref>, PredRNN <ref type="bibr" target="#b67">[67]</ref>, VarNet <ref type="bibr" target="#b26">[27]</ref>, Pre-dRNN++ <ref type="bibr" target="#b65">[65]</ref>, MSNET <ref type="bibr" target="#b30">[31]</ref>, E3d-LSTM <ref type="bibr" target="#b66">[66]</ref> and STM-FANet <ref type="bibr" target="#b25">[26]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation study</head><p>While SimVP is quite simple, we believe there still exists unapprehended parts. We are eager to know:</p><p>? Q1: Which architectural design plays the key role in improving performance? ? Q2: How the Conv kernel influence the performance? ? Q3: What roles do the Enc, Translator, and Dec play?  <ref type="table" target="#tab_0">-T-UNet  -#Groups  4  4  1  4  4  4  4  4  4</ref> 4 <ref type="figure" target="#fig_1">5,7,11)</ref>  <ref type="table">Table 7</ref>. Ablation study. S-UNet or T-UNet denotes the shortcut connection in the spatial or temporal encoder-decoder. #Groups is the number of convolutional groups. G and B indicate group normalization and batch normalization. <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11)</ref>+ct means the Conv kernels of the Inception module plus the translator's hidden dimension. Note that the spatial Enc and Dec are fixed on each dataset, seeing <ref type="table" target="#tab_6">Table.</ref> 10 (appendix) for detailed settings. Finally, we compare models with SimVP, and results with a gap of less than 0.5 are regarded as the same.</p><formula xml:id="formula_4">Norm G G G B G G G G G G Kernel (3) +ct - - - - - - - - - (5)+ct - - - - - - - - - (7)+ct - - - - - - - - - (11)+ct - - - - - - - - - (11)+2ct - - - - - - - - - (3,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting of Q1 and Q2</head><p>We perform the ablation study on Moving MNIST, TrafficBJ, and Human3.6. All models are trained up to 100 epochs, different from previous settings. For neural architecture design, we study whether using spatial UNet shortcut, temporal UNet shortcut, group convolution, and group normalization can bring performance gain.</p><p>As to the convolutional kernel, we study how the kernel size and hidden dimension affect the model performance. We report the MSE metric for all datasets. Setting of Q3 We represent submodules trained with n epochs as Enc n , Translator n , Dec n , mix them and evaluate the results at t = 20 (the last prediction) to reveal the role of each well-tuned module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer of Q1</head><p>Answer of Q3 As shown in <ref type="figure">Figure.</ref> 6, we conduct that the translator mainly focus on predict the position and content of the objects. The decoder is responsible for optimizing the shape of the foreground objects. The encoder can erase the background error by means of spatial UNet connection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose SimVP, a simpler yet effective CNN model for video prediction. We show that SimVP can achieve state-of-the-art results without introducing any complex modules, strategies and tricks. Meanwhile, the reduced computing cost makes it easy to scale up to more scenarios. We believe simpler is better, and SimVP may serve as a strong baseline and provide inspiration for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Dataset</head><p>Moving MNIST Moving MNIST <ref type="bibr" target="#b55">[55]</ref> is a standard benchmark consisting of two digits independently moving within the 64 ? 64 grid and bounced off the boundary. By assigning different initial locations and velocities to each digit, we can get an infinite number of sequences of length 20, thus enabling us to evaluate models without considering the data insufficiency issues. By default, models are trained to predict the future 10 frames conditioned on the previous 10 frames. Although the dynamics seem simple at first glance, accurately predicting consistent long-term future frames remains challenging for state-of-the-art models.</p><p>TrafficBJ TrafficBJ contains the trajectory data in Beijing collected from taxicab GPS with two channels, i.e. inflow or outflow defined in <ref type="bibr" target="#b75">[75]</ref>. We choose data from the last four weeks as testing data, and all data before that as training data. Following the setting in <ref type="bibr" target="#b69">[69]</ref>, we transform the data into [0, 1] via max-min normalization. Since the original data is between -1 and 1, the reported MSE and MAE are 1/4 and 1/2 of the original data respectively after transformation, which is consistent with previous literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b69">69]</ref>.</p><p>Human3.6 Human3.6 <ref type="bibr" target="#b23">[24]</ref> is a complex human pose dataset with 3.6 million samples, recording different activities such as taking photos, talking on the phone, posing, greeting, eating, etc. Similar to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b69">69]</ref>, only videos with "walking" scenario are used. Following previous works, we generate 4 future frames given the previous 4 RGB frames.</p><p>KITTI&amp;Caltech Pedestrian KITTI <ref type="bibr" target="#b16">[17]</ref> is one of the most popular datasets for mobile robotics and autonomous driving, as well as a benchmark for computer vision algorithms. It is composed by hours of traffic scenarios recorded with a variety of sensor modalities, including highresolution RGB, gray-scale stereo cameras, and a 3D laser scanner. CalTech Pedestrian <ref type="bibr" target="#b10">[11]</ref> is a driving dataset focused on detecting pedestrians. It is conformed of approximately 10 hours of 640 ? 480 30 FPS video taken from a vehicle driving through regular traffic in an urban environment, making a total of 250,000 annotated frames distributed in 137 approximately minute-long segments. We follow the same protocol of PredNet <ref type="bibr" target="#b36">[37]</ref> and CrevNet <ref type="bibr" target="#b73">[73]</ref> for preprocessing, training and evaluation. Models are trained on KITTI dataset to predict the next frame after 10frame warm-up and are evaluated on Caltech Pedestrian.</p><p>KTH The KTH dataset <ref type="bibr" target="#b53">[53]</ref> contains 25 individuals performing 6 types of actions, i.e., walking, jogging, running, boxing, hand waving and hand clapping. Following <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b66">66]</ref>, we use person 1-16 for training and 17-25 for testing. Models are trained to predict next 20 or 40 frames from the previous 10 observations. 7.2. Translator: should we use RNN, Transformer or CNN? <ref type="figure">Figure 7</ref>. Training dynamics of various translators on Moving MNIST using large learning rate 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Network structure</head><p>Denote N s , C s as the layer numbers and hidden dimensions of the spatial Encoder (or Decoder). Similarly, N t and C t are the layer numbers and hidden dimensions of the Translator's encoder (or decoder). We use NNI (Neural Network Intelligence) to search hyperparameters, and the search space is shown in <ref type="table" target="#tab_6">Table.</ref> 9. The final hyperparameter settings on various dataset can be found in <ref type="table" target="#tab_6">Table.</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1, i.e., (1) RNN-RNN-RNN (2) CNN-RNN-CNN (3) CNN-ViT-CNN, and (4) CNN-CNN-CNN. Some representative works are collected in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Different architectures for video prediction. Red and blue lines help to learn the temporal evolution and spatial dependency. SimVP belongs to the framework of CNN-CNN-CNN, which can outperform other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Training dynamics of translators on Moving MNIST and Human3.6. The x-axis is the training epochs and the y-axis is the evaluated MSE. SimVP converges faster than other methods in the early training phase, but CrevNet performs better in the long run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of Moving MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of Caltech (10 ? 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>The role of the Translator, Encoder and Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Some representative video prediction works since 2014.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The overall framework of SimVP. Both the Encoder, Translator, and Decoder are built upon CNN. The encoder stacks Ns ConvNormReLU block to extract spatial features, i.e., convoluting C channels on (H, W ). The translator employs Nt Inception modules to learn temporal evolution, i.e., convoluting T ?C channels on (H, W ). The decoder utilizes Ns unConvNormReLU blocks to reconstruct the ground truth frames, which convolutes C channels on (H, W ).</figDesc><table><row><cell cols="4">Global structure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Spatio-temporal features</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H</cell></row><row><cell>H</cell><cell>T</cell><cell>W</cell><cell cols="2">CNN</cell><cell>H'</cell><cell>T</cell><cell>W'</cell><cell cols="2">CNN (Translator)</cell><cell></cell><cell>T' H'</cell><cell>W'</cell><cell>CNN</cell><cell>T'</cell><cell>W</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Encoder)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Decoder)</cell></row><row><cell cols="2">Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Spatial features</cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell cols="4">Detailed structure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Inception</cell></row><row><cell></cell><cell>Conv2d</cell><cell>Groupnorm</cell><cell>LeakyRelu</cell><cell>Inception Inception</cell><cell></cell><cell></cell><cell cols="2">Inception Inception</cell><cell>unConv2d</cell><cell>Groupnorm</cell><cell>LeakyRelu</cell><cell></cell><cell>Conv</cell><cell>GroupConv2d GroupConv2d</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GroupConv2d</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Inception</cell><cell></cell><cell></cell><cell cols="2">Inception</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 2. pyramid features to provide state-of-the-art results on SSv2.</cell><cell cols="4">(b). In general, they focus on modifying the LSTM and encoding-decoding modules. Spatio-Temporal video autoencoder [46] incorporates Con-vLSTM and an optical flow predictor to capture changes over time. Conditional VRNN [6] combines CNN encoder and RNN decoder in a variational generating framework.</cell></row><row><cell cols="10">Video Swin Transformer [34] expands Swin Transformer</cell><cell cols="4">E3D-LSTM [66] applies 3D convolution for encoding and</cell></row><row><cell cols="10">from 2D to 3D, where the shiftable local attention schema</cell><cell cols="4">decoding and integrates it in latent RNNs for obtaining</cell></row><row><cell cols="10">leads to a better speed-accuracy trade-off. Most of the</cell><cell cols="4">motion-aware and short-term features. CrevNet [73] pro-</cell></row><row><cell cols="10">models above are designed for video classification; works</cell><cell cols="4">poses using CNN-based normalizing flow modules to en-</cell></row><row><cell cols="4">about video prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">code and decode inputs for information-preserving feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">transformations. PhyDNet [18] models physical dynamics</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">with CNN-based PhyCells. Recently, this framework has</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">attracted considerable attention, because the CNN encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">can extract decent and compressed features for accurate and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">efficient prediction.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CNN-ViT-CNN This framework introduces Vision</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Transformer (ViT) to model latent video dynamics. By</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">extending language transformer [60] to ViT [12], a wave</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">of research has been sparked recently. As to image</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">transformer, DeiT [57] and Swin Transformer [33] have</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">achieved state-of-the-art performance on various vision</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">tasks. The great success of image transformer has inspired</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">the investigation of video transformer. VTN [42] applies</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">sliding window attention on temporal dimension following</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">a 2D spatial feature extractor. TimeSformer and ViViT [2,4]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">study different space-time attention strategies and suggest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">that separately applying temporal and spatial attention can</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">achieve superb performance. MViT [14] extracts multiscale</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table. 2, including the number of training samples n train , number of testing samples n test , image resolution (C, H, W ), input sequence length T and forecasting sequence length T ? . The detailed dataset description can be found in the appendix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The statistics of datasets. The training or testing set has Ntrain or Ntest samples, each of which consists T or T ? images with the shape (C, H, W ).</figDesc><table><row><cell></cell><cell cols="2">N train N test</cell><cell>(C, H, W )</cell><cell>T</cell><cell>T ?</cell></row><row><cell>MMNIST</cell><cell cols="2">10000 10000</cell><cell>(1, 64, 64)</cell><cell>10</cell><cell>10</cell></row><row><cell>TrafficBJ</cell><cell>19627</cell><cell>1334</cell><cell>(2, 32, 32)</cell><cell>4</cell><cell>4</cell></row><row><cell>Human3.6</cell><cell>2624</cell><cell cols="3">1135 (3, 128, 128) 4</cell><cell>4</cell></row><row><cell>Caltech Pedestrian</cell><cell>2042</cell><cell cols="3">1983 (3, 128, 160) 10</cell><cell>1</cell></row><row><cell>KTH</cell><cell>5200</cell><cell cols="4">3167 (1, 128, 128) 10 20 or 40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Performance comparision of various methods on Moving MNIST. The source code of SimVP will be released soon. Computation comparision on Moving MNIST. We report the per sample memory overhead, per frame FLOPs, and total training time. For methods marked with * , we report the results reproduced by their official codes. Other results refer to<ref type="bibr" target="#b73">[73]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Conference</cell><cell cols="2">MSE SSIM</cell><cell>Github</cell></row><row><cell>ConvLSTM [71]</cell><cell cols="2">(NIPS 2015)</cell><cell cols="2">103.3 0.707</cell><cell>PyTorch</cell></row><row><cell>PredRNN [67]</cell><cell cols="2">(NIPS 2017)</cell><cell>56.8</cell><cell>0.867</cell><cell>PyTorch</cell></row><row><cell cols="3">PredRNN-V2 [68] (Arxiv 2021)</cell><cell>48.4</cell><cell>0.891</cell><cell>PyTorch</cell></row><row><cell cols="3">CausalLSTM [65] (ICML 2018)</cell><cell>46.5</cell><cell cols="2">0.898 Tensorflow</cell></row><row><cell>MIM [69]</cell><cell cols="2">(CVPR 2019)</cell><cell>44.2</cell><cell cols="2">0.910 Tensorflow</cell></row><row><cell>E3D-LSTM [66]</cell><cell cols="2">(ICLR 2018)</cell><cell>41.3</cell><cell cols="2">0.920 Tensorflow</cell></row><row><cell>PhyDNet [18]</cell><cell cols="2">(CVPR 2020)</cell><cell>24.4</cell><cell>0.947</cell><cell>PyTorch</cell></row><row><cell>CrevNet [73]</cell><cell cols="2">(ICLR 2020)</cell><cell>22.3</cell><cell>0.949</cell><cell>PyTorch</cell></row><row><cell>SimVP</cell><cell>-</cell><cell></cell><cell>23.8</cell><cell>0.948</cell><cell>PyTorch</cell></row><row><cell>Method</cell><cell>Memory</cell><cell cols="2">FLOPs</cell><cell cols="2">Training time</cell></row><row><cell>ConvLSTM</cell><cell>1043MB</cell><cell cols="2">107.4G</cell><cell>-</cell></row><row><cell>PredRNN</cell><cell cols="3">1666 MB 192.9 G</cell><cell>-</cell></row><row><cell cols="4">CausalLSTM 2017 MB 106.8 G</cell><cell>-</cell></row><row><cell>E3D-LSTM</cell><cell cols="3">2695 MB 381.3 G</cell><cell>-</cell></row><row><cell>CrevNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* 224 MB 1.652 G ? 10d (300k iters) PhyDNet* 200 MB 1.633 G ? 10d (2k epochs) SimVP* 412 MB 1.676 G ? 2d (2k epochs) 4.2. Translator: should we use RNN, Transformer or CNN?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>SimVP vs SOTA. The optimal(or suboptimal) results are marked by bold(or underlined). MSE? MAE? SSIM? MSE ? 100 ? MAE? SSIM? MSE / 10 ? MAE / 100 ? SSIM?</figDesc><table><row><cell></cell><cell cols="3">Moving MNIST</cell><cell></cell><cell>TrafficBJ</cell><cell></cell><cell></cell><cell>Human3.6</cell><cell></cell></row><row><cell>ConvLSTM</cell><cell>103.3</cell><cell>182.9</cell><cell>0.707</cell><cell>48.5</cell><cell>17.7</cell><cell>0.978</cell><cell>50.4</cell><cell>18.9</cell><cell>0.776</cell></row><row><cell>PredRNN</cell><cell>56.8</cell><cell>126.1</cell><cell>0.867</cell><cell>46.4</cell><cell>17.1</cell><cell>0.971</cell><cell>48.4</cell><cell>18.9</cell><cell>0.781</cell></row><row><cell>CausalLSTM</cell><cell>46.5</cell><cell>106.8</cell><cell>0.898</cell><cell>44.8</cell><cell>16.9</cell><cell>0.977</cell><cell>45.8</cell><cell>17.2</cell><cell>0.851</cell></row><row><cell>MIM</cell><cell>44.2</cell><cell>101.1</cell><cell>0.910</cell><cell>42.9</cell><cell>16.6</cell><cell>0.971</cell><cell>42.9</cell><cell>17.8</cell><cell>0.790</cell></row><row><cell>E3D-LSTM</cell><cell>41.3</cell><cell>86.4</cell><cell>0.910</cell><cell>43.2</cell><cell>16.9</cell><cell>0.979</cell><cell>46.4</cell><cell>16.6</cell><cell>0.869</cell></row><row><cell>PhyDNet</cell><cell>24.4</cell><cell>70.3</cell><cell>0.947</cell><cell>41.9</cell><cell>16.2</cell><cell>0.982</cell><cell>36.9</cell><cell>16.2</cell><cell>0.901</cell></row><row><cell>SimVP</cell><cell>23.8</cell><cell>68.9</cell><cell>0.948</cell><cell>41.4</cell><cell>16.2</cell><cell>0.982</cell><cell>31.6</cell><cell>15.1</cell><cell>0.904</cell></row><row><cell cols="5">? Q2: Does SimVP generalize well across different</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>datasets?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">? Q3: Does SimVP extend well to the case of flexible</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">predictive length?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table .</head><label>.</label><figDesc>SimVP can achieve SOTA results on lightweight benchmarks. From Table. 5, we observe that SimVP outperform all RNN baselines in all settings.</figDesc><table><row><cell>5. Six state-of-the-art RNN baselines are chosen</cell></row><row><cell>for comparison, including ConvLSTM [71], PredRNN [67],</cell></row><row><cell>CausalLSTM [65], MIM [69], E3D-LSTM [66] and PhyD-</cell></row><row><cell>Net [18]. We train SimVP on Moving MNIST, Human3.6,</cell></row><row><cell>and TrafficBJ for 2k, 100 and 80 epochs, respectively. We</cell></row><row><cell>use Adam optimizer and the learning rate is 0.01.</cell></row><row><cell>Answer of Q1</cell></row></table><note>The improvements on Moving MNIST and TrafficBJ are mod- est; On Human3.6, the relative improvement is significant. In addition, SimVP takes training time compared to pre- vious PhyDNet. Both performance and computing advan- tages support our answer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Results on Caltech Pedestrian dataset.</figDesc><table><row><cell></cell><cell cols="3">Caltech Pedestrian (10 ? 1)</cell></row><row><cell>Method</cell><cell>MSE?</cell><cell>SSIM?</cell><cell>PSNR?</cell></row><row><cell>BeyondMSE [40]</cell><cell>3.42</cell><cell>0.847</cell><cell>-</cell></row><row><cell>MCnet [62]</cell><cell>2.50</cell><cell>0.879</cell><cell>-</cell></row><row><cell>DVF [35]</cell><cell>-</cell><cell>0.897</cell><cell>26.2</cell></row><row><cell>Dual-GAN [32]</cell><cell>2.41</cell><cell>0.899</cell><cell>-</cell></row><row><cell>CtrlGen [19]</cell><cell>-</cell><cell>0.900</cell><cell>26.5</cell></row><row><cell>PredNet [37]</cell><cell>2.42</cell><cell>0.905</cell><cell>27.6</cell></row><row><cell>ContextVP [5]</cell><cell>1.94</cell><cell>0.921</cell><cell>28.7</cell></row><row><cell>GAN-VGG [54]</cell><cell>-</cell><cell>0.916</cell><cell>-</cell></row><row><cell>G-VGG [54]</cell><cell>-</cell><cell>0.917</cell><cell>-</cell></row><row><cell>SDC-Net [50]</cell><cell>1.62</cell><cell>0.918</cell><cell>-</cell></row><row><cell>rCycleGan [29]</cell><cell>1.61</cell><cell>0.919</cell><cell>29.2</cell></row><row><cell>DPG [16]</cell><cell>-</cell><cell>0.923</cell><cell>28.2</cell></row><row><cell>G-MAE [54]</cell><cell>-</cell><cell>0.923</cell><cell>-</cell></row><row><cell>GAN-MAE [54]</cell><cell>-</cell><cell>0.923</cell><cell>-</cell></row><row><cell>CrevNet [73]</cell><cell>-</cell><cell>0.925</cell><cell>29.3</cell></row><row><cell>STMFANet [26]</cell><cell>-</cell><cell>0.927</cell><cell>29.1</cell></row><row><cell>SimVP (ours)</cell><cell>1.56</cell><cell>0.940</cell><cell>33.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table.8, where we train SimVP for 100 epochs. The KTH dataset [53] contains 25 individuals performing 6 types of actions, i.e., walking, jogging, running, boxing, hand waving and hand clapping. Following [62, 66], we use person 1-16 for training and 17-25 for testing. Models are trained to predict next 20 or 40 frames from the previous 10 observations. Sixteen baselines are included, such as MCnet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Results on KTH dataset. SimVP extend well to the case of flexible predictive length. From Table. 8, we know that SimVP achieve state-of-the-art performance. Notebly, the PSNR improves by 11.8% and 19.5% in both (10 ? 20) and (10 ? 40) settings. This phenomenon further indicates that the performance degradation of SimVP is less than others on long-term prediction tasks.</figDesc><table><row><cell></cell><cell cols="4">KTH (10 ? 20) KTH (10 ? 40)</cell></row><row><cell>Method</cell><cell cols="4">SSIM? PSNR? SSIM? PSNR?</cell></row><row><cell>MCnet [62]</cell><cell>0.804</cell><cell>25.95</cell><cell>0.73</cell><cell>23.89</cell></row><row><cell cols="2">ConvLSTM [71] 0.712</cell><cell>23.58</cell><cell>0.639</cell><cell>22.85</cell></row><row><cell>SAVP [30]</cell><cell>0.746</cell><cell>25.38</cell><cell>0.701</cell><cell>23.97</cell></row><row><cell>VPN [28]</cell><cell>0.746</cell><cell>23.76</cell><cell>-</cell><cell>-</cell></row><row><cell>DFN [25]</cell><cell>0.794</cell><cell>27.26</cell><cell>0.652</cell><cell>23.01</cell></row><row><cell>fRNN [43]</cell><cell>0.771</cell><cell>26.12</cell><cell>0.678</cell><cell>23.77</cell></row><row><cell>Znet [74]</cell><cell>0.817</cell><cell>27.58</cell><cell>-</cell><cell>-</cell></row><row><cell>SV2Pi [3]</cell><cell>0.826</cell><cell>27.56</cell><cell>0.778</cell><cell>25.92</cell></row><row><cell>SV2Pv [3]</cell><cell>0.838</cell><cell>27.79</cell><cell>0.789</cell><cell>26.12</cell></row><row><cell>PredRNN [67]</cell><cell>0.839</cell><cell>27.55</cell><cell>0.703</cell><cell>24.16</cell></row><row><cell>VarNet [27]</cell><cell>0.843</cell><cell>28.48</cell><cell>0.739</cell><cell>25.37</cell></row><row><cell cols="2">SVAP-VAE [30] 0.852</cell><cell>27.77</cell><cell>0.811</cell><cell>26.18</cell></row><row><cell cols="2">PredRNN++ [65] 0.865</cell><cell>28.47</cell><cell>0.741</cell><cell>25.21</cell></row><row><cell>MSNET [31]</cell><cell>0.876</cell><cell>27.08</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">E3d-LSTM [66] 0.879</cell><cell>29.31</cell><cell>0.810</cell><cell>27.24</cell></row><row><cell cols="2">STMFANet [26] 0.893</cell><cell>29.85</cell><cell>0.851</cell><cell>27.56</cell></row><row><cell>SimVP (ours)</cell><cell>0.905</cell><cell>33.72</cell><cell>0.886</cell><cell>32.93</cell></row><row><cell>Answer of Q3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Answer of Q2 Larger kernel size and more model parameters lead to better performance. In Table. 7, from model 5 to model 8, with the increasing kernel size, we can see the significant performance gain. This improvement can be further enhanced by doubling the hidden dimension of model 8 to construct model 9. SimVP chooses multi-scale kernels, and the parameters of the Translator are 84% of model 9.</figDesc><table><row><cell>All of S-UNet, T-UNet, group convolu-</cell></row><row><cell>tion and group normalization can bring performance gain,</cell></row><row><cell>and the order of significance is: group convolution &gt;</cell></row><row><cell>group normalization ? S-UNet ? T-UNet. Please see Ta-</cell></row><row><cell>ble. 7 (from model 1 to model 4) for experimental evidence.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Search space Final hyperparameter setting.</figDesc><table><row><cell></cell><cell cols="5">Human MMNIST TrafficBJ Caltech KTH</cell></row><row><cell>C s</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>32</cell></row><row><cell>C t</cell><cell>64</cell><cell>512</cell><cell>256</cell><cell>128</cell><cell>128</cell></row><row><cell>N s</cell><cell>1</cell><cell>4</cell><cell>3</cell><cell>1</cell><cell>3</cell></row><row><cell>N t</cell><cell>5</cell><cell>3</cell><cell>2</cell><cell>3</cell><cell>4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The realistic running time is more reliable to FLOPs or MACs of the model, e.g., when RNN and CNN have the same FLOPs, RNN takes much more time due to its recurrent computation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgemnet This work is supported in part by the Science and Technology Innovation 2030 -Major Project (No. 2021ZD0150100) and National Natural Science Foundation of China (No. U21A20427).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards high resolution video generation with progressive growing of sliced wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02419</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11252</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contextvp: Fully context-aware video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="753" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved conditional vrnns for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7608" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmenting the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hsu-Kuang Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10915</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1174" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Multiscale vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Disentangling propagation and generation for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vincent Le Guen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Controllable video generation with sparse trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7854" to="7863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Prediction under uncertainty with error-encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04994</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04166</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic future prediction for video scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergal</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Gurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="767" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring spatialtemporal multi-frequency analysis for high-fidelity and temporal-consistency video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4554" to="4563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Varnet: Exploring variations for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shice</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5801" to="5806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting future frames using retrospective cycle gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Gyu</forename><surname>Yong-Hoon Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mutual suppression network for video prediction using disentangled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04810</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dual motion gan for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Video swin transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual structure using predictive generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06380</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Flexible spatio-temporal networks for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6523" to="6531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Diego de Las Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on largescale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04035</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling deep temporal dependencies with recurrent grammar cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Konda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1925" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Alejandro</forename><surname>Castro-Vargas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jose Garcia-Rodriguez, and Antonis Argyros. A review on deep learning techniques for video prediction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Viorica Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06309</idno>
		<title level="m">Spatio-temporal video autoencoder with differentiable memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabeau</forename><surname>Pr?mont-Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tele Hotloo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinu</forename><surname>Boney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent ladder networks</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Rakhimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10704</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Latent video transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sdc-net: Video prediction using spatially-displaced convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="718" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2830" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Tganv2: Efficient training of large models for video generation with multiple subsampling layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition</title>
		<meeting>the 17th International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Photo-realistic video prediction on natural videos of largely changing frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Shouno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08635</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation with contextual pyramid convlstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th acm international conference on multimedia</title>
		<meeting>the 27th acm international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2043" to="2051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08435</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6038" to="6046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08033</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to generate longterm future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Eidetic 3d lstm: A model for video prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Predrnn: A recurrent neural network for spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09504</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Memory in memory: A predictive neural network for learning higher-order nonstationarity from spatiotemporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02634</idno>
		<title level="m">Scaling autoregressive video models</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Predcnn: Predictive learning with cascade convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><surname>Kliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient and information-preserving future frame prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Easterbrook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Z-order recurrent neural networks for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jianmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep spatiotemporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
