<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technical University of Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technical University of Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technical University of Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-theart unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1% of the performance of indomain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. 1 A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TS-DAE and other recent approaches on four different datasets from heterogeneous domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence embedding techniques encode sentences into a fixed-sized, dense vector space such that semantically similar sentences are close. The most successful previous approaches like InferSent <ref type="bibr">(Conneau et al., 2017)</ref>, Universial Sentence Encoder (USE) <ref type="bibr">(Cer et al., 2018)</ref> and SBERT <ref type="bibr" target="#b10">(Reimers and Gurevych, 2019)</ref> heavily relied on labeled data to train sentence embedding models. However, for most tasks and domains, labeled data is not available and data annotation is expensive. To overcome this limitation, unsupervised approaches have been proposed which learn to embed sentences just using an unlabeled corpus for training.</p><p>We propose a new approach: Transformer-based Sequential Denoising Auto-Encoder (TSDAE). It significantly outperforms previous methods via an encoder-decoder architecture. During training, TS-DAE encodes corrupted sentences into fixed-sized vectors and requires the decoder to reconstruct the original sentences from this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embedding from the encoder. Later, at inference, we only use the encoder for creating sentence embeddings.</p><p>A crucial shortcoming of previous unsupervised approaches is the evaluation. Often, approaches are mainly evaluated on the Semantic Textual Similarity (STS) task from SemEval <ref type="bibr">Giorgi et al., 2021;</ref><ref type="bibr">Carlsson et al., 2021;</ref><ref type="bibr">Gao et al., 2021)</ref>. As we argue in Section 4, we perceive this as an insufficient evaluation. The STS datasets do not include sentences with domain specific knowledge, i.e., it remains unclear how methods will perform on more specific domains. Further, STS datasets have an artificial score distribution, and the performance on STS datasets does not correlate with downstream task performances <ref type="bibr" target="#b9">(Reimers et al., 2016)</ref>. In conclusion, it remains unclear, how well unsupervised sentence embedding methods will perform on domain specific tasks.</p><p>To answer this question, we compare TSDAE with previous unsupervised sentence embedding approaches on three different tasks (Information Retrieval, Re-Ranking and Paraphrase Identification), for heterogeneous domains and different text styles. We show that TSDAE can outperform other state-of-the-art unsupervised approaches by up to 6.4 points. TSDAE is able to perform on-par or even outperform existent supervised models like USE-large, which had been trained with a lot of labeled data from various datasets.</p><p>Further, we demonstrate that TSDAE works well for domain adaptation and as a pre-training task. We observe a significant performance improvement arXiv:2104.06979v3 [cs.CL] 10 Sep 2021 compared to other pre-training tasks like Masked Language Model (MLM).</p><p>Our contributions are three-fold:</p><p>? We propose a novel unsupervised method, TS-DAE based on denoising auto-encoders. We show that it outperforms the previous best approach by up to 6.4 points on diverse datasets.</p><p>? To the best of our knowledge, we are the first to compare recent unsupervised sentence embedding methods for various tasks on heterogeneous domains.</p><p>? TSDAE outperforms other methods including MLM by a large margin as a pre-training and domain adaptation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Supervised sentence embeddings utilize labels for sentence pairs which provide the information about the relation between the sentences. Since sentence embeddings are usually applied to measure the similarity of a sentence pair, the most direct way is to label this similarity for supervised training <ref type="bibr">(Henderson et al., 2017)</ref>. Many studies also find that natural language inference (NLI), question answering and conversational context datasets can successfully be used to train sentence embeddings <ref type="bibr">(Conneau et al., 2017;</ref><ref type="bibr">Cer et al., 2018)</ref>. The recently proposed Sentence-BERT <ref type="bibr" target="#b10">(Reimers and Gurevych, 2019)</ref> introduced pre-trained Transformers to the field of sentence embeddings. Although high-quality sentence embeddings can be derived via supervised training, the labeling cost is a major obstacle for practical usage, especially for specialized domains. Unsupervised sentence embeddings utilize only an unlabeled corpus during training. Recent work combined pre-trained Transformers with different training objectives to achieve state-of-the-art results on STS tasks. Among them, Contrastive Tension (CT) <ref type="bibr">(Giorgi et al., 2021</ref>) simply views the identical and different sentences as positive and negative examples, resp. and train two independent encoders; BERT-flow  trains model via debiasing embedding distribution towards <ref type="bibr">Gaussian;</ref><ref type="bibr">SimCSE (Gao et al., 2021)</ref> is based on contrastive learning <ref type="bibr">(Hadsell et al., 2006;</ref><ref type="bibr">Chen et al., 2020)</ref> and views the identical sentences with different dropout mask as the positive examples. For more details, please refer to Section 5. All of them  requires only independent sentences. By contrast, <ref type="bibr">DeCLUTR (Giorgi et al., 2021)</ref> utilizes sentencelevel contexts and requires long documents (2048 tokens at least) for training. This requirement is hardly met for many cases, e.g. tweets or dialogues. Thus, in this work we only consider methods which uses only single sentences during training.</p><p>Most previous work mainly evaluate only on Semantic Textual Similarity (STS) from the SemEval shared tasks. As we show in Section 4, the unsupervised approaches perform much worse than the out-of-the-box supervised pre-trained models even though they were not specifically trained for STS. Further, a good performance on STS does not necessarily correlate with the performance on downstream tasks <ref type="bibr" target="#b9">(Reimers et al., 2016)</ref>. It remains unclear how these methods perform on specific tasks and domains. To answer this, we compare three recent powerful unsupervised methods based on pre-trained Transformers including CT, SimCSE, BERT-flow and our proposed TSDAE on different tasks of heterogeneous domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequential Denoising Auto-Encoder</head><p>Although Sequential Denoising Auto-Encoder (SDAE) <ref type="bibr" target="#b14">(Vincent et al., 2010;</ref><ref type="bibr">Goodfellow et al., 2016;</ref><ref type="bibr">Hill et al., 2016)</ref> is a popular unsupervised method in machine learning, how to combine it with pre-trained Transformers for learning sentence embeddings remains unclear. In this section, we first introduce the training objective of TSDAE and then give the optimal configuration of TSDAE. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the architecture of TSDAE. TS-DAE train sentence embeddings by adding a certain type of noise (e.g. deleting or swapping words) to input sentences, encoding the damaged sentences into fixed-sized vectors and then reconstructing the vectors into the original input. Formally, the training objective is:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Objective</head><formula xml:id="formula_0">J SDAE (?) = E x?D [log P ? (x|x)] = E x?D [ l t=1 log P ? (x t |x)] = E x?D [ l t=1 log exp(h T t e t ) N i=1 exp(h T t e i ) ]</formula><p>where D is the training corpus, x = x 1 x 2 ? ? ? x l is the input sentence with l tokens,x is the corresponding damaged sentence, e t is the word embedding of x t , N is the vocabulary size and h t is the hidden state at decoding step t.</p><p>An important difference to original transformer encoder-decoder setup presented in <ref type="bibr" target="#b13">Vaswani et al. (2017)</ref> is the information available to the decoder: Our decoder decodes only from a fixed-size sentence representation produced by the encoder. It does not have access to all contextualized word embeddings from the encoder. This modification introduces a bottleneck, that should force the encoder to produce a meaningful sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TSDAE</head><p>The model architecture of TSDAE is a modified encoder-decoder Transformer where the key and value of the cross-attention are both confined to the sentence embedding only. Formally, the formulation of the modified cross-attention is:</p><formula xml:id="formula_1">H (k) = Attention(H (k?1) , [s T ], [s T ]) Attention(Q, K, V ) = softmax QK T ? d V</formula><p>where H (k) ? R t?d is the decoder hidden states within t decoding steps at the k-th layer, d is the size of the sentence embedding, [s T ] ? R 1?d is a one-row matrix including the sentence embedding vector and Q, K and V are the query, key and value, respectively. By exploring different configurations on the STS benchmark dataset <ref type="bibr">(Cer et al., 2017)</ref>, we discover that the best combination is:</p><p>(1) adopting deletion as the input noise and setting the deletion ratio to 0.6, (2) using the output of the [CLS] token as fixed-sized sentence representation (3) tying the encoder and decoder parameters during training. For the detailed tuning process, please refer to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Previous unsupervised sentence embedding learning approaches <ref type="bibr">(Giorgi et al., 2021;</ref><ref type="bibr">Carlsson et al., 2021;</ref><ref type="bibr" target="#b12">Su et al., 2021;</ref><ref type="bibr">Gao et al., 2021</ref>) primarily evaluated on the task of Semantic Textual Similarity (STS) with data from SemEval using Pearson or Spearman's rank correlation. We find the (sole) evaluation on STS problematic. As shown in <ref type="bibr" target="#b9">(Reimers et al., 2016)</ref>, performance on the STS dataset does not correlate with downstream task performance, i.e. an approach working well on the STS tasks must not be a good choice for downstream tasks. We confirm this with our experiments, the performance on the STS tasks does not correlate with the performance on other (real-world) tasks. See Section 6.1 for more details on this.</p><p>This has multiple reasons: First, the STS datasets consists of sentences which do not require domainspecific knowledge, they are primarily from news and image captions. It is unclear how approaches will work for domain-specific tasks. Second, the STS datasets have an artificial score distributiondissimilar and similar pairs appear roughly equally. For most real-word tasks, there is an extreme skew and only a tiny fraction of pairs are considered similar. Third, to perform well on the STS datasets, a method must rank dissimilar pairs and similar pairs equally well. In contrast, most real-world tasks, like duplicate questions detection, related paper finding, or paraphrase mining, only require to identify the few similar pairs out of a pool of millions of irrelevant combinations.</p><p>A further shortcoming of previous evaluation setups is just testing the case of unsupervised learning, ignoring labeled data that potentially exists. In many scenarios, some labeled data exists, either directly from the specific task or from other (similar) tasks. A good approach should also work if some labeled data is available.</p><p>Hence, we propose to evaluate unsupervised sentence embedding approaches in following three setups:</p><p>Unsupervised Learning: We assume we just have unlabeled sentences from the target task and tune our approaches based on these sentences.</p><p>Domain Adaptation: We assume we have unlabeled sentences from the target task and labeled sentences from NLI <ref type="bibr" target="#b0">(Bowman et al., 2015;</ref><ref type="bibr" target="#b15">Williams et al., 2018)</ref> and STS benchmark <ref type="bibr">(Cer et al., 2017)</ref> datasets. We test two setups: 1) Training on NLI+STS data, then unsupervised training to the target domain, 2) Unsupervised training on the target domain, then supervised training on NLI + STS.</p><p>Pre-Training: We assume we have a larger collection of unlabeled sentences from the target task and a smaller set of labeled sentences from the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate these three settings on different tasks from heterogeneous (specialized) domains. The tasks include Re-Ranking (RR), Information Retrieval (IR) and Paraphrase Identification (PI). In detail, the datasets used are as follows 2 :</p><p>AskUbuntu (RR task) is a collection of user posts from the technical forum AskUbuntu <ref type="bibr">(Lei et al., 2016)</ref>. Models are required to re-rank 20 candidate questions according to the similarity given an input post. The candidates are obtained via BM25 term-matching <ref type="bibr" target="#b11">(Robertson et al., 1994)</ref>. The evaluation metric is Mean Average Precision (MAP).</p><p>CQADupStack (IR task) is a question retrieval dataset of forum posts on various topics from Stack-Exchange <ref type="bibr">(Hoogeveen et al., 2015)</ref>. In detail, it has 12 forums including Android, English, gaming, geographic information system, Mathematica, physics, programmers, statistics, Tex, Unix, Webmasters and WordPress. Models are required to retrieve duplicate questions from a large candidate pool. The metric is MAP@100. We train a single model for all forums.</p><p>TwitterPara (PI task) consists of two similar datasets: the Twitter Paraphrase Corpus (PIT-2015) <ref type="bibr" target="#b16">(Xu et al., 2015)</ref> and the Twitter News URL Corpus (noted as TURL) <ref type="bibr">(Lan et al., 2017)</ref>. The dataset consists of pairs of tweets together with a crowd-annotated score if the pair is a paraphrase. The evaluation metric is Average Precision (AP) over the gold confidence scores and the similarity scores from the models.</p><p>SciDocs (RR task) is a benchmark consisting of multiple tasks about scientific papers <ref type="bibr">(Cohan et al., 2020)</ref>. In our experiments, we use the tasks of Cite: Given a paper title, identify the titles the paper is citing; Co-Cite (CC), Co-Read (CR), and Co-View (CV), for which we must find papers that are frequently co-cited/-read/-viewed for a given paper title. For all these tasks, given one query paper title, models are required to identify up to 5 relevant papers titles from up to 30 candidates. The negative examples were selected randomly. The evaluation metric is MAP.</p><p>For evaluation, sentences are first encoded into fixed-sized vectors and cosine similarity is used for sentence similarity. Since we focus on embeddings for sentences, we just use the titles from the AskUbuntu, CQADupStack and SciDocs datasets. For the datasets with sub-datasets or sub-tasks including CQADupStack, TwitterPara and SciDocs, the final score is derived by averaging the scores from each sub-dataset or sub-task.</p><p>For unsupervised training, we just use the sentences from the training split without any labels. The statistics for each dataset are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we compare our proposed TSDAE with other unsupervised counterparts and out-ofthe-box supervised pre-trained models on the above mentioned tasks. For comparison, we include three recent state-of-the-art unsupervised approaches: CT, SimCSE, and BERT-flow. We use the proposed hyper-parameters from the respective paper. Without other specification, BERT-base-uncased 3 is used as the base Transformer model. To eliminate the influence of randomness, we report the scores averaged over 5 random seeds. For other details, please refer to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Methods</head><p>We compare the approaches against avg. GloVe embeddings <ref type="bibr" target="#b7">(Pennington et al., 2014)</ref> and Sent2Vec <ref type="bibr" target="#b6">(Pagliardini et al., 2018)</ref>. The former generates sentence embeddings by averaging word embeddings trained on a large corpus from the general domain; the latter is also a bag-of-words model but trained on the in-domain unlabeled corpus. The unsupervised baseline of BERT-base-uncased with mean pooling is also in comparison. We further compare against existent pre-trained models: Universial Sentence Embedding (USE) , which was trained on multiple supervised datasets including NLI and community question answering. From the Sentence-Transformers package, we use SBERT-base-nli-v2 and SBERT-basenli-stsb-v2: These models were trained on SNLI + MultiNLI data using the Multiple-Negative Ranking Loss (MNRL) <ref type="bibr">(Henderson et al., 2017)</ref>  Mean Square Error (MSE) loss on the STS benchmark train set. Further we include BM25 using Elasticsearch for comparison.</p><p>To better understand the relative performance of these unsupervised methods, we also train SBERT models in an in-domain supervised manner and view their scores as the upper bound. For AskUbuntu, CQADupStack and SciDocs, where the relevant sentence pairs are labeled, the indomain SBERT models are trained with MNRL. MNRL is a cross-entropy loss with in-batch negatives. For a batch of relevant sentences pairs</p><formula xml:id="formula_2">{x (i) , y (i) } M i=1</formula><p>, MNRL views the labeled pairs as positive and the other in-batch combinations as negative. Formally, the training objective for each batch is:</p><formula xml:id="formula_3">J MNRL (?) = 1 M M i=1 log exp ?(f ? (x (i) ), f ? (y (i) )) M j=1 exp ?(f ? (x (i) ), f ? (y (j) ))</formula><p>where ? is a certain similarity function for vectors and f ? is the sentence encoder that embeds sentences. For TwitterPara, whose relevant scores are labeled, the MSE loss is adopted to train the in-domain models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MLM</head><p>Masked-Language-Model (MLM) is a fill-in-theblank task originally introduced by BERT: Words are masked from the input and the transformer network must predict the missing words. We use the original setup in Devlin et al. (2019) except the number of training steps (100K), the batch size (8) and the learning rate (5e-5). To derive a sentence embedding, we perform mean-pooling of the output token embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Contrastive Tension (CT)</head><p>CT <ref type="bibr">(Carlsson et al., 2021)</ref> finetunes pre-trained Transformers in a contrastive-learning fashion. For each sentence, it construct a binary cross-entropy loss by viewing the same sentence as the relevant and samples K random sentences as the irrelevant. To make the training process stable, for each sentence pair (a, b), CT uses two independent encoders f ? 1 and f ? 2 from the same initial parameter point to encode the sentence a and b, respectively. Formally, the learning objective is:</p><formula xml:id="formula_4">J CT (? 1 , ? 2 ) = E (a,b)?D [y log ?(f ? 1 (a) T f ? 2 (b)) + (1 ? y) log(1 ? ?(f ? 1 (a) T f ? 2 (b))]</formula><p>where y ? {0, 1} represents whether sentence a is identical to sentence b and ? is the Logistic function. Despite its simplicity, CT achieves state-ofthe-art unsupervised performance on the Semantic Textual Similarity (STS) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">SimCSE</head><p>Similar to CT, SimCSE (Gao et al., 2021) also views the identical sentences as the positive examples. The main difference is that SimCSE samples different dropout masks for the same sentence to generate a embedding-level positive pair and uses in-batch negatives. Thus, this learning objective is equivalent to feeding each batch of sentences to the shared encoder twice and applying the MNRL-loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">BERT-flow</head><p>Instead of fine-tuning the parameters of the pretrained Transformers, BERT-flow  aims at fully exploiting the semantic information encoded by these pre-trained models themselves via distribution debiasing. The paper of BERT-flow claims that the BERT word embeddings are highly relevant to the word frequency, which in turn influences the hidden states via the Masked Language Modeling (MLM) pre-training. This finally leads to biased sentence embeddings generated by the pooling over these hidden states. To solve this problem, BERT-flow inputs the biased sentence embedding into a trainable flow network f ? (Kingma and Dhariwal, 2018) for debiasing via fitting a standard Gaussian distribution, while keeping the parameters of the BERT model unchanged. Formally, the training objective is:</p><formula xml:id="formula_5">J BERT-flow (?) = E x?D [log p U (u)] (1) = E u [log(p Z (f ?1 ? (u))| det ?f ?1 ? (u) ?u |)] (2)</formula><p>where u is the biased embedding of sentence x and z = f ?1 ? (u) is the debiased sentence embedding which follows a standard Gaussian distribution. Equation 2 is derived by applying the changeof-variables theorem to Equation 1.</p><p>As BERT-flow does not update the parameters of the underlying Transformer network, we just reports scores for BERT-flow for unsupervised learning and domain adaptation NLI+STS ? target task. It is not suitable for the other evaluation setups we used. We re-implemented BERT-flow under the Pytorch framework, which can reproduce the reported results in the original paper. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Unsupervised learning: The results in <ref type="table" target="#tab_2">Table 2</ref> show that TSDAE can outperform the previous best approach (CT) by up-to 6.4 points (on Sci-Docs on average) and 2.6 points on average over all tasks. Surprisingly, a simple Masked-Language-Modeling (MLM) approach with mean pooling, which performs badly when evaluated on STS data, is the second best unsupervised approach, outperforming more recent approaches like CT, SimCSE, and BERT-flow on the selected tasks. TSDAE and MLM both removes words from the input, forcing the network to produce robust embeddings. In contrast, the input sentences for CT and SimCSE are not modified, resulting in less stable embeddings. Our experiments also show that out-of-the-box pretrained models (SBERT-base-nli-stsb-v2 and USElarge) achieve strong results on our tasks without any domain-specific fine-tuning, outperforming recent proposed unsupervised learning approaches.</p><p>Domain Adaptation: For all unsupervised methods, we find that first training on the target domain, and then training with labeled NLI+STS achieves better results than the opposite direction. For all methods, we observe a performance increase compared to only training on the target domain. On average, the performance improves by 1.3 points for TSDAE, 3.0 points for MLM, 0.6 points for CT, and 1.8 points for SimCSE. CT and SimCSE perform in this setting only slightly better than the out-of-the-box model SBERT-base-nli-stsb-v2.</p><p>Pre-training: In <ref type="figure" target="#fig_2">Figure 2</ref> we compare the pretraining performance of the tested approaches: We first pre-train on all available unlabeled sentences and then perform in-domain supervised training with different labeled training set sizes. Scores are reported by evaluation on the development sets. TSDAE outperforms MLM by a significant margin for all datasets except for AskUbuntu. There, MLM works slightly better. For the other datasets, TSDAE shows a clear out-performance to other pre-training strategies. The difference is quite consistent also for larger labeled training sets. We conclude, that TSDAE works well as pre-training method and can significantly improve the performance for later supervised training even for larger training datasets. CT and SimCSE don't perform well for pre-training, the results are far worse than using TSDAE/MLM or even starting from the pretrained SBERT-nli-stsb model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results on STS data</head><p>We sample sentences from Wikipedia as done by <ref type="bibr">Carlsson et al. (2021)</ref> and train a BERT-baseuncased model on this dataset with the different unsupervised training methods. In   plied to domain-specific real-world tasks, TSDAE and MLM are outperforming CT and SimCSE. We think these are due to the reasons mentioned in Section 4. Overall, we conclude that a strong performance on STS data is not a good indicator for good performance on domain-specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>We analyze how many training sentences are needed and if relevant content words are identified. For all the datasets except TwitterPara, the analysis is carried out on the development set. For TwitterPara, the test set is used, as it has no development split released by the original paper. All the hyper-parameters are chosen up-front without tuning to a particular dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Influence of Corpus Size</head><p>In certain domains, getting a sufficiently high number of (unlabeled) sentences can be challenging. Hence, data efficiency and deriving good sentence embeddings even with little unlabeled training data can be important.</p><p>In order to study this, we train the unsupervised approaches with different corpus sizes: Between at the end of each epoch and the best score on the development set is reported.</p><p>The results are shown in <ref type="figure">Figure 3</ref>. We observe that TSDAE is outperforming previous unsupervised learning methods often with as little as 1000 unlabeled sentences. With 10K unlabeled sentences, the downstream performance usually stagnates for all tested unsupervised sentence embedding methods. The only exception where more training data is helpful is for the CQADupStack task. This is expected, as the CQADupStack consists of 12 vastly different StackExchange forums, hence, requiring more unlabeled data to represent all domains well.</p><p>We conclude that comparatively little unlabeled data of ?10K sentences is needed to tune pretrained transformers to a specific domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Relevant Content Words</head><p>Not all word types play an equal role in determining the semantics of a sentence. Often, nouns are the critical content words in a sentence, while e.g. prepositions are less important and can be add / removed from a sentences without changing the content too much.</p><p>In this section, we investigate which word types are the most relevant for the different sentence embedding methods, i.e., which words (part-of-speech tags) mainly influence if a sentence pair is perceived as similar or not. We are especially interested if we observe differences between in-domain supervised approaches (SBERT-sup.), out-of-thebox pre-trained approaches, and unsupervised approaches.</p><p>To measure this, we select a sentence pair (a, b) that is labeled as relevant and find the word that maximally reduces the cosine-similarity score for the pair (a, b):</p><formula xml:id="formula_6">w =argmax w cossim(a, b)? min(cossim(a \ w, b), cossim(a, b \ w))</formula><p>among all words w that appear in either a or b. Then, we record the POS tag for? and compute the distribution of POS tags across all sentence pairs.  POS-tags are determined using CoreNLP . The result averaged over the four datasets is shown in <ref type="figure">Figure 4</ref>. For the result on each dataset, please refer to Appendix H. Comparing the indomain supervised model (SBERT-sup.) and the prior distribution of the POS tags, we find that nouns (NN) are by far the most relevant content words in a sentence, while function words such as prepositions (IN) and determinators (DT) have little influence on the model prediction. Surprisingly, we do not perceive significant differences between all the approaches. This is good news for the unsupervised methods (TSDAE, CT, Sim-CSE and BERT-flow) and show that they can learn which words types are critical in a sentence without access to labeled data. On the down side, unsupervised approaches might have issues for tasks where nouns are not the most critical content words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We mainly experiment with pre-trained Transformer encoders in this work. Besides single encoders, there are also pre-trained encoder-decoder models like BART <ref type="bibr" target="#b3">(Lewis et al., 2020)</ref> and T5 <ref type="bibr" target="#b8">(Raffel et al., 2020)</ref>. However, they are already extensively pre-trained with variants of auto-encoder loss on the general domain and they are suspected of overfitting the reconstruction behavior. To verify this idea, we also further train BART-base and T5-base models with TSDAE on the 4 domainspecific datasets. The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. We observe that BART and T5 can achieve much lower training loss (1.7 and 1.3 on average, resp.) than from scratch (3.4) or BERT (2.7), but they achieve rather bad test performance, even worse than from scratch. Compared with training from scratch (which is similar to <ref type="bibr" target="#b18">Zhang et al. (2018)</ref>), on the other hand, we find starting from BERT can reach to a much better balance point between loss fitting and generalization. Thus, we conclude that TSDAE is more suitable to start from single encoder checkpoints, which can utilize the pre-trained knowledge while avoiding overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we propose a new unsupervised sentence embedding learning method based on pretrained Transformers and sequential deoising autoencoder (TSDAE). We evaluate TSDAE on other, recent state-of-the-art unsupervised learning on four different tasks from heterogeneous (specialized) domains in three different settings: unsupervised learning, domain adaptation, and pretraining.</p><p>We observe that TSDAE performs well on the selected tasks and for the different settings, significantly outperforming other approaches.</p><p>Further, we show that the current evaluation of unsupervised sentence embedding learning approach, which is primarily done on the Semantic Textual Similarity (STS) task, is insufficient: A strong performance on STS does not correlate with a good performance on specific tasks. Many recent unsupervised approaches are not able to outperform out-of-the-box pre-trained models on the selected tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Optimal Configuration of TSDAE</head><p>To obtain the optimal configuration, we compare TSDAE models trained and evaluated on the general domain without bias towards any specific domain. The greedy search is applied by sequentially finding the best (1) noise type and ratio (2) pooling method and (3) weight tying scheme. Similar to the choice of CT and BERT-flow, we train the models on the combination of SNLI and MultiNLI without labels and evaluate the models on the STS benchmark with the metric of Spearman rank correlation. The maximum number of training steps is 30K and the models are evaluated every 1.5K training steps, reporting the best validation performance. Scores are obtained by calculating the average over 5 random seeds.</p><p>We first compare the scores of different noise types, fixing the noise ratio as 0.3 (i.e. 30% tokens are influenced) and the pooling method as CLS pooling. The results are show in <ref type="table" target="#tab_8">Table 5</ref>. This indicates deletion is the best noise type. We then tune the noise ratio of the deletion noise and the results are shown in <ref type="table" target="#tab_9">Table 6</ref>. This indicates 0.6 is the best noise ratio.   We then compare different pooling methods with the best setting so far. The results are shown in <ref type="table">Table 7</ref>. Since there is little difference between CLS and mean pooling and mean pooling loses the position information, the CLS pooling is chosen. Finally, we find that tying the encoder and the decoder can further improve the validation score to 79.15.</p><p>Method CLS Mean Max Score 78.77 78.84 78.17 <ref type="table">Table 7</ref>: Results with different pooling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Settings</head><p>We implement TSDAE, CT and BERT-flow based on Pytorch and Huggingface's Transformers 6 (version number: v3.1.0). For these three unsupervised methods, following the original papers, the number of training steps is 100K; the batch size is 8; the optimizers are AdamW, RMSProp and AdamW, respectively; the initial learning rates are 3e-5, 1e-5 and 1e-6, resp. The weight decay for BERT-flow is 0.01. The learning rate for CT follows a segmented-constant scheduling scheme: 1e-5 for step 1 to 500; 8e-6 for step 501 to 1000; 6e-6 for step 1001 to 1500; 4e-6 for step 1501 to 2000; 2e-6 for others. The pooling method for CT and BERT-flow is both mean pooling. Since CT trains two independent encoders and we find the second encoder has better performance, we use the second encoder for evaluation. For SimCSE, since its official hyper-parameter setting is very different from the other 3 methods, we use the official code 7 along with the default hyper-parameters. In detail, its hyper-parameters are: 1 epoch of training, batch size of 512, AdamW optimizer with learning rate 5e-5 and a linear layer on the CLS token embedding as the pooling method.</p><p>Since in the real-world scenario where the labeled data is expensive to obtain, applying early-stopping with a in-domain development set is impractical. Thus, in our unsupervised experiments, we do not use early-stopping with in-domain labeled data and indicate a fixed number of training steps 8 mentioned above instead.</p><p>We use the repository of sentence-transformers 9 (version number: v0.3.8) to train the in-domain supervised models. For them, the number of training epochs is 10; the maximum number of training steps is 20K; the batch size is 64; the similarity function ? is set to cosine similarity; early-stopping is applied by checking the validation performance. To eliminate the influence of randomness, we report the scores averaged over 5 random seeds for all the in-domain unsupervised and supervised models. All the pre-trained checkpoints used are listed in <ref type="table" target="#tab_11">Table 8</ref>.</p><p>For BM25, we use the implementation available on Elasticsearch 10 with the default settings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Results of Other Checkpoints</head><p>The results of other checkpoints besides BERT-base-uncased are shown in <ref type="table">Table 9</ref>. For all the methods, better results are achieved by using BERT checkpoints, which also makes TSDAE significantly outperforms others. We suppose this advantage comes from the additional pre-training task, next sentence prediction of the BERT models, which guides the model to learn from sentence-level contexts.  <ref type="table">Table 9</ref>: Evaluation of different checkpoints using average precision. '+/-' separates the mean value and standard deviation over scores of 5 random seeds. Best results within each group are underlined and the overall best results are bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Equivalent Labeling Work</head><p>The goal of unsupervised sentence embedding learning methods is to eliminate the need of labeled training data, which can be expensive in the creation. However, as shown in Section 6, approaches with sufficient in-domain labeled data significantly outperform unsupervised approaches.</p><p>As far as we know, previous work did not study the point of intersection between unsupervised and supervised approaches: If you only need few labeled examples to outperform unsupervised approaches, annotating those might be the more viable solution.</p><p>To find this intersection point, we train the in-domain supervised SBERT approach with varying size of labeled training data. Results are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. To estimate the intersection with more precision, we apply binary search. We set the search precision to the standard deviation of the target score over 5 random seeds.</p><p>The results are shown in <ref type="table" target="#tab_0">Table 10</ref>. To match the performance of TSDAE, 140 -6k annotated examples are required. CQADupStack and the TwitterParaphrase corpus, which compromise various domains, require more labeled data than AskUbuntu (1 domain). Surprisingly, SciDocs, which includes data from all type of scientific domains, the in-domain supervised approach outperforms unsupervised approaches with just 464 labeled examples. This dataset appears to be especially challenging for unsupervised approaches, as we observe a large performance gap between in-domain supervised and unsupervised approaches.</p><p>In an annotation experiment on the Twitter dataset, we measured that annotating 100 Tweet pairs takes about 20 minutes for an (experienced) annotator. Hence, the state-of-the-art unsupervised TSDAE approach achieves the same performance as a supervised approach with 0.5 -20 hours of annotation work for one annotator (2.5h -100h for 5 crowd annotators). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Usage for Pre-Training</head><p>The pre-training performance on AskUbuntu, CQADupStack and TwitterPara is shown in <ref type="figure" target="#fig_5">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Detailed Results of Semantic Textual Similarity</head><p>The detailed results of STS on each dataset are shown in <ref type="table" target="#tab_0">Table 11</ref> with the evaluation metric of Spearman's rank correlation. Note that the training set of STSb contains subsets of STS12-16, Thus, we do not include the scores of SBERT-base-nli-stsb-v2 on these datasets for reducing misunderstanding.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Influence of Corpus Size</head><p>The influence of corpus size for AskUbuntu, CQADupStack and TwitterPara is shown in <ref type="figure" target="#fig_6">Figure 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Influence of Different POS Tags</head><p>The influence of different POS tags on the output similarity scores for AskUbuntu, CQADupStack and TwitterPara is shown in <ref type="figure" target="#fig_7">Figure 7</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of TSDAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of different pre-training approaches (TSDAE/MLM/CT/SimCSE+SBERT) with increasing sizes of labeled training data (in thousands). SBERT: Training from the standard BERTbase-uncased checkpoint. TSDAE: Unsupervised baseline. Larger plots: Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The influence of the number of training sentences (in thousands) on the model performance. Larger plots: Appendix G. POS tag for the most relevant content word in a sentence, i.e. the word that mostly influences if a sentence pair is considered as similar. VB-AUX/-NAUX represents auxiliary/non-auxiliary verbs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>54.5 +/-0.2 10.5 +/-0.1 69.0 +/-0.1 53.5 +/-0.2 46.6 +/-0.1 DistilBERT-base TSDAE 59.2 +/-0.3 14.6 +/-0.1 73.9 +/-0.3 72.3 +/-0.9 54.9 +/-0.2 CT 57.7 +/-0.8 14.0 +/-0.3 66.4 +/-0.4 72.2 +/-0.7 52.3 +/-0.3 SimCSE 54.8 +/-0.7 12.3 +/-0.1 66.8 +/-0.6 65.9 +/-0.1 49.9 +/-0.3 BERT-flow 55.0 +/-0.2 11.0 +/-0.0 65.9 +/-0.0 70.5 +/-0.1 50.5 +/-053.7 +/-0.2 9.2 +/-0.1 69.3 +/-0.2 64.5 +/-0.1 49.2 +/-0.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The influence of the number of training sentences on the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The influence of the number of training sentences on the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The influence of different POS tags on the output similarity scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. The slash symbol '/' separates the numbers for development and test. Multiple subdatasets are included in CQADupStack, SciDocs and TwitterPara. CQADupStack has one sub-dataset for each of the 12 forums. The avg. #relevant, avg. #candidates and avg. length are all general statistics without distinguishing the sub-datasets.</figDesc><table><row><cell>and the</cell></row></table><note>3 Results for other checkpoints is reported in Appendix C</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>, we show the performance (Spearman's rank correlation) on the test set of the STS benchmark 5 along with the avg. performance on our four domain-specific tasks. See Appendix F for results on other STS datasets. We observe quite different behaviour when evaluating on STS data compared to evaluating on our domain specific tasks. On STS data, CT and Sim-CSE perform strongly, outperforming MLM and TSDAE by a large margin. However, when ap-Unsupervised learning based on BERT-base TSDAE 59.4 ? 14.5 ? 76.8 ? 69.2 73.0 71.4 ? 73.9 ? 75.0 ? 75.6 ? 74.0 ? 55.2 ? ? 74.5 ? 75.6 ? 78.6 ? 78.1 ? 78.2 ? 77.6 ? 56.5 ?</figDesc><table><row><cell>Method</cell><cell cols="2">AskU. CQADup.</cell><cell></cell><cell>TwitterP.</cell><cell></cell><cell></cell><cell></cell><cell>SciDocs</cell><cell></cell><cell>Avg.</cell></row><row><cell>Sub-task/-dataset</cell><cell></cell><cell></cell><cell cols="3">TURL PIT Avg.</cell><cell>Cite</cell><cell>CC</cell><cell>CR</cell><cell cols="2">CV Avg.</cell></row><row><cell>MLM</cell><cell>54.3</cell><cell>11.7</cell><cell>71.9</cell><cell cols="4">69.7 70.8 71.2 75.8</cell><cell>75.1</cell><cell cols="2">76.2 74.6 52.9</cell></row><row><cell>CT</cell><cell>56.3</cell><cell>13.3</cell><cell>74.6</cell><cell cols="4">70.4 72.5 63.4 67.1</cell><cell>70.1</cell><cell cols="2">69.7 67.6 52.4</cell></row><row><cell>SimCSE</cell><cell>55.9</cell><cell>12.4</cell><cell>74.5</cell><cell cols="4">62.5 68.5 62.5 65.1</cell><cell>67.7</cell><cell cols="2">67.6 65.7 50.6</cell></row><row><cell>BERT-flow</cell><cell>53.7</cell><cell>9.2</cell><cell>72.8</cell><cell cols="4">65.7 69.3 61.3 62.8</cell><cell>66.7</cell><cell cols="2">67.1 64.5 49.2</cell></row><row><cell cols="3">Domain adaptation: NLI+STS ? target task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSDAE</cell><cell>58.7</cell><cell>13.6</cell><cell>75.8</cell><cell>66.2</cell><cell>71</cell><cell cols="2">69.9  ? 73.8  ?</cell><cell>75  ?</cell><cell cols="2">75.7  ? 73.6  ? 54.2  ?</cell></row><row><cell>MLM</cell><cell>54.4</cell><cell>9.7</cell><cell>69.8</cell><cell>68.1</cell><cell>69</cell><cell cols="2">67.1 71.8</cell><cell>72.6</cell><cell cols="2">72.9 71.1 51.1</cell></row><row><cell>CT</cell><cell>57.9</cell><cell>14.2</cell><cell>75.6</cell><cell cols="4">70.6 73.1 62.3 66.2</cell><cell>68.5</cell><cell cols="2">68.9 66.5 52.9</cell></row><row><cell>SimCSE</cell><cell>56.6</cell><cell>13.8</cell><cell>73.4</cell><cell cols="7">65.9 69.7 61.8 63.7 67.01 66.7 64.8 51.2</cell></row><row><cell>BERT-flow</cell><cell>58.2</cell><cell>13.9</cell><cell>76.5</cell><cell>67.4</cell><cell>72</cell><cell cols="2">62.2 64.8</cell><cell>68.1</cell><cell>68</cell><cell>65.8 52.5</cell></row><row><cell cols="3">Domain adaptation: target task ? NLI+STS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">TSDAE 73.1  MLM 59.4  ? 14.4  ? 75.8 60.6 14.3 75.0 68.6 71.8 74.7 78.2</cell><cell>77.0</cell><cell cols="2">77.6 76.9 55.9</cell></row><row><cell>CT</cell><cell>56.4</cell><cell>13.4</cell><cell>75.9</cell><cell cols="4">68.9 72.4 66.5 69.6</cell><cell>70.6</cell><cell cols="2">72.2 69.7 53.0</cell></row><row><cell>SimCSE</cell><cell>56.2</cell><cell>13.1</cell><cell>75.5</cell><cell cols="4">67.3 71.4 65.5 68.5</cell><cell>70.0</cell><cell cols="2">71.4 68.9 52.4</cell></row><row><cell cols="3">Other previous unsupervised approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>53.4</cell><cell>13.3</cell><cell>71.9</cell><cell cols="4">70.5 71.2 58.9 61.3</cell><cell>67.3</cell><cell cols="2">66.9 63.6 50.4</cell></row><row><cell>Avg. GloVe</cell><cell>51.0</cell><cell>10.0</cell><cell>70.1</cell><cell cols="4">52.1 61.1 58.8 60.6</cell><cell>64.2</cell><cell cols="2">65.4 62.2 46.1</cell></row><row><cell>Sent2Vec</cell><cell>49.0</cell><cell>3.2</cell><cell>47.5</cell><cell cols="4">39.9 43.7 61.6 66.0</cell><cell>66.1</cell><cell cols="2">66.7 65.1 40.2</cell></row><row><cell>BERT-base-uncased</cell><cell>48.5</cell><cell>6.5</cell><cell>69.1</cell><cell cols="4">61.7 65.4 59.4 65.1</cell><cell>65.4</cell><cell cols="2">68.6 64.6 46.3</cell></row><row><cell cols="3">Out-of-the-box supervised pre-trained models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SBERT-base-nli-v2</cell><cell>53.4</cell><cell>11.8</cell><cell>75.4</cell><cell cols="4">69.9 72.7 66.8 70.0</cell><cell>70.7</cell><cell cols="2">72.8 70.1 52.0</cell></row><row><cell>SBERT-base-nli-stsb-v2</cell><cell>54.5</cell><cell>12.9</cell><cell>75.9</cell><cell cols="4">68.5 72.2 66.2 69.2</cell><cell>69.9</cell><cell cols="2">72.3 69.4 52.3</cell></row><row><cell>USE-large</cell><cell>(59.3)</cell><cell>(15.9)</cell><cell>77.1</cell><cell cols="4">69.8 73.5 67.1 69.5</cell><cell>71.4</cell><cell cols="2">72.6 70.2 54.7</cell></row><row><cell cols="3">In-domain supervised training (upper bound)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SBERT-supervised</cell><cell>63.8</cell><cell>16.3</cell><cell>81.6</cell><cell cols="4">75.8 78.7 90.4 91.2</cell><cell>86.2</cell><cell cols="2">83.6 87.9 61.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">STSb Specific Tasks</cell></row><row><cell>Unsupervised method</cell><cell></cell><cell></cell></row><row><cell>TSDAE</cell><cell>66.0</cell><cell>55.2</cell></row><row><cell>MLM</cell><cell>47.3</cell><cell>52.9</cell></row><row><cell>CT</cell><cell>73.9</cell><cell>52.4</cell></row><row><cell>SimCSE</cell><cell>73.8</cell><cell>50.6</cell></row><row><cell>BERT-flow</cell><cell>48.9</cell><cell>49.2</cell></row><row><cell cols="3">Out-of-the-box supervised pre-trained models</cell></row><row><cell>SBERT-base-nli-v2</cell><cell>83.9</cell><cell>52.0</cell></row><row><cell cols="2">SBERT-base-nli-stsb-v2 87.3</cell><cell>52.3</cell></row><row><cell>USE-large</cell><cell>80.9</cell><cell>54.7</cell></row></table><note>Evaluation using average precision. Results are averaged over 5 random seeds. The best results excluding the upper bound are bold. USE-large was trained with in-domain training data for AskUbuntu and CQADupStack (scores in italic). Our proposed TSDAE significantly outperforms other unsupervised and supervised out-of-the- box approaches. ? marks the cases where TSDAE outperforms both CT and SimCSE in all 5 runs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Performance (Spearman's rank correlation) on the STS benchmark test set. Specific tasks: Average performance from Table 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test performance/training loss of TSDAE models starting from different checkpoints. The results for BERT-base are copied fromTable 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>tension. In International Conference on Learning Representations, ICLR 2021, Vienna, Austria, May 3-7, 2021, pages 1-21. International Conference on Learning Representations.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.</cell></row><row><cell></cell><cell></cell><cell cols="2">2021. DeCLUTR: Deep contrastive learning for</cell></row><row><cell></cell><cell></cell><cell cols="2">unsupervised textual representations. In Proceed-</cell></row><row><cell></cell><cell></cell><cell cols="2">ings of the 59th Annual Meeting of the Association</cell></row><row><cell></cell><cell></cell><cell cols="2">for Computational Linguistics and the 11th Interna-</cell></row><row><cell cols="2">Daniel Cer, Mona Diab, Eneko Agirre, I?igo Lopez-</cell><cell cols="2">tional Joint Conference on Natural Language Pro-</cell></row><row><cell cols="2">Gazpio, and Lucia Specia. 2017. SemEval-2017</cell><cell cols="2">cessing (Volume 1: Long Papers), pages 879-895,</cell></row><row><cell cols="2">task 1: Semantic textual similarity multilingual and</cell><cell cols="2">Online. Association for Computational Linguistics.</cell></row><row><cell cols="2">crosslingual focused evaluation. In Proceedings</cell><cell></cell></row><row><cell cols="2">of the 11th International Workshop on Semantic</cell><cell cols="2">Ian Goodfellow, Yoshua Bengio, and Aaron Courville.</cell></row><row><cell cols="2">Evaluation (SemEval-2017), pages 1-14, Vancouver,</cell><cell cols="2">2016. Deep Learning. MIT Press. http://www.</cell></row><row><cell cols="2">Canada. Association for Computational Linguistics.</cell><cell>deeplearningbook.org.</cell></row><row><cell cols="2">Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,</cell><cell cols="2">Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006.</cell></row><row><cell cols="2">Nicole Limtiaco, Rhomni St. John, Noah Constant,</cell><cell cols="2">Dimensionality reduction by learning an invariant</cell></row><row><cell cols="2">Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,</cell><cell cols="2">mapping. In 2006 IEEE Computer Society Confer-</cell></row><row><cell cols="2">Brian Strope, and Ray Kurzweil. 2018. Univer-</cell><cell cols="2">ence on Computer Vision and Pattern Recognition</cell></row><row><cell cols="2">sal sentence encoder for english. In Proceedings</cell><cell cols="2">(CVPR 2006), 17-22 June 2006, New York, NY, USA,</cell></row><row><cell cols="2">of the 2018 Conference on Empirical Methods in</cell><cell>pages 1735-1742. IEEE Computer Society.</cell></row><row><cell cols="2">Natural Language Processing, EMNLP 2018: Sys-</cell><cell></cell></row><row><cell cols="2">tem Demonstrations, Brussels, Belgium, October 31</cell><cell cols="2">Matthew L. Henderson, Rami Al-Rfou, Brian Strope,</cell></row><row><cell cols="2">-November 4, 2018, pages 169-174. Association for</cell><cell cols="2">Yun-Hsuan Sung, L?szl? Luk?cs, Ruiqi Guo, Sanjiv</cell></row><row><cell>Computational Linguistics.</cell><cell></cell><cell cols="2">Kumar, Balint Miklos, and Ray Kurzweil. 2017. Ef-</cell></row><row><cell></cell><cell></cell><cell cols="2">ficient Natural Language Response Suggestion for</cell></row><row><cell cols="2">Ting Chen, Simon Kornblith, Mohammad Norouzi,</cell><cell>Smart Reply. arXiv preprint arXiv:1705.00652.</cell></row><row><cell cols="2">and Geoffrey E. Hinton. 2020. A simple framework</cell><cell></cell></row><row><cell cols="2">for contrastive learning of visual representations. In</cell><cell cols="2">Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.</cell></row><row><cell cols="2">Proceedings of the 37th International Conference on</cell><cell cols="2">Learning distributed representations of sentences</cell></row><row><cell cols="2">Machine Learning, ICML 2020, 13-18 July 2020,</cell><cell cols="2">from unlabelled data. In NAACL HLT 2016, The</cell></row><row><cell cols="2">Virtual Event, volume 119 of Proceedings of Ma-</cell><cell cols="2">2016 Conference of the North American Chapter of</cell></row><row><cell cols="2">chine Learning Research, pages 1597-1607. PMLR.</cell><cell cols="2">the Association for Computational Linguistics: Hu-</cell></row><row><cell></cell><cell></cell><cell cols="2">man Language Technologies, San Diego California,</cell></row><row><cell cols="2">Arman Cohan, Sergey Feldman, Iz Beltagy, Doug</cell><cell cols="2">USA, June 12-17, 2016, pages 1367-1377. The As-</cell></row><row><cell cols="2">Downey, and Daniel S. Weld. 2020. SPECTER:</cell><cell>sociation for Computational Linguistics.</cell></row><row><cell cols="2">document-level representation learning using</cell><cell></cell></row><row><cell>citation-informed transformers.</cell><cell>In Proceedings</cell><cell cols="2">Doris Hoogeveen, Karin M. Verspoor, and Timothy</cell></row><row><cell cols="2">of the 58th Annual Meeting of the Association</cell><cell cols="2">Baldwin. 2015. Cqadupstack: A benchmark data</cell></row><row><cell cols="2">for Computational Linguistics, ACL 2020, Online,</cell><cell cols="2">set for community question-answering research. In</cell></row><row><cell cols="2">July 5-10, 2020, pages 2270-2282. Association for</cell><cell cols="2">Proceedings of the 20th Australasian Document</cell></row><row><cell>Computational Linguistics.</cell><cell></cell><cell cols="2">Computing Symposium, ADCS 2015, Parramatta,</cell></row><row><cell></cell><cell></cell><cell cols="2">NSW, Australia, December 8-9, 2015, pages 3:1-3:8.</cell></row><row><cell cols="2">Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo?c</cell><cell>ACM.</cell></row><row><cell cols="2">Barrault, and Antoine Bordes. 2017. Supervised</cell><cell></cell></row><row><cell cols="2">learning of universal sentence representations from</cell><cell cols="2">Diederik P. Kingma and Prafulla Dhariwal. 2018.</cell></row><row><cell cols="2">natural language inference data. In Proceedings of</cell><cell cols="2">Glow: Generative flow with invertible 1x1 convolu-</cell></row><row><cell cols="2">the 2017 Conference on Empirical Methods in Nat-</cell><cell cols="2">tions. In Advances in Neural Information Process-</cell></row><row><cell cols="2">ural Language Processing, pages 670-680, Copen-</cell><cell cols="2">ing Systems 31: Annual Conference on Neural Infor-</cell></row><row><cell cols="2">hagen, Denmark. Association for Computational</cell><cell cols="2">mation Processing Systems 2018, NeurIPS 2018, De-</cell></row><row><cell>Linguistics.</cell><cell></cell><cell cols="2">cember 3-8, 2018, Montr?al, Canada, pages 10236-</cell></row><row><cell></cell><cell></cell><cell>10245.</cell></row><row><cell cols="2">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and</cell><cell></cell></row><row><cell cols="2">Kristina Toutanova. 2019. BERT: pre-training of</cell><cell cols="2">Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.</cell></row><row><cell cols="2">deep bidirectional transformers for language under-</cell><cell cols="2">A continuously growing dataset of sentential para-</cell></row><row><cell cols="2">standing. In Proceedings of the 2019 Conference</cell><cell cols="2">phrases. In Proceedings of the 2017 Conference on</cell></row><row><cell cols="2">of the North American Chapter of the Association</cell><cell cols="2">Empirical Methods in Natural Language Processing,</cell></row><row><cell cols="2">for Computational Linguistics: Human Language</cell><cell cols="2">EMNLP 2017, Copenhagen, Denmark, September 9-</cell></row><row><cell cols="2">Technologies, NAACL-HLT 2019, Minneapolis, MN,</cell><cell cols="2">11, 2017, pages 1224-1234. Association for Compu-</cell></row><row><cell cols="2">USA, June 2-7, 2019, Volume 1 (Long and Short Pa-</cell><cell>tational Linguistics.</cell></row><row><cell cols="2">pers), pages 4171-4186. Association for Computa-</cell><cell></cell></row><row><cell>tional Linguistics.</cell><cell></cell><cell cols="2">Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.</cell></row><row><cell></cell><cell></cell><cell cols="2">Jaakkola, Kateryna Tymoshenko, Alessandro Mos-</cell></row><row><cell cols="2">Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.</cell><cell cols="2">chitti, and Llu?s M?rquez. 2016. Semi-supervised</cell></row><row><cell cols="2">SimCSE: Simple contrastive learning of sentence</cell><cell>question retrieval with gated convolutions.</cell><cell>In</cell></row><row><cell cols="2">embeddings. arXiv preprint arXiv:2104.08821.</cell><cell cols="2">NAACL HLT 2016, The 2016 Conference of the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results with different noise types Score 77.81 77.70 77.75 78.02 78.25 78.77 78.19 77.69 75.67</figDesc><table><row><cell>Ratio</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results with different noise ratio.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Model checkpoints used in this work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Intersection point (number of labeled sentence pairs) between unsupervised TSDAE and in-domain supervised SBERT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Evaluation on the task of STS using Spearman's rank correlation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at: https://github.com/ UKPLab/sentence-transformers/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The dataset splits and the evaluation toolkit are available at: https://github.com/UKPLab/useb</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code available at: https://github.com/ UKPLab/pytorch-bertflow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In the original paper of BERT-flow, the mean pooling over the first and the last layer is used, which causes the discrepancy on the STS scores. However, for a comparable setting, as the choice of most of the previous work, we only consider the pooling over the last layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="128">and 65,536 sentences. For each experiment, we train a bert-base-uncased model with 10 epochs up to 100k training steps. The models are evaluated</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/huggingface/transformers 7 https://github.com/princeton-nlp/SimCSE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For SimCSE, the official code involves early-stopping on the STS-B development set. We do not change this setting for this method, since STS-B is not an in-domain dataset in our task-and domain-specific evaluation. 9 https://github.com/UKPLab/sentence-transformers 10 https://www.elastic.co/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Research Foundation (DFG) as part of the UKP-SQuARE project (grant GU 798/29-1), by the Eu- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Amaru Cuba Gyllensten, and Erik Ylip?? Hellqvist. 2021. Semantic re-tuning with contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelia</forename><surname>Gogoulou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.733</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="528" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Task-oriented intrinsic evaluation of semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Text REtrieval Conference</title>
		<meeting>The Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11-02" />
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15316</idno>
		<title level="m">Whitening Sentence Representations for Better Semantics and Faster Retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Bowman. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (PIT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/s15-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015</title>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual universal sentence encoder for semantic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jax</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Hern?ndez</forename><surname>?brego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning universal sentence representations with mean-max attention autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="4514" to="4523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">AskU. CQADup. TwitterP. SciDocs Avg</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">2333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
