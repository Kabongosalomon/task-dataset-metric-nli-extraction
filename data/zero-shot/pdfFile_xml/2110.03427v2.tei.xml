<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Highlights Is Attention always needed? A Case Study on Language Identification from Speech Is Attention always needed? A Case Study on Language Identification from Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-10">10 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanu</forename><surname>Mandal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santanu</forename><surname>Pal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indranil</forename><surname>Dutta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahidas</forename><surname>Bhattacharya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudip</forename><surname>Kumar Naskar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanu</forename><surname>Mandal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>188</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Raja S.C. Mallick Rd</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<postCode>700 032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santanu</forename><surname>Pal</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Wipro AI Lab</orgName>
								<orgName type="institution">Wipro India Limited</orgName>
								<address>
									<addrLine>Sarjapur Road</addrLine>
									<postCode>560 035</postCode>
									<settlement>Doddakannelli, Bangaluru</settlement>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indranil</forename><surname>Dutta</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Languages and Linguistics</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>188</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Raja S.C. Mallick Rd</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<postCode>700 032</postCode>
									<settlement>Kolkata</settlement>
									<country>India A R</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">T I C L E I N F O</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahidas</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Languages and Linguistics</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>188</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Raja S.C. Mallick Rd</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<postCode>700 032</postCode>
									<settlement>Kolkata</settlement>
									<country>India A R</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">T I C L E I N F O</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudip</forename><surname>Kumar Naskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>188</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Raja S.C. Mallick Rd</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<postCode>700 032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Highlights Is Attention always needed? A Case Study on Language Identification from Speech Is Attention always needed? A Case Study on Language Identification from Speech</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-10">10 Jul 2022</date>
						</imprint>
					</monogr>
					<note>? The proposed method uses Convolutional Neural Network (CNN), Convolutional Recurrent Neural Network (CRNN), and attention based CRNN for the task of Spoken Language Identification. ? Our proposed architecture provides state-of-the-art performance in languages that belong to the same language family as well as in noisy scenarios.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Language Identification Convolutional Neural Network Convolutional Recurrent Neural Net- work Attention Indian Languages</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Language Identification (LID), a recommended initial step to Automatic Speech Recognition (ASR), is used to detect a spoken language from audio specimens. In state-of-the-art systems capable of multilingual speech processing, however, users have to explicitly set one or more languages before using them. LID, therefore, plays a very important role in situations where ASR based systems cannot parse the uttered language in multilingual contexts causing failure in speech recognition. We propose an attention based convolutional recurrent neural network (CRNN with Attention) that works on Mel-frequency Cepstral Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some state-of-the-art approaches, namely Convolutional Neural Network (CNN) and Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed method. We performed extensive evaluation on thirteen different Indian languages and our model achieves classification accuracy over 98%. Our LID model is robust to noise and provides 91.2% accuracy in a noisy scenario. The proposed model is easily extensible to new languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is Attention always needed? A Case Study on Language Identification from Speech</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>From the inception of research in Natural Language Processing (NLP), researchers have specifically rely on Convolution Neural Networks (CNN) as it utilizes local features effectively. Earlier Recurrent Neural Network (RNN) was effectively used in different NLP domains, but recent use of Transformer has shown promising results which outperforms all previous experimental results. Attention based models are capable of capturing the content-based global interactions.</p><p>Transformer in Question-Answering domain, researcher <ref type="bibr" target="#b38">(Yamada et al., 2020)</ref> were able to outperform BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, SpanBERT <ref type="bibr" target="#b16">(Joshi et al., 2020)</ref>, XLNet <ref type="bibr" target="#b39">(Yang et al., 2019)</ref>, and ALBERT <ref type="bibr" target="#b19">(Lan et al., 2020)</ref>. In Machine Translation domain researcher <ref type="bibr" target="#b35">(Takase and Kiyono, 2021;</ref><ref type="bibr" target="#b12">Gu et al., 2019;</ref><ref type="bibr" target="#b3">Chen and Heafield, 2020)</ref> used Transformer and were able to outperform other state-of-the-art (sota) algorithms. In other domain like Language Modelling, Text Classification, Topic Modelling, Emotion Classification, Sentiment Analysis, etc Transformer has widely used.</p><p>In this work we focused on using different approaches for Spoken Language Identification. Humans are capable of recognizing almost immediately the language being used by a speaker for voicing an utterance. The task of automatic language identification (LID) is to automatically classify the language used by a speaker in his/her speech. In the era of Internet of Things, smart and intelligent assistants (e.g., Alexa 1 , Siri 2 , Cortona 3 , Google Assistant 4 , etc.) can interact with humans with some default language settings (mostly in English) and these smart assistants rely heavily on Automatic Speech Recognition (ASR). However, these virtual assistants fail to provide any assistance in multilingual contexts. To make such smart assistants robust, LID can be used so that the smart assistants can automatically recognize the speaker's language and change its language setting accordingly.</p><p>Our approach of identifying spoken language is limited to Indian Languages only because India is world second populated and seventh largest country in landmass and also have dynamic culture. Currently, India has 28 states and 8 Union Territories, where each states and Union Territories has its own language, but none of the language is recognised as the national language of the country. Only, English and Hindi is used as official language according to the Constitution of India Part XVII Chapter 1 Article 343 5 . Currently, only 22 languages have been accepted as regional languages. It can be seen from the <ref type="table" target="#tab_0">Table 1</ref> that different languages are spoken in different states, however, languages do not obey the geographical boundaries. Therefore, many of these languages, particularly in the neighboring regions, have multiple dialects which are amalgamation of two or more languages.</p><p>Such enormous linguistic diversity makes it difficult for the citizens to communicate in different parts of the country. Bilingualism and multilingualism are the norm in India. In this context, an LID system becomes a crucial component for any speech based smart assistant. The biggest challenge and hence an area of active innovation for Indian language is the reality that most of these languages are under resourced.</p><p>Every spoken language has its underlying lexical, speaker, channel, environment, and other variations. The likely differences among various spoken languages are in their phoneme inventories, frequency of occurrence of the phonemes, acoustics, the span of the sound units in different languages, and intonation patterns at higher levels. The overlap between the phoneme set of two or more familial languages makes it a challenge for recognition. The low-resource status of these languages makes the training of machine learning models doubly difficult. Every spoken language has its underlying lexical, speaker, channel, environment, and other variations. The likely differences among various spoken languages are in their phoneme inventories, frequency of occurrence of the phonemes, acoustics, the span of the sound units in different languages, and intonation patterns at higher levels. The overlap between the phoneme set of two or more familial languages makes it a challenge for recognition. The low-resource status of these languages makes the training of machine learning models doubly difficult. The purpose of our approach is yet to predict the correct spoken language regardless of the above-mentioned constraints.</p><p>In this work we proposed Language Identification method for Indian Languages using different approaches. Our LID methods cover 13 Indian languages 6 . Additionally our method is language agnostic. The main contributions of this work can be summarized as follows:</p><p>? The method uses Convolutional Neural Network (CNN), Convolutional Recurrent Neural Network (CRNN), and attention based CRNN for the task of LID. We tested 13 Indian languages achieving state-of-the-art accuracy.</p><p>? Our model also provides state-of-the-art performance in languages that belong to the same language family as well as in noisy scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Extraction of language dependent features for example prosody and phonemes was widely used to classify spoken languages <ref type="bibr" target="#b42">(Zissman, 1996;</ref><ref type="bibr" target="#b22">Mart?nez et al., 2011;</ref><ref type="bibr" target="#b8">Ferrer et al., 2010)</ref>. Following the success in speaker verification systems, identity vectors (i-vectors) have also been used as features in various classification architectures. Use of ivectors requires significant domain knowledge <ref type="bibr" target="#b5">(Dehak et al., 2011b;</ref><ref type="bibr" target="#b22">Mart?nez et al., 2011)</ref>. In recent trends researchers rely on neural networks for feature extraction and classification <ref type="bibr" target="#b20">(Lopez-Moreno et al., 2014;</ref><ref type="bibr" target="#b9">Ganapathy et al., 2014)</ref>. Researcher <ref type="bibr" target="#b28">Revay and Teschke (2019a)</ref> used the ResNet50  architecture for classifying languages by generating the log-Mel spectra for each raw audio. The architecture uses cyclic learning rate where learning rate increases and then decreases linearly. Maximum learning rate for a cycle is set by finding the optimal learning rate using fastai <ref type="bibr" target="#b15">(Howard and Gugger, 2020)</ref>.</p><p>Researcher <ref type="bibr" target="#b10">Gazeau and Varol (2018)</ref> established the use of Neural Network, Support Vector Machine, and Hidden Markov Model (HMM) to identify different languages. Hidden Markov models converts speech into a sequence of vectors and was used to capture temporal features in speech. Established LID systems <ref type="bibr" target="#b4">(Dehak et al., 2011a;</ref><ref type="bibr" target="#b22">Mart?nez et al., 2011;</ref><ref type="bibr" target="#b26">Plchot et al., 2016;</ref><ref type="bibr" target="#b41">Zazo et al., 2016)</ref> are based on identity vector (i-vectors) representations for language processing tasks. In <ref type="bibr" target="#b4">Dehak et al. (2011a)</ref>, i-vectors are used as data representations for a speaker verification task and fed to the classifier as the input. <ref type="bibr" target="#b4">Dehak et al. (2011a)</ref> applied Support Vector Machines (SVM) with cosine kernels as the classifier, while <ref type="bibr" target="#b22">Mart?nez et al. (2011)</ref> used logistic regression for the actual classification task. Recent years have found the use of feature extraction with neural networks, particularly with Long Short Term Memory (LSTM) <ref type="bibr" target="#b41">(Zazo et al., 2016;</ref><ref type="bibr" target="#b11">Gelly et al., 2016;</ref><ref type="bibr" target="#b21">Lozano-Diez et al., 2015)</ref>. These neural networks produce better accuracy while being simpler in design compared to the conventional LID methods <ref type="bibr" target="#b4">(Dehak et al., 2011a;</ref><ref type="bibr" target="#b22">Mart?nez et al., 2011;</ref><ref type="bibr" target="#b26">Plchot et al., 2016)</ref>. Recent trends in developing LID systems are mainly focused on different forms of LSTMs with DNNs. <ref type="bibr" target="#b26">Plchot et al. (2016)</ref> used a 3 layered Convolutional Neural Network where i-vectors were the input layer and softmax activation function as the output layer. <ref type="bibr" target="#b41">Zazo et al. (2016)</ref> used Mel Frequency Cepstral Coefficients (MFCCs) with Shifted Delta Coefficient features as information to a unidirectional layer which is directly connected to a softmax <ref type="bibr">6</ref> The study was limited to the number of Indian languages for which datasets were available classifier. <ref type="bibr" target="#b11">Gelly et al. (2016)</ref> used audio transformed to Perceptual Linear Prediction (PLP) coefficients and their 1 and 2 order derivatives as information for a Bidirectional LSTM in forward and backward directions. The forward and backward sequences generated from the Bidirectional LSTM were joined together and used to classify the language of the input samples. <ref type="bibr" target="#b21">Lozano-Diez et al. (2015)</ref> used Convolutional Neural Networks (CNNs) for their LID system. They transformed the input data as an image containing MFCCs with Shifted Delta Coefficient features. The image represents the time domain for the x-axis and frequency bins for the y-axis. <ref type="bibr" target="#b21">Lozano-Diez et al. (2015)</ref> used CNN as the feature extractor for the identity vectors. They achieved better performance when combining both the CNN features and identity vectors. <ref type="bibr" target="#b29">Revay and Teschke (2019b)</ref> used ResNet  architecture for language classification by generating spectrograms of each audio. Cyclic Learning <ref type="bibr" target="#b32">(Smith, 2018)</ref> was used where the learning rate increases and decreases linearly. <ref type="bibr" target="#b37">Venkatesan et al. (2018)</ref> utilised Mel-Frequency Cepstral Coefficients (MFCC) to infer aspects of speech signals from Kannada, Hindi, Tamil, and Telugu. They obtained accuracy of 76% and 73% using Support Vector Machines and Decision Tree classifiers, respectively, on 5 hours of training data. <ref type="bibr" target="#b23">Mukherjee et al. (2019)</ref> used Convolutional Neural Networks (CNN) For language identification on German, Spanish, and English. They used Filter Banks to extract features from frequency domain representations of the signal. <ref type="bibr" target="#b0">Aarti and Kopparapu (2017)</ref> experimented with several auditory features in order to determine the optimal feature set for a classifier to detect Indian Spoken Language. <ref type="bibr" target="#b31">Sisodia et al. (2020)</ref> evaluated Ensemble Learning models for classifying spoken languages such as German, Dutch, English, French, and Portuguese. Bagging, Adaboosting, random forests, gradient boosting, and additional trees were used in their ensemble learning models. <ref type="bibr" target="#b14">Heracleous et al. (2018)</ref> presented a comparative study of Deep Neural Networks (DNN) and Convolutional Neural Networks (CNN) for Spoken Language Identification (LID), with Support Vector Machines (SVM) as the baseline. They also presented the performance of the fusion of the mentioned methods. The NIST 2015 i-vector Machine Learning Challenge dataset was used to assess the system's performance with the goal of detecting 50 in-set languages. <ref type="bibr" target="#b2">Bartz et al. (2017)</ref> tackled the problem of Language Identification in the image domain rather than the typical acoustic domain. A hybrid Convolutional Recurrent Neural Network (CRNN) is employed for this, which acts on spectrogram images of the provided audio clips. <ref type="bibr" target="#b7">Draghici et al. (2020)</ref> tried to solve the task of Language Identification while using Mel-spectrogram images as input features. This strategy was employed in Convolutional Neural Network (CNN) and Convolutional Recurrent Neural Network (CRNN) in terms of performance. This work is characterized by a modified training strategy which provides equal class distribution and efficient memory utilisation. <ref type="bibr" target="#b9">Ganapathy et al. (2014)</ref> reported how they used bottleneck features from a Convolutional Neural Network for the LID task. Bottleneck features were used in conjunction with conventional acoustic features, and performance was evaluated. Experiments revealed that when a system with bottleneck features is compared to a system without them, average relative improvements of up to 25% are achieved. <ref type="bibr" target="#b41">Zazo et al. (2016)</ref> proposed an open-source, end-to-end, LSTM-RNN system that outperforms a more recent reference i-vector system by up to 26% when both are tested on a subset of the NIST Language Recognition Evaluation with 8 target languages.</p><p>Our research differs from the previous works on LID in the following aspects:</p><p>? Comparison of performance of CNN, CRNN, as well as CRNN with Attention.</p><p>? Extensive experiments with our proposed model shows its applicability both for close language as well as noisy speech scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Architecture</head><p>Our proposed architecture consists of three models.</p><p>? CNN based architecture</p><p>? CRNN based architecture</p><p>? CRNN with Attention based architecture</p><p>We made use of the capacity of CNNs to capture spatial information to identify languages from audio samples. In CNN based architecture our network uses four convolution layers, where each layer is followed by ReLU <ref type="bibr" target="#b24">(Nair and Hinton, 2010)</ref> activation function and max pooling with a stride of 3 and a pool size of 3. The kernel sizes and the number of filters for each convolution layer are (3, 512), (3, 512), (3, 256), and (3, 128), respectively. Mechanism used in our architecture is based on Hierarchical Attention Networks <ref type="bibr" target="#b40">(Yang et al., 2016)</ref>. In the Attention Mechanism, contexts of features are summarized with a bidirectional LSTM by going forward and backward.</p><formula xml:id="formula_0">??? ? = ????????????? ? ( ), ? [1, ] ???? = ?????????????? ( ), ? [ , 1] = [??? ? , ???? ]</formula><p>(1)</p><formula xml:id="formula_1">? = ?( ? + ) ?<label>(2)</label></formula><p>In equation 1, is the the number of audio specimen. The vector, , builds the base for the attention mechanism. The goal of the attention mechanism is to learn the model through training with randomly initialized weights and biases. The layer also ensures with the ? function that the network does not stall. The function keeps the input values between -1 and 1, and also maps zeros to near-zero values. The layer with ? function are again multiplied by trainable context vector . The context weight vector is randomly initiated and jointly learned during the training process. Improved vectors are represented by ? as shown in equation 2. Context vectors are finally calculated by providing a weight to each by dividing the exponential values of the previously generated vectors with the summation of all exponential values of previously generated vectors as shown in equation 3. To avoid division by zero, an epsilon is added to the denominator.</p><formula xml:id="formula_2">= ( ? ) ? ( ? ) +<label>(3)</label></formula><p>The sum of these importance weights concatenated with the previously calculated context vectors is fed to a linear layer with 13 output units serving as a classifier for the 13 languages.  </p><formula xml:id="formula_3">w i b i tanh() u i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Extraction</head><p>For feature extraction of spoken utterances we used MFCCs. For calculating MFCCs we used _ ? , frame size represented as _ , frame stride represented as _ , N-point Fast Fourier transform represented as , low frequency mel represented as , number of filter represented as , number of cepstral coefficients represented as , and cepstral lifter represented of values 0.97, 0.025 (25ms), 0.015 (15ms overlapping), 512, 0, 40, 13, and 22, respectively. We used frame size of 25 ms as typically frame sizes in speech processing domain uses 20ms to 40ms with 50% (in our case 15ms) overlapping between consecutive frames.</p><formula xml:id="formula_4">? = 2595 ? log 10 (1 + 0.5 ? 700 )<label>(4)</label></formula><p>We used low frequency mel (lf) as 0 and high frequency mel (hf) is calculated using the equation 4. lf and hf are used to generate the non-linear human ear perception of sound, by more discriminative of lower frequencies and less discriminative at higher frequencies.</p><formula xml:id="formula_5">? _ = [ [0], [1 ?] ? _ ? * [? ?1]]<label>(5)</label></formula><p>As shown in equation 5 emphasized signal is calculated by using pre-emphasis filter applied on signal using first order filter. Number of frames is calculated by taking the ceiling value of division of absolute value of difference between signal length ( _ ) and product of filter size ( _ ) and sample rate (sr) with the product of frame stride ( _ ) and sample rate (sr) as shown in equation 6. Signal length is the length of ? _ calculated in equation 5.</p><formula xml:id="formula_6">_ = ? | _ ? ( _ ? )| ( _ ? ) ?<label>(6)</label></formula><p>Using equation 7 _ is generated from concatenation of ? _ and zero value array of dimen-</p><formula xml:id="formula_7">sion ( _ _ ? ? _ ?)?1, where, _ _ ? is calculated by _ ? ( _ ? ) + ( _ ? ). _ = [ ? _ , [0] (( _ ?( _ ? )+( _ ? ))? _ )?1 ]<label>(7)</label></formula><p>Frames are calculated as shown in equation 8 from the _ elements where elements are the addition of array of positive natural number from 0 to _ ? repeated _ and transpose of array of size of _ where each element is the difference of ( _ ? ).</p><formula xml:id="formula_8">= _ [({ ? + ? 0 &lt; &lt; ( _ ? )} ) ( _ ,1) =0 + (({ ? = ( _ ? ) ? ( ? 1), ? {0, ? , _ ? ( _ ? )}} ) (( _ ? ),1) =0 ) ? ]<label>(8)</label></formula><p>Power frames shown in equation 9 are calculated square of absolute value of Discrete Fourier Transform (DFT) of product of hamming window and frames of each elements with NFFT.</p><formula xml:id="formula_9">= | (( ? (0.54 ? ( ? ( _ ? )?1 =0 0.46 ? cos 2 ( _ ? )?1 ))), )| 2 (9) _ = { ? = + ? ? ( + 2) ? 1 ? , ? { , ? , ? }}<label>(10)</label></formula><p>Mel points are the array where elements are calculated as shown in the equation 10, where i is the values belongs from lf to hf.</p><formula xml:id="formula_10">= ? ( + 1) ? (700 ? (10 _ 2595 ? 1)) _ ?<label>(11)</label></formula><p>From equation 11, bins are calculated where floor value of the elements are taken which is product of hertz points and + 1 divided by sample rate. Hertz points is calculated by multiplying 700 to subtraction of 1 from 10 power of _ 2595 .</p><formula xml:id="formula_11">( ) = ? ? ? ? ? ? ? 0 &lt; ( ? 1) ? ( ?1) ( )? ( ?1) ( ? 1) ? ? ( ) ( +1)? ( +1)? ( ) ( ) ? ? ( + 1) 0 &gt; ( + 1)<label>(12)</label></formula><p>Bins calculated from equation 11 are used to calculate filter banks shown in equation 12. Each filter in the filter bank is triangular, with a response of 1 at the central frequency and a linear drop to 0 till it meets the central frequencies of the two adjacent filters, where the response is 0. Finally mfcc is calculated shown in equation 13 by decorrelate the filter bank coefficients using Discrete Cosine Transform (DCT) to get a compressed representation of the filter banks. Sinusoidal liftering is applied to the mfcc to de-emphasize higher mfccs which improves to classify in noisy signals.</p><formula xml:id="formula_12">= (20 log 10 ( ? ? )) ? 1 + 2 sin { ? ? ? + , ? }<label>(13)</label></formula><p>MFCCs features of shape (1000, 13) generated from equation 13 is provided as input to the neural network which expects the same dimension followed by convolution layers as mentioned in section 3. Raw speech signal cannot be provided input to the architecture as it contains lots of noise data therefore extracting features from the speech signal and using it as input to the model will produce better performance than directly considering raw speech signal as input. Our motivation to use MFCCs features as the feature count is small enough to force us to learn the information of the sample. Parameters are related to the amplitude of frequencies and provide us with frequency channels to analyze the speech specimen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Benchmark Data</head><p>In the past two decades, development of LID methods has been largely fostered through NIST Language Evaluations (LREs). As a result, the most popular benchmarks for evaluating new LID models and methods are NIST LRE evaluation dataset <ref type="bibr" target="#b30">(Sadjadi et al., 2018)</ref>. The NIST LREs dataset mostly contains narrow-band telephone speech. Datasets are typically distributed by the Linguistic Data Consortium (LDC) and cost thousands of dollars. For example, the standard Kaldi <ref type="bibr" target="#b27">(Povey et al., 2011)</ref> recipe for LRE072 relies on 18 LDC SLR datasets that cost $15000 (approx) to LDC non-members. This makes it difficult for new research groups to enter the academic field of LID. Furthermore, the NIST LRE evaluations focus mostly on telephone speech.</p><p>As the NIST LRE dataset is not freely available we used the European Language Dataset <ref type="bibr" target="#b2">Bartz et al. (2017)</ref> which is open sourced. The European language (EU) dataset contains YouTube News data for 4 major European languages -English (en), French (fr), German (de) and Spanish (es). Statistics of the dataset are given in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Experimental Data</head><p>The Indian language (IL) dataset was acquired from the Indian Institute of Technology, Madras 7 . The dataset includes 13 widely used Indian languages.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Environment</head><p>We implemented our architecture using Tensorflow <ref type="bibr" target="#b1">(Abadi et al., 2016)</ref> backend. We split the Indian language dataset into training, validation, and testing set, containing 80%, 10%, and 10% of the data, respectively, for each language and gender.</p><p>For regularization, we apply dropout <ref type="bibr" target="#b33">(Srivastava et al., 2014)</ref> after Max-Pooling layer and Bi-Directional LSTM layer. We use the rate of 0.1. A 2 regularization with 10 ?6 weight is also added to all the trainable weights in the network. We train the model with Adam <ref type="bibr" target="#b17">(Kingma and Ba, 2015)</ref> optimizer with 1 = 0.9, 2 = 0.98, and = 10 ?9 and learning rate schedule <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, with 4k warm-up steps and peak learning rate of 0.05? ? where d is 128. Batch size of 64 with "Sparse Categorical Crossentrop" as the loss function were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on European Language</head><p>We evaluated our model in two environments -No Noise and White Noise. According to our intuition, in real life scenarios during prediction of language chances of capturing background noise of chatter and other sounds may happen. For the white noise evaluation setup, we mixed white noise to each test sample which has a strong audible presence but retains the identity of the language.  <ref type="table">Table 4</ref> compares the results of our models on the EU dataset with state-of-the-art models presented by <ref type="bibr" target="#b2">Bartz et al. (2017)</ref>. Proposed model by <ref type="bibr" target="#b2">Bartz et al. (2017)</ref> consists of CRNN and uses Google's Inception-v3 architecture <ref type="bibr" target="#b34">(Szegedy et al., 2016)</ref>. They experimented in four different environments -No Noise, White Noise, Cracking Noise, and Background Noise. All our evaluation results are rounded up to 3 digit after decimal point.</p><p>The CNN model failed to achieve competitive results; it provided accuracy of 0.948/0.871 in No Noise/White Noise. In CRNN architecture, our model provides accuracy of 0.967/0.912 on No Noise/White Noise scenario outperforming the state-of-the-art results of <ref type="bibr" target="#b2">Bartz et al. (2017)</ref>. Use of Attention improves over the Inception-v3 CRNN in No Noise scenario, however it does not perform well on White Noise</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Result on Indian Language Dataset</head><p>We evaluated our proposed architecture with the proposed architecture by <ref type="bibr" target="#b18">Kulkarni et al. (2022)</ref> on the same datasets. Our two architectures i.e. CRNN and CRNN with Attention was able to outperform the accuracy obtained by <ref type="bibr" target="#b18">Kulkarni et al. (2022)</ref> as shown in <ref type="table">Table 5</ref>. They used 6 Linear layers where units are <ref type="bibr">256,</ref><ref type="bibr">256,</ref><ref type="bibr">128,</ref><ref type="bibr">64,</ref><ref type="bibr">32,</ref><ref type="bibr">13</ref>, respectively in the CNN architecture, whereas the DNN architecture uses 3 LSTM layers having units 256, 256, 128, respectively followed by a dropout layer followed by 3 Time Distributed layer followed by Linear layer of 13 as units.</p><p>Accuracy DNN <ref type="bibr" target="#b18">(Kulkarni et al., 2022)</ref> 98.34% RNN <ref type="bibr" target="#b18">(Kulkarni et al., 2022)</ref> 98.43% CNN 98.3% CRNN 98.7% CRNN with Attention 98.7% <ref type="table">Table 5</ref> Comparative evaluation results (in terms of Accuracy) of our model and the model of <ref type="bibr" target="#b18">Kulkarni et al. (2022)</ref> on the Indian Language dataset</p><p>We evaluated system performance using the following evaluation metrics -Recall (TPR), Precision (PPV), f1score, and Accuracy. Since one of our major objectives was to measure the accessibility of the network to new languages, we introduced Data Balancing of training data for each class, as the number of samples available for each class may vary drastically. This is the case for the Indian Language Dataset as shown in <ref type="table" target="#tab_2">Table 3</ref> in which Kannada, Marathi and particularly Bodo have limited amount of data compared to the rest of the languages. To alleviate this data imbalance problem, we used class weight balancing as a dynamic method using scikit-learn <ref type="bibr" target="#b25">(Pedregosa et al., 2011)</ref>.  <ref type="table">Table 6</ref> Experimental Results for Indian Languages PPV, TPR, f1-score and Accuracy scores are reported in <ref type="table">Table 6</ref> for the three architectures -CNN, CRNN, and CRNN with Attention. From <ref type="table">Table 6</ref> it is clearly visible that both CRNN architecture and CRNN with Attention provide competitive results of 0.987 accuracy. <ref type="table" target="#tab_6">Table 7</ref>, 8, and 9 shows the confusion matrix for CNN, CRNN, and CRNN with Attention.     <ref type="table">Table 9</ref> Confusion matrix for CNN Assamese and Bengali have originated from the same language family and they share approximately the same phoneme set. However, Bengali and Tamil are from different language family but share similar phoneme set. For example, in Bengali cigar is churut and star is nakshatra while cigar in Tamil is charuttu and star in Tamil is natsattira, which is quite similar. Similarly, Meitei and Assamese share similar phonemes. On close study we observed that Hindi and Malayalam have also similar phoneme set as both the languages borrowed most of the vocabularies from Sanskrit. For example, 'arrogant' is Ahankar in Hindi and Ahankaram in Malayalam. Similarly, Sathyu or commonly spoken as Satya in Hindi means 'Truth', which is Sathyam in Malayalam. Also the word Sundar in Hindi is Sundaram in Malayalam, which means 'beautiful'. <ref type="table" target="#tab_0">Table 10</ref> shows the most common classification errors encountered during evaluation. </p><formula xml:id="formula_13">Assamese ? Meitei Bengali ? Assamese Bengali ? Meitei Bengali ? Tamil Hindi ? Malayalam</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Result on same language families on Indian Language Dataset</head><p>A deeper study into these 13 Indian languages led us to define five clusters of languages based on their phonetic similarity. Cluster internal languages are phonetically similar, close, and geographically contiguous, hence difficult to be differentiated. Bodo and Meitei are phonetically very much distant from any of the rest of the languages, thus they form singleton clusters. We carried out separate experiments for identification of the cluster internal languages for Cluster 1, 2 and 3, and the experimental results are presented in <ref type="table" target="#tab_0">Table 11</ref>.</p><p>It can be clearly observed from <ref type="table" target="#tab_0">Table 11</ref> that both CRNN architecture and CRNN with Attention provide competitive results for every language cluster. For cluster-1 CRNN architecture and CRNN with Attention provides accuracy of 0.98/0.974, for cluster-2 0.999/0.999, and for cluster-3 0.999/1, respectively. CNN architecture also provides comparable results to the other two architectures.    <ref type="table" target="#tab_0">Table 12</ref> Confusion matrix for Cluster 1</p><p>From <ref type="table" target="#tab_0">Table 12</ref>, we observed that Bengali gets confused with Assamese and Odia, which is quite expected since these two languages are spoken in neighbouring states and both of them share almost the same phonemes. For example, in Odia rice is pronounced as bhata whereas in Bengali pronounced as bhat, similarly fish in odia as machha whereas in Bengali it is machh. Both CRNN and CRNN with Attention perform well to discriminate between Bengali and Odia.  <ref type="table" target="#tab_0">Table 13</ref> Confusion matrix for Cluster 2</p><p>It can be observed from <ref type="table" target="#tab_0">Table 14</ref> that CNN makes a lot of confusion discriminating among these four languages. Both CRNN and CRNN with Attention prove to be better at discriminating among these languages.  From the results in <ref type="table" target="#tab_0">Table 11</ref>, 12, 13 and 14, it is quite clear that CRNN (Bi-Directional LSTM over CNN) and CRNN with Attention are more effective for Indian language identification and they perform almost at par. Another important observation is that it is harder to classify the languages in cluster 1 than the other two clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRNN and Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1.">Convolution Kernel Size</head><p>To study the effect of kernel sizes in the convolution layers, we sweep the kernel size with 3, 7, 17, 32, 65 of the models. We found that performance decreases with larger kernel sizes, as shown in <ref type="table" target="#tab_0">Table 15</ref>. On comparing the accuracy upto second decimal places kernel size 3 performs better than rest.  <ref type="table" target="#tab_0">Table 15</ref> Ablation study on convolution kernel sizes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2.">Automatic Class Weight vs Manual Class Weight</head><p>Balancing the data using class weights gives better accuracy for CRNN with Attention (98.7%) and CRNN (98.7%), compared to CNN (98.3%) shown in <ref type="table">Table 6</ref>. We study the efficacy of the architectures by manually balancing the datasets using 100 samples, 200 samples, and 571 samples drawn randomly from the dataset and the results of these experiments are presented in <ref type="table" target="#tab_0">Table 16</ref>, 17 and 18, respectively.    The objective of the study was to observe the performance of the architectures on increasing the sample size. Since Bodo language has the minimum data (571 samples) among all the languages in the dataset, we perform our experiments till 571 samples.</p><p>A comparison of the results in <ref type="table" target="#tab_0">Table 16</ref>, 17, and 18 reveals the following observations. ? All the models perform consistently better with more training data.</p><p>? CRNN and CRNN with attention perform consistently better than CNN.</p><p>? CRNN is less data hungry among the 3 models and it performs the best in the lowest data scenario.  <ref type="figure" target="#fig_6">Figure 3</ref> graphically shows the performance improvement over increasing data samples. The confusion matrices for the three architectures for the 3 datasets are presented in <ref type="figure" target="#fig_1">Table A.1, A.2, A.3, B.1, B.2, B</ref>.3, C.1, C.2, C.3 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>In this work, we proposed a language identification method using CRNN that works on MFCC features of speech signals. Our architecture efficiently identifies the language both in close language and noisy scenarios. We carried out extensive experiments and our architecture produced state-of-the-art results. Through our experiments, we have also shown our architecture's robustness to noise and its extensible to new languages. The model exhibits the overall best accuracy of 98.7% which improves over the traditional use of CNN (98.3%). CRNN with attention performs almost at par with CRNN, however the attention mechanism which incurs some additional computational overhead does not always result in improvement over CRNN. In future, we would like to extend our work by increasing the language classes with speech specimen recorded in different environments. We would also like to extend our work to check the usefulness of the proposed architecture on smaller time speech samples through which we can deduce the optimal time required to classify the languages with high accuracy. We would also like to test our method on language dialect identification.       <ref type="table" target="#tab_0">Predicted  PPV  TPR  f1  Score  as  bn  bd  gu  hi  kn  ml  mn  mr  or  rj  ta</ref>        <ref type="table" target="#tab_0">Predicted  PPV  TPR  f1  Score  as  bn  bd  gu  hi  kn  ml  mn  mr  or  rj  ta</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 Figure 1 :</head><label>11</label><figDesc>provides a schematic overview of the network architecture. In CRNN based architecture the Bi-Directional LSTM consisting of a single LSTM with 256 output units was used after the Convolution module. The Attention The figure presents our architecture consisting of Convolution module, Bi-Directional LSTM, and Attention Mechanism denoted in different blocks. Convolution module extracts features from the input audio. The output of the final convolution layer is provided to the Bi-Directional LSTM network as the input which is further connected to the Attention module for adequate learning. The Attention module is finally connected to a softmax classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematic diagram of the Attention Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>presents the schematic diagram of the Attention Module where is the input to the the module and output of the Bi-Directional LSTM layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>Cluster 1: Assamese, Bengali, Odia ? Cluster 2: Gujarati, Hindi, Marathi, Rajasthani ? Cluster 3: Kannada, Malayalam, Tamil, Telugu ? Cluster 4: Bodo ? Cluster 5: Meitei</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of model results for varying dataset size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>List of official languages as per the Eighth Schedule of the Constitution of India, as of 1 December 2007 with their language family and states spoken in.Table 1 describes the 22 languages designated as Official language according to the Eighth Schedule of the Constitution of India, as of 1 December 2007. Most of the Indian languages originated from Indo-Aryan and Dravidian language family.</figDesc><table><row><cell>Sl. No.</cell><cell>Language</cell><cell>Family</cell><cell>Spoken in</cell></row><row><cell>1</cell><cell>Assamese</cell><cell>Indo-Aryan</cell><cell>Assam</cell></row><row><cell>2</cell><cell>Bengali</cell><cell>Indo-Aryan</cell><cell>Assam, Jharkhand, Tripura, West Bengal</cell></row><row><cell>3</cell><cell>Bodo</cell><cell>Sino-Tibetan</cell><cell>Assam</cell></row><row><cell>4</cell><cell>Dogri</cell><cell>Indo-Aryan</cell><cell>Jammu and Kashmir</cell></row><row><cell>5</cell><cell>Gujarati</cell><cell>Indo-Aryan</cell><cell>Gujrat, Dadra and Nagar Haveli and Daman and Diu</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Andaman and Nicobar Islands, Bihar,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Chhattisgarh,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dadra and Nagar Haveli and Daman and Diu,</cell></row><row><cell>6</cell><cell>Hindi</cell><cell>Indo-Aryan</cell><cell>Delhi, Haryana, Himachal Pradesh, Jammu and Kashmir,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Jharkhand, Ladakh, Madhya Pradesh,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mizoram, Rajasthan, Uttar Pradesh,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Uttarakhand</cell></row><row><cell>7</cell><cell>Kannada</cell><cell>Dravidian</cell><cell>Karnataka</cell></row><row><cell>8</cell><cell>Kashmiri</cell><cell>Indo-Aryan</cell><cell>Jammu and Kashmir</cell></row><row><cell>9</cell><cell>Konkani</cell><cell>Indo-Aryan</cell><cell>Dadra and Nagar Haveli and Daman and Diu, Goa</cell></row><row><cell>10</cell><cell>Maithili</cell><cell>Indo-Aryan</cell><cell>Jharkhand</cell></row><row><cell>11</cell><cell>Malayalam</cell><cell>Dravidian</cell><cell>Kerala, Lakshadweep, Puducherry</cell></row><row><cell>12</cell><cell>Marathi</cell><cell>Indo-Aryan</cell><cell>Dadra and Nagar Haveli and Daman and Diu, Goa, Maharashtra</cell></row><row><cell>13</cell><cell>Meitei</cell><cell>Sino-Tibetan</cell><cell>Manipur</cell></row><row><cell>14</cell><cell>Nepali</cell><cell>Indo-Aryan</cell><cell>Sikkim, West Bengal</cell></row><row><cell>15</cell><cell>Odia</cell><cell>Indo-Aryan</cell><cell>Jharkhand, Odisha</cell></row><row><cell>16</cell><cell>Punjabi</cell><cell>Indo-Aryan</cell><cell>Delhi, Haryana, Punjab</cell></row><row><cell>17</cell><cell>Sanskrit</cell><cell>Indo-Aryan</cell><cell>Himachal Pradesh</cell></row><row><cell>18</cell><cell>Santali</cell><cell>Austroasiatic</cell><cell>Jharkhand</cell></row><row><cell>19</cell><cell>Sindhi</cell><cell>Indo-Aryan</cell><cell>Rajasthan</cell></row><row><cell>20</cell><cell>Tamil</cell><cell>Dravidian</cell><cell>Tamil Nadu</cell></row><row><cell>21</cell><cell>Telugu</cell><cell>Dravidian</cell><cell>Andhra Pradesh, Puducherry, Telangana</cell></row><row><cell>22</cell><cell>Urdu</cell><cell>Indo-Aryan</cell><cell>Bihar, Delhi, Jammu and Kashmir, Jharkhand, Telangana, Uttar Pradesh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Statistics</figDesc><table><row><cell>Language</cell><cell>Label</cell><cell>Total Samples</cell><cell>Average Duration (in seconds)</cell></row><row><cell>English</cell><cell>en</cell><cell>43,269</cell><cell>684.264</cell></row><row><cell>French</cell><cell>fr</cell><cell>67,689</cell><cell>492.219</cell></row><row><cell>German</cell><cell>de</cell><cell>48,454</cell><cell>1,152.916</cell></row><row><cell>Spanish</cell><cell>es</cell><cell>57,869</cell><cell>798.169</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell></row></table><note>of the European Language (EU) Dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>presents the statistics of this dataset which we used for our experiments.</figDesc><table><row><cell>Language</cell><cell>Label</cell><cell>Gender</cell><cell>Samples</cell><cell>Total Samples</cell><cell>Average Duration (in seconds)</cell></row><row><cell>Assamese</cell><cell>as</cell><cell>F M</cell><cell>8,713 8,941</cell><cell>17,654</cell><cell>5.587</cell></row><row><cell>Bengali</cell><cell>bn</cell><cell>F M</cell><cell>3,253 6,187</cell><cell>9,440</cell><cell>5.743</cell></row><row><cell>Bodo</cell><cell>bd</cell><cell>F</cell><cell>571</cell><cell>571</cell><cell>25.219</cell></row><row><cell>Gujarati</cell><cell>gu</cell><cell>F M</cell><cell>2,396 3,288</cell><cell>5,684</cell><cell>13.459</cell></row><row><cell>Hindi</cell><cell>hi</cell><cell>F M</cell><cell>2,318 2,318</cell><cell>4,636</cell><cell>8.029</cell></row><row><cell>Kannada</cell><cell>kn</cell><cell>F M</cell><cell>1,289 1,289</cell><cell>2,578</cell><cell>10.264</cell></row><row><cell>Malayalam</cell><cell>ml</cell><cell>F M</cell><cell>5,650 5,650</cell><cell>11,300</cell><cell>5.699</cell></row><row><cell>Manipuri</cell><cell>mn</cell><cell>F M</cell><cell>9,487 8,430</cell><cell>17,917</cell><cell>4.169</cell></row><row><cell>Marathi</cell><cell>mr</cell><cell>F</cell><cell>2,448</cell><cell>2,448</cell><cell>7.059</cell></row><row><cell>Odia</cell><cell>or</cell><cell>F M</cell><cell>3,578 3,573</cell><cell>7,151</cell><cell>4.4</cell></row><row><cell>Rajasthani</cell><cell>rj</cell><cell>F M</cell><cell>4,346 4,779</cell><cell>9,125</cell><cell>7.914</cell></row><row><cell>Tamil</cell><cell>ta</cell><cell>F M</cell><cell>3,243 3,717</cell><cell>6,960</cell><cell>10.516</cell></row><row><cell>Telugu</cell><cell>te</cell><cell>F M</cell><cell>4,043 2,481</cell><cell>6,524</cell><cell>15.395</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Statistics</figDesc><table /><note>of the Indian Language (IN) Dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Comparative evaluation results (in terms of Accuracy) of our model and the model of Bartz et al. (2017) on the YouTube News (EU) dataset</figDesc><table><row><cell></cell><cell>No Noise</cell><cell>White Noise</cell></row><row><cell>CRNN (Bartz et al., 2017)</cell><cell>0.91</cell><cell>0.63</cell></row><row><cell>Inception-v3 CRNN (Bartz et al., 2017)</cell><cell>0.96</cell><cell>0.91</cell></row><row><cell>CNN</cell><cell>0.948</cell><cell>0.871</cell></row><row><cell>CRNN</cell><cell>0.967</cell><cell>0.912</cell></row><row><cell>CRNN with Attention</cell><cell>0.966</cell><cell>0.888</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Confusion matrix for CRNN with Attention architecture From Table 7, 8, and 9 it can be observed that Assamese gets confused with Meitei; Bengali gets confused with Assamese, Meitei, Tamil and Telugu; and Hindi gets confused with Malayalam.</figDesc><table><row><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell>Predicted ml</cell><cell>mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Confusion matrix for CRNN</figDesc><table><row><cell></cell><cell></cell><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell>Predicted ml</cell><cell>mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row><row><cell></cell><cell>as</cell><cell>1744</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>1</cell><cell>13</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>0</cell><cell>0.991</cell><cell>0.988</cell><cell>0.989</cell></row><row><cell></cell><cell>bn</cell><cell>9</cell><cell>838</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>35</cell><cell>0</cell><cell>8</cell><cell>5</cell><cell>34</cell><cell>13</cell><cell>1</cell><cell>0.888</cell><cell>0.941</cell></row><row><cell></cell><cell>bd</cell><cell>0</cell><cell>0</cell><cell>57</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.966</cell><cell>1</cell><cell>0.983</cell></row><row><cell></cell><cell>gu</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>563</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0.996</cell><cell>0.991</cell><cell>0.994</cell></row><row><cell></cell><cell>hi</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>452</cell><cell>0</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0.991</cell><cell>0.974</cell><cell>0.983</cell></row><row><cell>Actual</cell><cell>kn ml mn</cell><cell>0 0 1</cell><cell>0 0 0</cell><cell>0 1 0</cell><cell>0 0 0</cell><cell>1 2 0</cell><cell>256 2 0</cell><cell>0 1122 0</cell><cell>0 0 1788</cell><cell>0 0 0</cell><cell>0 0 0</cell><cell>1 3 0</cell><cell>0 0 1</cell><cell>0 0 1</cell><cell>0.973 0.99 0.972</cell><cell>0.992 0.993 0.998</cell><cell>0.983 0.991 0.985</cell></row><row><cell></cell><cell>mr</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>244</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0.996</cell><cell>0.998</cell></row><row><cell></cell><cell>or</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>715</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.986</cell><cell>0.999</cell><cell>0.992</cell></row><row><cell></cell><cell>rj</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>908</cell><cell>0</cell><cell>0</cell><cell>0.986</cell><cell>0.996</cell><cell>0.991</cell></row><row><cell></cell><cell>ta</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>688</cell><cell>1</cell><cell>0.946</cell><cell>0.989</cell><cell>0.967</cell></row><row><cell></cell><cell>te</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>652</cell><cell>0.975</cell><cell>0.998</cell><cell>0.986</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 Most</head><label>10</label><figDesc></figDesc><table /><note>common errors</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11</head><label>11</label><figDesc>Experimental Results of LID for close languages.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 ,</head><label>12</label><figDesc>Table 13andTable 14presents the confusion matrix for cluster 1, cluster 2 and cluster 3, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">CRNN and Attention</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>as</cell><cell>Predicted bn</cell><cell>or</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row><row><cell></cell><cell>as</cell><cell>1766</cell><cell>0</cell><cell>0</cell><cell>0.962</cell><cell>1</cell><cell>0.981</cell></row><row><cell>Actual</cell><cell>bn</cell><cell>70</cell><cell>874</cell><cell>0</cell><cell>1</cell><cell>0.926</cell><cell>0.961</cell></row><row><cell></cell><cell>or</cell><cell>0</cell><cell>0</cell><cell>716</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CRNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>as</cell><cell>Predicted bn</cell><cell>or</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row><row><cell></cell><cell>as</cell><cell>1766</cell><cell>0</cell><cell>0</cell><cell>0.953</cell><cell>1</cell><cell>0.976</cell></row><row><cell>Actual</cell><cell>bn</cell><cell>88</cell><cell>856</cell><cell>0</cell><cell>1</cell><cell>0.907</cell><cell>0.951</cell></row><row><cell></cell><cell>or</cell><cell>0</cell><cell>0</cell><cell>716</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>as</cell><cell>Predicted bn</cell><cell>or</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row><row><cell></cell><cell>as</cell><cell>1766</cell><cell>0</cell><cell>0</cell><cell>0.953</cell><cell>1</cell><cell>0.976</cell></row><row><cell>Actual</cell><cell>bn</cell><cell>87</cell><cell>844</cell><cell>13</cell><cell>1</cell><cell>0.894</cell><cell>0.944</cell></row><row><cell></cell><cell>or</cell><cell>0</cell><cell>0</cell><cell>716</cell><cell>0.982</cell><cell>1</cell><cell>0.991</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell>Confusion matrix for Cluster 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16</head><label>16</label><figDesc>Experimental Results for Manually Balancing the Samples for each category to 100.</figDesc><table><row><cell>Language</cell><cell>PPV</cell><cell cols="3">CRNN with Attention TPR f1 Score Accuracy</cell><cell>PPV</cell><cell>TPR</cell><cell>CRNN f1 Score</cell><cell>Accuracy</cell><cell>PPV</cell><cell>TPR</cell><cell>CNN f1 Score</cell><cell>Accuracy</cell></row><row><cell>as</cell><cell>0.941</cell><cell>0.96</cell><cell>0.95</cell><cell></cell><cell>1</cell><cell>0.94</cell><cell>0.969</cell><cell></cell><cell>0.8</cell><cell>0.88</cell><cell>0.838</cell><cell></cell></row><row><cell>bn</cell><cell>0.909</cell><cell>1</cell><cell>0.952</cell><cell></cell><cell>1</cell><cell>0.96</cell><cell>0.98</cell><cell></cell><cell>0.92</cell><cell>0.92</cell><cell>0.92</cell><cell></cell></row><row><cell>bd</cell><cell>0.98</cell><cell>0.96</cell><cell>0.97</cell><cell></cell><cell>0.98</cell><cell>0.98</cell><cell>0.98</cell><cell></cell><cell>0.94</cell><cell>0.94</cell><cell>0.94</cell><cell></cell></row><row><cell>gu</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.918</cell><cell>0.9</cell><cell>0.909</cell><cell></cell></row><row><cell>hi</cell><cell>1</cell><cell>0.98</cell><cell>0.99</cell><cell></cell><cell>1</cell><cell>0.98</cell><cell>0.99</cell><cell></cell><cell>0.956</cell><cell>0.86</cell><cell>0.905</cell><cell></cell></row><row><cell>kn</cell><cell>1</cell><cell>0.98</cell><cell>0.99</cell><cell></cell><cell>1</cell><cell>0.98</cell><cell>0.99</cell><cell></cell><cell>0.878</cell><cell>0.86</cell><cell>0.869</cell><cell></cell></row><row><cell>ml</cell><cell>0.962</cell><cell>1</cell><cell>0.98</cell><cell>0.975</cell><cell>0.893</cell><cell>1</cell><cell>0.943</cell><cell>0.971</cell><cell>0.896</cell><cell>0.86</cell><cell>0.878</cell><cell>0.883</cell></row><row><cell>mn</cell><cell>0.979</cell><cell>0.92</cell><cell>0.948</cell><cell></cell><cell>0.907</cell><cell>0.98</cell><cell>0.942</cell><cell></cell><cell>0.754</cell><cell>0.92</cell><cell>0.829</cell><cell></cell></row><row><cell>mr</cell><cell>0.98</cell><cell>0.98</cell><cell>0.98</cell><cell></cell><cell>0.98</cell><cell>0.96</cell><cell>0.97</cell><cell></cell><cell>0.956</cell><cell>0.86</cell><cell>0.905</cell><cell></cell></row><row><cell>or</cell><cell>0.98</cell><cell>1</cell><cell>0.99</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.941</cell><cell>0.96</cell><cell>0.95</cell><cell></cell></row><row><cell>rj</cell><cell>0.96</cell><cell>0.96</cell><cell>0.96</cell><cell></cell><cell>1</cell><cell>0.96</cell><cell>0.98</cell><cell></cell><cell>0.86</cell><cell>0.86</cell><cell>0.86</cell><cell></cell></row><row><cell>ta</cell><cell>1</cell><cell>0.96</cell><cell>0.98</cell><cell></cell><cell>0.904</cell><cell>0.94</cell><cell>0.922</cell><cell></cell><cell>0.784</cell><cell>0.8</cell><cell>0.792</cell><cell></cell></row><row><cell>te</cell><cell>1</cell><cell>0.98</cell><cell>0.99</cell><cell></cell><cell>0.979</cell><cell>0.94</cell><cell>0.959</cell><cell></cell><cell>0.935</cell><cell>0.86</cell><cell>0.896</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 17</head><label>17</label><figDesc>Experimental Results for Manually Balancing the Samples for each category to 200.</figDesc><table><row><cell>Language</cell><cell>PPV</cell><cell cols="3">CRNN with Attention TPR f1 Score Accuracy</cell><cell>PPV</cell><cell>TPR</cell><cell>CRNN f1 Score</cell><cell>Accuracy</cell><cell>PPV</cell><cell>TPR</cell><cell>CNN f1 Score</cell><cell>Accuracy</cell></row><row><cell>as</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.983</cell><cell>0.983</cell><cell>0.983</cell><cell></cell><cell>0.967</cell><cell>1</cell><cell>0.983</cell><cell></cell></row><row><cell>bn</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.983</cell><cell>1</cell><cell>0.991</cell><cell></cell></row><row><cell>bd</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell></row><row><cell>gu</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.983</cell><cell>1</cell><cell>0.991</cell><cell></cell><cell>0.982</cell><cell>0.931</cell><cell>0.956</cell><cell></cell></row><row><cell>hi</cell><cell>0.983</cell><cell>0.983</cell><cell>0.983</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.893</cell><cell>0.862</cell><cell>0.877</cell><cell></cell></row><row><cell>kn</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.903</cell><cell>0.966</cell><cell>0.933</cell><cell></cell></row><row><cell>ml</cell><cell>1</cell><cell>0.966</cell><cell>0.982</cell><cell>0.988</cell><cell>0.983</cell><cell>1</cell><cell>0.991</cell><cell>0.985</cell><cell>0.914</cell><cell>0.914</cell><cell>0.914</cell><cell>0.945</cell></row><row><cell>mn</cell><cell>0.983</cell><cell>1</cell><cell>0.991</cell><cell></cell><cell>1</cell><cell>0.983</cell><cell>0.991</cell><cell></cell><cell>0.931</cell><cell>0.931</cell><cell>0.931</cell><cell></cell></row><row><cell>mr</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>0.982</cell><cell>1</cell><cell>0.991</cell><cell></cell><cell>0.965</cell><cell>0.982</cell><cell>0.973</cell><cell></cell></row><row><cell>or</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>0.983</cell><cell>0.991</cell><cell></cell><cell>1</cell><cell>0.966</cell><cell>0.982</cell><cell></cell></row><row><cell>rj</cell><cell>0.919</cell><cell>0.983</cell><cell>0.95</cell><cell></cell><cell>0.918</cell><cell>0.966</cell><cell>0.941</cell><cell></cell><cell>0.9</cell><cell>0.931</cell><cell>0.915</cell><cell></cell></row><row><cell>ta</cell><cell>0.964</cell><cell>0.931</cell><cell>0.947</cell><cell></cell><cell>0.964</cell><cell>0.914</cell><cell>0.938</cell><cell></cell><cell>0.879</cell><cell>0.879</cell><cell>0.879</cell><cell></cell></row><row><cell>te</cell><cell>1</cell><cell>0.983</cell><cell>0.991</cell><cell></cell><cell>1</cell><cell>0.983</cell><cell>0.991</cell><cell></cell><cell>0.982</cell><cell>0.931</cell><cell>0.956</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 18</head><label>18</label><figDesc>Experimental Results for Manually Balancing the Samples for each category to 571.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table A . 1</head><label>A1</label><figDesc>Confusion matrix of Manually Balancing the Samples for each category to 100 with CNN Confusion matrix of Manually Balancing the Samples for each category to 200 with CNN</figDesc><table><row><cell></cell><cell></cell><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell cols="2">Predicted ml mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell>PPV</cell><cell></cell><cell>TPR</cell><cell>f1 Score</cell></row><row><cell></cell><cell>as</cell><cell>58</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.967</cell><cell></cell><cell>1</cell><cell>0.983</cell></row><row><cell></cell><cell>bn</cell><cell>0</cell><cell>58</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.983</cell><cell></cell><cell>1</cell><cell>0.991</cell></row><row><cell></cell><cell>bd</cell><cell>0</cell><cell>0</cell><cell>56</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>gu</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>54</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.982</cell><cell cols="2">0.931</cell><cell>0.956</cell></row><row><cell></cell><cell>hi</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>50</cell><cell>0</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell>0.893</cell><cell cols="2">0.862</cell><cell>0.877</cell></row><row><cell>Actual</cell><cell>kn ml mn</cell><cell>0 0 1</cell><cell>0 0 0</cell><cell>0 0 0</cell><cell>0 0 0</cell><cell>0 0 0</cell><cell>56 4 1</cell><cell>2 53 0</cell><cell>0 0 54</cell><cell>0 0 0</cell><cell>0 0 0</cell><cell>0 1 0</cell><cell>0 0 1</cell><cell>0 0 1</cell><cell>0.903 0.914 0.931</cell><cell cols="2">0.966 0.914 0.931</cell><cell>0.933 0.914 0.931</cell></row><row><cell></cell><cell>mr</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>55</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.965</cell><cell cols="2">0.982</cell><cell>0.973</cell></row><row><cell></cell><cell>or</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>56</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell cols="2">0.966</cell><cell>0.982</cell></row><row><cell></cell><cell>rj</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>54</cell><cell>0</cell><cell>0</cell><cell>0.9</cell><cell cols="2">0.931</cell><cell>0.915</cell></row><row><cell></cell><cell>ta</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>3</cell><cell>51</cell><cell>0</cell><cell>0.879</cell><cell cols="2">0.879</cell><cell>0.879</cell></row><row><cell></cell><cell>te</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>54</cell><cell>0.982</cell><cell cols="2">0.931</cell><cell>0.956</cell></row><row><cell cols="2">Table A.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">Confusion matrix of Manually Balancing the Samples for each category to 571 with CNN</cell><cell></cell><cell></cell></row><row><cell cols="5">B. CRNN architecture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell cols="2">Predicted ml mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row><row><cell></cell><cell></cell><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell cols="2">Predicted ml mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell cols="2">PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row><row><cell></cell><cell>as</cell><cell>44</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>0</cell><cell>0.8</cell><cell></cell><cell>0.88</cell><cell>0.838</cell></row><row><cell></cell><cell>bn</cell><cell>1</cell><cell>46</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0.92</cell><cell></cell><cell>0.92</cell><cell>0.92</cell></row><row><cell></cell><cell>bd</cell><cell>0</cell><cell>0</cell><cell>47</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.94</cell><cell></cell><cell>0.94</cell><cell>0.94</cell></row><row><cell></cell><cell>gu</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>45</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell cols="2">0.918</cell><cell>0.9</cell><cell>0.909</cell></row><row><cell></cell><cell>hi</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>43</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell cols="2">0.956</cell><cell>0.86</cell><cell>0.905</cell></row><row><cell>Actual</cell><cell>kn ml mn</cell><cell>0 1 2</cell><cell>0 0 0</cell><cell>1 0 0</cell><cell>1 0 0</cell><cell>0 0 0</cell><cell>43 1 0</cell><cell>2 43 1</cell><cell>1 2 46</cell><cell>0 1 0</cell><cell>0 2 0</cell><cell>0 0 0</cell><cell>1 0 1</cell><cell>1 0 0</cell><cell cols="2">0.878 0.896 0.754</cell><cell>0.86 0.86 0.92</cell><cell>0.869 0.878 0.829</cell></row><row><cell></cell><cell>mr</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>43</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell cols="2">0.956</cell><cell>0.86</cell><cell>0.905</cell></row><row><cell></cell><cell>or</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>48</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell cols="2">0.941</cell><cell>0.96</cell><cell>0.95</cell></row><row><cell></cell><cell>rj</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>43</cell><cell>1</cell><cell>0</cell><cell>0.86</cell><cell></cell><cell>0.86</cell><cell>0.86</cell></row><row><cell></cell><cell>ta</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>40</cell><cell>0</cell><cell cols="2">0.784</cell><cell>0.8</cell><cell>0.792</cell></row><row><cell></cell><cell>te</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>43</cell><cell cols="2">0.935</cell><cell>0.86</cell><cell>0.896</cell></row><row><cell cols="2">Table A.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table B . 1</head><label>B1</label><figDesc>Confusion matrix of Manually Balancing the Samples for each category to 100 with CRNN</figDesc><table><row><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell>Predicted ml mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table B . 2</head><label>B2</label><figDesc>Confusion matrix of Manually Balancing the Samples for each category to 200 with CRNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table B . 3</head><label>B3</label><figDesc>Confusion matrix of Manually Balancing the Samples for each category to 571 with CRNN</figDesc><table><row><cell cols="7">C. CRNN with Attention architecture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell>Predicted ml mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table C . 1</head><label>C1</label><figDesc>Confusion matrix of Manually Balancing the Samples for each category to 100 with CRNN and Attention</figDesc><table><row><cell>as</cell><cell>bn</cell><cell>bd</cell><cell>gu</cell><cell>hi</cell><cell>kn</cell><cell>Predicted ml mn</cell><cell>mr</cell><cell>or</cell><cell>rj</cell><cell>ta</cell><cell>te</cell><cell>PPV</cell><cell>TPR</cell><cell>f1 Score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table C . 2</head><label>C2</label><figDesc>Confusion matrix of Manually Balancing the Samples for each category to 200 with CRNN and Attention</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table C . 3</head><label>C3</label><figDesc>Confusion matrix of Manually Balancing the Samples for each category to 571 with CRNN and Attention</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.mea.gov.in/Images/pdf1/Part17.pdf</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://www.iitm.ac.in/donlab/tts/database.php</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spoken indian language classification using artificial neural network -an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Aarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kopparapu</surname></persName>
		</author>
		<idno type="DOI">10.1109/SPIN.2017.8049987</idno>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Signal Processing and Integrated Networks (SPIN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="424" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, USENIX Association</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, USENIX Association<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language identification using deep convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<editor>El-Alfy, E.S.M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="880" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Approaching neural chinese word segmentation as a low-resource machine translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05348</idno>
		<ptr target="https://arxiv.org/abs/2008.05348" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASL.2010.2064307</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language recognition via i-vectors and dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2011-328</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="857" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doran</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<idno>doi:10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Solorio, T.</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Study on Spoken Language Identification Using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Draghici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abe?er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lukashevich</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411109.3411123</idno>
		<ptr target="https://doi.org/10.1145/3411109.3411123" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="253" to="256" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparison of approaches for modeling prosodic features in speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2010.5495632</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4414" to="4417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust language identification using convolutional neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Segbroeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2014-419</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1846" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic spoken language recognition with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gazeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Varol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology and Computer Science (IJITCS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Divide-and-Conquer Approach for Language Identification Based on Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Messaoudi</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-180</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3231" to="3235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Levenshtein transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/675f9820626f5bc0afb47b57890b466e-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="11179" to="11189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparative study on spoken language identification based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heracleous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yoneyama</surname></persName>
		</author>
		<idno type="DOI">10.23919/EUSIPCO.2018.8553347</idno>
	</analytic>
	<monogr>
		<title level="m">26th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2265" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fastai: A layered api for deep learning. Information 11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<idno type="DOI">10.3390/info11020108</idno>
		<ptr target="https://www.mdpi.com/2078-2489/11/2/108" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1853" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spoken language identification for native indian languages using deep learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Apte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Autonomous Systems</title>
		<editor>Chen, J.I.Z., Wang, H., Du, K.L., Suma, V.</editor>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="75" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1eA7AEtvS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic language identification using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2014.6854622</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5337" to="5341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An end-to-end approach to language identification in short utterances using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lozano-Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zazo-Candil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Toledano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2015-164</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="403" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language recognition in ivectors space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mat?jka</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2011-329</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="861" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spoken language recognition using cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shivam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIT48102.2019.00013</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Information Technology (ICIT)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning<address><addrLine>Omnipress, Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Duchesnay</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v12/pedregosa11a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BAT system description for NIST LRE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>F?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Novotn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pes?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<idno type="DOI">10.21437/Odyssey.2016-24</idno>
		<idno>doi:10.21437/Odyssey.2016-24</idno>
		<ptr target="https://doi.org/10.21437/Odyssey.2016-24" />
	</analytic>
	<monogr>
		<title level="m">Odyssey 2016: The Speaker and Language Recognition Workshop</title>
		<editor>Rodr?guez-Fuentes, L.J., Lleida, E.</editor>
		<meeting><address><addrLine>Bilbao, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-21" />
			<biblScope unit="page" from="166" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multiclass language identification using deep learning on spectral images of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Revay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teschke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04348</idno>
		<ptr target="http://arxiv.org/abs/1905.04348" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multiclass language identification using deep learning on spectral images of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Revay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teschke</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1905.04348</idno>
		<ptr target="https://arxiv.org/abs/1905.04348" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Performance Analysis of the 2017 NIST Language Recognition Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kheyrkhah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernandez-Cordero</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-69</idno>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1798" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ensemble learners for identification of spoken languages using mel frequency cepstral coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Sisodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nikhil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sathvik</surname></persName>
		</author>
		<idno type="DOI">10.1109/IDEA49133.2020.9170720</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Data, Engineering and Applications (IDEA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1803.09820</idno>
		<ptr target="https://arxiv.org/abs/1803.09820" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lessons on parameter sharing across layers in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiyono</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06022</idno>
		<ptr target="https://arxiv.org/abs/2104.06022" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic language identification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sangeetha</surname></persName>
		</author>
		<idno type="DOI">10.1109/CESYS.2018.8724070</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Communication and Electronics Systems (ICCES)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">LUKE: deep contextualized entity representations with entity-aware selfattention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
		<ptr target="doi:10.18653/v1/2020.emnlp-main.523" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Liu, Y.</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1174</idno>
		<ptr target="https://aclanthology.org/N16-1174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Language identification in short utterances using long short-term memory (lstm) recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lozano-Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toledano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0146917</idno>
		<idno>doi:10.1371/journal.pone.0146917</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0146917" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Comparison of four approaches to automatic language identification of telephone speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zissman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSA.1996.481450</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
