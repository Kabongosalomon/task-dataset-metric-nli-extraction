<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Cloudream Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Cloudream Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology * Corresponding</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
							<email>jiangbo.lu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Cloudream Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state -Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network (ConvNet) is first trained to predict HEMlets from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade method (about 20% on Human3.6M). The proposed method naturally supports training with "inthe-wild" images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation from a single image is an important problem in computer vision, because of its wide applications, e.g., video surveillance and human-computer interaction. Given an image containing a single person, 3D human pose inference aims to predict 3D coordinates of the human body joints. Recovering 3D information of human poses from a single image faces several challenges. The challenges are at least three folds: 1) reasoning 3D human poses from a single image is by itself very challenging due to the inherent ambiguities; 2) being a regression problem, existing approaches have not achieved a good balance between representation efficiency and learning effectiveness; 3) for "in-the-wild" images, both 3D capturing and manual labeling require a lot of efforts to obtain high-quality 3D annotations, making the training data extremely scarce.</p><p>For 2D human pose estimation, almost all best performing methods are detection based <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>. Detectionbased approaches essentially divide the joint localization task into local image classification tasks. The latter is easier to train, because it effectively reduces the feature and target dimensions for the learning system <ref type="bibr" target="#b27">[28]</ref>. Existing 3D pose estimation methods often use detection as an intermediate supervision mechanism as well. A straightforward strategy is to use volumetric heatmaps to represent the likelihood map of each 3D joint location <ref type="bibr" target="#b19">[20]</ref>. Sun et al. <ref type="bibr" target="#b27">[28]</ref> further proposed a differentiable soft-argmax operator that unifies the joint detection task and the regression task into an endto-end training framework. This significantly improves the state-of-the-art 3D pose estimation accuracy.</p><p>In this work, we propose a novel effective intermediate representation for 3D pose estimation -Part-Centric Heatmap Triplets (HEMlets) (as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>). The key idea is to polarize the 3D volumetric space around each distinct skeletal part, which has the two end-joints kinematically connected. Different from <ref type="bibr" target="#b18">[19]</ref>, our relative depth information is represented as three polarized heatmaps, corresponding to the different state of the local depth ordering of the part-centric joint pairs. Intuitively, HEMlets encodes the co-location likelihoods of pairwise joints in a dense perpixel manner with the coarsest discretization in the depth dimension. Instead of considering arbitrary joint pairs, we focus on kinematically connected ones as they possess semantic correspondence with the input image, and are thus a more effective target for the subsequent learning. In addition, the encoded relative depth information is strictly local for the part-centric joint pairs and suffers less from potential inconsistent data annotations.</p><p>The proposed network architecture is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. A ConvNet is first trained to learn the HEMlets and 2D joint heatmaps, which are then fed together with the high-level image features to another ConvNet to produce a volumetric heatmap for each joint. We leverage on the soft-argmax regression <ref type="bibr" target="#b27">[28]</ref> to obtain the final 3D coordinates of each joint. Significant improvements are achieved compared to the best competing methods quantitatively and qualitatively. Most notably, our HEMlets method achieves a record MPJPE of 39.9mm on Human3.6M <ref type="bibr" target="#b8">[9]</ref>, yielding about 20% improvement over the best-of-grade method <ref type="bibr" target="#b27">[28]</ref>.</p><p>The merits of the proposed method lie in three aspects:</p><p>? Learning strategy. Our method takes on a progressive learning strategy, and decomposes a challenging 3D learning task into a sequence of easier sub-tasks with mixed intermediate supervisions, i.e., 2D joint detection and HEMlets learning. HEMlets is the key bridging and learnable component leading to 3D heatmaps, and is much easier to train and less prone to overfitting. Its training can also take advantage of existing labeled datasets of relative depth ordering <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>? Representation power. HEMlets is based on 2D perjoint heatmaps, but extends them by a couple of additional heatmaps to encode local depth ordering in a dense per-pixel manner. It builds on top of 2D heatmaps but unleashes the representation power, while still allowing leveraging the ssoft-argmax regression <ref type="bibr" target="#b27">[28]</ref> for end-to-end learning.</p><p>? Simple yet effective. The proposed method features a simple network architecture design, and it is easy to train and implement. It achieves state-of-the-art 3D pose estimation results validated by the evaluations over all standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review the approaches that are based on deep ConvNets for 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Direct encoder-decoder</head><p>With the powerful feature extraction capability of deep ConvNets, many approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref> learn end-to-end Convolutional Neural Networks (CNNs) to infer human poses directly from the images. Li and Chen <ref type="bibr" target="#b11">[12]</ref> are the first who used CNNs to estimate 3D human pose via a multitask framework. Tekin et al. <ref type="bibr" target="#b28">[29]</ref> designed an auto encoder to model the joint dependencies in a high-dimensional feature space. Park et al. <ref type="bibr" target="#b17">[18]</ref> proposed fusing 2D joint locations with high-level image features to boost the estimation of 3D human pose. However, these single-stage methods are limited by the availability of 3D human pose datasets and cannot take advantage of large-scale 2D pose datasets that are vastly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transition with 2D joints</head><p>To avoid collecting 2D-3D paired data, a large number of works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref> decompose the task of 3D pose estimation into two independent stages: 1) firstly inferring 2D joint locations using well-studied 2D pose estimation methods, such as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b22">23]</ref>; 2) and then learning a mapping to lift them into the 3D space. These approaches mainly focus on tackling the second problem. For example, a simple fully connected residual network is proposed by Martinez et al. <ref type="bibr" target="#b12">[13]</ref> to directly recover 3D human pose from its 2D projection. Fang et al. <ref type="bibr" target="#b6">[7]</ref> considered prior knowledge of human body configurations and proposed human pose grammar, leading to better recovery of the 3D pose from only 2D joint locations. Yang et al. <ref type="bibr" target="#b34">[35]</ref> adopted an adversarial learning scheme to ensure the anthropometrical validity of the output pose and further improved the performance. Recently, by involving a reprojection mechanism, the proposed method in <ref type="bibr" target="#b32">[33]</ref> shows insensitivity to overfitting and accurately predicts the result from noisy 2D poses. Though promising results have been achieved by these twostage methods, a large gap exists between the 3D human pose and its 2D projections due to inherent ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">3D-aware intermediate states</head><p>To further bridge the gap between the 2D image and the target 3D human pose under estimation, some recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref> proposed to involve 3D-aware states for intermediate supervisions. Namely, a network is firstly trained to map the input image to these 3D-aware states, and then another network is trained to convert those states to the 3D joint locations. Finally, these two networks are combined and optimized jointly. A volumetric representation for 3D joint-heatmaps is proposed in <ref type="bibr" target="#b19">[20]</ref>, with which the 3D pose is regressed in a coarse-to-fine manner. However, regressing a probability grid in the 3D space globally is also a very challenging task. It usually suffers from quantization errors for the joint locations. To address this issue, Sun et al. <ref type="bibr" target="#b27">[28]</ref> exploited a soft-argmax operation and proposed an end-to-end training scheme for the 3D volumetric regression, achieving by far the best performance on 3D pose estimation. Inspired by <ref type="bibr" target="#b20">[21]</ref> that the relative depth ordering across joints is helpful for resolving pose ambiguities, Pavlakos et al. <ref type="bibr" target="#b18">[19]</ref> adopted a ranking loss for pairwise ordinal depth to train the 3D human pose predictor explicitly. A similar scheme of relative depth supervision is utilized in the work of <ref type="bibr" target="#b22">[23]</ref>. Forward-or-Backward Information (FBI), proposed in <ref type="bibr" target="#b24">[25]</ref>, is another kind of relative depth information but focuses more on the bone orientations.</p><p>In this work, we propose HEMlets, a novel representation that encodes both 2D joint locations and the part-centric relative depth ordering simultaneously. Experiments justify that this representation reaches by far the best balance between representation efficiency and learning effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">"In-the-wild" adaptation</head><p>All the aforementioned approaches are mainly trained on the datasets collected under indoor settings, due to the difficulty of annotating 3D joints for "in-the-wild" images <ref type="bibr" target="#b2">[3]</ref>. Thus, many strategies are developed to make domain adaptation. By exploiting graphics techniques, previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5]</ref> have synthesized a large "faked" dataset mimicking real images. Though these data benefit 3D pose estimation, they are still far from realistic, making the applicability limited. Recently, both Pavlakos et al. <ref type="bibr" target="#b18">[19]</ref> and Shi et al. <ref type="bibr" target="#b24">[25]</ref> proposed to label the relative depth relationship across joints instead of the exact 3D joint coordinates. This weak annotation scheme not only makes building largescale "in-the-wild" datasets feasible, but also provides 3Daware information for training the inference model in a weakly-supervised manner. With HEMlets representation, we can readily use these weakly annotated "in-the-wild" data for domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HEMlets Pose Estimation</head><p>We propose a unified representation of heatmap triplets to model the local information of body skeletal parts, i.e., kinematically connected joints, whereas the corresponding 2D image coordinates and relative depth ordering are considered. By such a representation, images annotated with relative depth ordering of skeletal parts can be treated equally with images annotated with 3D joint information. While the latter is usually very scarce, the former is relatively easy to obtain <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19]</ref>. In this section, we first present the proposed part-centric heatmap triplets and its encoding scheme. Then, we elaborate a simple network architecture that utilizes the part-centric heatmap triplets for 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Part-centric heatmap triplets</head><p>We divide the full body skeleton consisting of N = 18 joints into K = 14 parts as shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. Specifically, we use B to denote the set of skeletal parts, where B = {B 1 , B 2 , . . . , B K }. For each part, we denote the two associated joints as (p, c), with p being the parent node and c being the child node. The relative depth ordering, denoted as r(z p , z c ), can be then described as a tri-state function <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>:</p><formula xml:id="formula_0">r(z p , z c ) = ? ? ? ? ? 1 z p ? z c &gt; 0 |z p ? z c | &lt; ?1 z p ? z c &lt; ? ,<label>(1)</label></formula><p>where is used to adjust the sensitivity of the function to the relative depth difference. The absolute depths of the two joints p and c are denoted by z p and z c , respectively. We argue that directly using the discretized label as an intermediate state for learning the 3D pose from a 2D joint heatmap, as was done in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>, is not as effective. Since this abstraction tends to lose some important features encoded in the joints' spatial domain. Instead of elevating the problem straight away to the 3D volumetric space, we utilize an intermediate representation of the 3D-aware relationship of the parent joint p k and the child joint c k of a skeletal part B k . Provided with the supervision signals, we define polarized target heatmaps where a pair of normalized Gaussian peeks corresponding to the 2D joint locations are placed accordingly across three heatmaps (see <ref type="figure" target="#fig_1">Figure 2</ref>). We term them as the negative polarity heatmap T ?1 k , the zero polarity heatmap T 0 k and the positive polarity heatmap T +1 k with respect to the function value in Eq. (1). The parent joint p k is always placed in the zero polarity heatmap T 0 k . The child joint c k will appear in the negative/positive polarity heatmap, if its depth is larger/smaller than that of the parent joint p k (i.e., |r(z p , z c )| = 0). Both parent and child joints are co-located in the zero polarity heatmap if their depths are roughly the same (i.e., r(z p , z c ) = 0).</p><p>Formally, we denote the heatmap triplets of the skeletal part B k as the stacking of three heatmaps T ?1 k , T 0 k , T +1 k :</p><formula xml:id="formula_1">T k = Stack[T ?1 k , T 0 k , T +1 k ].<label>(2)</label></formula><p>Given 3D groundtruth coordinates of all joints, we can readily compute the heatmap triplets of each skeletal part. For easy reference, we shall refer to the part-centric heatmap triplets T k as HEMlets, and use it afterwards.</p><p>Discussions. Here we provide some understandings of HEMlets from a few perspectives. First, different from a joint-specific 2D heatmap that models the detection likelihood for each intended joint on the (x, y) plane, HEMlets models part-centric pairwise joints' co-location likelihoods on the (x, y) plane simultaneously with their ordinal depth relations. This helps to learn geometric constraints (e.g., bone lengths) implicitly. Second, by augmenting a 2D heatmap to a triplet of heatmaps, HEMlets learns and evaluates the co-location likelihood for a pair of connected joints (p, c) by the joint probability distribution P (x p , y p , x c , y c , r(z p , z c )) in a locally-defined volumetric space. In contrast, Pavlakos et al. <ref type="bibr" target="#b18">[19]</ref> relaxed the learning target and marginalized the 3D probability distributions independently for the (x, y) plane i.e., P (x p , y p ), P (x c , y c ) and the z-dimension, with the latter supervised independently by r(z p , z c ) based on a ranking loss. Third, by ex-ploiting the available supervision signals to a larger extent, HEMlets brings the benefit of making the knowledge more explicitly expressed and easier to learn, and bridges the gap in learning the 3D information from a given 2D image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D pose inference</head><p>Network architecture. We employ a fully convolutional network to predict the 3D human pose as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. A ResNet-50 <ref type="bibr" target="#b7">[8]</ref> backbone architecture is adopted for basic feature extraction. One of the two upsampling branches is used to learn the HEMlets and the 2D heatmaps of skeletal joints, and the other one is used to perform upsampling of the learned features to the same resolution as the output heatmaps. Both HEMlets and the 2D joint heatmaps are then encoded jointly by a 2D convolutional operation to form a latent global representation. Finally these global features are joined with the convolutional features extracted from the original image to predict a 3D feature map for each joint. We perform a soft-argmax operation <ref type="bibr" target="#b27">[28]</ref> to aggregate information in the 3D feature maps to obtain the 3D joint estimations.</p><p>HEMlets loss. Let us denote with T gt the groundtruth HEMlets of all skeletal parts and withT the corresponding prediction. We use a standard L 2 distance between T gt andT to compute the HEMlets loss as follows:</p><formula xml:id="formula_2">L HEM = (T gt ?T) ? 2 2 ,<label>(3)</label></formula><p>where denotes an element-wise multiplication, and ? is a binary tensor to mask out missing annotations.</p><p>Auxiliary 2D joint loss. As HEMlets essentially contains heatmap responses of 2D joint locations, we adopt a heatmap-based 2D joint detection scheme to facilitate HEMlets prediction. The L 2 loss of 2D joint prediction is computed as:</p><formula xml:id="formula_3">L 2D = N n=1 H gt n ?? n 2 2 ,<label>(4)</label></formula><p>where H gt n is the groundtruth 2D heatmap of the n-th 2D joint and? n is the corresponding network prediction.</p><p>Soft-argmax 3D joint loss. To avoid quantization errors and allow end-to-end learning, Sun et al. <ref type="bibr" target="#b27">[28]</ref> suggested a soft-argmax regression for 3D human pose estimation. Given learned volumetric features F n of size (h ? w ? d) for the n-th joint, the predicted 3D coordinates are given as:</p><formula xml:id="formula_4">[x n ,? n ,? n ] = v v ? Softmax(F n ),<label>(5)</label></formula><p>where v denotes a voxel in the volumetric feature space of F n . For robustness, we employ the L 1 loss for the regression of 3D joints. Specifically, the loss is defined as:</p><formula xml:id="formula_5">L 3D ? = N n=1 ( x gt n ?x n + y gt n ?? n + ? z gt n ?? n ),<label>(6)</label></formula><p>where the groundtruth 3D position of the n-th joint is given as (x gt n , y gt n , z gt n ). We use the same 2D and 3D mixed training strategy in <ref type="bibr" target="#b27">[28]</ref> (? ? {0, 1}): ? in Eq. (6) is set to 1 when the training data is from 3D datasets, and ? = 0 when the data is from 2D datasets.</p><p>Training strategy. For HEMlets prediction, We combine L HEM and L 2D for the intermediate supervision. The loss function is defined as:</p><formula xml:id="formula_6">L int = L HEM + L 2D .<label>(7)</label></formula><p>By using L HEM and L 2D jointly as supervisions, we allow training the network using images with 2D joint annotations and 3D joint annotations. By 3D joint annotation, we refer to annotations with exact 3D joint coordinates or relative depth ordering between part-centric joint pairs. The end-to-end training loss L tot is defined by combining L int with L 3D ? :</p><formula xml:id="formula_7">L tot = ? * L int + L 3D ? ,<label>(8)</label></formula><p>where ? = 0.05 in all our experiments.</p><p>Implementation details. We implement our method in PyTorch. The model is trained in an end-to-end manner using both images with 3D annotations (e.g., Human3.6M <ref type="bibr" target="#b8">[9]</ref> or HumanEva-I <ref type="bibr" target="#b25">[26]</ref>), and 2D annotations (MPII <ref type="bibr" target="#b0">[1]</ref>).</p><p>In our experiments, we adopt an adaptive value of in Eq. (1) for each skeletal part: k = 0.5 B k ( B k is the 3D Euclidean distance between the two end joints of the skeletal part B k ). The training data is further augmented with rotation (?30 ? ), scale (0.75?1.25), horizontal flipping (with a probability of 0.5) and color distortions. By using a batch size of 64, a learning rate of 0.001 and Adam optimization, the training took 100K iterations to converge. It took about a few days (2 ? 4) with four NVIDIA GTX 1080 GPUs to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform quantitative evaluation on three benchmark datasets: Human3.6M <ref type="bibr" target="#b8">[9]</ref>, HumanEva-I <ref type="bibr" target="#b25">[26]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b13">[14]</ref>. Ablation study is conducted to evaluate our design choices. We demonstrate that the proposed method shows superior generalization ability to in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation protocols</head><p>Human3.6M. Human3.6M <ref type="bibr" target="#b8">[9]</ref> contains 3.6 million RGB images captured by a MoCap System in an indoor environment, in which 7 professional actors were performing 15 activities such as walking, eating, sitting, making a phone call and engaging in a discussion, etc. We follow the standard protocol as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>, and use 5 subjects (S1, S5, S6, S7, S8) for training and the rest 2 subjects (S9, S11) for evaluation (referred to as Protocol #1). Some previous works reported their results with 6 subjects (S1, S5, S6, S7, S8, S9) used for training and only S11 for evaluation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref> (referred to as Protocol #2). Despite not using S9 as training data, we compare our results with these methods.</p><p>HumanEva-I. HumanEva-I <ref type="bibr" target="#b25">[26]</ref> is one of the early datasets for evaluating 3D human poses. It contains fewer subjects and actions compared to Human3.6M. Following <ref type="bibr" target="#b1">[2]</ref>, we train a single model on the training sequences of Subject 1, 2 and 3, and evaluate on the validation sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP</head><p>. This is a recent 3D human pose dataset which includes both indoor and outdoor scenes <ref type="bibr" target="#b13">[14]</ref>. Without using its training set, we evaluate our model trained from Human3.6M only on the test set. The results are reported using the 3DPCK and the AUC metric <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Evaluation metric. We follow the standard steps to align the 3D pose prediction with the groundtruth by aligning the position of the central hip joint, and use the Mean Per-Joint Position Error (MPJPE) between the groundtruth and the prediction as evaluation metrics. In some prior  <ref type="table">Table 1</ref>. Quantitative comparisons of the mean per-joint position error (MPJPE) on Human3.6M <ref type="bibr" target="#b8">[9]</ref> under Protocol #1 and Protocol #2, as well as using PA MPJPE as the evaluation metric. Similar to most of the competing methods (e.g., <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7]</ref>), our models were trained on the Human3.6M dataset and used also the extra MPII 2D pose dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref>, the pose prediction was further aligned with the groundtruth via a rigid transformation. The resulting MPJPE is termed as Procrustes Aligned (PA) MPJPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and comparisons</head><p>Human3.6M. We compare our method against state-ofthe-art under three protocols, and the quantitative results are reported in <ref type="table">Table 1</ref>. As can be seen, our method outperforms all competing methods on all action subjects for the protocols used. It is worth mentioning that our approach makes considerable improvements on some challenging actions for 3D pose estimation such as Sitting and Sitting Down. Thanks to HEMlets learning, our method demonstrates a clear advantage for handling complicated poses.</p><p>With a simple network architecture and little parameter tuning, we produce the most competitive results compared to previous works with carefully designed networks powered by e.g., adversarial training schemes or prior knowledge. On average, we improve the 3D pose prediction accuracy by 20% than that reported in Sun et al. <ref type="bibr" target="#b27">[28]</ref> under Protocol #1. We also report our performance using PA MPJPE as the evaluation metric, and compare with these methods that make use of S9 as additional training data. We still outperform all of them across all action subjects, even without utilizing S9 for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Walking Jogging S1 S2 S3 S1 S2 S3 Avg Simo-Serra et al. <ref type="bibr" target="#b26">[27]</ref>  HumanEva-I. With the same network architecture where only the HumanEva-I dataset is used for training, our results are reported in <ref type="table" target="#tab_1">Table 2</ref> under the popular protocol <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref>. Different from these approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref> which used extra 2D datasets (e.g., MPII) or pre-trained 2D detectors (e.g., CPM <ref type="bibr" target="#b33">[34]</ref>), our method still outperforms previous approaches.</p><p>MPI-INF-3DHP. We evaluate our method on the MPI-INF-3DHP dataset using two metrics, the PCK and AUC. The results are generated by the model we trained for Hu-man3.6M. In <ref type="table">Table 3</ref>, we compare with three recent methods which are not trained on this dataset. Our result of "Studio GS" is one percentage lower than <ref type="bibr" target="#b18">[19]</ref>. But our method outperforms all these methods with particularly large mar- gins for the "Outdoor" and "Studio no GS" sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>We study the influence on the final estimation performance of different choices made in our network design and the training procedure.</p><p>Alternative intermediate supervision. First, We examine the effectiveness of using HEMlets supervision. We evaluate the model trained without any intermediate supervision (Baseline), with 2D heatmap supervision only, with HEMlets supervision only, and with both 2D heatmap supervision and HEMlets supervision (Full). All of these design variants are evaluated with the same experimental setting (including training data, network architecture and L 3D ? loss definition) under Protocol #1 on Human3.6M.</p><p>The detailed results are presented in <ref type="table">Table 4</ref>. Using 2D heatmaps supervision for training, the prediction error is reduced by 3.0mm compared to the baseline. The HEMlets supervision provided 1.7mm lower mean error compared to the 2D heatmaps supervision. This validates the effectiveness of the intermediate supervision. By combining all these choices, our approach using HEMlets with 2D heatmap supervision achieves the lowest error. Without using the extra MPII 2D pose dataset, we repeated this study. Similar conclusions can still be drawn. But the gap between w/ HEMlets (excluding L 2D , 46.0mm) and Full (45.1mm) shrinks, suggesting the strength of the HEMlets representation in encoding both 2D and (local) 3D information.</p><p>To further illustrate the effectiveness of HEMlets representation, we provide a visual comparison in <ref type="figure" target="#fig_3">Fig. 4</ref>. Though the 2D joint errors of the two estimations are quite close, the method with HEMlets learning significantly improves the 3D joint estimation result and fixes the gross limb errors.</p><p>Regarding the runtime, tested on a NVIDIA GTX 1080 GPU, our full model (with a total parameter number of 47.7M) takes 13.3ms for a single forward inference, while the baseline model (with 34.3M parameters) takes 8.5ms.</p><p>Variants of HEMlets. We next experimented with some variants of HEMlets on Human3.6M and MPII 2D pose datasets. In the first variant, we use five-state heatmaps, referred to as 5s-HEM, where the child joint is placed to different layers of the heatmaps according to the angle of the associated skeletal part with respect to the imaging plane. Specifically, we define the five states corresponding to the (?90 ? , ?60 ? ), (?60 ? , ?30 ? ), (?30 ? , 30 ? ), (30 ? , 60 ? ) and (60 ? , 90 ? ) range, respectively. In the second variant, we place a pair of joints in the negative and positive polarity heatmaps respectively according to their depth ordering (i.e., the closer/farther joint will appear in the positive/negative polarity heatmap. If their depths are roughly the same, they are co-located in the zero polarity heatmap. We refer to this variant as 2s-HEM. We trained 5s-HEM, 2s-HEM and HEMlets with the Human3.6M dataset only. A comparison on the validation loss is given in <ref type="figure" target="#fig_4">Fig. 6</ref>. The other two variants produce inferior convergence compared to HEMlets under the same experiment setting.</p><p>Augmenting datasets. Many state-of-the-art approaches use a mixed training strategy for 3D human pose estimation. In addition to exploiting Human3.6M and MPII datasets, we study the effect of using augmenting datasets such as Ordinal <ref type="bibr" target="#b18">[19]</ref> and FBI <ref type="bibr" target="#b24">[25]</ref> for training. Firstly, we adapt the annotations of Ordinal and FBI datasets to the required form of HEMlets. Then we train our model using different combinations of these additional datasets. The comparisons on the MPI-INF-3DHP dataset <ref type="bibr" target="#b13">[14]</ref> are reported in <ref type="table">Table 5</ref>. We find augmenting datasets slightly increase the 3DPCK score for the trained model. Interestingly, training with FBI annotations attains a better 3DPCK score than Ordinal annotations. We suspect this is due to the amount of manual annotation errors related to different annotation schemes. <ref type="figure">Figure 5</ref>. Qualitative results on different validation datasets: the first two columns are from the test dataset of 3DHP <ref type="bibr" target="#b13">[14]</ref>. The other columns are from Leeds Sports Pose (LSP) <ref type="bibr" target="#b9">[10]</ref>. Our approach produces visually correct results even on challenging poses (last column). Generalization. For an evaluation of in-the-wild images from Leeds Sports Pose (LSP) <ref type="bibr" target="#b9">[10]</ref> and the validation set of MPI-INF-3DHP <ref type="bibr" target="#b13">[14]</ref>, we list some visual results predicted by our approach. As shown in <ref type="figure">Fig. 5</ref>, even for challenging data (e.g., self-occlusion, upside-down), our method yields visually correct pose estimations for these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a simple and highly effective HEMlets-based 3D pose estimation method from a single color image. HEMlets is an easy-to-learn intermediate representation encoding the relative forward-or-backward depth relation for each skeletal part's joints, together with their spatial co-location likelihoods. It is proved very helpful to bridge the input 2D image and the output 3D pose Dataset 3DPCK Base 75.3 w/ Ordinal <ref type="bibr" target="#b18">[19]</ref> 76.1 w/ FBI <ref type="bibr" target="#b24">[25]</ref> 76.9 w/ FBI [25] + Ordinal <ref type="bibr" target="#b18">[19]</ref> 76.5 <ref type="table">Table 5</ref>. Evaluation of 3DPCK scores by adding different augmenting datasets that provide relative depth ordering annotations. Base denotes using the base datasets (Human3.6M and MPII).</p><p>in the learning procedure. We demonstrated the effectiveness of the proposed method tested over the standard benchmarks, yielding a relative accuracy improvement of about 20% over the best-of-grade method on the Human3.6M benchmark. Good generalization ability is also witnessed for the presented approach. We believe the proposed HEMlets idea is actually general, which may potentially benefit other 3D regression problems e.g., scene depth estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the HEMlets-based 3D pose estimation. (a) Input RGB image. Our algorithm encodes (b) the 2D locations for the joints p and c, but also (c) their relative depth relationship for each skeletal part pc into HEMlets. (d) Output 3D human pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Part-centric heatmap triplets {T ?1 k , T 0 k , T +1 k } where p and c are the parent joint and the child joint. (a, b) Joints and skeletal parts. We locate the parent joint p of the k-th skeletal part B k at the zero polarity heatmap T 0 k (c-e). The child joint c is located, according to relative depth of p and c, in the positive (c), zero (d) and negative polarity heatmap (e), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The network architecture of our proposed approach. It consists of four major modules: (a) A ResNet-50 backbone for image feature extraction. (b) A ConvNet for image feature upsampling. (c) Another ConvNet for HEMlets learning and 2D joint detection. (d) A 3D pose regression module adopting a soft-argmax operation for 3D human pose estimation. (e) Details of the HEMlets learning module. "Feature concatenate" denotes concatenating the feature maps from the HEMlets learning branch and the upsampling branch together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>An example image with the detected joints overlaid and shown from a novel view, using different methods: (a) L 3D ? + L 2D (2D error: 15.2; 3D joint error: 81.3mm). (b) L 3D ? + L 2D + L HEM (2D error: 13.0; 3D error: 41.2mm). (c) Ground-truth. HEMlets learning helps fixing local part errors, see blue in (a) vs. red in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The validation loss of 5s-HEM, 2s-HEM and HEMlets, respectively. All are trained with the Human3.6M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Direct Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg LinKDE et al. [9] 132.7 183.6 132.3 164.4 162.1 205.9 150.6 171.3 151.6 243.0 162.1 170.7 177.1 96.6 127.9 162.1 Tome et al.. [31] 65.0 73.5 76.8 86.4 86.3 110.7 68.9 74.8 110.2 173.9 85.0 85.8 86.3 71.4 73.1 88.4 Rogez et al. [22] 76.2 80.2 75.8 83.3 92.2 105.7 79.0 71.7 105.9 127.1 88.0 83.7 86.6 64.9 84.0 87.7 Tekin et al.</figDesc><table><row><cell>Protocol #1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[30]</cell><cell>54.2 61.4</cell><cell>60.2 61.2 79.4 78.3 63.1 81.6 70.1</cell><cell>107.3</cell><cell>69.3 70.3 74.3 51.8 74.3 69.7</cell></row><row><cell cols="2">Martinez et al. [13] 53.3 60.8</cell><cell>62.9 62.7 86.4 82.4 57.8 58.7 81.9</cell><cell>99.8</cell><cell>69.1 63.9 67.1 50.9 54.8 67.5</cell></row><row><cell>Fang et al. [7]</cell><cell>50.1 54.3</cell><cell>57.0 57.1 66.6 73.3 53.4 55.7 72.8</cell><cell>88.6</cell><cell>60.3 57.7 62.7 47.5 50.6 60.4</cell></row><row><cell cols="2">Pavlakos et al. [19] 48.5 54.4</cell><cell>54.4 52.0 59.4 65.3 49.9 52.9 65.8</cell><cell>71.1</cell><cell>56.6 52.9 60.9 44.7 47.8 56.2</cell></row><row><cell>S?r?ndi et al. [24]</cell><cell>51.2 58.7</cell><cell>51.7 53.4 56.8 59.3 50.7 52.6 65.5</cell><cell>73.2</cell><cell>56.8 51.4 56.6 47.0 42.4 55.8</cell></row><row><cell>Sun et al. [28]</cell><cell>47.5 47.7</cell><cell>49.5 50.2 51.4 55.8 43.8 46.4 58.9</cell><cell>65.7</cell><cell>49.4 47.8 49.0 38.9 43.8 49.6</cell></row><row><cell>Ours</cell><cell>34.4 42.4</cell><cell>36.6 42.1 38.2 39.8 34.7 40.2 45.6</cell><cell>60.8</cell><cell>39.0 42.6 42.0 29.8 31.7 39.9</cell></row><row><cell>Protocol #2</cell><cell cols="4">Direct Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Nie et al. [17]</cell><cell>90.1 88.2</cell><cell>85.7 95.6 103.9 92.4 90.4 117.9 136.4</cell><cell>98.5</cell><cell>103.0 94.4 86.0 90.6 89.5 97.5</cell></row><row><cell>Chen et al.. [4]</cell><cell>53.3 46.8</cell><cell>58.6 61.2 56.0 58.1 41.4 48.9 55.6</cell><cell>73.4</cell><cell>60.3 45.0 76.1 62.2 51.1 57.5</cell></row><row><cell cols="2">Martinez et al. [13] 39.5 43.2</cell><cell>46.4 47.0 51.0 56.0 41.4 40.6 56.5</cell><cell>69.4</cell><cell>49.2 45.0 49.5 38.0 43.1 47.7</cell></row><row><cell>Fang et al. [7]</cell><cell>38.2 41.7</cell><cell>43.7 44.9 48.5 55.3 40.2 38.2 54.5</cell><cell>64.4</cell><cell>47.2 44.3 47.3 36.7 41.7 45.7</cell></row><row><cell cols="2">Pavlakos et al.. [19] 34.7 39.8</cell><cell>41.8 38.6 42.5 47.5 38.0 36.6 50.7</cell><cell>56.8</cell><cell>42.6 39.6 43.9 32.1 36.5 41.8</cell></row><row><cell>Yang et al. [35]</cell><cell>26.9 30.9</cell><cell>36.3 39.9 43.9 47.4 28.8 29.4 36.9</cell><cell>58.4</cell><cell>41.5 30.5 29.5 42.5 32.2 37.7</cell></row><row><cell>Ours</cell><cell>29.1 34.9</cell><cell>29.9 32.6 31.2 32.3 27.0 33.3 37.6</cell><cell>45.9</cell><cell>32.2 31.5 34.5 22.9 25.9 32.1</cell></row><row><cell>PA MPJPE</cell><cell cols="4">Direct Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Yasin et al. [36]</cell><cell cols="4">88.4 72.5 108.5 110.2 97.1 81.6 107.2 119.0 170.8 108.2 142.5 86.9 92.1 165.7 102.0 108.3</cell></row><row><cell>Sun et al.. [28]</cell><cell>36.9 36.2</cell><cell>40.6 40.4 41.9 34.9 35.7 50.1 59.4</cell><cell>40.4</cell><cell>44.9 39.0 30.8 39.8 36.7 40.6</cell></row><row><cell>Dabral et al. [6]</cell><cell>28.0 30.7</cell><cell>39.1 34.4 37.1 44.8 28.9 32.2 39.3</cell><cell>60.6</cell><cell>39.3 31.1 37.8 25.3 28.4 36.3</cell></row><row><cell>Ours</cell><cell>21.6 27.0</cell><cell>29.7 28.3 27.3 32.1 23.5 30.3 30.0</cell><cell>37.7</cell><cell>30.1 25.3 34.2 19.2 23.2 27.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Detailed results on the validation set of HumanEva-I<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell></cell><cell>65.1 48.6 73.5 74.2 46.6 32.2 56.7</cell></row><row><cell cols="2">Moreno-Noguer et al. [15] 19.7 13.0 24.9 39.7 20.0 21.0 26.9</cell></row><row><cell>Martinez et al. [13]</cell><cell>19.7 17.4 46.8 26.9 18.2 18.6 24.6</cell></row><row><cell>Fang et al. [7]</cell><cell>19.4 16.8 37.4 30.4 17.6 16.3 22.9</cell></row><row><cell>Pavlakos et al. [19]</cell><cell>18.8 12.7 29.2 23.5 15.4 14.5 18.3</cell></row><row><cell>Ours</cell><cell>13.5 9.9 17.1 24.5 14.8 14.4 15.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements. This work is supported in part by</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2385" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3467" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihye</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3d human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Matteo Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How robust is 3d human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Robotic Co-workers 4.0</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09241</idno>
		<title level="m">FBI-pose: Towards bridging the gap between 2d images and 3d human poses using forwardor-backward information</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hu-manEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A joint model for 2d and 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carme</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3634" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3941" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2500" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4627" to="4635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
