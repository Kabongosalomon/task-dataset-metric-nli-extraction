<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhua</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Intelligent Machines</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfu</forename><surname>Wang</surname></persName>
							<email>zfwang@ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Intelligent Machines</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Bottom-up</term>
					<term>Pose estimation</term>
					<term>Guiding Offset</term>
					<term>Heatmap</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple yet reliable bottom-up approach with a good trade-off between accuracy and efficiency for the problem of multi-person pose estimation. Given an image, we employ an Hourglass Network to infer all the keypoints from different persons indiscriminately as well as the guiding offsets connecting the adjacent keypoints belonging to the same persons. Then, we greedily group the candidate keypoints into multiple human poses (if any), utilizing the predicted guiding offsets. And we refer to this process as greedy offset-guided keypoint grouping (GOG). Moreover, we revisit the encodingdecoding method for the multi-person keypoint coordinates and reveal some important facts affecting accuracy. Experiments have demonstrated the obvious performance improvements brought by the introduced components. Our approach is comparable to the state of the art on the challenging COCO dataset under fair conditions. The source code and our pretrained model are publicly available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The problem of multi-person pose estimation is to localize the 2D skeleton keypoints (body joints) for all persons given a single image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. It is a fundamental task with many applications. Some recent work can address the problem of single-person pose estimation extremely well thanks to the development of convolutional neural networks (CNNs) specially designed for human pose estimation such as Hourglass Network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref> and HRNet <ref type="bibr" target="#b5">[6]</ref>. But the challenge becomes much more difficult when multiple persons appear in the scene at the same time. And it has not been solved well so far considering accuracy, speed and simplicity.</p><p>Approach taxonomy. Existing approaches for this problem can be roughly divided into two categories: top-down and bottom-up. Some recent top-down approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref> have achieved high accuracy. However, most of them have low prediction efficiency especially when many people exist 1 https://github.com/hellojialee/OffsetGuided in the scene, for they must rely on an advanced person detector to detect all the persons and estimate all the singleperson poses one by one. The bottom-up approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref> instead infer the keypoint positions and corresponding grouping cues of all persons indiscriminately using a feed-forward network (here, we mainly discuss CNN-based approaches) and then group the detected keypoints into individual human poses. The run time of the grouping process may be very fast and nearly constant regardless of the person number in the image. Therefore, developing bottom-up approaches is challenging but attractive for a better trade-off between accuracy and efficiency. In this paper, we mainly focus on the bottom-up approaches.</p><p>Keypoint coordinate encoding. Here, we only review the most important work. With the arrival of the deep learning era, DeepPose <ref type="bibr" target="#b14">[15]</ref> for the first time uses CNNs to regress the Cartesian coordinates of a fixed number of person keypoints directly. As a result, this approach cannot handle the situation of multiple persons. PersonLab <ref type="bibr" target="#b1">[2]</ref>, CenterNet <ref type="bibr" target="#b15">[16]</ref> and PifPaf <ref type="bibr" target="#b12">[13]</ref> decompose the task of keypoint localization into two subproblems at each position: binary classification to discriminate whether this current pixel is a keypoint, and offset regression to the ground-truth keypoint position. However, special techniques are essential to make their approaches work well, e.g., leveraging focal loss <ref type="bibr" target="#b16">[17]</ref> for the classification task and Laplace-based L1 loss <ref type="bibr" target="#b17">[18]</ref> for the offset regression task <ref type="bibr" target="#b12">[13]</ref>. By contrast, Tompson et al. <ref type="bibr" target="#b18">[19]</ref> and many later researchers use CNNs to predict the Gaussian response heatmaps of person keypoints and then obtain the keypoint coordinates by finding the local maximum responses in the heatmaps. But high-res feature maps are required to alleviate the precision decline of keypoint localization.</p><p>Keypoint grouping encoding. The encoding of keypoint grouping (or association) information is critical for the post-processing in bottom-up approaches. Here, we conclude the existing approaches for keypoint grouping into two categories: global grouping and greedy grouping. Prior work, such as DeeperCut <ref type="bibr" target="#b19">[20]</ref>, Associative Embedding <ref type="bibr" target="#b11">[12]</ref> and CenterNet <ref type="bibr" target="#b15">[16]</ref>, presents different approaches to encode the global keypoint grouping information. For instance, Associative Embedding employs Hourglass Networks <ref type="bibr" target="#b3">[4]</ref> to infer the identity tags for all candidate keypoints. These tags are grouping cues to cluster keypoints into individual human poses. And CenterNet <ref type="bibr" target="#b15">[16]</ref> proposes center offsets away from person centers as keypoint grouping cues. By contrast, Part Affinity Fields <ref type="bibr" target="#b10">[11]</ref>, PersonLab <ref type="bibr" target="#b1">[2]</ref> and PIFPAF <ref type="bibr" target="#b12">[13]</ref> are greedy grouping approaches, associating adjacent nodes in a human skeleton tree independently rather than finding the global solution of a global graph matching problem.</p><p>Some state-of-the-art (SOTA) bottom-up approaches are remarkable in estimation accuracy but have so complicated structure and many hyper-parameters affecting results that we cannot clearly figure out the contributions of introduced components or compare these approaches equally. In this work, we propose a bottom-up approach, which is easy to follow and works well in terms of accuracy, speed and clarity. We use Hourglass-104 <ref type="bibr" target="#b4">[5]</ref> as the inference model and select Cen-terNet <ref type="bibr" target="#b15">[16]</ref> as the baseline approach. The main contributions of this paper are as follows:</p><p>(1) We present a greedy keypoint grouping method, which we refer to as greedy offset-guided keypoint grouping (GOG). The adjacent keypoints of each person are connected by guiding offsets. The proposed GOG algorithm is robust and fast.</p><p>(2) We follow and improve the existing Gaussian heatmap encoding-decoding method for keypoint coordinates of the multi-person poses, ensuring precise keypoint localization.</p><p>(3) Our preliminary system has obtained obvious performance increases compared with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminary</head><p>Keypoint grouping information is essential for multi-person pose estimation. Here, we propose a novel grouping encoding for keypoints as the form of guiding offsets to greedily "connect" the adjacent keypoints belonging to the same persons. A guiding offset illustrated in <ref type="figure" target="#fig_0">Fig.1</ref> is the displacement vector starting at a keypoint J f rom of a person and pointing to his/her next keypoint J to . An overview of our approach is shown in <ref type="figure" target="#fig_1">Fig.2</ref>, which is mainly inspired by Simple Pose <ref type="bibr" target="#b2">[3]</ref> and CenterNet <ref type="bibr" target="#b15">[16]</ref> but different from them or other bottomup approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref> based on offsets.</p><p>PersonLab <ref type="bibr" target="#b1">[2]</ref> and PIFPAF <ref type="bibr" target="#b12">[13]</ref> detect an arbitrary root keypoint of a person and use bidirectional offsets to generate a tree-structured kinematic graph of that person. Merely the root keypoint coordinate is obtained in the confidence maps of classification while the subsequent keypoint coordinate is regressed directly on the basis of the offset fields and the former keypoint location. Abundant root keypoints are detected as the seeds of the kinematic graphs to ensure they can detect as many human poses as possible. Hence, they have to apply non-maximum suppression (NMS) to remove redundant person poses. CenterNet <ref type="bibr" target="#b15">[16]</ref> detects the center point of each person as the root "node" and localizes keypoints using classification heatmaps and refinement offsets. CenterNet only as- sociates the center point with the keypoints which are within the same person bounding box using center offsets.</p><formula xml:id="formula_0">J guid ? X Y J from J to Guiding Offset L (a) (b)</formula><p>However, both precise offset regression to keypiont locations and localization of person center points are vague visual tasks. Previous CNN-based work such as DeepPose <ref type="bibr" target="#b14">[15]</ref> and Tompson et al. <ref type="bibr" target="#b18">[19]</ref> has already suggested that inferring keypiont heatmaps is easier than inferring keypoint Cartesian coordinates. The vague ground-truth keypoint of a person is different from his/her bounding box or segmentation boundary with clear visual concepts. As a result, we localize all keypoints using Gaussian response heatmaps. As for keypoint grouping, we infer a set of guiding offsets at the positions close to every keypoint pointing to an adjacent keypoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Definition of Gaussian Responses</head><p>We use the most popular coordinate encoding-decoding method for keypoint localization, i.e., Gaussian response heatmaps. Assuming I ? R W ?H?3 is the input image of width W and height H, the ground-truth keypoint heatmap for the network output is G * ? [0, 1] W R ? H R ?C , in which R is the output stride and C is the number of keypoint types. In our approach, R = 4 for Hourglass Networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref> and C = 17 for COCO dataset. Considering a pixel value at g(u, v) in the c-th ground-truth keypoint heatmap R W R ? H R , before generating the ground truth at g, we map g to the positiong(x, y) in the input image space R W ?H?C using this transformation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>: g(x, y) = g(u, v)?R+R/2?0.5. If the ground-truth keypoint of type c at the position p ? R W ?H?C is the nearest keypoint away fromg, we generate the ground-truth Gaussian response value at g as:</p><formula xml:id="formula_1">G * (g(u, v), ? k ) = exp(? (gx?px) 2 +(gy?py) 2 2? 2 k )</formula><p>, where ? k is fixed for different keypoint types for simplicity. The coordinate transformation from g tog is critical for keypoint localization precision because we hold the perspective everywhere in this work that each pixel in the image occupies a 1 ? 1 "area" and that pixel value lies exactly at the center of the pixel cell. During decoding (testing), we correspondingly upsample the output heatmaps of the network R times using bicubic interpolation and then find the local maximums in heatmaps as detected keypoints. It should be notated that the groundtruth keypoint coordinates are integers in the COCO dataset. And in this case, original keypoint locations can be restored in this way. On the contrary, the theoretical error keypoint localization is inevitable when we use bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Definition of Guiding Offsets</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig.1 (a)</ref>, the ground-truth guiding offset originated from keypoint J f rom to kyepoint J to is a displacement vector calculated as {J to (x) ? J f rom (x), J to (y) ? J f rom (y)}. We place a set of ground-truth guiding offsets around the "start" keypoint J f rom . A human pose skeleton with 19 "limbs" is defined same as in PIFPAF <ref type="bibr" target="#b12">[13]</ref>. A limb in the human skeleton is a "connection" between two adjacent keypoints. Consequently, we use 19 types of guiding offsets to associate keypoints unless mentioned otherwise.</p><p>During decoding, we only use the guiding offsets predicted at the local maximum positions in the keypoint heatmaps as the grouping cues. Supposing we have two candidate keypoints in <ref type="figure" target="#fig_0">Fig.1 (a)</ref> to be paired and they have Gaussian response values J f rom (s) and J to (s) respectively. We measure the connection score between them as:</p><formula xml:id="formula_2">S(J f rom , J to ) = J f rom (s) ? J to (s) ? exp(? ?l L ),</formula><p>where L is the length between J f rom and J to , ?l is the "error" distance between J guid and J to . Here, we refer to the connection of the paired keypoints with high connection confidence as a candidate "limb".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Greedy Keypoint Grouping</head><p>We have already described the guiding offsets and the human pose skeleton. Assuming we have detected a keypoint J f rom and a keypoint J to of the adjacent types in <ref type="figure" target="#fig_0">Fig.1 (a)</ref>, they are paired and connected as a limb if the connection score S(J f rom , J to ) is above a threshold. The adjacent keypoints in the human skeleton are paired and measured independently. Greedy offset-guided keypoint grouping (GOG) is the process to assemble the collected limbs, i.e., the adjacent keypoint connections, into individual human skeletons as shown at the Bottom of <ref type="figure" target="#fig_1">Fig.2</ref>. Our GOG algorithm is inspired by Simple Pose <ref type="bibr" target="#b2">[3]</ref> and CenterNet <ref type="bibr" target="#b15">[16]</ref>, which is greedy and pretty fast. Note that one keypoint can not be connected to two or more keypoints of the same type. The details can be found in our source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Network Structure and Losses</head><p>Following CenterNet <ref type="bibr" target="#b15">[16]</ref>, we employ Hourglass-104 as the backbone network that consists of two stacked hourglass modules <ref type="bibr" target="#b3">[4]</ref> and has the depth of 104. We only add 1 ? 1 convolutional layers into the backbone to regress the desired <ref type="table">Table 1</ref>. Results of accuracy and speed on the COCO validation set. We use single-scale inference and the same computer with a 2080 Ti GPU to test speed equally. The faster speed in parenthesis is obtained using AMP FP16 inference. outputs. The focal L 2 loss <ref type="bibr" target="#b2">[3]</ref> is applied to supervise the regression of keypoint heatmaps while only the guiding offsets close to "start" keypoints are supervised by the L 1 loss and normalized by individual scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>Implementation details. Our system is implemented using Python and Pytorch. We present experiments on the COCO dataset. It consists of the training set, the test-dev set, and the validation set. Our models are trained on the training set and evaluated on the validation set (excluding images without people) and test-dev set. We set ? k of the Gaussian response kernel to 7. The supervision area of the guiding offsets around the "start" keypoint is set to 7 ? 7. The training images are cropped to 512 ? 512 patches with random scaling, rotation, flipping and translation. We fine-tune the pre-trained Hourglass-104 in <ref type="bibr" target="#b15">[16]</ref> using the Adam optimizer with the batch size of 32, the initial learning rate of 1e-3 for less than 150 epochs. For evaluation we use single-scale inference, keeping the same as CenterNet <ref type="bibr" target="#b15">[16]</ref>, PersonLab <ref type="bibr" target="#b1">[2]</ref>, and PIFPAF <ref type="bibr" target="#b12">[13]</ref> for fair comparisons. Ablation studies. It is essential to learn the upper limit of our system. We replace the network outputs with the groundtruth values and obtain the theoretical upper bound: 86.6% AP on the COCO validation set. If we define more limbs in the human skeleton, let us say 44 limbs, to connect adjacent keypoints, the upper bound reaches 91.0% AP. It further increases to 95.5% AP when we set the output stride R to 1.</p><p>Please refer to <ref type="table">Table 1</ref>, the baselin CenterNet and all our results are based on Hourglass-104 with single-scale inference. The long side of the input image is resized to a fixed number. Our approach (Ours 2) significantly exceeds the baseline (+2.1% AP) on the COCO validation set under equal conditions. And the enhanced result, Ours 3 with more limbs and a larger input, achieves 67.5% AP as well as good speed. The results in <ref type="table">Table 1</ref> suggest that our approach has obtained a satisfying trade-off between accuracy and speed.</p><p>Keypoint coordinate encoding-decoding. We validate different encoding-decoding methods for multi-person key-   <ref type="table" target="#tab_1">Table 2</ref>, in which, "qnt" represents quantifying the numerical keypoint to an integer coordinate before generating the Gaussian distribution, "ro" is short for refinement offsets improving localization precision. Here, flip augmentation is not used. DARK <ref type="bibr" target="#b6">[7]</ref> works pretty well in some top-down approaches, and we have modified and applied it to our system. Experiments indicate that our encoding-decoding method is best in average precision. DARK assumes that the predicted keypoint responses obey a Gaussian distribution, but the network outputs do not meet this assumption in bottom-up approaches, in which inferred Gaussian responses are often far from ground-truth values. Refinement offsets failed in improving our result (?0.3% AP), suggesting that naive offset regression to precise keypoint locations is not accurate enough. As expected in Section 2.2, using bilinear interpolation in heatmap upsampling leads to a large accuracy drop (?2.1% AP).</p><p>Results. We compare our approach with the reproducible SOTA bottom-up approaches in <ref type="table" target="#tab_2">Table 3</ref>. Though our results are not always the best, our preliminary system has advantages taking into account accuracy, speed, and simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We have presented a bottom-up approach for the problem of multi-person pose estimation, which is simple enough yet achieves a respectable trade-off between accuracy and efficiency. Specifically, we have revisited and improved the encoding-decoding method for multi-person keypoint coordinates in heatmaps, leading to more precise localization. And we have proposed the guiding offsets between adjacent keypoints as the keypoint grouping cues and assemble human skeletons greedily. Experiments on the challenging COCO dataset have demonstrated the advantages and much room for accuracy improvement of our preliminary system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Definition of guiding offsets. (a) The ground-truth guiding offset placed at the position of keypoint J f rom points to its adjacent keypoint J to . Actually, J f rom and J to are in different heatmap channels, the figure is just for description. In practice, the inferred guiding offset points to the floatingpoint position J guid . (b) The inferred guiding offsets at the area around the "right hip" keypoints. They guide to the corresponding "right ankle" keypoints of the same individuals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Responses of "left shoulder" (b) Responses of "left hip" (c) Guiding offsets from "left shoulder" to "left hip" (d) Candidate keypoints and limbs (f) Final result (e) Greedy keypoint grouping Overview of the proposed approach. Top-left, top-middle and top-right: we infer the keypoints and the guiding offsets simultaneously using Hourglass-104<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref>. There are 17 types of keypoint heatmaps and 19 types of guiding-offset feature maps. Bottom-left: next, the top k (k = 32 in all our experiments) scoring candidate keypoints and limbs (connections of paired keypoints) of each type and with high confidence are collected. Bottom-middle: subsequently, they are grouped into individual human poses greedily using our GOG algorithm. Bottom-right: finally we measure the confidence of each human pose by averaging the response values of its keypoints and filter out the human poses with low confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies of the encoding-decoding methods for keypoint coordinates on the COCO validation set.</figDesc><table><row><cell></cell><cell>DARK [7]</cell><cell>qnt</cell><cell cols="4">qnt + ro Ours Ours + ro Ours w/ bilinear</cell></row><row><cell>AP</cell><cell>54.5</cell><cell>56.2</cell><cell>62.8</cell><cell>64.4</cell><cell>64.1</cell><cell>62.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on the COCO test-dev set. We report singlescale inference results. Entries marked with "*" are produced by corresponding official source code and models.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP M</cell><cell>AP L</cell><cell>Input</cell></row><row><cell cols="2">CenterNet  *  [16], Hourglass-104 63.0</cell><cell>58.9</cell><cell cols="2">70.4 ?640</cell></row><row><cell>Simple Pose  *  [3], IMHourglass</cell><cell>64.6</cell><cell>60.3</cell><cell>71.6</cell><cell>768</cell></row><row><cell>PersonLab [2], RestNet-101</cell><cell>65.5</cell><cell>61.3</cell><cell>71.5</cell><cell>1401</cell></row><row><cell>PIFPAF  *  [13], RestNet-101</cell><cell>64.9</cell><cell>60.6</cell><cell>71.2</cell><cell>641</cell></row><row><cell>Bottom-up HRNet-W32 [14]</cell><cell>64.1</cell><cell>57.4</cell><cell cols="2">73.9 &gt;640</cell></row><row><cell>Ours (final)</cell><cell>64.7</cell><cell>60.7</cell><cell>70.4</cell><cell>640</cell></row><row><cell>Ours (final)</cell><cell>65.6</cell><cell>63.3</cell><cell>68.8</cell><cell>768</cell></row><row><cell>point coordinates in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Benchmarking and error diagnosis in multi-instance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple pose: Rethinking and improving a bottom-up approach for multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>1, 2.1, 2.2, 2.4, 1, 2.5</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
	<note>1, 2.2, 2, 2.5</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3711" to="3719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Shih En Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>1, 2.1, 2.3, 1</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in arXiv preprint arXiv: 1904.07850, 2019. 1, 2.1, 2.2, 2, 2.4, 2.5, 1, 3, 3</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
