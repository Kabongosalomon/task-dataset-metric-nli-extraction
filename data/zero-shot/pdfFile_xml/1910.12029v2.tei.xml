<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PoseLifter: Absolute 3D human pose lifting network from a single noisy 2D human pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><forename type="middle">Yong</forename><surname>Chang</surname></persName>
							<email>jychang@kw.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Kwangwoon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PoseLifter: Absolute 3D human pose lifting network from a single noisy 2D human pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study presents a new network (i.e., PoseLifter) that can lift a 2D human pose to an absolute 3D pose in a camera coordinate system. The proposed network estimates the absolute 3D location of a target subject and generates an improved 3D relative pose estimation compared with existing pose-lifting methods. Using the PoseLifter with a 2D pose estimator in a cascade fashion can estimate a 3D human pose from a single RGB image. In this case, we empirically prove that using realistic 2D poses synthesized with the real error distribution of 2D body joints considerably improves the performance of our PoseLifter. The proposed method is applied to public datasets to achieve state-of-theart 2D-to-3D pose lifting and 3D human pose estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What information can we acquire from the sparse semantic points of a single image? <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows a 2D human pose that consists of a set of 2D joints. Human activities can be easily recognized from such sparse 2D joint information <ref type="bibr" target="#b14">[15]</ref>, based on which automated algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> have been proposed. What about the 3D human pose shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b)? How accurately can a 3D human pose (i.e., 3D joint coordinates) be reconstructed using only projected geometric information without appearance features? This ill-posed problem <ref type="bibr" target="#b16">[17]</ref>, namely, the automatic lifting of 2D joint coordinates in a single image to 3D space, has been addressed in previous studies, and successful methods have been proposed <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">31]</ref>. However, existing methods generate only the relative 3D pose, i.e., the 3D joint coordinates after the translation transform is applied to move the reference joint to the origin. To address this issue, we propose a novel 2D-to-3D pose-lifting network (i.e., PoseLifter) that can produce an absolute 3D pose in which the coordinates of all joints are defined on a camera coordinate system.</p><p>An absolute pose can be decomposed into the absolute coordinates of a reference joint (i.e., the root) and the relative pose to the root. To obtain an absolute 3D pose, we use the normalized 2D pose, 2D location, and 2D scale obtained by decomposing the input 2D pose. From the obtained 2D information, location and scale enable the calculation of the root's 3D coordinates. Specifically, location and scale provide approximate information about the (X, Y ) and Z (i.e., depth) coordinates of the target human subject in the camera coordinate system, respectively. The normalized 2D pose can also be used to estimate the root-relative 3D pose as in existing methods <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this study, we propose to use all of the decomposed 2D information for the estimation of the absolute 3D pose (i.e., the root coordinates and relative 3D pose). This leads to the following advantages. First, under perspective projection, the distance from the camera to the human subject (i.e., root depth) is proportional to the ratio of the human scale in real space to the scale in the 2D image. The real scale can vary greatly depending on the posture of the human subject. For example, the scale in the squatting posture is relatively small compared to the stretching posture. This variation adds difficulty in determining the depth from only the 2D scale. However, a normalized 2D pose can provide additional information about the real scale, which helps in  <ref type="figure">Figure 2</ref>: For a given input RGB image, the root's absolute depth is proportional to the size of the human subject, as shown in (a). Even if the size of the human subject is fixed, the ambiguity in determining the absolute root depth remains due to the focal length. The focal length in terms of pixel dimensions (?) is the product of the focal length in terms of physical dimensions (f ) and the number of pixels per unit distance (m): ? = f ? m. Under the assumption of fixed subject size, the absolute root depth corresponding to a given input RGB image is proportional to f and m, shown in (b) and (c), respectively. Note that when m is doubled (m 2 = 2m 1 ), the actual physical size occupied by the image in the image sensor is halved, which doubles the absolute root depth (Z 2 = 2Z 1 ).</p><p>an accurate calculation of the depth of the root.</p><p>Secondly, determination of the root-relative 3D pose can also be aided by the 2D location and scale information. This is because the additional 2D information can alleviate the ill-posedness of the problem. For example, under the perspective projection assumption, the 2D projection of a human subject with a fixed root-relative 3D pose changes with the subject's 3D location. This further deepens the ambiguities in estimating the correct root-relative 3D pose from a given 2D pose. However, 2D location and scale provide approximate information about the subject's 3D location, thus mitigating the ambiguities of root-relative 3D pose estimation. Therefore, we propose a method of estimating root coordinates and root-relative 3D pose simultaneously using all of the normalized 2D pose, 2D location, and 2D scale information.</p><p>Between the two goals, determining the absolute depth of the root is a significantly unconstrained problem. Technically, the ambiguities of the human subject size and camera focal length do not allow the absolute depth of the root to be uniquely determined, as illustrated in <ref type="figure">Figure 2</ref>. Specifically, the absolute depth of root is proportional to both the size of human subject S and the camera focal length ? = f ? m, in which f and m are the focal length in terms of physical dimension and the number of pixels per unit distance, respectively. These ambiguities are solved in this study as follows. First, in the proposed PoseLifter, the size of the human subject is learned implicitly from datasets, which resolves the size ambiguity. To handle the focal length ambiguity, PoseLifter outputs the canonical root depth normalized by the focal length instead of the real depth. If additional focal length information is available, then the root's real depth can be obtained from the canonical depth.</p><p>The proposed PoseLifter can be applied to estimate the absolute 3D pose of a person from a single 2D image. A simple approach to achieve this goal is to sequentially combine a 2D human pose estimator and the PoseLifter. Specifically, the 2D human pose estimator will generate a 2D pose from the input 2D image. The resulting 2D pose will then be fed into the PoseLifter to generate the corresponding absolute 3D pose. However, the results of 2D pose estimation are unreliable because of inevitable errors. A recent study <ref type="bibr" target="#b34">[32]</ref> indicated that such errors exhibit a similar distribution regardless of the type of 2D pose estimator used. Thus, a method for improving the performance of existing 2D pose estimators by utilizing such distribution was proposed <ref type="bibr" target="#b26">[25]</ref>. In this regard, we propose to analyze the error of a 2D pose estimator and synthesize the input of the PoseLifter for learning following the resulting error statistics.</p><p>In summary, we propose the following technical improvements to 3D human pose estimation.</p><p>? The first is the implementation of the normalization layer, which is the first layer of the proposed PoseLifter. Our novel normalization layer normalizes the input 2D pose and adds the 2D location and scale information of the target subject as intermediate features. These added features enable the estimation of the root's absolute 3D coordinates and considerably improve the performance of root-relative 3D pose estimation.</p><p>? The second is the canonical root depth that is independent of the camera focal length. This new representation allows the PoseLifter to be applied to any test image with an unknown focal length.</p><p>? The last one focuses on the connection between 2D pose estimation and 3D pose-lifting modules. In our method, the error of a 2D pose estimator is realistically synthesized, and the result is used to learn the 3D pose lifter, thereby making the PoseLifter robust to 2D pose estimation errors. Consequently, the proposed approach achieves state-of-the-art performance on two large-scale 3D human pose datasets, namely, Human3.6M <ref type="bibr" target="#b12">[13]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b23">[22]</ref>.</p><p>The remainder of this paper is organized as follows. Section 2 introduces related studies. Section 3 describes the proposed 2D-to-3D pose-lifting method (i.e., PoseLifter). Section 4 explains the application of this method to 3D human pose estimation from a single RGB image. Section 5 presents the experimental results. Section 6 provides the conclusions drawn from the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. 2D-to-3D human pose lifting</head><p>In earlier studies, 3D human pose is typically modeled as a linear combination of sparse basis poses. In general, the re-projection error between the 2D projections of 3D joints and input 2D joints is minimized using optimization methods, such as greedy orthogonal matching pursuit <ref type="bibr" target="#b33">[31]</ref>, alternating direction <ref type="bibr" target="#b43">[41]</ref>, and convex relaxation-based alternating direction method of multipliers <ref type="bibr" target="#b47">[45]</ref>, along with a prior model of physically possible 3D poses <ref type="bibr" target="#b0">[1]</ref>. After optimization, the 3D joints in a world coordinate system and the viewpoint information of an orthographic camera are obtained simultaneously.</p><p>In recent studies, a large 2D/3D dataset was used to learn the regression function that directly converts the input 2D to the output 3D pose based on a camera coordinate system. A neural network is typically adopted for this purpose. The normalized 2D joint coordinates <ref type="bibr" target="#b20">[21]</ref> and a Euclidean distance matrix <ref type="bibr" target="#b27">[26]</ref> are proposed as inputs for the network. All the aforementioned methods yield a relative pose based on the root, whereas our proposed method allows the acquisition of an absolute 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D human pose estimation from a single image</head><p>Recent deep learning-based methods can be divided into direct image-to-pose estimation and cascade approaches. The direct approach produces an output 3D pose directly from an input single RGB image. These outputs exhibit various forms, such as 3D joint coordinates <ref type="bibr" target="#b17">[18]</ref>, bonebased representations <ref type="bibr" target="#b38">[36]</ref>, and volumetric heatmaps <ref type="bibr" target="#b31">[30]</ref>. Recent studies <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b39">37]</ref> proposed methods that directly regress 3D coordinates while maintaining the advantages of a volumetric heatmap representation through a softargmax <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">28]</ref> operation. In particular, the methods presented in <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b36">34]</ref> are related to our method, given that the approaches calculate the absolute 3D coordinates of the root. The previous methods differs from our method, which uses only 2D joint information for pose lifting, whereas the methods of <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b36">34]</ref> require an RGB image as input. Absolute root coordinates were also computed in <ref type="bibr" target="#b23">[22]</ref>. To do so, the method in <ref type="bibr" target="#b23">[22]</ref> initially computes the root-relative pose using the network learned through transfer learning and then estimates the absolute location of the root through a post-processing step based on a closed-form formula. In <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b36">34]</ref>, only the evaluation of the root-relative pose is performed, and the performance of absolute root location estimation is not reported.</p><p>The cascade approach consists of two steps: (1) creating a 2D pose through a 2D pose estimator and (2) lifting this 2D pose to output a 3D pose. The acquisition of a 3D pose from a 2D pose is accomplished via neural networkbased regression <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">26]</ref> or by fitting a 3D morphable model to 2D joints <ref type="bibr" target="#b2">[3]</ref>. The cascade approach is flexible and easy to use owing to its modular design. Different types of datasets can also be used for learning 2D pose estimating and 2D-to-3D pose-lifting modules. Meanwhile, in the direct approach, the entire process of generating an output 3D pose from an input RGB image is optimized in an end-to-end fashion by a single cost function, which is considered to bring relatively high performance. Our method is a regression-based cascade approach, which nevertheless achieves better 3D human pose estimation performance than most existing direct approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Error analysis of 2D human pose estimation</head><p>In <ref type="bibr" target="#b34">[32]</ref>, prediction errors in 2D human pose estimation were categorized into various types, thereby experimentally proving that existing state-of-the-art methods produce similar error distributions. On the basis of taxonomy, a joint with a small error from ground truth is considered good. The error of a joint that is near the ground truth but larger is called a jitter. Inversion and swap represent errors due to confusion with semantically similar joints belonging to the same and different persons, respectively. Lastly, a miss corresponds to a considerably large error that does not belong to previous cases. The method proposed in <ref type="bibr" target="#b26">[25]</ref> refines the prediction results of existing 2D pose estimators by using estimation error statistics. In contrast to this previous study that uses a simple empirical ratio of error types, our method estimates a specific error distribution to make 3D pose lifting robust.</p><formula xml:id="formula_0">2D Pose: =1 Normalization ? =1 , ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PoseLifter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input and output for PoseLifter</head><p>The objective of the pose-lifting problem is to lift a given 2D pose</p><formula xml:id="formula_1">{p i } J i=1 to a 3D pose {P i } J i=1</formula><p>. p i ? R 2 and P i ? R 3 represent the locations of the ith joint in an input image coordinate system and a camera coordinate system, respectively. J denotes the number of joints. First, we apply the two-step normalization procedure to the input 2D pose. In the first step, we convert 2D coordinates p i = [x i , y i ] to p i = [x i , y i ] for all joints i as follows:</p><formula xml:id="formula_2">x i = x i ? c x ; y i = y i ? c y ,<label>(1)</label></formula><p>where (c x , c y ) denotes the principal point of the camera. This transformation makes the coordinates of the 2D joint independent of the principal point of the camera. If the principal point of a test image is unknown, then the approximate image center can be used. According to our experiments on Human3.6M and MPI-INF-3DHP, this approximation does not yield a quantitatively significant performance difference.</p><p>In the second step, we follow the previous pose-lifting methods <ref type="bibr" target="#b20">[21]</ref> and further normalize the input 2D joints to zero mean and unit variance as follows:</p><formula xml:id="formula_3">p i = (p i ? u) ? ,<label>(2)</label></formula><p>where u and ? denote the mean vector and standard deviation, respectively:</p><formula xml:id="formula_4">u = J i=1 p i /J,<label>(3)</label></formula><formula xml:id="formula_5">? = J i=1 p i ? u 2 2 /J.<label>(4)</label></formula><p>Note that u and ? represent the approximated 2D location and scale of the subject in the image, respectively. Under a perspective projection assumption, the 2D location and scale information provide a clue to the 3D location of the subject and allow the estimation of an absolute 3D pose. Therefore, we define a normalization layer that transforms the input (2J)-dimensional vector p = [p 1 , . . . , p J ] into a (2J + 3)-dimensional vector concatenated with normalized 2D coordinates, mean vector, and standard deviation. We then set this layer as the first layer of our PoseLifter. Notably, the method of <ref type="bibr" target="#b20">[21]</ref> does not perform normalization through the principal point, nor does it use the location and scale of the target subject. Our objective in pose lifting is to obtain an absolute 3D</p><formula xml:id="formula_6">pose {P i } J i=1 . It can be decomposed into the root's abso- lute coordinates R = [R x , R y , R z ] ? R 3 and the relative 3D pose {P i } J i=1 to the root: P i = R +P i . We estimat? R z = Rz</formula><p>? obtained by dividing the z-component R z of the root via the focal length ? instead of its absolute 3D coordinates.R z means a depth value that is independent of the focal length of the camera and is thus named canonical root depth in this study. Therefore, 2D-to-3D pose lifting can be formulated as the problem of finding a 3D regression function h : R 2J ? ? R 3J?2 that maps the input 2D pose p ? R 2J into the canonical root depthR z ? R and the root-relative 3D poseP ? R 3J?3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Human Pose Estimator</head><p>PoseLifter <ref type="bibr">Input</ref>   </p><formula xml:id="formula_7">r x = ? R x R z + c x ; r y = ? R y R z + c y ,<label>(5)</label></formula><p>which can be rearranged as follows:</p><formula xml:id="formula_8">R x = (r x ? c x ) R z ? = (r x ? c x )R z ; R y = (r y ? c y ) R z ? = (r y ? c y )R z .<label>(6)</label></formula><p>Thus the x, y-components R x and R y of the absolute root coordinates are determined by the canonical depthR z and the root's 2D coordinates r = [r x , r y ]. Note that the absolute x and y-coordinates of the root can be calculated without focal length information. Meanwhile, the absolute root depth R z is obtained as follows:</p><formula xml:id="formula_9">R z = ?R z .<label>(7)</label></formula><p>Therefore, for any test image, we can reconstruct up to the canonical root depthR z , which can be promoted to the absolute root depth R z with the help of focal length information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network structure</head><p>We describe the structure of our PoseLifter that realizes the 3D regression function h, which is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Our network is primarily based on the residual block proposed in <ref type="bibr" target="#b20">[21]</ref>. This block iteratively passes the input vector to batch normalization <ref type="bibr" target="#b11">[12]</ref>, dropout <ref type="bibr" target="#b37">[35]</ref>, rectified linear unit <ref type="bibr" target="#b28">[27]</ref>, and linear layers twice, and then adds the result to the residual connection <ref type="bibr" target="#b10">[11]</ref> output. The dropout probability and feature dimension are set to 0.5 and 4096, respectively.</p><p>First, the input (2J)-dimensional vector is normalized to a (2J + 3)-dimensional vector via the normalization layer. The latter is then converted to a 4096-dimensional feature vector through a linear layer. This vector passes through two residual blocks and finally outputs a (3J ? 2)dimensional vector via a linear layer. The first number represents the canonical root depthR z , while the following 3(J ? 1) numbers represent the root-relative pose vector P, except for the root.</p><p>Subsequently, we supervise our PoseLifter by using the ground truth root depth R * z , focal length ?, and relative pos? P * for learning the 3D regression function h. In particular, the following cost function based on L1 loss is minimized:</p><formula xml:id="formula_10">L = 1 N N i=1 |R (i) z ? R * (i) z ? |+? 1 N N i=1 P (i) ?P * (i) 1 ,<label>(8)</label></formula><p>where superscript i is the index of the sample, and N denotes the total number of training samples. The first term causes our network to output a canonical root depth that is independent of the focal length. ? is a parameter for adjusting the relative strength between the two loss functions for the canonical root depth and the root-relative pose. This parameter is set to 10 3 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Cascading with 2D pose estimator</head><p>An absolute 3D pose can be obtained from a single RGB image by using the PoseLifter presented in the previous section. The basic concept is to combine a state-of-the-art 2D human pose estimation method and PoseLifter in a cascade fashion, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Similar to other 3D human pose estimation studies, we assume that a single RGB image and a bounding box that contains the target subject in the image are provided as inputs. First, we crop the input image using the bounding box and then resize the resulting image to 256 ? 256. The resized image is then fed into a state-of-the-art 2D human pose estimator to obtain a 2D pose. Thereafter, the obtained 2D pose is transformed into the original image's coordinate system and fed into our PoseLifter to yield the canonical root depthR z and the rootrelative poseP. If focal length information is available, the canonical root depth can be converted to the absolute root coordinates R = (R x , R y , R z ) through Equations <ref type="formula" target="#formula_8">(6)</ref> and <ref type="bibr" target="#b6">(7)</ref>. Finally, the absolute root coordinates and the rootrelative pose are transformed into an absolute 3D pose using P i = R +P i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">2D human pose estimation</head><p>The recently proposed heatmap regression network <ref type="bibr" target="#b44">[42]</ref> and integral regression <ref type="bibr" target="#b39">[37]</ref> are used for 2D human pose estimation from a single RGB image. The method presented in <ref type="bibr" target="#b44">[42]</ref> removes the last two layers of a residual neural network (i.e., ResNet) <ref type="bibr" target="#b10">[11]</ref> and adds three deconvolution layers and a 1 ? 1 convolution layer to the back. This modified network receives an image with a size of 3 ? 256 ? 256 and generates output heatmaps with a size of J ? 64 ? 64, which intuitively represent probability distributions for the 2D locations of each joint.</p><p>The process of obtaining 2D joint coordinates from heatmaps usually depends on the argmax operation. Argmax has two drawbacks: (1) vulnerability to quantization errors and (2) nondifferentiability. Therefore, a method that can directly calculate the 2D joint coordinates from a heatmap with sub-pixel accuracy in a differentiable manner has been proposed for integral regression <ref type="bibr" target="#b39">[37]</ref>. The basic concept of this method is to normalize a heatmap to a probability distribution and then apply the expectation operation to the result.</p><p>In this study, we construct a 2D human pose estimator by attaching an integral module to the back of the heatmap regression network based on the ResNet backbone. This estimator outputs the heatmaps and 2D joint coordinates from an input RGB image. The mean squared error (MSE) and L1 losses are used to supervise the heatmaps and 2D coordinates, respectively, as a cost function for estimator learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Synthesizing the input for training PoseLifter</head><p>The result of the 2D pose estimator is used as the input for our PoseLifter to generate an absolute 3D pose. Such input 2D pose is imperfect and includes an estimation error. We propose to model the estimation error and use the sampled realistic 2D pose based on the error distribution for the learning of PoseLifter to make the system robust to error.</p><p>To obtain the error statistics of 2D pose estimation, we learn a 2D human pose estimator by using data that is generated by excluding one of the human subjects that constitute the training set. The estimation error is obtained by applying the learned model to the excluded human subject data, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. We propose that a simple mixture model consisting of Gaussian and uniform distributions is well suited for such an empirical error distribution. Following the error taxonomy in <ref type="bibr" target="#b34">[32]</ref>, the Gaussian and uniform distributions can account for the error of the inlier joint, such as good or jitter, and the error of the outlier joint, such as inversion or miss, respectively.</p><p>The mixture model for error e = (e x , e y ) is as follows:</p><formula xml:id="formula_11">p(e) = ? exp ? 1 2 (e ? ?) T ? ?1 (e ? ?) 2? |?| +(1??) 1 v ,<label>(9)</label></formula><p>where ? is the mixing parameter, v is the normalization constant of the uniform distribution, and ? = (? x , ? y ) and ? = diag(? 2 x , ? 2 y ) represent the mean vector and covariance matrix of the Gaussian distribution, respectively. We initially set v to 100 ? 100 = 10000, assuming that the pixel range of the error due to the outlier is [?50, 50]. The remaining parameters of the mixture model are determined by minimizing the negative log likelihood as follows:</p><formula xml:id="formula_12">N LL = ? i log p(e (i) ),<label>(10)</label></formula><p>where superscript i is the index of the sample. This minimization process can be realized by using the expectation maximization (EM) algorithm <ref type="bibr" target="#b5">[6]</ref>.</p><p>To obtain the initial estimate of the mean and covariance of the Gaussian distribution, a single Gaussian is fitted to the given error data, and ? is initialized to 0.9. <ref type="figure" target="#fig_4">Figure 5</ref> shows that our mixture model effectively explains the empirical error data. The error model obtained through the preceding procedure is used to synthesize the 2D input pose of the training set required for PoseLifter learning, thereby making the resulting PoseLifter robust to real 2D pose estimation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>PyTorch <ref type="bibr" target="#b30">[29]</ref> is used as the deep learning framework in all our experiments. Our code to reproduce all the results of this paper is available in 1 . PoseLifter and the 2D pose estimator are learned separately through the following processes.</p><p>PoseLifter. First, the cost function of Equation <ref type="formula" target="#formula_10">(8)</ref> is minimized using the Rmsprop optimization algorithm <ref type="bibr" target="#b40">[38]</ref> for PoseLifter learning. Learning rate, batch size, and number of epochs are set to 10 ?3 , 64, and 300, respectively. Learning rate is reduced to 10 ?4 after 200 epochs. Except for random horizontal flipping, no data augmentation is performed in 2D-to-3D pose lifting experiments. In the case of 3D human pose estimation from a single RGB image, the input synthesis process in Section 4.2 functions as an additional data augmentation procedure.</p><p>2D pose estimator. The ResNet152 backbone-based network for 2D pose estimation is initialized with pretrained weights from the ImageNet dataset <ref type="bibr" target="#b35">[33]</ref>. Next, the sum of the MSE heatmap and L1 coordinate losses in Section 4.1 is minimized using the Rmsprop algorithm. In this case, learning rate, batch size, and number of epochs are set to 10 ?4 , 48, and 60, respectively. Learning rate is reduced to 10 ?5 after 30 epochs. For data augmentation, a small random translation of [?4, 4] pixel range, random horizontal flipping, and 40% random color jittering are applied to the input RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Dataset and evaluation metrics</head><p>The datasets and metrics used to evaluate the performance of the proposed method are as follows.</p><p>Human3.6M. The Human3.6M dataset <ref type="bibr" target="#b12">[13]</ref> is used as the primary dataset for evaluating the proposed method. For the evaluation metric, we use the mean per joint position error (MPJPE), which is defined as the mean of the Euclidean distances between the corresponding joints after aligning the root joints of ground truth and the estimated 3D pose. Moreover, the PA-MPJPE, which is calculated after applying Procrustes alignment <ref type="bibr" target="#b8">[9]</ref> to the two 3D poses, is adopted as an additional metric. The two metrics are used to evaluate the root-relative pose computed using our method. To evaluate the absolute location of the root joint, we introduce the mean of the Euclidean distance between the prediction R and the ground truth R * , i.e., the mean of the root position error (MRPE), as a new metric:</p><formula xml:id="formula_13">M RP E = 1 N N i=1 ||R (i) ? R (i) * || 2 ,<label>(11)</label></formula><p>where superscript i is the index of the sample, and N denotes the total number of test samples. Two protocols adopted from existing works are used to evaluate the proposed method. In Protocol 1, Subjects 1, 5, 6, 7, and 8 and Subjects 9 and 11 are used as the training and test sets, respectively. Evaluation is performed through the MPJPE metric. In Protocol 2, Subjects 1, 5, 6, 7, 8, and 9 and Subject 11 are adopted as the training and test sets, respectively. The PA-MPJPE metric is used for evaluation.</p><p>MPI-INF-3DHP. We additionally use the MPI-INF-3DHP dataset <ref type="bibr" target="#b23">[22]</ref> to evaluate the proposed method. This dataset consists of approximately 1.3M frames acquired using a commercial marker-less motion capture system with multiple cameras. Approximately 190K frames obtained by sampling the training set every 5 frames are used for the learning of the proposed model. Meanwhile, the original test set that consists of 2935 frames is used for evaluation.</p><p>In addition, 3DPCK, which extends the existing percentage of correct keypoints (PCK) <ref type="bibr" target="#b41">[39,</ref><ref type="bibr" target="#b42">40]</ref> to 3D, and the area under curve (AUC), which is calculated for several PCK thresholds, are used as evaluation metrics. To compute 3DPCK, whether the distance between the corresponding joints is less than 150mm is checked after the roots of the predicted and ground truth 3D poses are aligned. The data acquisition environment for the test set can be divided into a studio with a green screen (StudioGS), a studio without a green screen (StudioNoGS), and outdoor (Outdoor). 3DPCK and AUC results are reported for each case. <ref type="table">Table 1</ref> shows the focal length information of the cameras used to acquire the Human3.6M and MPI-INF-3DHP datasets. Unlike Human3.6M datasets with similar focal lengths, the MPI-INF-3DHP dataset has considerably different focal length values. Specifically, in the case of the Outdoor data, its focal length value is 1683.98 pixels, which is approximately 11% different from those of other test data (1499.21 pixels) or training data (1497.39 pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">2D-to-3D pose lifting</head><p>In this subsection, we present the performance of the proposed PoseLifter for 2D-to-3D pose lifting.</p><p>Canonical root depth estimation. Under the perspective projection assumption, the canonical root depth is inversely proportional to the image scale for human subjects with a constant real scale. This inverse proportion property is learned implicitly by the proposed PoseLifter, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>, where the predicted canonical root depth is approximately in inverse proportion to the human scale in image space (i.e., standard deviation of the 2D pose). However, the various postures of the human subject lead to variations in real scale, making the estimation of the canonical root depth using only the inverse proportion property difficult. This problem can be resolved by our method, which allows us to implicitly compute the variation of the real scale  <ref type="table">Table 2</ref>: 2D-to-3D pose lifting performances are shown for our PoseLifter and its variants and other existing methods, in which the Human3.6M dataset is used. Protocol 1 is adopted and the unit of all numbers is mm. Performance analysis. We first evaluate the performance of our PoseLifter for 2D-to-3D pose lifting. To achieve this, we train and test our PoseLifter using ground truth 2D pose data. <ref type="table">Table 2</ref> presents the performance of our PoseLifter with and without location and scale information. To implement the latter, we modify our normalization layer to output only normalized 2D pose {p i } J i=1 , except for mean u and standard deviation ?. The experimental results indicate that using location and scale information is essential in estimating an accurate absolute root position. In this case, MRPE is 98.84mm. Moreover, the location and scale information required for absolute root estimation considerably reduces the root-relative pose error (i.e., MPJPE) from 44.86mm to 38.38mm. This result demonstrates the effectiveness of our proposed PoseLifter in estimating the absolute pose (i.e., absolute root coordinates and root-relative pose).</p><p>Quantitative comparison. Subsequently, a quantitative comparison is performed between the proposed method and existing methods for 2D-to-3D pose lifting. The results are provided in <ref type="table">Table 2</ref>. The proposed method outperforms optimization-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">31,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b48">46]</ref> and the more recent regression-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">26]</ref> in terms of rootrelative 3D pose estimation while allowing the acquisition of absolute location information, which the other methods are incapable of doing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Cascading with 2D pose estimator</head><p>In this subsection, we present the performance of the proposed cascade approach with a 2D pose estimator in es-  <ref type="table">Table 3</ref>: The results of our cascade approach are presented along the input generating strategy for PoseLifter learning. The Human3.6M dataset is used. "Loc.&amp;Scale" indicates that the 2D location and scale information is utilized in PoseLifter. "2D estimate" means to use the output of the 2D pose estimator as a training set. "Single Gaussian" and "Mixture model" represent the error model used for synthesizing the 2D pose.</p><p>timating 3D human pose from a single RGB image. <ref type="table">Table 3</ref> provides the results for the Human3.6M dataset. As with 2D-to-3D pose lifting, the approximate 2D location and scale information of the target subject boosts the performance of the root-relative pose and the absolute root location, which are evaluated through MPJPE and MRPE, respectively. Synthesizing 2D pose for PoseLifter. As shown in Table 3, the input 2D pose data used for PoseLifter learning play an important role in the performance of 3D pose estimation. First, the use of ground truth 2D pose provides the worst results. This result is improved by using real 2D pose estimates obtained by applying the 2D pose estimator to training images for PoseLifter learning. Evidently, the use of realistic input data for learning improves the performance of the model. However, such input data are fixed for a given specific training data, thereby limiting their variability. One approach to overcome this problem is to synthesize the input data in accordance with the underlying distribution. The experiment that uses the error model obtained by analyzing the actual 2D pose error statistics demonstrates that the single Gaussian model does not substantially improve performance. <ref type="figure" target="#fig_4">Figure 5</ref> shows that the single Gaussian model estimates a larger standard deviation than necessary because of outliers. By contrast, our proposed mixture model yields considerably improved results for all the evaluation metrics. The results clearly prove our hypothesis that the synthesis of realistic inputs is beneficial for the performance of the model.</p><p>Root location estimation. To estimate the absolute location of the root, our learning-based method relies on a largescale 2D/3D pose dataset. In <ref type="bibr" target="#b23">[22]</ref>, a method was proposed for analytically calculating the root position from given 2D and root-relative 3D poses without learning. This method assumes a weak perspective projection and is based on a linear least square formulation. We refer to the supplementary  <ref type="table">Table 4</ref>: Quantitative results of root location estimation are given for the methods in <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b25">24]</ref> and ours, in which the Hu-man3.6M dataset is used. MRPE-X, MRPE-Y, and MRPE-Z represent the mean of the errors in the X, Y , and Z axes, respectively.</p><p>material of <ref type="bibr" target="#b23">[22]</ref> for the formula for calculating the depth Z of the root. The remaining X and Y coordinates are obtained using a back-projection formula. <ref type="table">Table 4</ref> presents a quantitative comparison between the analytic approach presented in <ref type="bibr" target="#b23">[22]</ref> and our learning-based approach, in which 2D and 3D root-relative poses obtained through our method are used for fair comparison. Our method exhibits a better Z error than the analytic method. For the errors in the X and Y directions, our method produces nearly the same result as the analytic method, which relies on the constraint that the 3D point should be located on the back-projecting ray. This finding shows that our pose-lifting method implicitly enforces such constraints through learning. The recently proposed learning-based method in <ref type="bibr" target="#b25">[24]</ref> shows better results than ours. We believe this is because our method only relies on 2D poses, while the other approach can utilize image features. Quantitative comparison. <ref type="table" target="#tab_6">Table 5</ref> provides the quantitative results of the performance of recently proposed methods for 3D human pose estimation from a single RGB image. The proposed method achieves comparable performance with state-of-the-art methods for Protocols 1 and 2. In particular, our method outperforms all cascade approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b33">31</ref>] that consist of a sequential combination of a 2D pose estimator and a 3D lifter. Two direct methods in <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b39">37]</ref> yield better results than ours for Protocols 1 and 2, respectively. However, several methods <ref type="bibr" target="#b39">[37,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b46">44]</ref>, including the aforementioned method <ref type="bibr" target="#b39">[37]</ref>, require the ground truth for the absolute depth of the root joint to produce a 3D pose result. This constraint is attributed to these methods using a back-projection formula to transform the joint's estimated image coordinates x and y to 3D coordinates X and Y , which require the absolute depth of each joint. By contrast, our method exhibits the advantage of not requiring the ground truth because the absolute location of the root is estimated. The qualitative results for some of the images in the test dataset are shown in <ref type="figure">Figure 7</ref>.</p><p>MPI-INF-3DHP. Subsequently, we present the evaluation results of the proposed method for the MPI-INF-3DHP dataset, as shown in <ref type="table" target="#tab_7">Table 6</ref>. We use an additional 2D pose   dataset, i.e., MPII, to learn the 2D pose estimator following <ref type="bibr" target="#b23">[22]</ref>. PoseLifter is learned using either Human3.6M or MPI-INF-3DHP. The input synthesis method presented in Section 4.2 is applied to this process.</p><p>In contrast with the Human3.6M dataset, the focal length parameters of the cameras used to acquire the training and test sets for the MPI-INF-3DHP dataset are considerably different, which is shown in <ref type="table">Table 1</ref>. By the focal length ambiguity, this allows a single input 2D pose to be mapped to multiple absolute root depths corresponding to different focal length parameters. Therefore, learning to regress the absolute root depth directly, based on a dataset containing different focal length images, becomes a seriously ill-posed problem. The canonical depth representation proposed in Section 3.2 resolves this problem by causing our network to regress the canonical root depth normalized by the focal length, as shown in <ref type="table" target="#tab_7">Table 6</ref>. The use of the canonical depth representation significantly reduces MRPE. <ref type="table" target="#tab_7">Table 6</ref> also presents the quantitative comparison between the proposed approach and the existing state-of-theart methods. For the experiments using the Human3.6M dataset, our method performs better than recent state-of-the-art methods based on a geometric constraint <ref type="bibr" target="#b46">[44]</ref> and adversarial learning <ref type="bibr" target="#b45">[43]</ref>. This finding shows that the proposed model can be generalized to unseen test data not used for learning. When the MPI-INF-3DHP dataset is used for learning, the proposed method outperforms all the other approaches. <ref type="figure" target="#fig_7">Figure 8</ref> shows the qualitative results for some test images.</p><p>Qualitative results for in-the-wild images. The proposed method is applied to the in-the-wild images of the COCO dataset <ref type="bibr" target="#b18">[19]</ref>. To detect bounding boxes that contain persons from an input image, we use Mask R-CNN <ref type="bibr" target="#b9">[10]</ref>, which is pre-trained from the COCO dataset. We then estimate the absolute 3D pose by feeding the image and detected bounding boxes to our cascade model. The focal length is manually selected. <ref type="figure">Figure 9</ref> shows the estimated 3D poses. The proposed method can be used in conjunction with the object detector to perform successful 3D pose estimation on challenging in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this study, we propose a novel pose-lifting method (i.e., PoseLifter) for estimating a 3D human pose from a 2D human pose. Unlike previous methods, the proposed method enables the acquisition of an absolute pose (i.e., a root-relative pose with absolute root coordinates) in a camera coordinate system and achieves state-of-the-art pose lifting performance. We additionally propose a simple cascade approach, that is, a sequential combination of a 2D pose estimator and PoseLifter, for 3D human pose estimation from a single RGB image. In this case, utilizing the error statistics of 2D pose estimation for PoseLifter learning is essential and contributes to the state-of-the-art 3D pose estimation performance of the proposed approach. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) A 2D human pose consisting of a set of several joints is overlaid on a RGB image. (b) Our goal is to estimate the 3D human pose in the camera coordinate system from such sparse 2D joint information in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Structure of the proposed PoseLifter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Overview of the proposed 3D human pose estimation method from a single RGB image. The root with absolute coordinates R = [R x , R y , R z ] is projected to image coordinates r = [r x , r y ] through a camera with focal length ? and principal point (c x , c y ) as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The error distribution of the right hand joint in the x axis is shown in the blue histogram. The dotted and solid lines show the single Gaussian model and the proposed mixture model fitted to such empirical data, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The standard deviation (?) of the 2D pose of the test samples in the Human3.6M dataset and the canonical root depth (R z ) predicted by our PoseLifter are plotted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>using the input normalized 2D pose for a more accurate estimation of the canonical root depth. For example, for Figures 6(a) and (b) with different image scales, the proposed method estimates similar canonical root depths, taking into account variations in real scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results of our method are shown for the MPI-INF-3DHP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Input 2D pose Loc.&amp;Scale MPJPE PA-MPJPE MRPE</figDesc><table><row><cell>GT</cell><cell>64.97</cell><cell>45.47</cell><cell>680.38</cell></row><row><cell>GT</cell><cell>61.53</cell><cell>45.45</cell><cell>239.35</cell></row><row><cell>2D estimate</cell><cell>58.90</cell><cell>43.54</cell><cell>209.44</cell></row><row><cell>Single Gaussian</cell><cell>56.01</cell><cell>44.78</cell><cell>164.51</cell></row><row><cell>Mixture model</cell><cell>53.14</cell><cell>42.63</cell><cell>144.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Direct. Discuss Eating Greet Phone Pose Purch. Sit SitD. Smoke Photo Wait Walk WalkD. WalkT. Avg. GT</figDesc><table><row><cell>Protocol 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Direct approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kanazawa CVPR'18 [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.0</cell></row><row><cell>Mehta 3DV'17 [22]</cell><cell>57.5</cell><cell>68.6</cell><cell>59.6</cell><cell>67.3</cell><cell>78.1</cell><cell>56.9</cell><cell>69.1</cell><cell cols="3">98.0 117.5 69.5</cell><cell>82.4</cell><cell>68.0</cell><cell>55.3</cell><cell>76.5</cell><cell>61.4</cell><cell>72.9</cell></row><row><cell>Pavlakos CVPR'17 [30]</cell><cell>67.4</cell><cell>72.0</cell><cell>66.7</cell><cell>69.1</cell><cell>72.0</cell><cell>65.0</cell><cell>68.3</cell><cell>83.7</cell><cell>96.5</cell><cell>71.7</cell><cell>77.0</cell><cell>65.8</cell><cell>59.1</cell><cell>74.9</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell>S?r?ndi ECCVW'18 [34]</cell><cell>63.6</cell><cell>65.5</cell><cell>56.0</cell><cell>62.1</cell><cell>64.0</cell><cell>60.7</cell><cell>64.8</cell><cell>76.7</cell><cell>93.0</cell><cell>63.3</cell><cell>69.7</cell><cell>62.0</cell><cell>68.8</cell><cell>61.3</cell><cell>54.1</cell><cell>65.7</cell></row><row><cell>Zhou ICCV'17 [44]</cell><cell>54.8</cell><cell>60.7</cell><cell>58.2</cell><cell>71.4</cell><cell>62.0</cell><cell>53.8</cell><cell>55.6</cell><cell cols="3">75.2 111.6 64.1</cell><cell>65.5</cell><cell>66.0</cell><cell>63.2</cell><cell>51.4</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell>Sun ICCV'17 [36]</cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>53.1</cell><cell>53.6</cell><cell>71.7</cell><cell>86.7</cell><cell>61.5</cell><cell>67.2</cell><cell>53.4</cell><cell>47.1</cell><cell>61.6</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell>Yang CVPR'18 [43]</cell><cell>51.5</cell><cell>58.9</cell><cell>50.4</cell><cell>57.1</cell><cell>62.1</cell><cell>49.8</cell><cell>52.7</cell><cell>69.2</cell><cell>85.2</cell><cell>57.4</cell><cell>65.4</cell><cell>58.4</cell><cell>60.1</cell><cell>43.6</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell>Moon ICCV'19 [24]</cell><cell>51.5</cell><cell>56.8</cell><cell>51.2</cell><cell>52.2</cell><cell>55.2</cell><cell>47.7</cell><cell>50.9</cell><cell>63.3</cell><cell>69.9</cell><cell>54.2</cell><cell>57.4</cell><cell>50.4</cell><cell>42.5</cell><cell>57.5</cell><cell>47.7</cell><cell>54.4</cell></row><row><cell>S?r?ndi ECCVW'18 [34]*</cell><cell>49.1</cell><cell>54.6</cell><cell>50.4</cell><cell>50.7</cell><cell>54.8</cell><cell>47.4</cell><cell>50.1</cell><cell>67.5</cell><cell>78.4</cell><cell>53.1</cell><cell>57.4</cell><cell>50.7</cell><cell>54.0</cell><cell>46.1</cell><cell>40.1</cell><cell>54.2</cell></row><row><cell>Sun ECCV'18 [37]</cell><cell>47.5</cell><cell>47.7</cell><cell>49.5</cell><cell>50.2</cell><cell>51.4</cell><cell>43.8</cell><cell>46.4</cell><cell>58.9</cell><cell>65.7</cell><cell>49.4</cell><cell>55.8</cell><cell>47.8</cell><cell>38.9</cell><cell>49.0</cell><cell>43.8</cell><cell>49.6</cell></row><row><cell>Cascade approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zhou TPAMI'18 [47]</cell><cell>68.7</cell><cell>74.8</cell><cell>67.8</cell><cell>76.4</cell><cell>76.3</cell><cell>84.0</cell><cell>70.2</cell><cell cols="3">88.0 113.8 78.0</cell><cell>98.4</cell><cell>90.1</cell><cell>62.6</cell><cell>75.1</cell><cell>73.6</cell><cell>79.9</cell></row><row><cell>Martinez ICCV'17 [21]</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>78.4</cell><cell>59.1</cell><cell>49.5</cell><cell>65.1</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang AAAI'18 [8]</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>53.4</cell><cell>55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>73.3</cell><cell>57.7</cell><cell>47.5</cell><cell>62.7</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Ours</cell><cell>44.8</cell><cell>48.2</cell><cell>48.5</cell><cell>51.5</cell><cell>54.5</cell><cell>47.9</cell><cell>47.8</cell><cell>60.7</cell><cell>76.4</cell><cell>52.5</cell><cell>64.4</cell><cell>50.8</cell><cell>39.0</cell><cell>55.3</cell><cell>42.2</cell><cell>52.5</cell></row><row><cell>Protocol 2</cell><cell cols="7">Direct. Discuss Eating Greet Phone Pose Purch.</cell><cell>Sit</cell><cell cols="4">SitD. Smoke Photo Wait</cell><cell cols="4">Walk WalkD. WalkT. Avg. GT</cell></row><row><cell>Direct approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kanazawa CVPR'18 [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.8</cell></row><row><cell>Sun ECCV'18 [37]</cell><cell>36.9</cell><cell>36.2</cell><cell>40.6</cell><cell>40.4</cell><cell>41.9</cell><cell>34.9</cell><cell>35.7</cell><cell>50.1</cell><cell>59.4</cell><cell>40.4</cell><cell>44.9</cell><cell>39.0</cell><cell>30.8</cell><cell>39.8</cell><cell>36.7</cell><cell>40.6</cell></row><row><cell>Yang CVPR'18 [43]</cell><cell>26.9</cell><cell>30.9</cell><cell>36.3</cell><cell>39.9</cell><cell>43.9</cell><cell>28.8</cell><cell>29.4</cell><cell>36.9</cell><cell>58.4</cell><cell>41.5</cell><cell>47.4</cell><cell>30.5</cell><cell>42.5</cell><cell>29.5</cell><cell>32.2</cell><cell>37.7</cell></row><row><cell>Moon ICCV'19 [24]</cell><cell>32.5</cell><cell>31.5</cell><cell>41.5</cell><cell>36.7</cell><cell>36.3</cell><cell>31.9</cell><cell>33.2</cell><cell>36.5</cell><cell>44.4</cell><cell>36.7</cell><cell>38.7</cell><cell>31.2</cell><cell>25.6</cell><cell>37.1</cell><cell>30.5</cell><cell>35.2</cell></row><row><cell>Cascade approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ramakrishna ECCV'12 [31]</cell><cell cols="16">137.4 149.3 141.6 154.3 157.7 141.8 158.1 168.6 175.6 160.4 158.9 161.7 174.8 150.0 150.2 157.3</cell></row><row><cell>Bogo ECCV'16 [3]</cell><cell>62.0</cell><cell>60.2</cell><cell>67.8</cell><cell>76.5</cell><cell>92.1</cell><cell>73.0</cell><cell cols="4">75.3 100.3 137.3 83.4</cell><cell>77.0</cell><cell>77.3</cell><cell>79.7</cell><cell>86.8</cell><cell>87.7</cell><cell>82.3</cell></row><row><cell cols="2">Moreno-Noguer CVPR'17 [26] 66.1</cell><cell>61.7</cell><cell>84.5</cell><cell>73.7</cell><cell>65.2</cell><cell>60.9</cell><cell cols="3">67.3 103.5 74.6</cell><cell>92.6</cell><cell>67.2</cell><cell>69.6</cell><cell>78.0</cell><cell>71.5</cell><cell>73.2</cell><cell>74.0</cell></row><row><cell>Zhou TPAMI'18 [47]</cell><cell>47.9</cell><cell>48.8</cell><cell>52.7</cell><cell>55.0</cell><cell>56.8</cell><cell>49.0</cell><cell>45.5</cell><cell>60.8</cell><cell>81.1</cell><cell>53.7</cell><cell>65.5</cell><cell>51.6</cell><cell>50.4</cell><cell>54.8</cell><cell>55.9</cell><cell>55.3</cell></row><row><cell>Martinez ICCV'17 [21]</cell><cell>39.5</cell><cell>43.2</cell><cell>46.4</cell><cell>47.0</cell><cell>51.0</cell><cell>41.4</cell><cell>40.6</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2</cell><cell>56.0</cell><cell>45.0</cell><cell>38.0</cell><cell>49.5</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Fang AAAI'18 [8]</cell><cell>38.2</cell><cell>41.7</cell><cell>43.7</cell><cell>44.9</cell><cell>48.5</cell><cell>40.2</cell><cell>38.2</cell><cell>54.5</cell><cell>64.4</cell><cell>47.2</cell><cell>55.3</cell><cell>44.3</cell><cell>36.7</cell><cell>47.3</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell>Ours</cell><cell>32.1</cell><cell>34.9</cell><cell>43.4</cell><cell>36.9</cell><cell>35.4</cell><cell>35.1</cell><cell>30.8</cell><cell>34.3</cell><cell>57.3</cell><cell>40.4</cell><cell>44.9</cell><cell>35.1</cell><cell>24.9</cell><cell>46.6</cell><cell>30.0</cell><cell>37.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>A quantitative comparison of our approach and other recent methods for 3D human pose estimation from a single RGB image is illustrated. "*" indicates additional Pascal VOC dataset<ref type="bibr" target="#b6">[7]</ref> is used for training. "GT" means that the root's ground truth depth has been used during the estimation process. The Human3.6M dataset is used. MPJPE and PA-MPJPE are adopted for Protocols 1 and 2, respectively.</figDesc><table><row><cell>Method</cell><cell cols="5">3D Dataset StudioGS StudioNoGS Outdoor All 3DPCK</cell><cell>AUC</cell><cell>MRPE</cell></row><row><cell>Yang [43]</cell><cell>H3.6M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.0</cell><cell>32.0</cell><cell>-</cell></row><row><cell>Zhou [44]</cell><cell>H3.6M</cell><cell>71.1</cell><cell>64.7</cell><cell>72.7</cell><cell>69.2</cell><cell>32.5</cell><cell>-</cell></row><row><cell>Ours w/o canonical depth</cell><cell>H3.6M</cell><cell>81.1</cell><cell>73.5</cell><cell>72.5</cell><cell>76.2</cell><cell>40.0</cell><cell>920.1</cell></row><row><cell>Ours</cell><cell>H3.6M</cell><cell>81.6</cell><cell>73.6</cell><cell>72.5</cell><cell>76.5</cell><cell>40.2</cell><cell>421.3</cell></row><row><cell>Mehta [22]</cell><cell>INF</cell><cell>84.1</cell><cell>68.9</cell><cell>59.6</cell><cell>72.5</cell><cell>36.9</cell><cell>-</cell></row><row><cell>Mehta [22]</cell><cell>INF+H3.6M</cell><cell>84.6</cell><cell>72.4</cell><cell>69.7</cell><cell>76.5</cell><cell>40.8</cell><cell>-</cell></row><row><cell>Mehta [23]</cell><cell>INF+H3.6M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.6</cell><cell>40.4</cell><cell>-</cell></row><row><cell>Kanazawa [16]</cell><cell>INF+H3.6M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.9</cell><cell>36.5</cell><cell>-</cell></row><row><cell>Ours w/o canonical depth</cell><cell>INF</cell><cell>91.3</cell><cell>78.2</cell><cell>64.6</cell><cell>79.9</cell><cell>42.3</cell><cell>296.9</cell></row><row><cell>Ours</cell><cell>INF</cell><cell>91.4</cell><cell>82.9</cell><cell>73.7</cell><cell>83.9</cell><cell>45.0</cell><cell>217.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The comparison results of the proposed and other methods are shown for the MPI-INF-3DHP dataset. "Ours w/o canonical depth" means that the canonical depth representation is not used.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonparametric feature matching based conditional random fields for gesture recognition from multimodal video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1612" to="1625" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple prior-free method for non-rigid structure-from-motion factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="122" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Visual motion perception. Scientific American</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Determination of 3d human body postures from a single view. Computer Vision, Graphics, and Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="148" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Computer Vision (ACCV)</title>
		<meeting>Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02322</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Figure 7: Qualitative results of our method are shown for the Human3</title>
		<imprint/>
	</monogr>
	<note>6M dataset</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>2017. 10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Camera distanceaware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Posefix: Modelagnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Qualitative results of our method are shown for the in-the-wild images of the COCO dataset</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Benchmarking and error diagnosis in multi-instance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04987</idno>
		<title level="m">Synthetic occlusion augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on 3d human pose estimation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="901" to="914" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
