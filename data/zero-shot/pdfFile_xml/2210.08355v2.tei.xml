<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Kobayashi</surname></persName>
							<email>kobayasi@lr.</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
							<email>tsutomu.hirao.kp@hco.ntt.co.jp</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NTT Communication Science Laboratories</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
							<email>kamigaito@lr.</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
							<email>masaaki.nagata.et@hco.ntt.co.jp</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NTT Communication Science Laboratories</orgName>
								<orgName type="institution">NTT Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To promote and further develop RST-style discourse parsing models, we need a strong baseline that can be regarded as a reference for reporting reliable experimental results. This paper explores a strong baseline by integrating existing simple parsing strategies, top-down and bottom-up, with various transformer-based pre-trained language models. The experimental results obtained from two benchmark datasets demonstrate that the parsing performance strongly relies on the pretrained language models rather than the parsing strategies. In particular, the bottom-up parser achieves large performance gains compared to the current best parser when employing DeBERTa. We further reveal that language models with a span-masking scheme especially boost the parsing performance through our analysis within intra-and multi-sentential parsing, and nuclearity prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Rhetorical Structure Theory (RST) <ref type="bibr" target="#b30">(Mann and Thompson, 1987)</ref> is one of the most influential theories for representing the discourse structure behind a document. According to the theory, a document is represented as a recursive constituent tree that indicates the relation between text spans consisting of a single elementary discourse unit (EDU) or contiguous EDUs. The label of a nonterminal node describes the nuclearity status, either nucleus or satellite, of the text span, and the edge indicates the rhetorical relation between the text spans ( <ref type="figure">Figure 1)</ref>.</p><p>RST-style discourse parsing (hereafter RST parsing) is a fundamental task in NLP and plays an essential role in several downstream tasks, such as text summarization <ref type="bibr" target="#b27">(Liu and Chen, 2019;</ref>, question-answering <ref type="bibr" target="#b11">(Gao et al., 2020)</ref>, and sentiment analysis <ref type="bibr" target="#b5">(Bhatia et al., 2015)</ref>. In most cases, the performance of an RST parsing method has been evaluated on the largest English treebank,  <ref type="figure">Figure 1</ref>: Example RST-style discourse tree, obtained from WSJ_1100 in the RST discourse treebank (Lynn <ref type="bibr" target="#b29">Carlson, 2002)</ref>, consisting of six the RST discourse treebank (RST-DT) <ref type="bibr" target="#b29">(Lynn Carlson, 2002)</ref>, as the benchmark dataset. The evaluation measures used include the micro-averaged F1 score of unlabeled spans, that of nuclearity-labeled spans, that of rhetorical relation-labeled spans, and that of fully labeled spans, based on standard Parseval <ref type="bibr" target="#b31">(Morey et al., 2017)</ref>, when using gold EDU segmentation.</p><p>There are two major strategies for RST parsing: top-down and bottom-up. The former builds RST trees by splitting a larger text span consisting of EDUs into smaller ones recursively. The latter builds trees by merging two adjacent text spans. Non-neural parsers with classical handcrafted features prefer the bottom-up strategy <ref type="bibr" target="#b7">(duVerle and Prendinger, 2009;</ref><ref type="bibr" target="#b9">Feng and Hirst, 2012;</ref><ref type="bibr" target="#b38">Wang et al., 2017)</ref>. On the other hand, recent neural parsers prefer the top-down strategy <ref type="bibr" target="#b20">(Kobayashi et al., 2020;</ref><ref type="bibr" target="#b22">Koto et al., 2021;</ref><ref type="bibr" target="#b32">Nguyen et al., 2021;</ref><ref type="bibr" target="#b43">Zhang et al., 2021)</ref>, while a few parsers employ the bottom-up strategy . With advances in neural network models, such neural parsers obtain significant gains over the non-neural parsers.</p><p>Several techniques have been proposed to boost RST parsing performance: <ref type="bibr" target="#b32">Nguyen et al. (2021)</ref> and <ref type="bibr" target="#b35">Shi et al. (2020)</ref> introduced beam search and <ref type="bibr" target="#b22">Koto et al. (2021)</ref> exploited dynamic oracles in search to improve their parsing algorithms. <ref type="bibr" target="#b20">Kobayashi et al. (2020)</ref> used a model ensemble with multiple runs, <ref type="bibr" target="#b43">Zhang et al. (2021)</ref> introduced adversarial training, and <ref type="bibr" target="#b21">Kobayashi et al. (2021)</ref> and  exploited silver data to improve parameter optimization.</p><p>Furthermore, pre-trained language models are playing an important role in improving the parsing performance. <ref type="bibr" target="#b35">Shi et al. (2020)</ref>, <ref type="bibr" target="#b32">Nguyen et al. (2021)</ref>, and <ref type="bibr" target="#b43">Zhang et al. (2021)</ref> employed XLNet  to obtain better vector representations for arbitrary text spans consisting of EDU(s). As a result, the current best top-down parser, <ref type="bibr" target="#b43">Zhang et al. (2021)</ref> with XLNet, achieved the fully labeled span F1 score of 53.8 with standard Parseval. The method has a gain of 4-5 points compared to the best non-neural parser <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>. However, it is still unclear what contributed the most to the improvement among the models' various factors such as parsing strategies, pre-trained language models, and a model ensemble. Therefore, we need a simple but strong baseline for different parsing strategies along with a pre-trained language model to clarify the effects of the parsing strategies and pre-trained models. The baseline will contribute to building more reliable experiments for revealing the effectiveness of newly proposed methods. <ref type="bibr">1</ref> This paper aims to build strong baselines for RST parsing, based on two simple open-source top-down <ref type="bibr" target="#b20">(Kobayashi et al., 2020)</ref> and bottomup  parsers, employing transformer-based pre-trained language models, without incorporating any of our own mechanisms. <ref type="bibr">2</ref> The experimental results on RST-DT (Lynn <ref type="bibr" target="#b29">Carlson, 2002)</ref> and Instructional Discourse Treebank (Instr-DT) <ref type="bibr" target="#b36">(Subba and Di Eugenio, 2009)</ref> with various pre-trained language models demonstrated that the parsing performance strongly relies on the performance of the pre-trained language models rather than the parsing strategies. While the current trend is a top-down parser, a bottom-up parser with DeBERTa <ref type="bibr" target="#b14">(He et al., 2021)</ref>, one of the current state-of-the-art pre-trained language models, achieved the best score, which is higher than those of the current state-of-the-art parsers. Further, our analysis based on intra-and multi-sentential parsing, and nuclearity prediction revealed that pre-trained language models with a span-masking scheme improve parsing performance more than those with a token-masking scheme. We will release our code at https://github.com/ nttcslab-nlp/RSTParser_EMNLP22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early studies on RST parsing were based on nonneural supervised learning methods with handcrafted features. As parsing strategies, bottom-up greedy algorithms <ref type="bibr" target="#b7">(duVerle and Prendinger, 2009;</ref><ref type="bibr" target="#b9">Feng and Hirst, 2012)</ref>, shift-reduce <ref type="bibr">(Wang et al., 2017), CRFs (Feng and</ref><ref type="bibr" target="#b10">Hirst, 2014)</ref>, and CKY-like parsing algorithms <ref type="bibr" target="#b17">(Joty et al., 2013</ref><ref type="bibr" target="#b18">(Joty et al., , 2015</ref> have been employed. In particular, Wang et al.'s shiftreduce parser <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>, based on SVMs, achieved the best results among the non-neural statistical models on the RST-DT. The method first builds nuclearity-labeled RST trees and then assigns relation labels between two adjacent spans consisting of a single or multiple EDUs.</p><p>Inspired by the success of neural networks in many NLP tasks, several early neural networkbased models have been proposed for RST parsing <ref type="bibr" target="#b16">(Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b23">Li et al., 2014</ref><ref type="bibr" target="#b24">Li et al., , 2016</ref>. However, as reported by <ref type="bibr" target="#b31">Morey et al. (2017)</ref>, while some neural approaches outperformed classical approaches, it was not by a large margin.</p><p>Recent end-to-end neural RST parsing models with sophisticated language models, such as GloVe and ELMo, achieved better performance. They used vector representations of text spans based on the LSTMs whose inputs are word embeddings from the language models. <ref type="bibr" target="#b42">Yu et al. (2018)</ref> proposed a bottom-up parser, based on the shift-reduce algorithm, that leverages the information from their neural dependency parsing model within a sentence for RST parsing. The method outperformed traditional non-neural methods and obtained a remarkable relation-labeled span F1 score of 49.2. As another approach, a top-down neural parser based on a sequence-to-sequence (seq2seq) framework was proposed  for use only at the sentence level. The method parses a tree in a depth-first manner with a pointer-generator net- work. <ref type="bibr" target="#b44">Zhang et al. (2020)</ref> extended the method and applied it to document-level RST parsing. <ref type="bibr" target="#b20">Kobayashi et al. (2020)</ref> proposed another top-down RST parsing method, based on a minimal spanbased approach, that splits a span into smaller ones recursively and exploits multiple granularity levels in a document. Then, they demonstrated the impact of the model ensemble. <ref type="bibr" target="#b22">Koto et al. (2021)</ref> extended <ref type="bibr">Kobayashi et al.'s parser (2020)</ref> by introducing dynamic oracles as well as a new penalty for segmentation loss, which is based on the current tree depth and the number of EDUs in the input. The latter two methods also outperformed traditional non-neural methods. More recently, neural RST parsing models with transformer-based pre-trained language models, such as SpanBERT and XLNet, have been proposed.   They also reported that performance was boosted by XLNet, and the following current best scores were obtained: 76.3, 65.5, 55.6, and 53.8 for unlabeled, nuclearity-, relation-, and fully labeled span F1 scores, respectively.</p><p>As another approach,  and <ref type="bibr" target="#b21">Kobayashi et al. (2021)</ref> proposed a pre-training and fine-tuning framework for RST parsing. They obtained silver data from automatically parsed largescale data and used them to pre-train their models. Then, they fine-tuned the models with gold data. <ref type="table">Table 1</ref> summarizes the previous best non-neural parser and recent end-to-end neural RST parsers with performance that can be considered state-ofthe-art. We can see that the RST parsing models with the transformer-based language models outperformed the other models regardless of the parsing strategy. The performance improvements are remarkable, especially for the relation-labeled and fully labeled span F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">End-to-end Neural RST Parsing</head><p>We employed the span-based parser <ref type="bibr" target="#b20">(Kobayashi et al., 2020;</ref><ref type="bibr" target="#b22">Koto et al., 2021)</ref> for the top-down parsing strategy and the shift-reduce transition parser , an end-to-end variant of <ref type="bibr">Wang et al.'s parser (2017)</ref>, for the bottom-up parsing strategy. These parsers were chosen here for their simple architecture and their open code. Overviews of the parsers are shown in Figs. 2 and 3. Both parsers basically consist of simple Feed-Forward Networks (FFNs) and BERTbased embeddings. In this study, we used a twolayer perceptron with the GELU activation function and a dropout layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vector Representations for Text Spans</head><p>Before describing the parsing models, we explain how to obtain a vector representation for an arbitrary text span by using BERT-based language models. Our procedure for obtaining the vector representation is a simplified variant of <ref type="bibr">Guz and Carenini's method (2020)</ref>.</p><p>First, we transform a document into a sequence of subwords, {t 1 , t 2 , . . . , t n }. Then, we obtain the vector representation for each subword in the sequence {w 1 , w 2 , . . . , w n } using a language model. Following , the vector representation of a text span u i:j , consisting of the i-th EDU to the j-th EDU, is obtained by averaging the vector of both edge subwords, u i:j = (w b(i) + w e(j) )/2, where b(i) returns the index of the leftmost subword in the i-th EDU and e(j) re- turns that of the rightmost subword in the j-th EDU. A document longer than the maximum allowed length of BERT (512 subwords) is embedded with sliding windows with 30-subword over-wrapping.</p><formula xml:id="formula_0">h i:k h k+1:j u i:k u k+1:j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Top-down Parsing</head><p>Since the minimal span-based parser does not require any additional module like a decoder, as in the pointer-network-based top-down parsers, it is suitable for a comparison with a bottom-up parser of the shift-reduce algorithm, which consists of three simple FFNs, as we describe in Section 3.3. This top-down parser splits each span into smaller ones recursively until the span becomes a single EDU. We modified the code 3 to utilize transformerbased embeddings and simplified it by excluding organizational features that represent sentence and paragraph boundary information. By following <ref type="bibr" target="#b22">Koto et al. (2021)</ref>, we introduced a biaffine layer for span splitting and a loss penalty. For each position k in a span consisting of the i-th EDU to the j-th EDU, a scoring function, s split (i, j, k), is defined as follows:</p><formula xml:id="formula_1">s split (i, j, k) = h i:k Wh k+1:j + v left h i:k + v right h k+1:j ,<label>(1)</label></formula><p>where W is a weight matrix and v left , v right are weight vectors corresponding to the left and right spans, respectively. Here, h i:k and h k+1:j are defined as follows:</p><formula xml:id="formula_2">h i:k = FFN left (u i:k ), (2) h k+1:j = FFN right (u k+1:j ),<label>(3)</label></formula><p>3 https://github.com/nttcslab-nlp/ Top-Down-RST-Parser Then, the span is split at position k that maximizes Eq. <ref type="formula" target="#formula_1">(1)</ref></p><formula xml:id="formula_3">:k = argmax i?k&lt;j s split (i, j, k).<label>(4)</label></formula><p>When splitting a span at position k, the score of the nuclearity status and relation labels for the two spans is defined as follows:</p><formula xml:id="formula_4">s label (i, j,k, ) = h i:k W hk +1:j +v left h i:k +v right hk +1:j ,<label>(5)</label></formula><p>where W is a weight matrix for a specific label , and v left and v right are weight vectors corresponding to the left and right spans for the label , respectively. While the correct split position in the training data is used fork in training time, the position predicted with Eq. (4) is used in testing time.</p><p>Then, the label that maximizes Eq. <ref type="formula" target="#formula_4">(5)</ref> is assigned to the spans:</p><formula xml:id="formula_5">= argmax ?L s label (i, j,k, ),<label>(6)</label></formula><p>where L denotes a set of valid nuclearity status combinations, {N-S,S-N,N-N}, for predicting the nuclearity and a set of relation labels, {Elaboration, Condition,. . .}, for predicting the relation. Note that the weight parameters W and FFNs for the nuclearity and relation labeling are learned separately. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bottom-up Parsing</head><p>Formally, in shift-reduce parsing, a parsing state is denoted as a tuple (S, Q), where S is a stack and Q is a queue that contains incoming EDUs. Each element in S can be a completed constituent or a terminal. At each step, the parser chooses one of the following actions with a neural classifier and updates the state:</p><p>? SHIFT: pop the first EDU off the queue and push it onto the stack.</p><p>? REDUCE: pop two elements from the stack and push a new constituent that has the popped subtrees as its children onto the stack as a single composite item. e 1:3 e 4:6</p><formula xml:id="formula_6">e i-2:i-1 e i+1 ? e i u s1 u q ? u s0</formula><p>Stack Queue e 1 e 2 e 3 e 4 e 5 e 6 e i-2 e i-1 In the REDUCE action, nuclearity status and relation labels are predicted by different neural classifiers. That is, RST trees are built in three stages: First, unlabeled trees are built and then nuclearity status and relation labels are assigned independently. Note that previous shift-reduce parsers were based on the two-stage approach, which means they first build nuclearity-labeled RST trees and then assign relation labels to the trees. To fairly compare top-down and bottom-up approaches, we employed the three-stage approach both in topdown and bottom-up parsing. Our experiments demonstrated that there is no significant difference between the performances of the two-stage and three-stage approaches.</p><formula xml:id="formula_7">0 1 0 FFN act FFN rel Concatenation Language model t 1 t n t b(j-2) t e(j-1) t b(j) t e(j) w 1 w n w b(j-2) w e(j-1) w b(j) w e(j) t b(j+1) t e(j+1) w b(j+1) w e(j+1)</formula><p>Our bottom-up parser has three classifiers, FFN act , FFN nuc , and FFN rel , for predicting action, nuclearity, and rhetorical relations, respectively. The difference among them is only the output dimension related to the number of classes; specifically, the output dimension of the action classifier is two (shift or reduce), that of the nuclearity classifier is three (N-S, N-N, S-N), and that of the relation classifier is the number of the rhetorical relations utilized in the dataset.</p><formula xml:id="formula_8">s * =FFN * (Concat(u s 0 , u s 1 , u q 0 )),<label>(7)</label></formula><p>where function "Concat" concatenates the vectors received as the arguments. u s 0 is the vector representation of a text span stored in the first position of the stack S, u s 1 is that in the second position of S, and u q 0 is that in the first position of the queue Q. Weights for each FFN and the language model used for span embeddings are trained by optimizing the cross-entropy loss of s act , s nuc , and s rel .   <ref type="bibr" target="#b14">(He et al., 2021)</ref>.</p><p>Note that we do not utilize organizational features as in top-down parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Pre-trained Language Models</head><p>Since most of the transformer-based pre-trained language models originated in BERT, we employed BERT and four of its variants as language models to obtain vector representations for text spans. <ref type="table" target="#tab_3">Table 2</ref> shows the size of the dataset and the tasks for their pre-training. BERT: is trained with two tasks: (1) a masked language model (MLM); 15% of the tokens in the training data are randomly masked, and then the model is trained to predict the masked tokens, and (2) a next sentence prediction (NSP) task; the model is trained to correctly predict the following sentence for a given sentence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>As in previous studies, we transformed RST-trees into right-heavy binary trees <ref type="bibr" target="#b34">(Sagae and Lavie, 2005</ref>) and evaluated the system results with microaveraged F 1 scores of Span, Nuclearity, Relation, and Full, based on Standard-Parseval <ref type="bibr" target="#b31">(Morey et al., 2017)</ref>. Span, Nuclearity, Relation, and Full were used to evaluate unlabeled, nuclearity-, relation-, and fully labeled tree structures, respectively. Since neural models heavily rely on their given initialization, we report average scores and standard deviations of three runs with different seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Configurations</head><p>We implemented all models based on Py-Torch <ref type="bibr" target="#b33">(Paszke et al., 2019)</ref> with PyTorch Lightning <ref type="bibr" target="#b8">(Falcon et al., 2019)</ref> and used language models from Transformers <ref type="bibr" target="#b39">(Wolf et al., 2020)</ref>. We used base models, such as BERT-base-cased, RoBERTa-base, for all the experiments. The dimension of hidden layers in FFNs was set to 512, and the dropout rate was set to 0.2. By following , we employed span-based batch rather than document-based batch. The minibatch size is 5 spans/action. We optimized all models with the AdamW <ref type="bibr" target="#b28">(Loshchilov and Hutter, 2017)</ref> optimizer. We used a learning rate of 1e-5 for language models and 1e-5/2e-4 for other parameters 5 such as FFN and biaffine layers. We scheduled the learning rate by linear warm-up, which increases the learning rate linearly during the first epoch and then decreases it linearly to 0 until the final epoch. We trained the model up to 20 epochs and applied early stopping with a patience of 5 by monitoring the fully labeled span F1 score on the development dataset. Details of other hyperparameters are in Appendix A. <ref type="table" target="#tab_6">Table 3</ref> shows the results with different pre-trained language models. The scores on RST-DT are better than those on Instr-DT. This is attributed to the size of the datasets. Instr-DT is significantly smaller than RST-DT while the number of rhetorical relations is larger. In fact, standard deviations on Instr-DT are larger than those on RST-DT. However, the tendencies of the experimental results on both datasets are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The results obtained from paired bootstrap resampling tests 6 between top-down and bottom-up parsers whilst fixing the language model show that significant differences are found only in Span and Nuc. for XLNet on RST-DT, and Rel. and Full for SpanBERT on Instr-DT, respectively.</p><p>On the other hand, the performance of the parsers varies widely depending on their language model when fixing the parsing strategy. To investigate the significant differences among parsers, we performed multiple comparison tests based on the paired bootstrap resampling tests while controlling the false discovery rate <ref type="bibr" target="#b4">(Benjamini and Hochberg, 1995</ref>   <ref type="table">Table 4</ref>: Results for intra-sentential parsing with various language models (RST-Parseval).</p><p>the current best parser <ref type="bibr" target="#b43">(Zhang et al., 2021</ref>) by 1.5, 2.5, 1.7, and 1.6 points, respectively. 7 Furthermore, most parsers yield a performance comparable to current state-of-the-art parsers. We believe that the results have a significant impact in the RST-parsing community. Since we built our baseline parsers based on a simple architecture, as described in Section 3, we can conduct more reliable experiments to reveal the effectiveness of newly proposed methods on top of them without any concern regarding the choice of pre-trained language models or parsing strategies.</p><p>While the evaluation results demonstrate that we successfully built baseline parsers, they also raise the following questions for us: (1) Why did De-BERTa, trained with the half-size dataset (85G), outperform RoBERTa, trained with the most extensive dataset (161G) (2) Why did SpanBERT consistently outperform BERT with significant differ-ences, even though they are trained with the same dataset (16GB). It is well known that pre-trained language models trained with large-scale datasets boost the performance <ref type="bibr" target="#b19">(Kaplan et al., 2020)</ref>; however, the above results do not necessarily agree with the finding.</p><p>We believe that the results may be due to a span-masking scheme, a common feature between SpanBERT and DeBERTa. With the span-masking scheme, randomly generated spans consisting of a sequence of tokens with the length up to 5 (for SpanBERT) or 3 (for DeBERTa) are masked, and then the language models are trained to predict the span boundary tokens in the mask. That is, the span-masking scheme is considered more contextsensitive than the token-masking scheme. Thus, pre-trained language models with a span-masking scheme are suitable for obtaining vector representations for long text spans consisting of EDUs.</p><p>To discuss the impact of the span-masking scheme in more detail, we evaluated our parsers in terms of intra-and multi-sentential parsing performance. <ref type="table">Tables 4 and 5 show the results. 8 From  Table 4</ref>, we can see that the tendency of the results is quite different from that in <ref type="table" target="#tab_6">Table 3</ref>; the differences among parsers are smaller than those in <ref type="table" target="#tab_6">Table 3</ref>. Particularly, the differences among the four methods except for BERT are within 1 point for Full on RST-DT. Other noteworthy points include that the scores of BERT are close to those of the other methods, and DeBERTa often did not achieve the best scores. In contrast, <ref type="table">Table 5</ref> emphasizes the effectiveness of DeBERTa and SpanBERT. De-BERTa completely outperformed the other methods with large differences, and the differences between SpanBERT and BERT became larger. To obtain better results in multi-sentential parsing, we need good representations for longer text spans over sentences. Thus, we believe that the span-masking scheme would help generate better representations for the longer text spans. The results also reveal that there is still much more room for further improvement than intra-sentential parsing.</p><p>Finally, we show another piece of evidence for the effectiveness of the span-masking scheme in <ref type="table" target="#tab_7">Table 6</ref>, which demonstrates the performance of nuclearity prediction among N-S, S-N, and N-N. N-N relations originally occur in n-array (n &gt; 2) trees in many cases. Therefore, we need good representations for longer text spans to detect N-N relations accurately. From the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>As shown in the experimental results, our approach would not perform well with insufficient training data. For example, the performance obtained from Instr-DT was inferior to that obtained from RST-DT in Rel. and Full. The results were caused by the small amount of training data and many rhetorical relations. Since the annotation costs for RST are considerable, how we obtain enough high-quality data is a significant issue for building RST parsers for new domains and languages. Furthermore, since our parsers rely on a large-scale pre-trained language model, they do not perform well for languages that are not ready to use the pre-trained language model, such as low-resource languages. In future work, we should improve the domain/language portability, and we believe the following are practical approaches:  A Hyperparameters <ref type="table" target="#tab_10">Table 7</ref> shows the hyperparameters utilized in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation in Intra-and Multi-sentential Parsing</head><p>Because human annotators sometimes build RSTtrees with disregarding sentence boundaries, some RST-trees have span boundaries that disagree with span boundaries in sentences. <ref type="figure">Figure 4</ref> shows an example. In the gold tree, the subtree consisting of e 3 and e 4 straddles the boundary between s 1 and s 2 . Parsers also sometimes disregard sentence boundaries when building RST-trees. In the predicted tree, the subtree consisting of e 11 and e 12 straddles the boundary between s 4 and s 5 . Therefore, we make a best effort to find sentences in the parse trees. We extract subtrees whose root nodes correspond to sentences when evaluating a predicted tree in terms of intra-sentential parsing. In the example, we extract the subtrees corresponding to s 3 , s 4 , and s 5 from the gold tree and s 1 , s 2 , and s 3 from the predicted tree. However, in this case, s 1 and s 2 are ignored even though they form valid subtrees in the predicted tree. So we give a unary tree whose leaf node is a sentence for s 1 and s 2 for the gold tree. Similarly, we give a unary tree for s 4 and s 5 for the predicted tree (see the middle row in <ref type="figure">Figure 4</ref>). As a result, the leaf nodes of a gold RST-tree do not necessarily have a one-to-one correspondence with those of a predicted tree. Thus, we apply RST-Parseval to evaluate predicted trees in terms of intra-sentential parsing.</p><p>We replace subtrees corresponding to sentences as leaf nodes when evaluating a predicted tree in multi-sentential parsing. In the example, the subtrees dominating e 7 to e 9 , e 10 to e 11 , and e 12 to e 13 in the gold tree are respectively replaced with the leaf nodes s 3 , s 4 , and s 5 . Since the gold RST-tree does not have valid subtrees dominating e 1 to e 3 and e 4 to e 6 , we do not replace them as s 1 and s 2 , respectively. That is, subtrees that cannot be converted into leaf nodes as sentences are left as they are. Similarly, the subtrees dominating e 1 to e 3 and e 4 to e 6 in the predicted tree are respectively replaced as leaf nodes s 1 and s 2 . We also do not replace e 10 to e 11 and e 12 to e 13 as s 4 and s 5 (see the bottom row in <ref type="figure">Figure 4</ref>). The transformation may break down the one-to-one correspondence between leaf nodes of gold and predicted RST-trees. Thus, we also apply RST-Parseval to evaluate predicted trees in terms of multi-sentential parsing.</p><p>e 1 e 2 e 3 e 4 e 5 e 6 e 7 e 8 e 9 s 1 s 2 s 3 s 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document:</head><p>A gold parse tree A system prediction e 10 e 11 e 12 e 13 s 5 e 1 e 2 e 3 e 4 e 5 e 6 e 7 e 8 e 9 e 10 e 11 e 12 e 13 e 1 e 2 e 3 e 4 e 5 e 6 e 7 e 8 e 9 e 10 e 11 e 12 s 3 s 4 s 5 e 13 s 1 s 2 s 3 s 1 s 2 e 7 e 8 e 9 e 7 e 8 e 9 e 4 e 5 e 6 s 4 s 5 e 1 e 2 e 3 e 10 e 11 e 12 e 13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root</head><p>Root Root Root Root Root Root Root Root Root e 1 e 2 e 3 e 4 e 5 e 6 e 10 e 11 e 12 e 13 s 1 s 2 s 3 s 3 s 4 s 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-sentential</head><p>Multi-sentential s 1 s 2 s 4 s 5 s 1 s 2 s 4 s 5 <ref type="figure">Figure 4</ref>: Example of decomposing a RST tree into intra-and multi-sentential trees.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>extended Wang et al.'s bottom-up parser (2017) by replacing SVMs with a neural classifier and employing SpanBERT to obtain representations for text spans. The performance was greatly improved: They achieved nuclearity-labeled and relation-labeled span F1 scores of 64.6 and 51.4, respectively. Shi et al. (2020) extended Lin et al.'s top-down model (2019) by introducing layer-wise beam search and XL-Net. Nguyen et al. (2021) also introduced beam search in their seq2seq-based top-down model and reported that XLNet greatly contributed to improving performance. Zhang et al. (2021) improved a seq2seq-based top-down model by exploiting adversarial training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Top-down parsing model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FFN nuc Figure 3 :</head><label>nuc3</label><figDesc>Bottom-up parsing model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>EDUs: e 1 :[Westinghouse Electric Corp. said], e 2 :[it will buy Shaw-Walker Co.], e 3 :[Terms weren't disclosed.], e 4 :[Shaw-Walker,], e 5 :[based in Muskegon, Mich.,], e 6 :[makes metal files and desks, and seating and office systems furniture.].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Data size and task for pre-training. The data size is from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>While the dataset used for train-ing is a subset of that used for RoBERTa, it performs consistently better on various NLP tasks.</figDesc><table><row><cell>5 Experimental Settings</cell></row><row><cell>5.1 Datasets</cell></row><row><cell>We used the RST-DT and Instr-DT to evaluate the</cell></row><row><cell>performance of the parsers. RST-DT contains 385</cell></row><row><cell>documents selected from the Wall Street Journal. It</cell></row><row><cell>is officially divided into 347 documents as the train-</cell></row><row><cell>ing dataset and 38 documents as the test dataset.</cell></row><row><cell>The number of rhetorical relation labels utilized in</cell></row><row><cell>the dataset is 18. Since there is no official develop-</cell></row><row><cell>ment dataset, we used 40 documents of the training</cell></row><row><cell>dataset as the development dataset by following a</cell></row><row><cell>previous study (Heilman and Sagae, 2015). Instr-</cell></row><row><cell>DT contains 176 documents of the home-repair</cell></row><row><cell>instruction domain. The number of rhetorical re-</cell></row><row><cell>lation labels in the dataset is 39. Since there are</cell></row><row><cell>no official development and test datasets, we used</cell></row><row><cell>126, 25, and 25 documents for training, develop-</cell></row><row><cell>ment, and test datasets, respectively. We used gold</cell></row><row><cell>EDU segmentation for both datasets by following</cell></row><row><cell>conventional studies.</cell></row><row><cell>BERT is trained on</cell></row><row><cell>Book Corpus and English Wikipedia.</cell></row><row><cell>RoBERTa: is trained with longer and larger</cell></row><row><cell>batches over more data and longer sequences. It</cell></row><row><cell>further removes NSP and dynamically changes the</cell></row><row><cell>masking pattern applied to the training data. In</cell></row><row><cell>addition to the dataset used for training BERT, the</cell></row><row><cell>training dataset here includes CC-News, OpenWeb-</cell></row><row><cell>Text, and Stories as well.</cell></row><row><cell>XLNet: is a generalized autoregressive pre-trained</cell></row><row><cell>language model, known as a permuted language</cell></row><row><cell>model (PLM). It is trained by maximizing the ex-</cell></row><row><cell>pected likelihood over all permutations of the fac-</cell></row><row><cell>torization order of the input text to approximately</cell></row><row><cell>consider bidirectional context. In addition to the</cell></row><row><cell>dataset used for training BERT, the training dataset</cell></row><row><cell>includes Giga5, ClueWeb, and CC as well.</cell></row><row><cell>SpanBERT: is trained with (1) a masked language</cell></row><row><cell>model with random spans (contiguous tokens) and</cell></row><row><cell>(2) span boundary token prediction in the masked</cell></row><row><cell>span. Unlike the original BERT, SpanBERT does</cell></row><row><cell>not include the NSP task. The dataset used for</cell></row><row><cell>training is the same as that for BERT.</cell></row></table><note>DeBERTa: is a modified variant of RoBERTa. It uses disentangled attention and an enhanced mask decoder. During training, it masks spans randomly as for SpanBERT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>? 66.6 (0.18) ? 55.8 (0.34) ? 53.8 (0.37) ? 75.7 (0.45) ? 56.1 (0.55) ? 48.7 (0.63) ? 41.5 (0.47) ? XLNet 77.8 (0.16) ? 67.4 (0.05) ? 57.0 (0.38) ? 54.8 (0.41) ? 74.3 (0.13) ? 55.2 (0.40) ? 47.0 (0.57) ? 40.2 (0.51) ? SpanBERT 76.5 (0.59) ? 65.4 (0.37) ? 54.5 (1.01) ? 52.2 (0.84) ? 73.7 (0.92) ? 54.5 (0.88) ? 42.7 (0.19) ? 36.7 (0.22) ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">RST-DT</cell><cell></cell><cell></cell><cell cols="2">Instr-DT</cell></row><row><cell></cell><cell></cell><cell>Span</cell><cell>Nuc.</cell><cell>Rel.</cell><cell>Full</cell><cell>Span</cell><cell>Nuc.</cell><cell>Rel.</cell><cell>Full</cell></row><row><cell cols="2">Zhang et al.</cell><cell>76.3</cell><cell>65.5</cell><cell>55.6</cell><cell>53.8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Top-down</cell><cell cols="9">BERT RoBERTa 77.3 (0.13) DeBERTa 69.8 (0.64) 59.1 (0.48) 48.3 (0.43) 46.6 (0.52) 65.3 (0.81) 44.6 (1.32) 37.6 (0.38) 30.9 (0.33) 78.5 (0.14) ? 67.9 (0.23) ? 56.6 (0.11) ? 54.4 (0.23) ? 77.3 (0.66) ? 57.9 (0.47) ? 50.0 (0.38) ? 43.4 (0.26) ?</cell></row><row><cell>Bottom-up</cell><cell cols="9">BERT RoBERTa XLNet SpanBERT 76.1 (0.37) ? 65.3 (0.54) ? 54.9 (0.13) ? 52.7 (0.32) ? 72.9 (1.26) ? 53.8 (1.97) ? 46.0 (1.28) ? 40.5 (1.32) ? 68.3 (0.60) 57.8 (0.69) 47.8 (1.04) 46.0 (0.82) 66.6 (1.04) 46.3 (1.63) 39.5 (1.59) 32.9 (1.20) 76.1 (0.33) ? 66.5 (0.23) ? 55.4 (0.38) ? 53.7 (0.15) ? 73.2 (0.85) ? 55.5 (0.25) ? 47.9 (0.70) ? 41.4 (0.38) ? 76.1 (0.67) ? 65.9 (0.19) ? 56.3 (0.55) ? 54.2 (0.48) ? 73.6 (1.31) ? 56.4 (1.59) ? 47.4 (0.47) ? 40.7 (1.24) ?</cell></row><row><cell></cell><cell>DeBERTa</cell><cell cols="8">77.8 (0.31) 68.0 (0.48) 57.3 (0.21) 55.4 (0.36) 77.8 (0.59) 60.0 (1.26) 51.4 (1.40) 44.4 (1.20)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">). The results show that DeBERTa signifi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">cantly outperformed BERT, SpanBERT, and some-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">times significantly outperformed RoBERTa and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">XLNet in top-down parsing. In contrast, it signifi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">cantly outperformed BERT, RoBERTa, XLNet, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">SpanBERT in bottom-up parsing. RoBERTa and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">XLNet obtained similar results; they significantly</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">outperformed BERT, and sometimes significantly</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">outperformed SpanBERT. While SpanBERT only</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">significantly outperformed BERT, it sometimes has</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">comparable performance to RoBERTa and XLNet.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">In particular, the bottom-up parser with DeBERTa</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">outperformed Span, Nuc., Rel., and Full scores of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results with various language models (Standard-Parseval). Standard deviations for three runs are shown in parentheses. The best score is indicated in bold.indicates significantly better than any model except De-BERTa. ? indicates significantly better than BERT, XLNet, and SpanBERT. ? indicates significantly better than BERT, RoBERTa, and SpanBERT. ? indicates significantly better than BERT and SpanBERT. ? indicates significantly better than BERT.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">RST-DT</cell><cell></cell><cell></cell><cell cols="2">Instr-DT</cell></row><row><cell></cell><cell>Span</cell><cell>Nuc.</cell><cell>Rel.</cell><cell>Full</cell><cell>Span</cell><cell>Nuc.</cell><cell>Rel.</cell><cell>Full</cell></row><row><cell>Top-down</cell><cell cols="8">BERT RoBERTa XLNet SpanBERT 94.1 (0.15) 88.8 (0.19) 79.4 (0.49) 78.5 (0.39) 85.5 (0.68) 71.9 (0.69) 52.4 (1.33) 48.5 (1.60) 92.6 (0.53) 85.7 (0.41) 75.4 (0.45) 74.7 (0.54) 82.7 (0.91) 69.1 (0.42) 52.7 (1.40) 47.4 (1.09) 94.1 (0.46) 88.4 (0.46) 79.6 (0.17) 78.7 (0.11) 85.9 (0.82) 72.3 (1.14) 56.9 (1.00) 52.3 (1.08) 94.8 (0.39) 89.5 (0.39) 80.5 (0.59) 79.5 (0.53) 85.8 (0.23) 73.2 (0.90) 58.4 (1.06) 54.2 (1.43) DeBERTa 94.2 (0.33) 89.0 (0.16) 80.1 (0.43) 79.1 (0.32) 86.9 (0.34) 73.6 (0.41) 58.2 (0.23) 53.8 (0.38)</cell></row><row><cell>Bottom-up</cell><cell cols="8">BERT RoBERTa XLNet SpanBERT 93.9 (0.24) 88.2 (0.19) 79.3 (0.37) 78.4 (0.29) 84.4 (0.53) 71.6 (1.52) 58.6 (0.50) 54.4 (1.01) 91.9 (0.34) 84.4 (0.31) 74.4 (0.37) 73.8 (0.30) 84.5 (0.42) 69.4 (1.17) 53.7 (1.04) 48.8 (1.42) 94.4 (0.12) 89.0 (0.34) 80.4 (0.47) 79.7 (0.51) 85.4 (0.99) 72.4 (0.65) 57.8 (0.49) 53.4 (0.60) 94.7 (0.31) 89.4 (0.24) 81.2 (0.27) 80.4 (0.34) 85.8 (0.84) 74.3 (1.79) 59.0 (1.88) 54.9 (2.64) DeBERTa 94.6 (0.38) 89.8 (0.65) 81.0 (0.64) 80.2 (0.70) 87.3 (0.69) 76.0 (0.90) 60.7 (1.53) 56.7 (1.46)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results for nuclearity prediction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Parameter search space in our experiments.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Building strong baselines can be recently regarded as an essential issue for various NLP tasks<ref type="bibr" target="#b25">(Liang et al., 2019;</ref><ref type="bibr" target="#b37">Suzuki et al., 2018;</ref><ref type="bibr" target="#b6">Denkowski and Neubig, 2017)</ref>.2 Note that we introduced some minor modifications, as we mention in Section 3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4"><ref type="bibr" target="#b22">Koto et al. (2021)</ref> reported that jointly predicting nuclearity and relation labels would improve performance. However, this might be due to the high-frequency label bias, and the total performance degraded in macro averaging. Thus, we predict them independently.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The learning rate was determined by using the development dataset.6  In this paper, we set the significance level (?) to 0.05.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We compared vanilla top-down and bottom-up parsers under the same conditions. We do not try to discuss which could be better as a parser being decorated with new methodologies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Note that Standard-Parseval cannot be applied in this setting because leaf nodes in the gold and predicted RSTtrees are not necessarily in one-to-one correspondence. See Appendix B for more detail.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Part of this work was supported by JSPS KAK-ENHI Grant Numbers P21H03505.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dt Span</forename><surname>Instr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nuc</surname></persName>
		</author>
		<idno>0.77) 76.4 (0.80) 64.7 (0.50) 48.0 (1.01) 47.1 (1.02) XLNet 71.9 (1.00) 54.2 (1.20) 40.3 (1.23) 39.5 (1.04) 76.8 (1.17) 65.1 (0.73</idno>
		<imprint>
			<biblScope unit="volume">72</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Rel. Full Span Nuc. Rel. Full</note>
	<note>XLNet 73.8 (0.61) 55.7 (0.96) 41.7 (0.84) 40.5 (0.76) 76.4 (0.12) 64.3 (0.34) 44.3 (1.11) 43.2 (1.06) SpanBERT. 9 (1.41) 38.0 (2.02) 36.4 (1.94) RoBERTa 72.1 (0.11) 53.9 (0.81) 39.9 (0.59) 38.9. 4 (1.76) 45.2 (1.76) SpanBERT 71.7 (0.43) 53.4 (0.96) 39.6 (1.24) 38.7 (1.23) 76.2 (1.29) 64.1 (1.36) 45.6 (1.01) 44.7 (0.94) DeBERTa 74.3 (0.65) 57.2 (1.08) 42.9 (0.42) 41.8 (0.55) 80.6 (0.67) 68.3 (1.44) 50.2 (0.79) 48.8 (0.64</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Table 5: Results for multi-sentential parsing with various language models (RST-Parseval)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rst-Dt Instr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dt</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N-S S-N N-N N-S S-N N-N</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: A practical and powerful approach to multiple testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosef</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2212" to="2218" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stronger baselines for trustable results in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3203</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel discourse parser based on support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pytorch lightning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning,3" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text-level discourse parsing with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discern: Discourse-aware entailment reasoning network for conversational machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.191</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2439" to="2449" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coreference for discourse parsing: A neural approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorii</forename><surname>Guz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.codi-1.17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computational Approaches to Discourse</title>
		<meeting>the First Workshop on Computational Approaches to Discourse</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unleashing the power of neural discourse parsers -a context and structure aware approach using large scale pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorii</forename><surname>Guz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.337</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3794" to="3805" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeBERTa: Decodingenhanced BERT with disentangled attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast rhetorical structure theory discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<idno>abs/1505.02425</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CODRA: A novel discriminative framework for rhetorical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00226</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="435" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>abs/2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Top-down rst parsing utilizing granularity levels in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6321</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8099" to="8106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving neural RST parsing model with silver agreement subtrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1600" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Top-down discourse parsing via sequence labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajri</forename><surname>Koto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="715" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1220</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discourse parsing with attention-based hierarchical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Strong and simple baselines for multimodal utterance embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><forename type="middle">Chong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1267</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2599" to="2609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A unified linear-time framework for sentence-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathyusha</forename><surname>Jwalapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4190" to="4200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting discourse-level segmentation for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5415</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization</title>
		<meeting>the 2nd Workshop on New Frontiers in Summarization<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary Ellen Okurowski Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<title level="m">RST Discourse Treebank. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rhetorical structure theory: A theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thompson</surname></persName>
		</author>
		<idno>ISI/RS-87-190</idno>
		<imprint>
			<date type="published" when="1987" />
			<publisher>USC/ISI</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How much progress have we made on RST discourse parsing? a replication study of recent results on the RST-DT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1319" to="1324" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">RST parsing from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Tung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1613" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A classifier-based parser with linear run-time complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology</title>
		<meeting>the Ninth International Workshop on Parsing Technology<address><addrLine>Vancouver, British Columbia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An end-to-end document-level neural discourse parser exploiting multi-granularity representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2012.11169</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An effective discourse parser that uses rich linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boulder</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="566" to="574" />
		</imprint>
	</monogr>
	<note>Proceedings of Human Language Tech</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An empirical study of building a strong baseline for constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A two-stage parsing method for text-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="184" to="188" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discourse-aware neural extractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5021" to="5031" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transition-based neural RST parsing with implicit syntax features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial learning for discourse rhetorical structure parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.305</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3946" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A top-down neural architecture towards text-level parsing of discourse rhetorical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6386" to="6395" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
