<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Descent for Visual 3D Human Pose and Shape</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
							<email>andreiz@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
							<email>egbazavan@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
							<email>mihaiz@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<email>wfreeman@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>sukthankar@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<email>sminchisescu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Descent for Visual 3D Human Pose and Shape</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present deep neural network methodology to reconstruct the 3d pose and shape of people, including hand gestures and facial expression, given an input RGB image. We rely on a recently introduced, expressive full body statistical 3d human model, GHUM, trained end-to-end, and learn to reconstruct its pose and shape state in a self-supervised regime. Central to our methodology, is a learning to learn and optimize approach, referred to as HUman Neural Descent (HUND), which avoids both second-order differentiation when training the model parameters, and expensive state gradient descent in order to accurately minimize a semantic differentiable rendering loss at test time. Instead, we rely on novel recurrent stages to update the pose and shape parameters such that not only losses are minimized effectively, but the process is meta-regularized in order to ensure endprogress. HUND's symmetry between training and testing makes it the first 3d human sensing architecture to natively support different operating regimes including self-supervised ones. In diverse tests, we show that HUND achieves very competitive results in datasets like H3.6M and 3DPW, as well as good quality 3d reconstructions for complex imagery collected in-the-wild.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic 3d human sensing from images and video would be a key, transformative enabler in areas as diverse as clothing virtual apparel try-on, fitness, personal well-being, health or rehabilitation, AR and VR for improved communication or collaboration, self-driving systems with emphasis to urban scenarios, special effects, human-computer interaction or gaming, among others. Applications in shopping, telepresence or fitness would increase human engagement and stimulate collaboration, communication, and the economy, during a lock-down.</p><p>The rapid progress in 3D human sensing has recently relied on volumetric statistical human body models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43]</ref> and supervised training. Most, if not all, state of the art architectures for predicting 2d, e.g., body keypoints <ref type="bibr" target="#b4">[5]</ref> or 3d, e.g., body joints, kinematic pose and shape <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref> rely, ab initio, at their learning core, on complete supervision. For 2d methods this primarily enters as keypoint or semantic segmentation annotations by humans, but for complex 3D articulated structures human annotation is both impractical and inaccurate. Hence for most methods, supervision comes in the form of synchronous 2d and 3d ground truth, mostly available in motion capture datasets like Human3.6M <ref type="bibr" target="#b12">[13]</ref> and more recently also 3DPW <ref type="bibr" target="#b40">[41]</ref>. Supervision-types aside, the other key ingredient of any successful system is the interplay between 3d initialization using neural networks and non-linear optimization (refinement) based on losses computed over image primitives like keypoints, silhouettes, or body part semantic segmentation maps. No existing feedforward system, particularly a monocular one, achieves both plausible 3d reconstruction and veridical image alignment 1 without non-linear optimizationa key component whose effectiveness for 3d pose estimation has been long since demonstrated <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>The challenge faced by applying non-linear optimization in high-dimensional problems like 3d human pose and shape estimation stems from its complexity. On one hand, firstorder model state updates are relatively inefficient for very ill-conditioned problems like monocular 3d human pose estimation where Hessian condition numbers in the 10 ?3 are typical <ref type="bibr" target="#b33">[34]</ref>. Consequently, many iterations are usually necessary for good results, even when BFGS approximations are used. On the other hand, nonlinear output state optimization is difficult to integrate as part of parameter learning, since correct back-propagation would require potentially complex, computationally expensive second-order updates, for the associated layers. Such considerations have inspired some authors <ref type="bibr" target="#b18">[19]</ref> to replace an otherwise desirable integrated learning process, with a dual system approach, where multiple non-linear optimization stages, supplying potentially improved 3d output state targets, are interleaved with classical supervised learning based on synchronized 2d and 3d data obtained by imputation. Such intuitive ideas have been shown to be effective practically, but remain expensive in training, and lack not just an explicit, integrated cost function, but also a consistent learning procedure to guarantee progress, in principle. Moreover, applying the system symmetrically, during testing, would still require potentially expensive non-linear optimization for precise image alignment.</p><p>In this paper, we take a different approach and replace the non-linear gradient refinement stage at the end of a classical 3d predictive architecture with neural descent, in a model called HUND (Human Neural Descent). In HUND, recurrent neural network stages refine the state output (in this case the 3d human pose and shape of a statistical GHUM model <ref type="bibr" target="#b42">[43]</ref>) based on previous state estimates, loss values, and a context encoding of the input image, similarly in spirit to non-linear optimization. However, differently from models relying on gradient-based back-ends, HUND can be trained end-toend using stochastic gradient descent, offers no asymmetry between training and testing, supports the possibility of potentially more complex, problem-dependent step updates compared to non-linear optimization, and is significantly faster. Moreover, by using such an architecture, symmetric in training and testing, with capability of refinement and self-consistency, we show, for the first time, that a 3d human pose and shape estimation system trained from monocular images can entirely bootstrap itself. The system would thus no longer necessarily require, the completely synchronous supervision, in the form of images and corresponding 3d ground truth configurations that has been previously unavoidable. Experiments in several datasets, ablation studies, and qualitative results in challenging imagery support and illustrate the main claims.</p><p>Related Work: There is considerable prior work in 3d human modeling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7]</ref>, as well as the associated learning and optimization techniques <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3]</ref>. Systems combining either random 3d initialization or prediction from neural networks with non-linear optimization using losses expressed in terms of alignment to keypoints and body semantic segmentation masks exist <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b18">19]</ref>. Black-box optimization has gained more interest in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, usually deployed in the context of meta-learning <ref type="bibr" target="#b10">[11]</ref>. Our work is inspired in part by that of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> in which the authors introduce recurrent mechanisms to solve optimization problems, albeit in a different domain and for other representations than the ones considered in this work. <ref type="bibr" target="#b27">[28]</ref> uses a neural network to directly regress the pose and shape parameters of a 3d body model from predicted body semantic segmentation. The network is trained in a mixed supervision regime, with either full supervision for the body model parameters or a weak supervision based on a 2d reprojection loss. <ref type="bibr" target="#b41">[42]</ref> propose to learn a series of linear regressors over SIFT <ref type="bibr" target="#b24">[25]</ref> features that produce descent directions analogous to an optimization algorithm for face alignment. Training is fully supervised based on 2d landmarks. Similarly, <ref type="bibr" target="#b38">[39]</ref> learn a recurrent network, that given an input image of a face, iteratively refines face landmark predictions. The network is trained fully supervised and operates only in the 2d domain. In <ref type="bibr" target="#b37">[38]</ref>, a cascade of linear regressors are learned to refine the 3d parameters of a 3d face model. Training is done over the entire dataset at a time (multiple persons with multiple associated face images) on synthetic data, in a simulated, mixed supervision regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>We describe the elements of the proposed methodology, including the statistical 3D human body model GHUM, as well as the associated learning and reconstruction architecture used. We also cover the fusion of multiple architectures in order to obtain accurate estimates for the full body including hand gestures and facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Statistical 3D Human Body Model GHUM</head><p>We use a recently introduced statistical 3d human body model called GHUM <ref type="bibr" target="#b42">[43]</ref>, to represent the pose and the shape of the human body. The model has been trained endto-end, in a deep learning framework, using a large corpus of over 60,000 diverse human shapes, and 540, 000 human motions, consisting of 390,000 samples from CMU and 150,000 samples from Human3.6M (subjects S1, S5, S6, S7, S8). The model has generative body shape and facial expressions ? = (? b , ? f ) represented using deep variational auto-encoders and generative pose ? = (? b , ? lh , ? rh ) for the body, left and right hands respectively represented using normalizing flows <ref type="bibr" target="#b45">[46]</ref>. We assume a separable prior on the model pose and shape state p(?, ?) = p(?) + p(?) where Gaussian components with 0 mean and unit I covariance, as typical in variational encoder and normalizing flow models. Given a monocular RGB image as input, our objective is to infer the pose ? ? R Np?1 and shape ? ? R Ns?1 state variables, where N p is the number of posing variables and N s is the length of the shape code, respectively. A posed mesh M(?, ?) has N v associated 3d vertices V = {v i , i = 1 . . . N v } with fixed topology given by the GHUM template. Because the rigid transformation of the model in camera space -represented by a 6d rotation <ref type="bibr" target="#b49">[50]</ref> r ? R 6?1 and a translation vector t ? R 3?1 -are important and require special handling, we will write them explicitly. The posed mesh thus writes M(?, ?, r, t). These estimates (and at later stages similar ones obtained recursively), together with the value of a semantic alignment loss Lu, expressed in terms of keypoint correspondences and differentiable rendering measures between model predictions and associated image structures, are fed into multiple refining RNN layers, with shared parameters ?, and internal memory (hidden state) m. The alignment losses (which can be unsupervised, weakly-supervised or self-supervised, depending on available data) at multiple recurrent stages M are aggregated into a learning loss L l , optimized as part of the learning-to-learn process. The parameters are obtained using stochastic gradient descent, as typical in deep learning. The model produces refined state estimates s with precise image alignment, but does not require additional gradient calculations for the recurrent stages neither in training (e.g., second-order parameter updates), nor during testing (first-order state updates). It is also extremely efficient computationally compared to models relying on nonlinear state optimization at test time. <ref type="figure">Figure 2</ref>. Our complete full body 3d sensing HUND network combines a feed-forward architecture to detect landmarks and semantically segment body parts with an attention mechanism that further processes the face, hands and the rest of the body as separate HUND predictive networks, with results fused in order to obtain the final, full body estimate. See <ref type="figure" target="#fig_0">fig.1</ref> for the architecture of an individual HUND network. Camera model. We assume a pinhole camera with intrinsics C = [f x , f y , c x , c y ] and associated perspective projection operator x 2d = ?(x 3d , C), where x 3d is any 3d point. During training and testing, intrinsics for the full input image are approximated, f x = max(H, W ), f y = max(H, W ), c x = W/2, c y = H/2, where H, W are the input dimensions. Our method works with cropped boundingboxes of humans, re-scaled to a fixed size of 480 ? 480, therefore we need to warp the image intrinsics C into the corresponding crop intrinsics C c</p><formula xml:id="formula_0">[C c 1] = K[C 1] ,<label>(1)</label></formula><p>where K ? R 5?5 is the scale and translation matrix, adapting the image intrinsics C. By using cropped intrinsics, we effectively solve for the state of the 3d model (including global scene translation) in the camera space of the input image. For multiple detections in the same image, the resulting 3d meshes are estimated relative to a common world coordinate system, into the same 3d scene. At test time, when switching C c with C, the 3d model projection will also align with the corresponding person layout in the initial image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning Architecture</head><p>The network takes as input a cropped human detection and resizes it to 480 ? 480. A multi-stage sub-network produces features F ? R 60?60?256 , keypoint detection heatmaps K ? R 60?60?137 and body-part segmentation maps B ? R 60?60?15 . These are embedded into a lowdimensional space, producing a code vector s c -the superscript c stands for context, i.e. the optimization's objective function context. We also append the cropped camera intrinsics C c to this context vector. At training time, a estimate s 0 of the initial GHUM state s = [? , ? , r , t ] is also produced. To simulate model refinement 2 , we employ a Recurrent Neural Network module RNN ? (s c , s i , m i ), where m i is the memory (hidden state) at refinement stage i, and unroll the updates into M stages (see <ref type="figure" target="#fig_0">fig.1</ref>)</p><formula xml:id="formula_1">s i m i = RNN ? (s i?1 , m i?1 , L i?1 u , s c ).<label>(2)</label></formula><p>The loss at each stage i is computed based on the labeling available at training time in the form of either 2d or 3d annotations. When both are missing, we are training with self-supervision. The self-supervised loss at each unit 2 HMR <ref type="bibr" target="#b15">[16]</ref> uses several recursive output layers on top of a CNN prediction. However HMR does not use a formal RNN to recursively refine outputs based on a memory structure encompassing the previous estimates, the image reprojection (keypoint and semantic) error and the image feature code, as we do, which is the equivalent of a complete non-linear optimization context. Nor do we use a discriminator for pose as HMR, but instead rely on the kinematic normalizing flow prior of GHUM. Hence our approach is methodologically very different. See ?3 for quantitative evaluation. processing stage i can be expressed as</p><formula xml:id="formula_2">L i u (s, K, B) = ? k L k (s i , K) + ? b L b (s i , B) + l(? i , ? i ),<label>(3)</label></formula><p>where l = ? log(p), L k is a 2d keypoint alignment loss, L b is a 2d semantic body part alignment (defined in terms of differentiable rendering), and M is the total number of training LSTM stages, while ? k and ? b are cross-validated scalar values which balance the loss terms.</p><p>The keypoint alignment loss, L k , measures the reprojection error of the GHUM's model 3d joints w.r.t. the predicted 2d keypoints. The loss is defined as the 2d mean-per-joint position error (MPJPE)</p><formula xml:id="formula_3">L k (s t , K) = 1 N j Nj i j i (K) ? ?(J i (s t ), C c ) 2 . (4)</formula><p>with N j keypoints, j i (K) is the 2d location of the i-th 2d keypoint extracted from the the K heatmap, and J i (s t ) is the i-th 3d keypoint computed by posing the GHUM model at s t .</p><p>The body-part alignment loss, L b , uses the current prediction s t to create a body-part semantic segmentation image I(M(s t ), C c ) ? R H?W ?15 . Then we follow a soft differentiable rasterization process <ref type="bibr" target="#b22">[23]</ref> to fuse probabilistic contributions of all predicted mesh triangles of the model, at its current state, with respect to the rendered pixels. In this way, gradients can flow to the occluded and far-range vertices. To be able to aggregate occlusion states and semantic information, we append to each mesh vertex its semantic label, as a one-hot vector {0, 1} 15?1 , and a constant alpha value of 1. The target body part semantic probability maps B are also appended with a visibility value, equal to the foreground probability ? [0, 1] H?W ?1 . The loss is the mean-per-pixel absolute value of the difference between the estimated and predicted semantic segmentation maps</p><formula xml:id="formula_4">L b (s t , B) = 1 HW HW i B i ? I(M(s t ), C c ) i 1 .<label>(5)</label></formula><p>For body shape and pose, we include two regularizers, proportional to the negative log-likelihood of their associated Gaussian distributions</p><formula xml:id="formula_5">l(?) = ? log p(?) = ? 2 2 , l(?) = ? log p(?) = ? 2 2 .<label>(6)</label></formula><p>When 3d supervision is available, we use the following unit training loss L i f , as well as, potentially, the other ones previously introduced in (3) for the self-supervised regime</p><formula xml:id="formula_6">L i f (s) = ? m L m (M(s i ), M) + ? 3d L 3d (J(s i ), J),</formula><p>where L m represents the 3d vertex error between the ground-truth mesh M and a predicted one, M(s i )obtained by posing the GHUM model using the predicted state s i ; L 3d is the 3d MPJPE between the 3d joints recovered from the predicted GHUM parameters, J(s i ), and the ground-truth 3d joints, J; ? m and ? 3d are scalar values that balance the two terms.</p><p>For learning, we consider different losses L l , including 'sum', 'last', 'min' or 'max', as follows</p><formula xml:id="formula_7">L ? u (s, K, B) = M i=1 L i u (s i , K, B) L ? u (s, K, B) = L M u (s M , K, B) L min u (s, K, B) = M min i=1 L i u (s i , K, B) L max u (s, K, B) = M max i=1 L i u (s i , K, B)<label>(7)</label></formula><p>We also consider an observable improvement (OI) loss for</p><formula xml:id="formula_8">L l [11] L oi u = M i=1 min{L i u ? min j&lt;i L j u , 0}.<label>(8)</label></formula><p>Multiple HUND networks for Body Pose, Shape, and Facial Expressions. Capturing the main body pose but also hand gestures and facial expressions using a single network is challenging due to the very different scales of each region statistics. To improve robustness and flexibility we rely on 4 part networks, one specialized for facial expressions, two for the hands, and one for the rest of the body. Based on an initial person keypoint detection and semantic segmentation, we drive attention to face and hand regions as identified by landmarks and semantic maps, in order to process those features in more detail. This results in multiple HUND networks being trained, with estimates for the full body shape and pose fused in a subsequent step from parts ( <ref type="figure">fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>View of Experimental Protocols. There is large variety of models and methods now available for 3d human sensing research, including body models like SMPL <ref type="bibr" target="#b23">[24]</ref> and GHUM <ref type="bibr" target="#b42">[43]</ref>, or reconstruction methods like DMHS <ref type="bibr" target="#b29">[30]</ref>, HMR <ref type="bibr" target="#b15">[16]</ref>, SPIN <ref type="bibr" target="#b18">[19]</ref> etc., set aside methods that combine random initialization or neural network prediction and nonlinear refinement <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">47]</ref>. To make things even more complex, some models are pre-trained on different 2d or 3d datasets and refined on others. A considerable part of this development has a historical trace, with models built on top of each-other and inheriting their structure and training sets, as available at different moments in time. Set that aside, multiple protocols are used for testing. For Human3.6M <ref type="bibr" target="#b12">[13]</ref> only, there are at least 4: the ones originally proposed by the dataset creators, on the withheld test set of Human3.6M (or the representative subset Human80K <ref type="bibr" target="#b11">[12]</ref>) as well as others, created by various authors, known as protocol 1 and 2 by re-partitioning the original training and validation sets for which ground truth is available. Out of these 2, only protocol 1 is sufficiently solid in the sense of providing a reasonably large and diverse test set for stable statistics (e.g., 110,000 images from different views in P1 vs. 13,700 in P2, from the same camera, at the same training set size of 312,000 configurations for both). Hence we use P1 for ablations and the official Human3.6M test set for more relevant comparisons. For some of the competing methods, e.g. SPIN <ref type="bibr" target="#b18">[19]</ref>, HMR <ref type="bibr" target="#b15">[16]</ref> we ran the code from the released github repositories ourselves on the Human3.6M test set since numbers were not reported in the original publications. Results are presented in table 2. We will also use 3DPW <ref type="bibr" target="#b40">[41]</ref> for similar reasons, or rather, in the absence of other options in the wild (30,150 training and 33,000 testing configurations). Testing all other model combinations would be both impractical and irrelevant, especially for new models like GHUM where most prior combinations are unavailable and impossible to replicate. As a matter of principle, 3D reconstruction models can be evaluated based on the amount of supervision received, be it 2d (for training landmark detectors #2d det or, additionally, for direct 3d learning #2d), # 3d, or synchronized #2d-3d annotations, the number of images used for self-supervision #I, as well as perhaps number of parameters and run-time. In addition, ablations for each model, e.g., HUND, would offer insights into different components and their relevance. We argue in support of this being one scientifically sound way of promoting diversity in the creation of new models and methods, rather than closing towards premature methodological convergence, weakly supported by unsustainable, ad-hoc, experimental combinatorics. For our self-supervised (SS) experiments, we employ two datasets containing images in-the-wild, COCO2017 <ref type="bibr" target="#b21">[22]</ref> (30,000 images) and OpenImages <ref type="bibr" target="#b20">[21]</ref> (24,000), with no annotations in training and testing. We refer to weaklysupervised (WS) experiments as those where ground truth annotations are available, e.g. human body keypoints. We do not rely on these but some other techniques including HMR and SPIN do, hence we make this distinction in order to correctly reflect their supervision level.</p><p>For fully supervised (FS) experiments, we employ H3.6M and 3DPW. Because we work with the newly released GHUM model, we retarget the mocap raw marker data from H3.6M to obtain accurate 3d mesh supervision for our model <ref type="bibr" target="#b42">[43]</ref>. Because the ground-truth of 3DPW is provided as SMPL 3d meshes, we fit the GHUM model by using an objective function minimizing vertex-to-vertex distances between the two corresponding meshes. Architecture Implementation. To predict single-person  <ref type="table">Table 1</ref>. Performance of different pose and shape estimation methods on the H3.6M dataset, with training/testing based on the representative protocol P1 (for self-supervised variants this only indicates the images used in testing). MPJPE-PA and MPJPE are expressed in mm. We also report the global translation of the body as this is supported by our fully perspective camera model (N.B. this is not supported by other methods which use an orthographic perspective model). We also compare different annotations used in the construction of different models, with a split into 2d (further differentiated into #2d det for training the joint landmarks and #2d for training the 3d learning algorithm), 3d and synchronized 2d-3d. The last column gives the number of images for self-supervised variants, e.g., HUND(SS), which do not use either 2d image keypoints or synchronized images and 3d mocap during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MPJPE HMR (FS+WS) <ref type="bibr" target="#b15">[16]</ref> 89 SPIN (FS+WS) <ref type="bibr" target="#b18">[19]</ref> 68 HUND (FS+SS) 66 <ref type="table">Table 2</ref>. Results of different methods on the H3.6M official held-out test set. We achieve better results on a large test set of 900k images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MPJPE-PA (mm) MPJPE (mm) HMR (FS+WS) <ref type="bibr" target="#b15">[16]</ref> 81.3 130.0 SPIN (FS+WS) <ref type="bibr" target="#b18">[19]</ref> 59.2 96.9 ExPose (FS+WS) <ref type="bibr" target="#b6">[7]</ref> 60.7 93.4 HUND (SS) 63.5 90.4 HUND (FS+SS) 57.5 81.4 <ref type="table">Table 3</ref>. Results on the 3DPW test set for different methods. Notice that a self-supervised version of HUND produces lower errors compared to the best supervised HMR implementation that includes not just synchronized 2d ? 3d training sets but also images with 2d annotation ground truth. A HUND model that includes asynchronous 2d-3d supervision, in addition to just unlabeled images, achieves the lowest error, and uses less training data than any other competitive method -see also table <ref type="bibr">1.</ref> keypoints and body part segmentation, we train a multi-task network with ResNet50 <ref type="bibr" target="#b8">[9]</ref> backbone (the first CNN in our pipeline, see <ref type="figure" target="#fig_0">fig.1 and fig.2</ref>) <ref type="bibr" target="#b29">[30]</ref>. We have 137 2d keypoints as in <ref type="bibr" target="#b3">[4]</ref> and 15 body part labels, as in <ref type="bibr" target="#b31">[32]</ref>. This network has 34M trainable parameters. The self-supervised training protocol for HUND assumes only images are available and we predict 2d body keypoints and body part labels during training, in addition to body shape and pose regularizers. For the embedding model (the second CNN in the pipeline) predicting s c , we use a series of 6 convolutional layers with pooling, followed by a fully connected layer. We use M = 5 LSTM [10] stages as RNNs for HUND and we set the number of units to 256, which translates to 525k parameters. In total there are 950k trainable parameters for the 3D reconstruction model. We train with a batch size of 32 and a learning rate of 10 ?4 for 50 epochs. For experiments where we train HUND using FS+SS, we use a mixed schedule, alternating between self-supervised and fully-supervised batches. Training takes about 72 hours on a single Nvidia Tesla P100 GPU. The runtime of our prediction network for a single image is 0.035s and 0.02s for HUND, on an Nvidia RTX 2080 GPU. Evaluation and discussion. Multiple experiments are run for different regimes. Quantitative results are presented in tables 1, 2 and 3 for Human3.6M and 3DPW respectively. A detailed analysis of optimization behavior for one image is given in <ref type="figure">fig. 3</ref> as well as, in aggregate, in <ref type="figure" target="#fig_1">fig. 4</ref>. Visual reconstructions at different HUND optimization stages, for several images, are given in <ref type="figure" target="#fig_3">fig. 6</ref>.  <ref type="table">Table 4</ref>. Impact assessment of different meta-losses used in HUND (FS), trained on the Human3.6M dataset, following protocol 1. The last and sum losses perform similarly well, with others following at a distance.</p><p>We also study the impact of different meta-learning losses, as given in <ref type="formula" target="#formula_7">(7)</ref> and <ref type="formula" target="#formula_8">(8)</ref>, on the quality of results of HUND. We use a HUND (FS) model trained and evaluated on Hu-man3.6M (protocol 1). From table 4 we observe that the last (L ? f ) and sum (L ? f ) losses perform best, whereas others produce considerably less competitive results, by some margin, for this problem. Finally, we show qualitative visual 3d reconstruction results, from several viewpoints, for a variety of difficult poses and backgrounds in <ref type="figure" target="#fig_2">fig. 5</ref>. Please see our Sup. Mat. for videos! Ethical Considerations. Our methodology aims to decrease bias by introducing flexible forms of self-supervision which would allow, in principle, for system bootstrapping and adaptation to new domains and fair, diverse subject distributions, for which labeled data may be difficult or impossible to collect upfront. Applications like visual surveillance and person identification would not be effectively supported currently, given that model's output does not provide sufficient <ref type="figure">Figure 3</ref>. Behavior of different optimization methods including standard non-linear gradient-based BFGS, HUND(5), as well as variants of HUND(i), i ? 5, initializing BFGS, in order to assess progress and the quality of solutions obtained along the way (left). Corresponding cumulative run-times are shown on the right. Observe that HUND produces a good quality solution orders of magnitude faster than gradient descent (note log-scales on both plots). End refinement using gradient descent improves results, although we do not recommend a hybrid approach-here we only show different hybrids for insight. This shows one optimization trace for a model initialized in A-pose and estimated given one image from Human3.6M, but such behavior is typical of aggregates, see e.g., <ref type="figure" target="#fig_1">fig. 4</ref>. See also <ref type="figure" target="#fig_3">fig. 6</ref> for visual illustrations of different configurations sampled by HUND during optimization. On the left we show per-joint angle averages w.r.t. ground truth. On the right we show running times in aggregate for different types of optimization. One can see that BFGS descent under a keypoint+prior loss tends to be prone to inferior local optima compared to different HUND hybrids, which on average find significantly better solutions. The plot needs to be interpreted in proper context, as aggregates meant to show distance and run-time statistics per iteration. Hence, they may not be entirely representative of any single run, but for a singleton see e.g., <ref type="figure">fig. 3</ref>. detail for these purposes. This is equally true of the creation of potentially adversely-impacting deepfakes, as we do not include an appearance model or a joint audio-visual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We have presented a neural model, HUND, to reconstruct the 3d pose and shape of people, including hand gestures and facial expressions, from image data. In doing so, we rely on an expressive full body statistical 3d human model, GHUM, to capture typical human shape and motion regularities. Even so, accurate reconstruction and continuous learning are challenging because large-scale diverse 3d supervision is difficult to acquire for people, and because the most efficient inference is typically based on non-linear image fitting. This is however difficult to correctly 'supra'-differentiate, to second order, in training and expensive in testing. To address such challenges, we rely on self-supervision based on differentiable rendering within learning-to-learn approaches based on recurrent networks, which avoid expensive gradient descent in testing, yet provide a surrogate for robust loss minimization. HUND is tested and achieves very competitive results for datasets like H3.6M and 3DPW, as well as for complex poses, collected in challenging outdoor conditions. HUND's learning-to-learn and optimize capabilities, and symmetry between training and testing, can make it the first architecture to demonstrate the possibility of bootstraping a plausible 3d human reconstruction model without initial, synchronous (2d, 3d) supervision.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of our Human Neural Descent (HUND) architecture for learning to estimate the state s of a generative human model GHUM (including shape ? and pose ?, as well as person's global rotation r and translation t) from monocular images. Given an input image, a first CNN extracts semantic feature maps for body keypoints (K) and part segmentation (B), as well as other features (F). These, in turn feed, into a second stage CNN that learns to compute a global context code s c as well as an initial estimate of the model state s0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Optimization statistics for different methods, aggregated over 100 different poses (estimation runs) from Human3.6M. We initialize in an A-pose and perform monocular 3d pose and shape reconstruction for GHUM under a HUND (FS+SS) model, as well as non-linear optimization baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Visual 3d reconstruction results obtained by HUND. Given initial 2d predictions for body, face and hand keypoints, and initial predictions for semantic body part labelling, the neural descent network predicts the 3d GHUM pose and shape parameters. Best seen in color. For other examples and videos see our Sup. Mat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Visual 3d pose and shape configurations of GHUM sampled by HUND during optimization. First column shows the input image, columns 2-6 illustrate GHUM estimates at each HUND stage. Columns 7 and 8 show visualizations of the GHUM state from different viewpoints, after HUND terminates. Columns 9, 10 and 11 show close up views for the reconstructed face expressions, left and right hands.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To be understood in the classical model-based vision sense of best fitting the model predictions to implicitly or explicitly-associated image primitives (or landmarks), within modeling accuracy.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, G Martinez</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to learn without gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">G?mez</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12929" to="12941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Meta-learning in neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hu-man3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chris Manafas, and Georgios Tzimiropoulos. 3d human body reconstruction from a single image via volumetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Vibe: Video inference for human body pose and shape estimation. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01786</idno>
		<title level="m">Soft rasterizer: A differentiable renderer for image-based 3d reasoning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Iasonas Kokkinos Riza Alp Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neverova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating Articulated Human Motion with Covariance Scaled Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="371" to="393" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kinematic jump processes for monocular 3d human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Marquez</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regressing 3d face shapes from arbitrary image sets with disentanglement in shape space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bill Freeman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rahul Sukthankar, and Cristian Sminchisescu. GHUM &amp; GHUML: Generative 3D human shape and articulated pose models. CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7760" to="7770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bill Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes -The Importance of Multiple Scene Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07035</idno>
		<title level="m">On the continuity of rotation representations in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
