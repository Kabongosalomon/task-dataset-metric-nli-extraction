<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CenterNet++ for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
						</author>
						<title level="a" type="main">CenterNet++ for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object detection</term>
					<term>bottom-up</term>
					<term>anchor-free</term>
					<term>deep learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are two mainstreams for object detection: top-down and bottom-up. The state-of-the-art approaches mostly belong to the first category. In this paper, we demonstrate that the bottom-up approaches are as competitive as the top-down and enjoy higher recall. Our approach, named CenterNet, detects each object as a triplet keypoints (top-left and bottom-right corners and the center keypoint). We firstly group the corners by some designed cues and further confirm the objects by the center keypoints. The corner keypoints equip the approach with the ability to detect objects of various scales and shapes and the center keypoint avoids the confusion brought by a large number of false-positive proposals. Our approach is a kind of anchor-free detector because it does not need to define explicit anchor boxes. We adapt our approach to the backbones with different structures, i.e., the 'hourglass' like networks and the the 'pyramid' like networks, which detect objects on a single-resolution feature map and multi-resolution feature maps, respectively. On the MS-COCO dataset, CenterNet with Res2Net-101 and Swin-Transformer achieves APs of 53.7% and 57.1%, respectively, outperforming all existing bottom-up detectors and achieving state-of-the-art. We also design a real-time CenterNet, which achieves a good trade-off between accuracy and speed with an AP of 43.6% at 30.5 FPS. https://github.com/Duankaiwen/PyCenterNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the current era, there mainly exist two categories for object detection: the bottom-up detection approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b68">[69]</ref> and the top-down approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b45">[46]</ref>. People believed that the bottom-up approaches may be time-consuming and introduces more false positives, while the top-down approaches have gradually evolved into the mainstream approaches due to their effectiveness in practice. All the top-down approaches model each object as a prior point or a pre-defined anchor box, then predicts the corresponding offsets to the bounding box. They enjoy the ability to perceive the objects as a whole, which simplify the post-process of generating bounding boxes. However, they usually suffer the difficulty to perceive objects with peculiar shapes (e.g., the aspect ratios of the objects are large). <ref type="figure">Fig. 1(a)</ref> shows a case that a top-down approach fails to cover the 'train'. We will give a detailed analysis for this problem in Section 3.1.</p><p>On the other hand, we find that the bottom-up approaches are potentially better in locating objects with arbitrary geometry, and thus have a higher recall. But the traditional bottom-up approaches usually generate many false positives as well, which fails to represent objects accurately. Take the CornerNet <ref type="bibr" target="#b29">[30]</ref> as an example, one of the most representative bottom-up approaches, it models each object using a pair of corner keypoints and achieves state-of-the-art object detection accuracy. Nevertheless, the performance of CornerNet is still restricted by its relatively weak ability to refer to the global information of an object. That is, because each object is constructed by a pair of corners, the algorithm sensitively detects the boundaries of  <ref type="figure">Fig. 1. (a)</ref> The top-down approach Faster R-CNN <ref type="bibr" target="#b47">[48]</ref> often fails to locate the object precisely that has a peculiar shape (an extreme aspect ratio). Blue and red bounding-boxes indicate false positives and false negatives, respectively. (b) We visualize the top 100 bounding boxes (according to the MS-COCO dataset standard) of CornerNet <ref type="bibr" target="#b29">[30]</ref>, which shows a large number of false positives. Blue and red bounding-boxes indicate true positives and false positives, respectively. (c) The idea of the proposed CenterNet. We show that correct predictions can be determined by checking the central parts.</p><p>objects without being aware of which pairs of keypoints that should be grouped into objects. Consequently, as shown in <ref type="figure">Fig. 1(b)</ref>, CornerNet often generates incorrect bounding boxes, most of which could be easily filtered out with some complementary information, e.g., the aspect ratio.</p><p>Driven by the analysis of the bottom-up approaches, our core opinion is that the bottom-up approaches would be necessary and be as competitive as the top-down approaches, as long as we improve their ability to perceive arXiv:2204.08394v1 [cs.CV] 18 Apr 2022 the global information of an object. In this paper, we present a low-cost yet effective solution named CenterNet, a strong bottom-up object detection approach, that detects each object as a triplet keypoints (top-left and bottom-right corners and the center keypoint). CenterNet explores the central part of a proposal, i.e., the region that is close to the geometric center of a box, with one extra keypoint. We intuit that if a predicted bounding box has a high IoU with the ground-truth box, then the probability that the center keypoint in the central region of the bounding box will be predicted as the same class is high, and vice versa. Thus, during inference, after a proposal is generated as a pair of corner keypoints, we determine if the proposal is indeed an object by checking if there is a center keypoint of the same class falling within its central region. The idea is shown in <ref type="figure">Fig. 1(c)</ref>.</p><p>We design two frameworks to adapt to the networks with different structures, the first could fit the 'hourglass' like networks, which detect objects on a single-resolution feature map. This kind of networks are very popular in the keypoint estimation task, e.g., the hourglass network <ref type="bibr" target="#b42">[43]</ref>, which we apply the network to better predict the corners and center keypoints. We also design our framework to fit the 'pyramid' like networks, which detect objects on multi-resolution feature maps. This brings two advantages: Stronger generality-most of networks have 'pyramid' structures, e.g., the ResNet <ref type="bibr" target="#b24">[25]</ref> and its variants; Higher detection accuracy-objects with different scales are detected in different receptive fields. Although the pyramid structure has been widely applied in the top-down approaches, it is, as far as we know, the first time that used in the bottom-up approaches.</p><p>We evaluate the proposed CenterNet on the MS-COCO dataset <ref type="bibr" target="#b36">[37]</ref>, one of the most popular benchmarks for largescale object detection. CenterNet, with Res2Net-101 <ref type="bibr" target="#b17">[18]</ref> and Swin-Transformer <ref type="bibr" target="#b38">[39]</ref>, achieves APs of 53.7 and 57.1, respectively, outperforming all existing bottom-up detectors by a large margin. We also design a real-time CenterNet, which achieves a good trade-off between accuracy and speed with an AP of 43.6 at 30.5 FPS. CenterNet is quite efficient yet closely matches state-of-the-art performance of the other top-down approaches.</p><p>The preliminary version of this paper appeared as <ref type="bibr" target="#b12">[13]</ref>. In this extended journal version, we improve the work from the following aspects. (i) The original CenterNet is only applied to the Hourglass network <ref type="bibr" target="#b42">[43]</ref> as the backbone, which all the objects are only detected in a single-resolution feature map. We extend the idea of CenterNet to make it work in the network that has the pyramid structure, which allows CenterNet detecting objects in multi-resolution feature maps. To this end, we propose new methods for detecting keypoints (including corners and center keypoints) and grouping keypoints, respectively. (ii) In this version, due to our new design of CenterNet, we could try more backbones that have the pyramid structures, such as ResNet <ref type="bibr" target="#b24">[25]</ref>, ResNext <ref type="bibr" target="#b60">[61]</ref> and Res2Net <ref type="bibr" target="#b17">[18]</ref>. We even report the detection results using the backbone of the Transformer <ref type="bibr" target="#b38">[39]</ref>. The experimental results show the detection accuracy is significantly improved by introducing the pyramid structure, which allow the network using the richer receptive fields to detect objects. (iii) We present a real-time CenterNet in this journal version, which achieves a better accuracy/speed trade-off among the popular detectors.</p><p>The main contributions of this work can be summarized as follows:</p><p>? We propose a strong bottom-up object detection approach named CenterNet. It detects each object as a triplet keypoints, which enjoys the ability in locating objects with arbitrary geometry and to perceive the global information within objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We design two frameworks to adapt to the networks with different structures, which enjoys a stronger generality: Our approach could fit almost all of networks.</p><p>? Without bells and whistles, CenterNet achieves stateof-the-art detection accuracy among the bottom-up approaches and closely matches state-of-the-art performance of the other top-down approaches.</p><p>? With properly reduced the structure complexity, Cen-terNet achieves a satisfying trade-off between accuracy and speed. We demonstrate that the bottom-up approaches are necessary and as competitive as the top-down approaches.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews related work, and Section 3 details the proposed CenterNet. Experimental results are given in Section 4, followed by the conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Object detection involves locating and classifying the objects. In the deep learning era, powered by deep convolutional neural networks, object detection approaches can be roughly categorized into two main types of pipelines, namely, top-down approaches and bottom-up approaches. Top-down approaches first find the proposals that represent the whole objects, then further determine the classes and the bounding boxes of the objects by classifying and regressing the proposals. The proposals could be further divided into anchor-based and anchor-free according to the different forms of proposals.</p><p>Anchor-based proposals, which are also called anchors, start with rectangles that have different predefined sizes, scales and shapes. They are uniformly distributed on the feature maps and are trained to regress to the desired place with the help of ground-truth objects. Among which, some approaches pay more attention to the quality of detection results. One of the most representative approaches is R-CNN <ref type="bibr" target="#b21">[22]</ref>. It divides the process of object determination into two stages. Some meaningful proposals are selected out in the first stage, and are further verified in the second stage. After that, a lot of works are proposed to expand it, such as SPPNet <ref type="bibr" target="#b23">[24]</ref>, Fast R-CNN <ref type="bibr" target="#b20">[21]</ref>, Faster R-CNN <ref type="bibr" target="#b47">[48]</ref>, Cascade R-CNN <ref type="bibr" target="#b2">[3]</ref>, MR-CNN <ref type="bibr" target="#b19">[20]</ref>, ION <ref type="bibr" target="#b0">[1]</ref>, OHEM <ref type="bibr" target="#b51">[52]</ref>, HyperNet <ref type="bibr" target="#b28">[29]</ref>, CRAFT <ref type="bibr" target="#b61">[62]</ref>, R-FCN <ref type="bibr" target="#b8">[9]</ref>, FPN <ref type="bibr" target="#b34">[35]</ref>, Libra R-CNN <ref type="bibr" target="#b43">[44]</ref>, Mask R-CNN <ref type="bibr" target="#b22">[23]</ref>, Fitness-NMS <ref type="bibr" target="#b57">[58]</ref>, Grid R-CNN <ref type="bibr" target="#b40">[41]</ref>, TridentNet <ref type="bibr" target="#b33">[34]</ref>, etc. Some other approaches pay more attention to the detection speed. They usually do not have the proposal verification stage. The representative approaches includes SSD <ref type="bibr" target="#b37">[38]</ref>, DSSD <ref type="bibr" target="#b16">[17]</ref>, RON <ref type="bibr" target="#b27">[28]</ref>, YOLOv2 <ref type="bibr" target="#b45">[46]</ref>, RetinaNet <ref type="bibr" target="#b35">[36]</ref>, RefineDet <ref type="bibr" target="#b64">[65]</ref>, AlignDet <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr">TABLE 1</ref> The average recall (AR) of top-down and bottom-up approaches on the MS coco validation dataset. In this experiment, we report the AR computed by targets of different aspect ratios and different sizes. To eliminate the influence of other factors on AR, we exclude the impacts of bounding-box categories and scores on recall and compute it by allowing at most 1000 object proposals. AR 1+ , AR 2+ , AR 3+ and AR 4+ denote box area in the ranges of 96 2 , 200 2 , 200 2 , 300 2 , 300 2 , 400 2 , and 400 2 , +? , respectively. 'X' and 'HG' denote ResNeXt and Hourglass, respectively.  <ref type="bibr" target="#b66">[67]</ref>, GFL <ref type="bibr" target="#b32">[33]</ref>, FreeAnchor <ref type="bibr" target="#b65">[66]</ref>, FSAF <ref type="bibr" target="#b70">[71]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Despite the great success of the application of the anchors, they suffer many drawbacks, e.g., a large number of anchors are often required to ensure a sufficiently high IoU (intersection over union) rate with the ground-truth objects, and the size and aspect ratio of each anchor box need to be manually designed. Therefore, the very neat anchor-free proposals are proposed. The anchor-free proposals abandon the anchors and represent objects as the points within objects. The key of the anchor-free proposals is to the accurately predict the labels of the relative sparse points and the distances from the points to the object borders. Typical approaches including YOLO series <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b44">[45]</ref>, FCOS <ref type="bibr" target="#b55">[56]</ref>, Objects as Points <ref type="bibr" target="#b67">[68]</ref>, FoveaBox <ref type="bibr" target="#b26">[27]</ref>, SAPD <ref type="bibr" target="#b69">[70]</ref>, RepPoints series <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b62">[63]</ref>, etc. Bottom-up approaches detects the individual parts of objects instead of perceiving the objects as a whole. Subsequently, the individual parts that belongs to the same object will be grouped together by some trainable post-process algorithms. The bottom-up approach dates back to the pre deep learning era, Felzenszwalb et al. represent objects using mixtures of multi-scale deformable part models which know as DPM <ref type="bibr" target="#b15">[16]</ref>. Recently, the keypoint estimation <ref type="bibr" target="#b41">[42]</ref> inspires the object detection to recognize the objects by detecting and grouping the keypoints, e.g., CornerNet <ref type="bibr" target="#b29">[30]</ref> and CornerNet-lite <ref type="bibr" target="#b30">[31]</ref> detects objects as paired corners, while ExtremeNet <ref type="bibr" target="#b68">[69]</ref> detects four extreme points (topmost, left-most, bottom-most, right-most) of an object. Since the bottom-up approaches do not need the anchors, they belong to a kind of the anchor-free detectors. Most of the Bottom-up approaches are based on a state-of-the-art keypoint estimation framework <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b59">[60]</ref>, which also brings some drawbacks, e.g., they rely too much on the heatmap with high-resolution, the inference usually too slow, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline and Motivation</head><p>We notice that there are two important choices of object detection, top-down or bottom-up? Based on the discussions in the above sections, we infer that bottom-up approaches have better potential in locating objects with arbitrary geometry, and thus a higher recall. This is because most top-down approaches work by the anchors, which is very empirical (e.g., to improve efficiency, only the anchors with common sizes and aspect ratios are considered) and their shapes and locations are relatively fixed, although the subsequent bounding-box regression process could slightly change their states. Therefore, the detectors tend to miss the object with a peculiar shape. <ref type="figure">Fig. 1</ref>(a) has shown a typical example.</p><p>We also provide a quantitative study, shown in <ref type="table">Table 1</ref>. Three representative approaches as well as our work are evaluated on the MS-COCO validation dataset. <ref type="table">Table 1</ref> shows that the top-down approaches report significantly lower recalls than the bottom-up approaches, especially for the object with peculiar geometry, e.g., with the scale that larger than 300 2 pixels or aspect ratio that larger than 5 : 1. This is no surprise because, on the one hand, for Faster R-CNN <ref type="bibr" target="#b47">[48]</ref>, a typical anchor-based top-down approach, there are no pre-defined anchors could match these objects. On the other hand, for FCOS <ref type="bibr" target="#b55">[56]</ref>, a typical anchor-free topdown approach, it is difficult to accurately regress the long distances from the border to the proposal. Since the bottomup approaches usually detects the individual parts of objects and group them into the objects, they somewhat avoid this problem. Moreover, we report the results of the proposed CenterNet, which demonstrates that CenterNet inherits the merits of the bottom-up approaches for locating objects flexibly, especially with peculiar geometries.</p><p>Although the bottom-up approaches enjoy a high recall, they often generate many false positives. Take Cor-nerNet <ref type="bibr" target="#b29">[30]</ref> as an example, it produces two heatmaps for detecting corners: a heatmap of top-left corners and a heatmap of bottom-right corners. The heatmaps represent the locations of keypoints of different categories and assigns a confidence score for each keypoint. Besides, it also predicts an embedding and a group of offsets for each corner (as shown in <ref type="figure" target="#fig_2">Fig 2)</ref>. The embeddings are used to identify if two corners are from the same object. The offsets learn to remap the corners from the heatmaps to the input image. For generating object bounding boxes, top-k left-top corners and bottom-right corners are selected from the heatmaps according to their scores, respectively. Then, the distance of the embedding vectors of a pair of corners is calculated to determine if the paired corners belong to the same object.   An object bounding box is generated if the distance is less than a threshold. The bounding box is assigned a confidence score, which equals to the average scores of the corner pair.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we provide a deeper analysis of CornerNet. We count the AF 1 (average false discovery) rate of Corner-Net on the MS-COCO validation dataset, defined as the proportion of the incorrect bounding boxes. The quantitative results demonstrate the incorrect bounding boxes account for a large proportion even at low IoU thresholds, e.g., CornerNet obtains 32.7% AF at IoU = 0.05. This means in average, 32.7 out of every 100 object bounding boxes have IoU lower than 0.05 with the ground-truth. The small incorrect bounding boxes are even more, which achieves 60.3% AF. One of the possible reasons lies in that CornerNet cannot look into the regions inside the bounding boxes. To make CornerNet <ref type="bibr" target="#b29">[30]</ref> perceive the visual patterns in bounding boxes, one potential solution is to adapt CornerNet into a two-stage detector, which uses the RoI pooling <ref type="bibr" target="#b20">[21]</ref> to look into the visual patterns in bounding boxes. However, it is known that such a paradigm is computationally expensive.</p><p>In this paper, we propose a highly efficient alternative called CenterNet to explore the visual patterns within each bounding box. For detecting an object, our approach uses a triplet, rather than a pair, of keypoints. By doing so, our approach only pays attention to the center information, the cost of our approach is minimal, but partially inherits the functionality of RoI pooling. Furthermore, we design two frameworks, which detect objects on a single-resolution feature map and on multi-resolution feature maps, respectively. The former is applied to the keypoint estimation network, which we hope to detect the corners and the center keypoints better. While the later is more popular in the object 1. AF = 1 ? AP, where AP denotes the average precision at IoU = [0.05 : 0.05 : 0.5] on the MS-COCO dataset. Also, AF i = 1 ? AP i , where AP i denotes the average precision at IoU = i/100,</p><formula xml:id="formula_0">AF scale = 1 ? AP scale , where scale = {small, medium, large}, denotes the scale of object.</formula><p>detection due the better generality and richer detection receptive fields. The two frameworks are slightly different in design details, we will give a detailed introduction in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Detection as Keypoint Triplets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Single-resolution detection framework</head><p>Inspired by the pose estimation, we apply the networks that commonly used in pose estimation to better detect the corners and center keypoints, most of which detect the keypoints on a single-resolution feature map, e.g., the hourglass network <ref type="bibr" target="#b42">[43]</ref>. The overall network architecture is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. We represent each object by a center keypoint and a pair of corners. Specifically, we embed a heatmap for the center keypoints on the basis of CornerNet and predict the offsets of the center keypoints. Then, we use the method proposed in CornerNet <ref type="bibr" target="#b29">[30]</ref> to generate top-k bounding boxes. However, to effectively filter out the incorrect bounding boxes, we leverage the detected center keypoints and resort to the following procedure: (1) select top-k center keypoints according to their scores; (2) use the corresponding offsets to remap these center keypoints to the input image; (3) define a central region for each bounding box and check if the central region contains center keypoints. Note that the class labels of the checked center keypoints should be same as that of the bounding box; (4) if a center keypoint is detected in the central region, we will preserve the bounding box. The score of the bounding box will be replaced by the average scores of the three points, i.e., the top-left corner, the bottom-right corner and the center keypoint. If there are no center keypoints detected in its central region, the bounding box will be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-resolution detection framework</head><p>The overall network architecture is shown in <ref type="figure">Figure 3</ref>. It starts with a backbone (e.g., the ResNet <ref type="bibr">[</ref>  <ref type="figure">Fig. 3</ref>. Multi-resolution detection framework of CenterNet. A convolutional backbone network outputs three feature maps, which are C3-C5, to connect a feature pyramid structure (FPN). Then FPN outputs P3-P7 feature maps as the final prediction layers. In each prediction layer, we use heatmap and regression to predict the keypoints, respectively. In the heatmap-based prediction, we predict three light binary heatmaps for predicting corners and center keypoints. In the regression-based prediction, to decouple the top-left and bottom-right corners, we divide the ground-truth boxes into four sub-ground-truth boxes along the geometric center, and we select the top-left and bottom-right sub-ground-truth boxes to supervise the regression, respectively. During the inference, the regressed vectors act as a cue to find the closest keypoints on the corresponding heatmaps to refine the locations of the keypoints. Finally, the predicted keypoint triplets are used to determine the final bounding boxes.</p><p>etc.) that extracts features from the input image. We select C3-C5 feature maps from the backbone as the input of a feature pyramid structure (FPN). Then the FPN outputs P3-P7 feature maps as the final prediction layers. In each prediction layer, we use heatmap and regression to predict the keypoints, respectively. In the heatmap-based prediction, we predict three light binary heatmaps for predicting corners and center keypoints. The resolution of the heatmap is the same with the prediction layer, therefore, we predict a extra offset for each keypoint to learn to remap the keypoint from the heatmap to the input image. In the regressionbased prediction, to decouple the top-left and bottom-right corners, we divide the ground-truth boxes into four subground-truth boxes along the geometric center, and we select the top-left and bottom-right sub-ground-truth boxes to supervise the regression, respectively. Take the regression of the top-left box as a example, we select some feature points within the top-left sub-ground-truth boxes, each selected feature point predicts two vectors, which point to the topleft corners and center keypoint. Moreover, we also assign each selected feature point a class label to supervise the classification. We apply the common anchor-free detection methods to train the network to predict the sub-bounding boxes (such as FCOS <ref type="bibr" target="#b55">[56]</ref> and RepPoints <ref type="bibr" target="#b7">[8]</ref>). As a side comment, here we emphasize that the regression accuracy of sub-bounding boxes will be higher than that of complete bounding boxes, because <ref type="table">Table 1</ref> shows that anchor-free methods like FCOS <ref type="bibr" target="#b55">[56]</ref> suffer from the low accuracy of the long distance regression, while our the sub-bounding boxes effectively halve the regression distances.</p><p>During the inference, the regressed vectors act as a cue to find the closest keypoints on the corresponding heatmaps to refine the locations of the keypoints. Next, each valid pair of keypoints defines a bounding box. Here by valid we mean that two keypoints belong to the same class (i.e., the corresponding top-left and bottom-right sub-bounding box of the same class), and the x and y coordinates of the top-left point are smaller than that of the bottom-right point, respectively. Finally, we define a central region for each bounding box and check if the central region contains both the predicted center keypoints. If there is at most one center keypoint detected in its central region, the bounding box will be removed. The score of the bounding box will be replaced by the average scores of the the points, i.e., the topleft corner, the bottom-right corner and the center keypoints. Then tl x , tl y , br x , br y , ctl x , ctl y , cbr x and cbr y should satisfy the following relationship:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Central region definition</head><formula xml:id="formula_1">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ctlx =</formula><p>(n + 1)tlx + (n ? 1)brx 2n ctly = (n + 1)tly + (n ? 1)bry 2n cbrx = (n ? 1)tlx + (n + 1)brx 2n cbry = (n ? 1)tly + (n + 1)bry 2n</p><p>where n is odd and determines the scale of the central region j. In this paper, n is set to be 3 and 5 for the scales of bounding boxes less than and greater than 150, respectively. <ref type="figure">Figure 4</ref> shows two central regions when n = 3 and n = 5, respectively. According to Equation <ref type="formula" target="#formula_2">(1)</ref>, we could determine a scale-aware central region and then check whether the central region contains center keypoints.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enriching Center and Corner Information</head><p>Both center keypoints and corners have rigorous geometric relationships with the objects but contains limited visual patterns of objects. We train the network in a fully supervised way to learn the geometric relationships and the limited visual features so as to locate keypoints. If we introduce more visual patterns for both center keypoints and corners, they will be better detected. Center pooling. The geometric centers of objects do not always convey very recognizable visual patterns (e.g., the human head contains strong visual patterns, but the center keypoint is often in the middle of the human body). To address this issue, we propose center pooling to capture richer and more recognizable visual patterns. <ref type="figure" target="#fig_5">Figure 5(a)</ref> shows the principle of center pooling. The detailed process of center pooling is as follows: the backbone outputs a feature map and to determine whether a pixel in the feature map is a center keypoint, we need to find the maximum value in both the horizontal and vertical directions and add these values together. By doing so, center pooling helps improve the detection of center keypoints. Cascade corner pooling. Corners are often outside objects, which lack local appearance features. CornerNet <ref type="bibr" target="#b29">[30]</ref> uses corner pooling to address this issue. The principle of corner pooling is shown in <ref type="figure" target="#fig_5">Figure 5</ref>(b). Corner pooling aims to find the maximum values on the boundary directions to determine corners. However, this makes corners sensitive to edges. To address this problem, we need to enable corners to extract features from central regions of the object. The principle of cascade corner pooling is presented in <ref type="figure" target="#fig_5">Figure 5</ref>(c).   Cascade corner pooling first looks along a boundary to find a maximum boundary value and then looks inside the box along with the location of the boundary maximum value 2 to find an internal maximum value; finally, the two maximum values are added together. By cascade corner pooling, the corners obtain both the boundary information and the visual patterns of objects. Both center pooling and the cascade corner pooling could be easily achieved by applying the corner pooling <ref type="bibr" target="#b29">[30]</ref> in different directions. <ref type="figure" target="#fig_6">Figure 6</ref>(a) shows the structure of the center pooling module. To take a maximum value in a specific direction, e.g., the horizontal direction, we only need to connect the left pooling and the right pooling in sequence. <ref type="figure" target="#fig_6">Figure 6(b)</ref> shows the structure of a cascade top corner pooling module, in which the white rectangle denotes a 3?3 convolution followed by batch normalization. Compared with the top corner pooling in CornerNet <ref type="bibr" target="#b29">[30]</ref>, a left corner pooling is added before the top corner pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Inference</head><p>Training. We train CenterNet on 8 Tesla V100 (32GB) GPUs. For single-resolution detection framework, our direct baseline is CornerNet <ref type="bibr" target="#b29">[30]</ref>. Following it, we use the stacked hourglass network (Hourglass) <ref type="bibr" target="#b42">[43]</ref> with 52 and 104 layers as the backbone -the latter has two hourglass modules while the former has only one. All modifications on the hourglass architecture, made by <ref type="bibr" target="#b29">[30]</ref>, are preserved. The network is trained from scratch when we use the Hourglass as the backbone. In addition, to show that the framework generalizes to other network architectures, we investigate another backbone named HRNet <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, which enjoys the ability to maintain high-resolution representations throughout feature extraction. The resolution of the input image is 511 ? 511, leading to heatmaps of the size 128 ? 128. We use the data augmentation strategy presented in <ref type="bibr" target="#b29">[30]</ref> to train a robust model. Adam <ref type="bibr" target="#b25">[26]</ref> is used to optimize the training loss:</p><formula xml:id="formula_3">Ls = L co kp + L ce kp + ?L co pull + ?L co push + ? (L co off + L ce off ),<label>(2)</label></formula><p>where L co kp and L ce kp denote the focal losses, which are used to train the network to detect corners and center keypoints, respectively. L co pull is a "pull" loss for corners, which is used to minimize the distance of the embedding vectors that belongs to the same objects. L co push is a "push" loss for corners 2. For the topmost, leftmost, bottommost and rightmost boundary, look vertically towards the bottom, horizontally towards the right, vertically towards the top and horizontally towards the left, respectively.</p><p>that is used to maximize the distance of the embedding vectors that belong to different objects. L co off and L ce off are 1losses <ref type="bibr" target="#b20">[21]</ref>, which are used to train the network to predict the offsets of corners and center keypoints, respectively. ?, ? and ? denote the weights for corresponding losses and are set to 0.1, 0.1 and 1, respectively. L kp , L pull , L push and L off are all defined in CornerNet, and we suggest referring to <ref type="bibr" target="#b29">[30]</ref> for details. We use a batch size of 48. The maximum number training epochs is 100. We use a learning rate of 2.5 ? 10 ?4 for the first 88 epochs and then continue training 12 epochs with a rate of 2.5 ? 10 ?5 .</p><p>For multi-resolution detection framework, we use ResNet <ref type="bibr" target="#b24">[25]</ref>, Res2Net <ref type="bibr" target="#b17">[18]</ref>, ResNeXt <ref type="bibr" target="#b60">[61]</ref> and Swin-Transformer <ref type="bibr" target="#b38">[39]</ref> with the weights pre-trained on Ima-geNet <ref type="bibr" target="#b10">[11]</ref> as our backbones, respectively. The FPN <ref type="bibr" target="#b34">[35]</ref> structure is applied to output the detection layers with different scales. Both single-scale and multi-scale training strategies are applied. For the single-scale training, the shorter side of each input image is 800, while for the multi-scale training, the shorter side of each input image is randomly selected from a range of <ref type="bibr">[480,</ref><ref type="bibr">960]</ref>. We use the data augmentation strategy presented in <ref type="bibr" target="#b62">[63]</ref> to train a robust model. Stochastic gradient descent (SGD) is used to optimize the training loss:</p><formula xml:id="formula_4">Lm = 1 2 L tl cls + L br cls +? 2 L tl reg + L br reg +? L co kp + L ce kp +? (L co off + L ce off ),<label>(3)</label></formula><p>where L br cls and L br cls denote the focal losses, which are used to train the network to classify the the top-left and bottomright sub-bounding boxes, respectively. L tl reg and L br reg denote the GIoU loss <ref type="bibr" target="#b48">[49]</ref>, which are used to train the network to regress the the top-left and bottom-right sub-bounding boxes, respectively.?,? and? denote the weights for corresponding losses and are set to 2, 0.25 and 1.0, respectively. We use a batch size of 16. The maximum number training epochs is 24. We use a learning rate of 0.01 for the first 16 epochs and then the learning rate decays by a factor of 10 after the 16th epoch and the 22th epoch, respectively. Inference. For single-resolution detection framework, we following <ref type="bibr" target="#b29">[30]</ref>. For the single-scale testing, we input both the original and horizontally flipped images with the original resolutions into the network. For multi-scale testing, we input both the original and horizontally flipped images with resolutions of 0.6, 1, 1.2, 1.5 and 1.8. We select top 70 center keypoints, top 70 top-left corners and top 70 bottom-right corners from the heatmaps to detect the bounding boxes. We flip the bounding boxes detected in the horizontally flipped images and mix them into the original bounding boxes. Soft-NMS <ref type="bibr" target="#b1">[2]</ref> is used to remove the redundant bounding boxes. We finally select the top 100 bounding boxes according to their scores as the final detection results.</p><p>For multi-resolution detection framework, we follow <ref type="bibr" target="#b62">[63]</ref>. For the single-scale testing, we resize each image to a shorter side of 800 to input the network, while for the multi-scale testing, we first resize each image to a shorter side of [400, 600, 800, 1000, 1200, 1400], then the detection results from all scales are merged. A NMS with a threshold of 0.6 is applied to remove the redundant results. Flip argumentation is added to only the multi-scale evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Relationship to Prior Works</head><p>Our approach inherits advantages of bottom-up and topdown approaches. Top-down approaches have the ability of perceiving the global visual contents within proposals, but they usually suffer from the low location accuracies especially for the peculiar shapes. Bottom-up approaches enjoy the ability in locating objects with arbitrary geometries, but often exist many incorrect bounding boxes (false positives). Our approach uses a triplet of keypoints to represent an object, which still is a bottom-up approach but could perceive visual contents within proposals and the cost of is minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset, Metrics and Baseline</head><p>We evaluate our method on the MS-COCO dataset <ref type="bibr" target="#b36">[37]</ref>. This dataset contains 80 categories and more than 1.5 million object instances. A large number of small objects makes it a very challenging dataset. We use the 'train2017' set (i.e., 110K training images) for training and testing the results on the test-dev set. We use 'val2017' set as the validation set to perform ablation studies and visualization experiments.</p><p>The MS-COCO dataset <ref type="bibr" target="#b36">[37]</ref> uses AP and AR metrics to characterize the performance of a detector. AP represents the average precision rate, which is computed over ten different IoU thresholds (i.e., 0.5 : 0.05 : 0.95) and all categories. AR represents the maximum recall rate, which is computed over a fixed number of detections (i.e., 1, 10 and 100 ) per image and averaged over all categories and the ten different IoU thresholds. Additionally, AP and AR can be used to evaluate the performance under different object scales, including small objects (area &lt; 32 2 ), medium objects (32 2 &lt; area &lt; 96 2 ) and large objects (area &gt; 96 2 ). AP is considered the single most important metric on the MS-COCO dataset.  detectors by a large margin and ranks among the top of state-of-the-art top-down detectors. The abbreviations are: 'R' -ResNet <ref type="bibr" target="#b24">[25]</ref>, 'X' -ResNeXt <ref type="bibr" target="#b60">[61]</ref>, 'HG' -Hourglass network <ref type="bibr" target="#b42">[43]</ref>, 'R2' -Res2Net <ref type="bibr" target="#b17">[18]</ref>, ' ?' -multi-scale testing, 'SR' -detecting objects on the single-resolution feature map, 'MR' -detecting objects on the multi-resolution feature maps. The biggest improvement comes from small objects. For instance, SR-CenterNet (Hourglass-52) improves the AP for small objects by 5.5% (single-scale) and by 6.4% (multiscale). For the backbone Hourglass-104, the improvements are 6.2% (single-scale) and 8.1% (multi-scale), respectively. This benefit stems from the center information modeled by the center keypoints: the smaller the scale of an incorrect bounding box, the lower the probability that a center keypoint can be detected in the central region. <ref type="figure">Figure 7</ref> CenterNet also leads to a large improvement in re-ducing medium and large incorrect bounding boxes. As <ref type="table" target="#tab_6">Table 3</ref> shows, SR-CenterNet (Hourglass-104) improves the single-scale testing AP by 4.7% (from 42.7% to 47.4%) and 3.5% (from 53.9% to 57.4%) for medium and large bounding boxes, respectively. <ref type="figure">Figure 7</ref>(c) and <ref type="figure">Figure 7(d)</ref> show qualitative comparisons of the reduction of medium and large incorrect bounding boxes. Notably, the AR is also significantly improved compared with baselines, with the best performance achieved with multi-scale testing. This is because our approach removes many incorrect bounding boxes, which is equivalent to improving the confidence of those bounding boxes with accurate locations but relatively low scores. The performance of CenterNet is also competitive with those of top-down approaches, e.g., the single-scale testing AP of SR-CenterNet (Hourglass-52) is comparable to the top-down approach RefineDet <ref type="bibr" target="#b64">[65]</ref> (41.6% vs. 41.8%) and that of MR-CenterNet (Res2Net-101) is comparable to GFLV2 <ref type="bibr" target="#b31">[32]</ref> (53.7% vs. 53.3%). The multi-scale testing AP of 57.1% achieved by MR-CenterNet (Swin-L) closely matches the state-of-the-art AP of 58.7% achieved by the top-down approach Swin Transformer <ref type="bibr" target="#b38">[39]</ref>. We present qualitative detection results in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with State-of-the-art Detectors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-resolution Detection Improves Precision</head><p>As shown in <ref type="table" target="#tab_6">Table 3</ref>, the proposed MR-CenterNet further improves object detection accuracy. For instance, at the same network depth (Hourglass-52 vs. ResNet-50), MR-CenterNet improves the AP of objects by 4.8%. And thanks to the strong generality of the framework of MR-CenterNet, we are able to apply stronger backbones for CenterNet.</p><p>We also design two comparative experiments to confirm the contribution of multi-resolution detection in MR-CenterNet. For the first experiment, it is a control experiment, so we use the default network. For the second experiment, we augment the network with two up-convolutional networks and the skip connections from lower layers to the output to allow for a higher-resolution output. The resolution of the output layer is 1/8 times of the input image.  Therefore, all the objects are detected in single-resolution detection layer. The diagrams of the two network structures are shown in <ref type="figure" target="#fig_10">Figure 9</ref>. <ref type="table" target="#tab_9">Table 4</ref> reports the detection results of the two experiment on the MS COCO validation dataset, MR-CenterNet with multi-resolution detection </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone</head><p>Structure AP MR-CenterNet R-50 <ref type="figure" target="#fig_10">Figure 9</ref> (a) 45.7 MR-CenterNet R-50 <ref type="figure" target="#fig_10">Figure 9</ref> (b) 40.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 5</head><p>We report the inference speed of real-time CenterNet vs. other typical detectors on the MS-COCO dataset. 'RT': real-time, 'AP val ': AP on the MS coco validation dataset, 'AP val ': AP on the MS coco test-dev dataset. For fair comparison, here we follow FCOS <ref type="bibr" target="#b55">[56]</ref> to measure FCOS-RT, YOLOv3 and our real-time CenterNet in an end-to-end manner, which is from prepossessing to the final output boxes. Our CenterNet achieves a good trade-off between accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone FPS AP val APtest YOLOv3 <ref type="bibr" target="#b46">[47]</ref> Darknet layers achieves higher accuracy. This is because the multiresolution detection structure provides richer receptive field for detecting objects with different scales, which helps to improve the detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Real-time CenterNet</head><p>We also design a real-time version of CenterNet, which is called CenterNet-RT. CenterNet-RT is based on the multiresolution detection framework of CenterNet (show in <ref type="figure">Figure 3)</ref>, we apply the following tricks to speed up Center-Net: (i). Reduce the resolution of the input image, which from 800 to 512 and 1333 to 736 for shorter side and the maximum longer side, respectively; (ii). Inspired by FCOS-RT <ref type="bibr" target="#b55">[56]</ref> and BlendMask-RT <ref type="bibr" target="#b5">[6]</ref>, we remove P 6 and P 7 levels to further save the calculation, since the low resolution input image makes the higher feature levels P 6 and P 7 less important. (iii). Remove the corner and center keypoint detection heatmap. For CenterNet-RT, we only use the regression results of corner and center keypoint as the final predictions of the keypoints. This saves a lot of time which spent on the cascade corner pooling and finding the closest keypoints on the heatmap. (iv). During training, replace the SGD optimizer with the AdamW <ref type="bibr" target="#b39">[40]</ref> optimizer and increase the number of training epochs to 32. We test the inference speed of our method on an NVIDIA Tesla-V100 GPU. We also follow FCOS <ref type="bibr" target="#b55">[56]</ref> to measure FCOS-RT, YOLOv3 and our real-time CenterNet in an end-to-end manner, which is from prepossessing to the final output boxes.</p><p>The results are shown in <ref type="table">Table 5</ref>. CenterNet-RT achieves a good trade-off between accuracy and speed and is still competitive among other typical methods. In our conference version, CenterNet (i.e., SR-CenterNet) performs slow, which the inference speed is less than 7 FPS. In this paper, we equip CenterNet with a pyramid structure and detect objects in multi-resolution feature layers, and the improve- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Incorrect Bounding Box Reduction</head><p>The AP <ref type="bibr" target="#b36">[37]</ref> metric reflects how many high quality object bounding boxes (usually IoU 0.5) a network can predict, but cannot directly reflect how many incorrect object bounding boxes (usually IoU 0.5) a network generates. The AF rate is a suitable metric, which reflects the proportion of the incorrect bounding boxes. <ref type="table" target="#tab_11">Table 6</ref> shows the AF rates for CornerNet and CenterNet. CornerNet generates many incorrect bounding boxes even at IoU = 0.05 threshold, i.e., CornerNet511-52 and CornerNet511-104 obtain 35.2% and 32.7% AF rate, respectively. On the other hand, Cor-nerNet generates more small incorrect bounding boxes than medium and large incorrect bounding boxes, which reports 62.5% for CornerNet511-52 and 60.3% for CornerNet511-104, respectively. Our CenterNet decreases the AF rates at all criteria via exploring central regions. For instance, CenterNet511-52 and CenterNet511-104 decrease AF 5 by both 4.5%. In addition, the AF rates for small bounding boxes decrease the most, which are 9.5% by CenterNet511-52 and 9.6% by CenterNet511-104, respectively. This is also the reason why the AP improvement for small objects is more prominent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>Our work has contributed three components, including central region exploration, center pooling and cascade corner pooling. To analyze the contribution of each individual component, an ablation study is given here. The baseline is CornerNet511-52 <ref type="bibr" target="#b29">[30]</ref>. We add the three components to the baseline one by one and follow the default parameter setting detailed in Section 4.1. The results are given in <ref type="table" target="#tab_12">Table 7</ref>. Central region exploration. To understand the importance of the central region exploration (see CRE in the table), we add a center heatmap branch to the baseline and use a triplet of keypoints to detect bounding boxes. For the center keypoint detection, we only use conventional convolutions. As presented in the third row in <ref type="table" target="#tab_12">Table 7</ref>, we improve the AP by 2.3% (from 37.6% to 39.9%). However, we find that the improvement for the small objects (that is 4.6%) is more significant than that for other object scales. The improvement for large objects is almost negligible (from 52.2% to 52.3%). This is not surprising because, the number of small incorrect bounding boxes are larger and they usually do not contain the center keypoints of objects, which are more likely to benefit from filtering by center keypoints. Center pooling. To demonstrate the effectiveness of proposed center pooling, we then add the center pooling module to the network (see CTP in the table). The fourth row in <ref type="table" target="#tab_12">Table 7</ref> shows that center pooling improves the AP by 0.9% (from 39.9% to 40.8%). Notably, with the help of center pooling, we improve the AP for large objects by 1.4% (from 52.2% to 53.6%), which is much higher than the improvement using conventional convolutions (i.e., 1.4% vs. 0.1%). It demonstrates that our center pooling is effective in detecting center keypoints of objects, especially for large objects. Our explanation is that center pooling can extract richer internal visual patterns, and larger objects contain more accessible internal visual patterns. <ref type="figure">Figure 7</ref>(e) shows the results of detecting center keypoints without/with center pooling. We can see the conventional convolution fails to locate the center keypoint for the cow, but with center pooling, it successfully locates the center keypoint. Cascade corner pooling. We replace corner pooling <ref type="bibr" target="#b29">[30]</ref> with cascade corner pooling to detect corners (see CCP in the table). The second row in <ref type="table" target="#tab_12">Table 7</ref> shows the results that we test on the basis of CornerNet511-52. We find that cascade corner pooling improves the AP by 0.7% (from 37.6% to 38.3%). The last row shows the results that we test on the basis of CenterNet511-52, which improves the AP by 0.5% (from 40.8% to 41.3%). The results of the second row show there is almost no change in the AP for large objects (i.e., 52.2% vs. 52.2%), but the AR is improved by 1.8% (from 74.0% to 75.8%). This suggests that cascade corner pooling helps to obtain more internal visual patterns in large objects, but too rich visual patterns may interfere with its perception for the boundary information, leading to many inaccurate bounding boxes. After equipping with our CenterNet, the inaccurate bounding boxes are effectively suppressed, which improves the AP for large objects by 2.2% (from 53.6% to 55.8%). <ref type="figure">Figure 7</ref>(f) shows the result of detecting corners with corner pooling or cascade corner pooling. We can see that cascade corner pooling can successfully locate a pair of corners for the cat on the left while corner pooling cannot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Error Analysis</head><p>The exploration of visual patterns within each bounding box depends on the center keypoints. In other words, once a center keypoint is missed, the proposed CenterNet would miss the visual patterns within the bounding box. To understand the importance of center keypoints, we replace the predicted center keypoints with the ground-truth values and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we propose CenterNet, a new bottom-up object detection approach that detects objects using a triplet, including one center keypoint and two corners. Our approach addresses the problem that the traditional bottomup approach lack an additional look into the cropped regions by exploring the visual patterns within each proposed region with minimal costs. Besides, we also extend the Cen-terNet on a framework that has pyramid structure to allow for better detecting multi-scale objects. The experimental results show CenterNet outperforms all existing bottom-up approaches by a large margin and is competitive among the top-down approaches especially in recall ability. We also design some real-time models for CenterNet, which achieves a good trade-off between accuracy and speed. The most important take-away is that we prove that bottom-up approaches are more flexible in locating objects with arbitrary geometries, and an additional look into each proposed region is necessary for improving precision. We hope that CenterNet could attract more attention to promote a further exploration of bottom-up approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Kaiwen Duan, Honggang Qi and Qingming Huang are with the School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China. ? Song Bai is with Huazhong University of Science and Technology. ? Lingxi Xie and Qi Tian are with Huawei Inc.. ? Honggang Qi and Qingming Huang are the corresponding authors (email: {hgqi,qmhuang}@ucas.ac.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>512x512</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Single-resolution detection framework of CenterNet. A convolutional backbone network applies cascade corner pooling and center pooling to output two corner heatmaps and a center keypoint heatmap, respectively. Note that the heatmaps are multi-class heatmaps, which means that the number of channel of each heatmap equals to the number of the classes in the dataset. Similar to CornerNet, a pair of detected corners and the similar embeddings are used to detect a potential bounding boxes. Then the detected center keypoints are used to determine the final bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The size of the central region in the bounding box affects the detection results. For example, small central regions lead to a low recall rate for small bounding boxes, while large central regions lead to a low precision for large bounding boxes. Therefore, we propose a scale-aware central region to adaptively fit the size of bounding boxes. The scaleaware central region tends to generate a relatively large central region for a small bounding box and a relatively small central region for a large bounding box. Let tl x and tl y denote the coordinates of the top-left corner of i and br x and br y denote the coordinates of the bottom-right corner of i. Define a central region j. Let ctl x and ctl y denote the coordinates of the top-left corner of j and cbr x and cbr y denote the coordinates of the bottom-right corner of j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 . 5 .</head><label>45</label><figDesc>(a) The central region when n = 3. (b) The central region when n = The solid rectangle and the shaded region denote the predicted bounding box and the scalable central region, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Center pooling takes the maximum values in both horizontal and vertical directions. (b) Corner pooling only takes the maximum values in boundary directions. (c) Cascade corner pooling takes the maximum values in both boundary directions and internal directions of objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The structures of the center pooling module (a) and the cascade top corner pooling module (b). We achieve center pooling and cascade corner pooling by combining corner pooling in different directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) and Figure 7(b) show qualitative comparisons that demonstrate the effectiveness of CenterNet in reducing small incorrect bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>(a) and (b) show that the small incorrect bounding boxes are significantly reduced by the modeling center information. (c) and (d) show that the center information works to reduce the medium and large incorrect bounding boxes. (e) shows the results of the detection of the center keypoints without/with center pooling. (f) shows the results of the detection of corners with corner pooling and cascade corner pooling. The blue boxes above denote the ground-truth. The red boxes and dots denote the predicted bounding boxes and keypoints, respectively. Some qualitative detection results on the MS-COCO validation dataset. Only detections with scores higher than 0.5 are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>(a) In the first experiment, the network outputs multi-resolution detection layers. (b) In the second experiment, the network is equipped with two up-convolutional networks and the skip connections from lower layers to the output to allow for a single-resolution detection layer. The number in the box denotes the stride.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="8">Average false discovery (AF) rate (%) of CornerNet. AF reflects the</cell></row><row><cell cols="8">distribution of incorrect bounding boxes (false positives). The results</cell></row><row><cell cols="8">suggest the false positives account for a large proportion.</cell></row><row><cell>Method</cell><cell>AF</cell><cell>AF 5</cell><cell>AF 25</cell><cell>AF 50</cell><cell>AF S</cell><cell>AF M</cell><cell>AF L</cell></row><row><cell>CornerNet</cell><cell>37.8</cell><cell>32.7</cell><cell>36.8</cell><cell>43.8</cell><cell>60.3</cell><cell>33.2</cell><cell>25.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 shows</head><label>3</label><figDesc>a comparison with state-of-the-art detectors on the MS-COCO test-dev set. Compared with the baseline CornerNet<ref type="bibr" target="#b29">[30]</ref>, the proposed CenterNet achieves a remarkable improvement. For example, SR-CenterNet (Hourglass-52) reports a single-scale testing AP of 41.6%, an improvement of 3.8% over 37.8%, and a multi-scale testing AP of 43.5%, an improvement of 4.1% over 39.4%, achieved by CornerNet under the same setting. When using the deeper backbone (i.e., Hourglass-104), the AP improvement over CornerNet are 4.4% (from 40.5% to 44.9%) and 4.9% (from 42.1% to 47.0%) under the single-scale and multiscale testing, respectively. We also report the detection results of MR-CenterNet, which obtain significant improvements again on the basis of SR-CenterNet, e.g., with the backbone of Res2Net-101, MR-CenterNet reports APs of 51.5% for single-scale testing and 53.7% for multi-scale testing, respectively. The current Transformer based topdown detectors<ref type="bibr" target="#b38">[39]</ref> push the accuracy to a new level, so we explore the application of the Transformer based backbone to the bottom-up approaches. After equipped with the Transformer based backbone, CenterNet reports APs of 53.2% for single-scale testing and 57.1% for multiscale testing, respectively, surpassing all other bottom-up approaches to the best of our knowledge.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3</head><label>3</label><figDesc>Performance comparison (%) with the state-of-the-art methods on the MS-COCO test-dev dataset. CenterNet outperforms all existing bottom-up</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 4</head><label>4</label><figDesc>The detection results using different network structures on the MS COCO validation dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6</head><label>6</label><figDesc>Comparison of the average false discovery rates (%) of CornerNet and CenterNet on the MS-COCO validation dataset. The results suggest CenterNet avoids a large number of incorrect bounding boxes, especially for small incorrect bounding boxes. CenterNet achieves 45.7% AP at 14.5 FPS. We further propose the CenterNet-RT on the basis of MR-CenterNet, achieving results of 43.2% AP at 30.5 FPS. This accuracy is competitive with SR-CenterNet (HG-104) but the inference speed is ? 6 times faster.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AF</cell><cell>AF5</cell><cell>AF25</cell><cell>AF50</cell><cell>AF S</cell><cell>AF M</cell><cell>AF L</cell></row><row><cell>CornerNet</cell><cell>HG-52</cell><cell>40.4</cell><cell>35.2</cell><cell>39.4</cell><cell>46.7</cell><cell>62.5</cell><cell>36.9</cell><cell>28.0</cell></row><row><cell>SR-CenterNet</cell><cell>HG-52</cell><cell>35.1</cell><cell>30.7</cell><cell>34.2</cell><cell>40.8</cell><cell>53.0</cell><cell>31.3</cell><cell>24.4</cell></row><row><cell>CornerNet</cell><cell>HG-104</cell><cell>37.8</cell><cell>32.7</cell><cell>36.8</cell><cell>43.8</cell><cell>60.3</cell><cell>33.2</cell><cell>25.1</cell></row><row><cell>SR-CenterNet</cell><cell>HG-104</cell><cell>32.4</cell><cell>28.2</cell><cell>31.6</cell><cell>37.5</cell><cell>50.7</cell><cell>27.1</cell><cell>23.0</cell></row><row><cell cols="9">ment is significant for speed and accuracy. With ResNet-50,</cell></row><row><cell>MR-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 7</head><label>7</label><figDesc>Ablation study on the major components of CenterNet511-52 on the MS-COCO validation dataset. The CRE denotes central region exploration, the CTP denotes center pooling, and the CCP denotes cascade corner pooling.</figDesc><table><row><cell>CRE</cell><cell>CTP</cell><cell>CCP</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell><cell>APS</cell><cell>APM</cell><cell>APL</cell><cell>AR1</cell><cell cols="3">AR10 AR100 ARS</cell><cell>ARM</cell><cell>ARL</cell></row><row><cell></cell><cell></cell><cell></cell><cell>37.6</cell><cell>53.3</cell><cell>40.0</cell><cell>18.5</cell><cell>39.6</cell><cell>52.2</cell><cell>33.7</cell><cell>52.2</cell><cell>56.7</cell><cell>37.2</cell><cell>60.0</cell><cell>74.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>38.3</cell><cell>54.2</cell><cell>40.5</cell><cell>18.6</cell><cell>40.5</cell><cell>52.2</cell><cell>34.0</cell><cell>53.0</cell><cell>57.9</cell><cell>36.6</cell><cell>60.8</cell><cell>75.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>39.9</cell><cell>57.7</cell><cell>42.3</cell><cell>23.1</cell><cell>42.3</cell><cell>52.3</cell><cell>33.8</cell><cell>54.2</cell><cell>58.5</cell><cell>38.7</cell><cell>62.4</cell><cell>74.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40.8</cell><cell>58.6</cell><cell>43.6</cell><cell>23.6</cell><cell>43.6</cell><cell>53.6</cell><cell>33.9</cell><cell>54.5</cell><cell>59.0</cell><cell>39.0</cell><cell>63.2</cell><cell>74.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>41.3</cell><cell>59.2</cell><cell>43.9</cell><cell>23.6</cell><cell>43.8</cell><cell>55.8</cell><cell>34.5</cell><cell>55.0</cell><cell>59.2</cell><cell>39.1</cell><cell>63.5</cell><cell>75.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 8</head><label>8</label><figDesc>Error analysis of center keypoints via using ground-truth. we replace the predicted center keypoints with the ground-truth values, the results suggest there is still room for improvement in detecting center keypoints.evaluate performance on the MS-COCO validation dataset.Table 8 shows that using the ground-truth center keypoints improves the AP from 41.3% to 56.5% for CenterNet511-52 and from 44.8% to 58.1% for CenterNet511-104, respectively. APs for small, medium and large objects are improved by 15.5%, 16.5%, and 14.5% for CenterNet511-52 and 14.5%, 14.1%, and 13.3% for CenterNet511-104, respectively.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>SR-CenterNet w/o GT</cell><cell>HG-52</cell><cell>41.3</cell><cell>59.2</cell><cell>43.9</cell><cell>23.6</cell><cell>43.8</cell><cell>55.8</cell></row><row><cell>SR-CenterNet w/ GT</cell><cell>HG-52</cell><cell>56.5</cell><cell>78.3</cell><cell>61.4</cell><cell>39.1</cell><cell>60.3</cell><cell>70.3</cell></row><row><cell>SR-CenterNet w/o GT</cell><cell>HG-104</cell><cell>44.8</cell><cell>62.4</cell><cell>48.2</cell><cell>25.9</cell><cell>48.9</cell><cell>58.8</cell></row><row><cell>SR-CenterNet w/ GT</cell><cell>HG-104</cell><cell>58.1</cell><cell>78.4</cell><cell>63.9</cell><cell>40.4</cell><cell>63.0</cell><cell>72.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blendmask: Top-down meets bottomup for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8573" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Revisiting feature alignment for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01570</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reppoints v2: Verification meets regression for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10519" to="10528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Corner proposal network for anchor-free, two-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Location-sensitive visual recognition with cross-iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Res2net: A new multiscale backbone architecture. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer science</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5936" to="5944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cornernetlite: Efficient keypoint based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08900</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6054" to="6063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning object detectors from scratch with gated recurrent feature pyramids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00886</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<title level="m">Beyond skip connections: Top-down modulation for object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fcos: A simple and strong anchor-free object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Denet: Scalable realtime object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6877" to="6885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08036</idno>
		<title level="m">Scaled-yolov4: Scaling cross stage partial network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Craft objects from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6043" to="6051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9259" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bottomup object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Zhiqiang Shen, and Marios Savvides. Soft anchor-point object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12448</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
