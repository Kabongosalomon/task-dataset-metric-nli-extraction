<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLUDA : Contrastive Learning in Unsupervised Domain Adaptation for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Midhun</forename><surname>Vayyat</surname></persName>
							<email>midhun.vayyat@mercedes-benz.com</email>
							<affiliation key="aff0">
								<orgName type="department">Mercedes Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaswin</forename><surname>Kasi</surname></persName>
							<email>kasi.jaswin@mercedes-benz.com</email>
							<affiliation key="aff0">
								<orgName type="department">Mercedes Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuraag</forename><surname>Bhattacharya</surname></persName>
							<email>anuraag.bhattacharya@mercedes-benz.com</email>
							<affiliation key="aff0">
								<orgName type="department">Mercedes Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaib</forename><surname>Ahmed</surname></persName>
							<email>shuaib.ahmed@mercedes-benz.com</email>
							<affiliation key="aff0">
								<orgName type="department">Mercedes Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Tallamraju</surname></persName>
							<email>rahul.tallamraju@mercedes-benz.com</email>
							<affiliation key="aff0">
								<orgName type="department">Mercedes Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLUDA : Contrastive Learning in Unsupervised Domain Adaptation for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose CLUDA, a simple, yet novel method for performing unsupervised domain adaptation (UDA) for semantic segmentation by incorporating contrastive losses into a student-teacher learning paradigm, that makes use of pseudo-labels generated from the target domain by the teacher network. More specifically, we extract a multi-level fused-feature map from the encoder, and apply contrastive loss across different classes and different domains, via source-target mixing of images. We consistently improve performance on various feature encoder architectures and for different domain adaptation datasets in semantic segmentation. Furthermore, we introduce a learned-weighted contrastive loss to improve upon on a state-of-the-art multi-resolution training approach in UDA. We produce state-of-the-art results on GTA ? Cityscapes (74.4 mIOU, +0.6) and Synthia ? Cityscapes (67.2 mIOU, +1.4) datasets. CLUDA effectively demonstrates contrastive learning in UDA as a generic method, which can be easily integrated into any existing UDA for semantic segmentation tasks. Please refer to the supplementary material for the details on implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is an important task in computer vision with applications ranging from autonomous driving to medical image analysis. Most of the existing approaches in semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b46">47]</ref> use deep learning models in a fully supervised setting. This requires a lot of annotated data for accurate predictions. However, manual annotation of class labels is very difficult, as it may take around 1.5 hours of human labour to annotate a single driving scene image from the cityscapes dataset <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19]</ref>. Hence, it is unrealistic for some application areas (e.g. autonomous driving) to use manual annotations for fully supervised training settings. One promising way to circumvent the issue of manual annotation is instead to train models on synthetically rendered, and auto-annotated images. High-resolution photorealistic images can be generated from advanced computer graphics software/game engines <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42]</ref>. A major advantage is that a large amount of annotated data can be rendered in a relatively short amount of time. However, it has been demonstrated systematically in previous work that models trained only on synthetic data do not generalize well on real-world data <ref type="bibr" target="#b58">[59]</ref>. Despite being photo-realistic, synthetically generated data suffers from domain shift, as the underlying data distribution of real and synthetic images are different. Hence, using models trained on synthetic data (source domain) for real-world applications would require sophisticated domain adaptation strategies to generalize to the real-world (real domain).</p><p>Extensive research on domain adaptation has been carried out in the literature. In scenarios where manual annotations are available, along with synthetic data, semisupervised and weakly-supervised domain adaptation methods have been explored <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b59">60]</ref>. How-ever, when the manual annotation is difficult or tedious to obtain, unsupervised domain adaptation (UDA) techniques have been proposed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>Generally, UDA is performed in two steps. Models are first trained on source domain data (synthetic data). These models are subsequently used to generate the pseudo-labels on the target domain data (real-world data). Since the pseudo-labels are noisy and the learnt features for the two domains are distinct, a second step in the form of adversarial training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref> or self-learning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b58">59]</ref> is employed. The general architecture of such models is as follows. An encoder with feature extractor backbone (ResNet-101 <ref type="bibr" target="#b14">[15]</ref>, VGG-19 <ref type="bibr" target="#b35">[36]</ref>, and more recently SegFormer <ref type="bibr" target="#b18">[19]</ref>), followed by a classifier that predicts dense labels. Most recent domain adaptation methods for semantic segmentation like DAFormer <ref type="bibr" target="#b18">[19]</ref>, HRDA <ref type="bibr" target="#b19">[20]</ref> use transformer backbones to extract features and a cross-entropy loss to train the classifier. Intuitively cross-entropy loss aims only at bringing similar features together while ignoring to differentiate features across distinct classes. This results in scattered feature representations that might overlap across classes leading to mis-classification. Hence, it is important to separate features corresponding to different classes while accumulating features belonging to the same class in the latent space.</p><p>In this paper, we introduce contrastive learning along with cross-entropy loss for unsupervised domain adaptation called (CLUDA) to keep similar features in latent space together while separating away distinct features. This results in a richer, compact and well separated feature space, thereby, making it easier for final classification (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Based on the above intuition we argue that intra-class compactness and inter-class separability of features in source domain remain similar in the target domain. Although contrastive learning has been recently applied for semantic segmentataion in both supervised <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47]</ref> and unsupervised settings <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref>, to the best of our knowledge, this paper is the first to study unsupervised domain adaptation for semantic segmentation using contrastive learning.</p><p>In this paper, we demonstrate that the proposed method can be used to enhance the domain adaptation capabilities for semantic segmentation on state-of-the-art encoder architectures. We showcase state-of-the-art results by applying contrastive losses to transformer-based UDA methods, like DAFormer and HRDA.</p><p>(Contributions:) We propose the usage of contrastive losses for unsupervised domain adaptation in semantic segmentation. Our method increases intra-class semantic similarity and decreases inter-class similarity across source and target domains. To the best of our knowledge, we are the first to study unsupervised domain adaptation for semantic segmentation using contrastive learning. We also show that our proposed approach outperforms the current state-of-the-art on different datasets such as GTA ? Cityscapes and Synthia ? Cityscapes, where ? indicates mapping from source to target domain datasets. Moreover, to overcome the shape-bias problem in transformers, we propose a novel masking strategy for stuff and thing classes separately. We also demonstrate the flexibility of our method by applying it to two state-of-the-art UDA approaches DAFormer <ref type="bibr" target="#b18">[19]</ref> and multi-resolution HRDA <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Semantic Segmentation</head><p>Most of the existing works in semantic segmentation use per-pixel cross-entropy loss between the predicted label and ground truth, as first shown in <ref type="bibr" target="#b26">[27]</ref>. Besides cross-entropy loss and its variants <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b0">1]</ref>, Dice loss has been used, mainly to detect the boundaries of the objects in the image for better segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b37">38]</ref>. Also, there are few works on feature-distribution-based losses, as introduced in [47, 55].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">UDA in Semantic Segmentation</head><p>Some methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref> explores domain adaptive segmentation along with image-to-image translation using cyclic loss <ref type="bibr" target="#b56">[57]</ref>. Since the target annotation is not available, to generate pseudo label, a hard threshold is used to eliminate low confidence pixel predictions from the predicted label. The work <ref type="bibr" target="#b24">[25]</ref> proposes source-free domain adaptation using a model pre-trained only on source dataset. This model will serve as a source dataset generator, which later helps in domain adaptation to target data, via adversarial loss training.</p><p>Recently <ref type="bibr" target="#b22">[23]</ref> proposed a patch-wise intra-image contrastive learning for both semi-supervised and unsupervised domain adaptation. Images are divided into patches, and features extracted from these patches are used in contrastive learning. In <ref type="bibr" target="#b43">[44]</ref>, depth estimation is used as an auxiliary task to alleviate the domain shift. It uses the depth estimation discrepancies in two domains to refine the target pseudo labels and guide the domain adaptation. In another recent work <ref type="bibr" target="#b53">[54]</ref>, the pseudo label predictions were rectified exploiting the distance between features and their respective class centroids. Furthermore <ref type="bibr" target="#b46">[47]</ref> improves upon <ref type="bibr" target="#b53">[54]</ref> by aligning untrusted and trusted pixel regions using adversarial training. Along with cross-domain adaptation, one can perform cross-region adaptation in this manner. Broadly speaking, to improve the feature extraction capability of the encoder we explore the contrastive learning approach which has shown some promising results recently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Contrastive Losses for Semantic Segmentation</head><p>In <ref type="bibr" target="#b5">[6]</ref>, Sumit et. al. invented the concept of contrastive learning by bringing similar samples together, while separating different labels apart. Florian et. al. <ref type="bibr" target="#b34">[35]</ref> introduced triplet-loss which takes a sample and brings it closer to its class anchor called positive anchor and at the same time drives it farther from other class anchors called negative anchors. Contrastive loss techniques can also be differentiated based on the kind of metric-learning that is used to cluster the features. Some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref> use Euclidean distance to compute the distance between two features, while others <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b46">47]</ref> use cosine similarity between the unit-normalized features to align similar features and separate dissimilar features apart.</p><p>Contrastive learning in semantic segmentation have only been recently introduced in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47]</ref>. Since semantic segmentation is a dense prediction task each pixel is a feature and the cluster representation is learned at pixel level. While <ref type="bibr" target="#b44">[45]</ref> exploits the dense contrastive learning in a fully supervised setting, <ref type="bibr" target="#b21">[22]</ref> takes a semisupervised approach and <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b45">46]</ref> follows the unsupervised approach. Similar to <ref type="bibr" target="#b44">[45]</ref> we exploit the inter-image semantic similarity and dissimilarity between classes but extend it to unsupervised domain adaptation. These works attempt to align the same class features and distinguish different class features in the feature space. All of these works use the feature generated at the last layer of the feature extractor. We attempt to combine the features using contrastive losses by using all the features generated by the hierarchical features extractors like <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b25">26]</ref>. Recent works have focused on unsupervised contrastive learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref> because of its capability to learn feature representation in the absence of human supervision. We define unsupervised contrastive loss objective in section 4.1. We utilize the existing InfoNCE <ref type="bibr" target="#b11">[12]</ref> loss for supervised contrastive loss for source training. For domain adaptation we modify the same loss by hinging it on model prediction confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Prerequisites</head><p>Given a source (synthetic) dataset {X s = x n i=1 } with segmentation labels {Y s = y n i=1 }, we train a deep learning model that learns from the source and is expected to produce high accuracy semantic maps for the target (real) images</p><formula xml:id="formula_0">{X t = x m i=1 } without annotation data.</formula><p>Here, both X s and X t share same C classes. Generally a semantic segmentation model follows an encoder-decoder architecture, where encoder is a feature extractor f followed by a decoder that has a classifier g. A semantic segmentation map is given by h = g(f (x)), which predicts classes for each pixel in the image. The network is optimised using categorical crossentropy with ground-truth and pseudo ground-truth for real and synthetic data respectively.</p><formula xml:id="formula_1">l s ce = ? H?W i=1 C c=1 y (i,c) s log(p (i,c) s ),<label>(1)</label></formula><formula xml:id="formula_2">l t ce = ? H?W i=1 C c=1? (i,c) t log(p (i,c) t ),<label>(2)</label></formula><p>where y</p><formula xml:id="formula_3">(i,c) s is the source ground-truth label,? (i,c) t</formula><p>is the target pseudo-label and p (i,c) t is the softmax probability of x (i) t pixel belonging to class c. Since we cannot use real ground-truth we use self-training <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref> to infer pseudo-labels. The pseudo-label is derived from teacher network h t with the following formulation -p</p><formula xml:id="formula_4">(i,c) t = [i = argmax(h t (x t ) c )], where [.] is the Iversion bracket.</formula><p>For self-training with only cross-entropy loss, we exactly adopt the approach taken in DAFormer <ref type="bibr" target="#b18">[19]</ref>. It consists of a student-teacher network, where pseudo-labels of target images are predicted by the teacher, while student network sees a source-target-mixed-image. A mixedground-truth for the mixed-image is then constructed by overlaying the source ground-truth(properly aligning as per the mixing) on pseudo-labels predicted by teacher network. Using this mixed-ground-truth the student network is trained, while freezing the teacher network. Teacher network is indirectly updated via exponential-moving-average (EMA) of the weights from the student network. Following DAFormer, we control EMA update weights using a fixed hyper-parameter ?, and as well use, rare class sampling (RCS) to alleviate the issue of long-tail representation of the classes in the source data. Moreover, we use Ima-geNet <ref type="bibr" target="#b8">[9]</ref> feature distance loss to prevent the network from over-fitting to the source data. For finer details we redirect the readers to the aforementioned paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Approach</head><p>Building upon motivations described in previous sections, we introduce contrastive loss for UDA for semantic segmentation tasks. Generally, for contrastive learning, we need intermediate feature-vectors to represent the object, which for semantic segmentation are individual pixels. The feature extractor maps each pixel into a higherdimensional latent space. Contrastive losses are a function of these higher-dimensional embeddings of pixels and their corresponding labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Contrastive Losses</head><p>There are quite a few variants of contrastive losses(CL), like region-aware CL <ref type="bibr" target="#b20">[21]</ref>, positive-negative equal CL <ref type="bibr" target="#b42">[43]</ref>, intra-image pixel-wise CL <ref type="bibr" target="#b55">[56]</ref>, cross-image pixel-wise CL <ref type="bibr" target="#b46">[47]</ref>, centroid-aware CL, and distribution-aware CL <ref type="bibr" target="#b47">[48]</ref>. For CLUDA, we follow <ref type="bibr" target="#b44">[45]</ref>, that for a given feature-vector v i is given by,</p><formula xml:id="formula_5">L I N CE ( v, P, N ) = 1 |P i | v + ?P i ? log e ( v i . v + /? ) e ( v i . v + /? ) + v ? ?N i e ( v i . v ? /? ) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>We sample source images using rare class sampling (RCS) and use a fixed ImageNet pre-trained encoder for preventing the student network from forgetting real features following <ref type="bibr" target="#b18">[19]</ref>. We fuse the encoder hierarchical features and compute contrastive losses on the fused features. For DA training we use the self learning approach to generate the pseudo label but hinge the contrastive loss computed for domain mixed images on the teacher net prediction confidence.</p><p>Here v + is the positive anchor, v ? is the negative anchor and ? is the temperature hyper-parameter to control the magnitude of the similarity. The summation is normalized by the cardinality of all the positive samples considered for v i , indicated by |P i |. N i is the set of all negative samples considered for v i . We use an explicit form of the above loss, that computes the similarity between every pair of pixel-features f i , f j across the batch following <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55]</ref>. Since we domain adapt from source synthetic-data to target real-world data we consider pixels from a source image x s and a source-target-mixed image x m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Contrastive Source Training</head><p>Feature extractors like ResNet-101 <ref type="bibr" target="#b14">[15]</ref>, SegFormer <ref type="bibr" target="#b48">[49]</ref> produces hierarchical feature maps at different scales with different embedding dimensions. Existing works perform contrastive losses over the final feature map, and neglect the remaining prior feature maps which capture finer details of a scene, that are semantically important. The final feature map captures coarser details only, <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b14">15]</ref>. We propose to make use of all the feature maps generated by the feature extractor within a contrastive loss. In DAFormer <ref type="bibr" target="#b18">[19]</ref>, the generated feature maps are fused after channel aligning, and shape aligning, to produce a single fused-feature tensor F f used , which is later mapped to N -channel segmentation logits. F f used tensor for a given image is of size H/? ? W/? ? C, where H, W are height and width of the image, ? &gt; 1 is some down-scaling factor depending on the encoder-architecture used. C is the embedding size of pixel-feature vectors f i . Typically, C are high-dimensional vectors of order 128, 256, 512 etc. Due to GPU memory constraints we reduce the dimensions of F f used , following <ref type="bibr" target="#b54">[55]</ref> by down-scaling (H/??W/?) ? 65?65. In equation 3, we take v + and v ? from the ground-truth that is available for purely source images x s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Contrastive Domain Adaptation Training</head><p>We follow <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b18">19]</ref> to do DA training, where image augmentation happens using color-jitter, blur, saturation, along with ClassMix <ref type="bibr" target="#b29">[30]</ref>. Source-domain random-classoverlapping means, we randomly sample a class from source image x s , and overlay all of the pixels of chosen class onto the target image and generate augmented image x m . This allows the network to view pixels from source and target domain simultaneously, and hence assist in domain adaptation. Since it is applied on each image in a batch independently, we get sufficient number of features-vectors of same class, from both source and target domain. We aim to align features belonging to same class from both the domains across a batch, and separate features belonging to different classes for both domains across the batch. In UDA, the target semantic labels are not available so we rely on the pseudo-labels (as previously described in section 3). In CLUDA, for the augmented images, the contrastive losses are weighted based on teacher network pseudo-label confidence on target images. Without such re-weighting, the model produces inconsistent results as can be seen in <ref type="figure">Fig. 3</ref>. For computing the weighing-factor ? conf , we first compute the number of target-pixels whose softmax-probabilities are above a certain threshold ?.</p><formula xml:id="formula_6">? conf = 1 H/? ? W/? i?T 1 [max k P i,k &gt;?] ,<label>(4)</label></formula><p>where T denotes all the pixels in pure target image, k runs over probability of each class. Then the contrastive loss is given by,</p><formula xml:id="formula_7">L CL = 1 H ? W s?S L I N CE ( v s , P s , N s )<label>(5)</label></formula><formula xml:id="formula_8">+ ? conf t?T L I N CE ( v t , P s,t , N s,t ) ,</formula><p>where, S is the set of all source features in x m , T is the set of all the target features in x m , P s /N s indicates positives samples/negative samples from source, whereas P s,t /N s,t indicates positive/negative samples from both source and target pixels in x m image. We observe that the pseudolabel prediction confidence increases over the course of the training as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, as the model sees more and more source and target images. This loss L CL we add to the CE losses and ImageNet feature-distance loss mentioned in Section 3.3 in <ref type="bibr" target="#b18">[19]</ref>,</p><formula xml:id="formula_9">L = l s ce + l t ce + L CL + ?L F D<label>(6)</label></formula><p>where L F D is the Euclidean-loss between student-network features and fixed ImageNet-features on only source images. Note, for discussions in subsection 5.5, we compute contrastive loss of stuff-classes and thing-classes differently. For the definition of stuff-and thing-classes we follow Caesar et. al. <ref type="bibr" target="#b2">[3]</ref>. We don't include comparisons between stuff and thing-classes features in the equation 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multi-Resolution CLUDA</head><p>Recently <ref type="bibr" target="#b19">[20]</ref> proposed unsupervised domain adaptation in semantic segmentation using multi-resolution images. A low-resolution entire image that captures the long-range contextual dependencies, and a high-resolution image-crop that helps in predicting high-quality segmentation maps of the smaller-classes. The N -class prediction logits of both low resolution and high resolution are fused using a learned scale-attention. For UDA, the target pseudo-label is generated by fusing together segmentation prediction from overlapping sliding windows on the target image, which is then used in self-learning.</p><p>Contrastive loss on just the low-resolution image is ineffective. See <ref type="table">Table 6</ref>. We posit that the features learnt by the feature-extractor for low and high-resolution images is different and hence learning cluster representation <ref type="figure">Figure 3</ref>: CLUDA module for multi-resolution UDA. Note that the figures shows just the contrastive learning module using multi-resolution, rest of the architecture remains same as HRDA <ref type="bibr" target="#b19">[20]</ref> and that we have mentioned only those elements in the legend that are new in this architecture, rest of the elements are same as <ref type="figure">Fig. 2</ref>. Please refer to equation 7 for the loss L M R CL . of just the low-resolution features is not sufficient as the cross-entropy loss will force the model to learn from both low and high-resolution predictions. Therefore, we propose a novel strategy to utilize both the low and highresolution features for CLUDA. We combine the per-pixel contrastive-loss from low and high-resolution images, using a learned scale weight for each pixel. Effectively, these weights quantify the confidence for considering features from high-resolution crops to compare against features from low-resolution. Then the contrastive loss in equation 5 is replaced by,</p><formula xml:id="formula_10">L M R CL (x l m , x h m , A s , A m ) = 1 H/? ? W/? s?S ? s L I N CE ( v s , P (l) s , N (l) s ) + s?S (1 ? ? s )L I N CE ( v s , P (h) s , N (h) s ) + ? conf t?T ? m L I N CE ( v t , P (l) t , N (l) t ) + ? conf t?T (1 ? ? m )L I N CE ( v t , P (h) t , N (h) t ) .<label>(7)</label></formula><p>Here, ? s ? A s is the weightage per pixel when purely source image is passed, and ? m ? A m is the weightage when augmented image is passed. N (h) /P (h) represent negative/positive features respectively from high-resolution crop, while N (l) /P (l) represent low-resolution crop's negative/positive features respectively. Note that, the weightage is 1 for pixels present in low resolution crop and not available in the high-resolution image after cropping. Also note, we don't compare high-resolution features against each other, i.e there is no CL terms of two high-resolution feature-vectors in the loss function, in equation 7.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training Methodology</head><p>A general semantic segmentation model consisted of a CNN-based feature-extractor, but recently, works have also explored transformer-based feature-extractors ( refer to Table 4 for architectures used). The feature-extractor outputs features that are embedded hierarchically. This is followed by a CNN-based simple decoder mentioned in section 4.2. The decoder fuses the feature-embeddings into a single feature-map. Then a classifier maps the fused-feature-map into N -channel softmax output. The model is trained using CE loss on the computed softmax output. Additionally in each iteration, we compute the contrastive losses from the fused-features for source and augmented images separately. In a single iteration, we first train the student-network using the source image, followed by predicting the targetimage pseudo-label from the teacher-network. For DA, we mix the source and target images and segmentation labels following <ref type="bibr" target="#b39">[40]</ref>. The student-network is then trained using the domain-mixed-augmented images and their respective pseudo-labels. The teacher-network weights are updated using the exponential moving average (EMA) of studentnetwork weights. For maximizing the performance of contrastive losses, we use stuff-thing masks, refer to section 5.5, larger-embedding size, refer to section supplementary material. For contrastive loss computed on mixed images, we re-weight the loss using pseudo-label confidence. Refer to section 5.4. For multi-resolution DA, first we compute the contrastive loss with low-resolution image features, followed by contrastive losses between low-resolution and high-resolution image features. To compute the final contrastive loss for the batch of source and domain-mixedimages, we combine the two losses using learned weights. Note that the weights are applied only to the features from the image crops that are common in both high and lowresolution image features. We keep much of the training methodology same as HRDA <ref type="bibr" target="#b19">[20]</ref> except the addition of the feature-weights-learning network and the contrastive loss itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>Datasets: We perform our experiments on GTA-5 <ref type="bibr" target="#b31">[32]</ref>, SYNTHIA <ref type="bibr" target="#b32">[33]</ref>, Cityscapes <ref type="bibr" target="#b6">[7]</ref> datasets. Please refer to supplementary material for details on image resolution and dataset size.</p><p>Network Architecture: For DAFormer <ref type="bibr" target="#b18">[19]</ref> + CLUDA, we use Swin-L <ref type="bibr" target="#b25">[26]</ref> as the backbone, while for HRDA <ref type="bibr" target="#b19">[20]</ref> + CLUDA, we use Segformer-B5 <ref type="bibr" target="#b48">[49]</ref>. For both sets of experiments, we use a simple MLP decoder <ref type="bibr" target="#b18">[19]</ref> with embedding dimension of 512. The same decoder is used in the segmentation head, scaled attention head as well as feature weight prediction head in HRDA <ref type="bibr" target="#b19">[20]</ref> + CLUDA.</p><p>Training: We follow the same training regime as followed in DAFormer <ref type="bibr" target="#b18">[19]</ref> and HRDA <ref type="bibr" target="#b19">[20]</ref> for DAFormer <ref type="bibr" target="#b18">[19]</ref> + CLUDA and HRDA <ref type="bibr" target="#b19">[20]</ref> + CLUDA respectively. However we take C = 512 as the dimension of the fused feature map. We train our model using AdamW <ref type="bibr" target="#b27">[28]</ref> with learning rate of 6 ? 10 ?5 for encoder and a learning rate of 6 ? 10 ?4 for decoder and keep betas as (0.9, 0.999), weight decay of 0.01, and a batch size of 2. For self-training we keep the value of EMA weight update parameter ? = 0.999, for learning-rate optimization we follow polynomial-learning rate reduction. We increase the learning-rate for the first 1500 iterations with a warmup rate of 10 ?6 and follow the polynomial-reduction of the learning-rate <ref type="bibr" target="#b18">[19]</ref> after that. In HRDA <ref type="bibr" target="#b19">[20]</ref> + CLUDA, the learned feature weight obtained from the feature weight prediction head has the dimension 1?h?w where h?w is the spatial dimension of the feature map.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with existing UDA methods</head><p>We begin by comparing the proposed approach with existing UDA methods. We show that CLUDA improves the existing method by a margin of +0.6 mIoU in GTA ? Cityscapes in <ref type="table" target="#tab_1">Table 1</ref> and +1.4 mIoU in SYNTHIA ? Cityscapes in <ref type="table" target="#tab_2">Table 2</ref>. Class-wise improvements can be seen in 12 of the 19 classes in GTA ? Cityscapes, Where major improvements can be seen in difficult classes like Sidewalk, Fence, etc., and in 13 of the 16 classes in SYN-THIA ? Cityscapes, as also supported quantitatively in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>DAFormer <ref type="bibr" target="#b18">[19]</ref> DAFormer <ref type="bibr" target="#b18">[19]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison of network architectures with CLUDA</head><p>We perform ablation studies with different backbones network architectures in <ref type="table" target="#tab_6">Table 4</ref>. Our method shows consistent improvement over DAFormer <ref type="bibr" target="#b18">[19]</ref> irrespective of the backbone architecture, which implied that our methods are agnostic to network architecture. The best results are obtained with Swin-L[26] backbone. <ref type="table" target="#tab_4">Table 3</ref> shows why weighting the contrastive loss computed on the domain-mixed-features is important. Backpropagation signals from the contrastive loss in the early stage of training are very high, when the model predictions are highly unreliable, and are detrimental to the overall performance. Hence, we hinge the loss weights on the teacher-network prediction confidence computed from equation 4. This regularizes the model learning by applying low weights when the prediction confidence is low. The weights increase as the teacher-network becomes more confident. Further, <ref type="figure" target="#fig_1">Fig. 4</ref> shows how the teacher network makes more confident predictions as the training progresses. The performance drops in the absence of confidence weights, as can be seen in <ref type="figure" target="#fig_1">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Influence of Confidence Weights on Contrastive loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CL type mIoU</head><p>S-S + T-T + S-T 67.4 S-S 68.9 S-S + T-T 69 <ref type="table">Table 5</ref>: Ablation of different strategies used while computing contrastive loss over the fused feature maps. We adopt three different strategies and compare the performance of our method. In S-S (stuff-stuff), the contrastive losses are only computed between stuff class features. In S-S + T-T (stuff-stuff + thing-thing) contrastive losses are computed separately over stuff class features and thing class features. In S-S + T-T + S-T (Stuff-Stuff + Thing-Thing + Stuff-Thing), we compute contrastive loss taking into account every pair of feautes irrespective of stuff or thing class. All experiments are conducted on GTA?Cityscapes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Influence of of separation of features into stuffthing classes</head><p>Transformer-based feature-extractors suffer from shapebias <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. We address this issue by applying the contrastive loss separately on stuff class and thing class features in the fused feature map. In other words, contrastive loss is computed taking into account pairwise thing class features and pairwise stuff class features separately, while ignoring stuff-thing feature pairs. In <ref type="table">Table 5</ref>, we compare different strategies used while applying contrastive losses on the features. We acheive the best performance when we treat stuff class features and thing class separately as discussed above. Adopting this strategy reduces the probability of pixels belonging to stuff classes getting misclassified as thing class and vice-a-versa. For eg., The sign boards are often misclassified as traffic signal (supporting figure is provided in the supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative Analysis</head><p>We provide visual illustrations of how CLUDA improves performance of state-of-the-art UDA methods in semantic segmentation. <ref type="figure" target="#fig_2">Fig. 5</ref> shows imporvements achieved by Daformer <ref type="bibr" target="#b18">[19]</ref>+CLUDA in single resolution and by HRDA <ref type="bibr" target="#b19">[20]</ref>+CLUDA in multi-resolution. One can clearly notice the significant improvements in segmentation of difficult classes such as Fence, Sidewalks, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods mIoU</head><p>HRDA <ref type="bibr" target="#b19">[20]</ref> 73.8 HRDA <ref type="bibr" target="#b19">[20]</ref>+CLUDA <ref type="table">(LR only)</ref> 72.2 HRDA <ref type="bibr" target="#b19">[20]</ref>+CLUDA (LR + HR) 72.9 HRDA <ref type="bibr" target="#b19">[20]</ref>+CLUDA (Weighted(LR + HR)) 74.4 <ref type="table">Table 6</ref>: Comparison of different ways of combining high resolution(HR) and low resolution(LR) features obtained from HR crops and LR crops respectively. In (LR only), the features from LR crops are considered only. In (LR + HR), both LR features and HR features are taken into account and corresponding contrastive losses are added. In (weighted(LR + HR)), we combine the respective contrastive losses by weighing them using a learned feature weight map. The experiments are conducted on GTA ? Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Influence of weighting contrastive losses computed on low resolution and high resolution features</head><p>It is important to take into account contrastive losses computed on both low resolution(LR) as well as high resolution(HR) features. However, naively adding the two may not be the best possible strategy to combine the said losses, as can be seen in the <ref type="table">Table 6</ref>. We weigh the contrastive losses computed on LR and HR features and add them as per equation 7, which gives the best performance. Although rows 2 and 3 of <ref type="table">Table 6</ref> suggest that using contrastive losses over LR and HR features is not beneficial, by simply letting the model learn the weights for combining losses from LR and HR features gives us a boost in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we presented CLUDA, a simple and effective strategy to improve UDA in semantic segmentation using contrastive loss. CLUDA can be used along with existing cross-entropy-based losses to train models. We also showed that CLUDA can be used with different featureextractor architectures with marginal extra memory consumption. For multi-resolution DA, we extended CLUDA by using learned-feature weighting to combine contrastive loss computed on multiple resolutions. Overall CLUDA produced the state-of-the-art model performance of 74.4 mIoU in GTA ? Cityscapes with a gain of +0.6 and 67.2 mIoU in SYNTHIA ? Cityscapes with a gain of +1.4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Feature representation learned by DAFormer[19] vs CLUDA. Classes in their respective representation colors are following: Road, Sidewalk, Building, Wall, Fence, Pole, T. Light, T. Sign, Vegetation, Terrain, Sky, Person, Rider, Car, Truck, Bus, Train, Motorcycle, Bicycle. CLUDA shows significant improvement in feature representation, stuff classes like Sky, Building, Vegetation are clearly separated. Even difficult classes like Fence, Terrain, Motorcycle, Bicycle are well separated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Shows improvement in the mIoU score when the contrastive losses applied on the domain-mixed features are weighted according to the confidence score computed using equation 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative analysis of CLUDA on GTA ? Cityscapes with existing SOTA methods. Note that CLUDA uses only low resolution images and hence must be compared with DAFormer for fair comparison. Multi-resolution CLUDA uses both high and low resolution crops.Fig. showsthe performance improvement in classes like Sidewalk, Fence, Bus, etc highlighted using dotted boxes. For more qualitative analysis on GTA ? Cityscapes and SYNTHIA ? Cityscapes please refer to supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MethodsRoad S.Walk Build. Wall Fence Pole T. Light T. Sign Veget. Terrain Sky Person Rider Car Truck Bus Train M.Bike Bike mIoU AdaptSeg[41] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 CBST[59] 91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9 DACS[40] 89.9 39.7 87.9 30.7 39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.2 0.0 27.3 34.0 52.1 CorDA[44] 94.7 63.1 87.6 30.7 40.6 40.2 47.8 51.6 87.6 47.0 89.7 66.7 35.9 90.2 48.9 57.5 0.0 39.8 56.0 56.6 BAPA[24] 94.4 61.0 88.0 26.8 39.9 38.3 46.1 55.3 87.8 46.1 89.4 68.8 40.0 90.2 60.4 59.0 0.0 45.1 54.2 57.4 ProDA[54] 87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.5</cell></row><row><cell>DAFormer[19]</cell><cell cols="3">95.7 70.2 89.4 53.5 48.1 49.6 55.8</cell><cell cols="4">59.4 89.9 47.9 92.5 72.2 44.7 92.3 74.5 78.2 65.1 55.9 61.8</cell><cell>68.3</cell></row><row><cell>DAFormer[19] + CLUDA(Ours)</cell><cell cols="3">97.5 78.8 88.8 60.8 52 47.1 51.9</cell><cell>50.3 89.7</cell><cell>51</cell><cell>94</cell><cell>71</cell><cell>48.6 93.1 82 84.1 71.4 58.9 60.7 70.11</cell></row><row><cell>HRDA [20]</cell><cell cols="2">96.4 74.4</cell><cell>91 61.6 51.5 57.1 63.9</cell><cell cols="4">69.3 91.3 48.4 94.2 79.0 52.9 93.9 84.1 85.7 75.9 63.9 67.5</cell><cell>73.8</cell></row><row><cell>HRDA[20] + CLUDA(Ours)</cell><cell>97.1</cell><cell>78</cell><cell>91 60.3 55.3 56.3 64.3</cell><cell cols="4">71.5 91.2 51.1 94.7 78.4 52.9 94.5 82.8 86.5 73</cell><cell>64.2 69.7</cell><cell>74.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with existing UDA methods on GTA ? Cityscapes.</figDesc><table><row><cell>Methods</cell><cell cols="7">Road S.Walk Build. Wall Fence Pole T. Light T. Sign Veget. Sky Person Rider Car Bus M.Bike Bike mIoU</cell></row><row><cell>AdaptSeg[41]</cell><cell>79.2 37.2 78.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.9</cell><cell>10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3</cell><cell>37.2</cell></row><row><cell>CBST[59]</cell><cell cols="5">68.0 29.9 76.3 10.8 1.4 33.9 22.8</cell><cell>29.5 77.6 78.3 60.6 28.3 81.6 23.5 18.8 39.8</cell><cell>42.6</cell></row><row><cell>DACS[40]</cell><cell cols="5">80.6 25.1 81.9 21.5 2.9 37.2 22.7</cell><cell>24.0 83.7 90.8 67.5 38.3 82.9 38.9 28.5 47.6</cell><cell>48.3</cell></row><row><cell>CorDA[44]</cell><cell cols="5">93.3 61.6 85.3 19.6 5.1 37.8 36.6</cell><cell>42.8 84.9 90.4 69.7 41.8 85.6 38.4 32.6 53.9</cell><cell>55.0</cell></row><row><cell>BAPA[24]</cell><cell cols="5">91.7 53.8 83.9 22.4 0.8 34.9 30.5</cell><cell>42.8 86.8 88.2 66.0 34.1 86.6 51.3 29.4 50.5</cell><cell>53.3</cell></row><row><cell>ProDA[54]</cell><cell cols="5">93.3 61.6 85.3 19.6 5.1 37.8 36.6</cell><cell>42.8 84.9 90.4 69.7 41.8 85.6 38.4 32.6 53.9</cell><cell>55.0</cell></row><row><cell>DAFormer[19]</cell><cell cols="5">84.5 40.7 88.4 41.5 6.5 50.0 55.0</cell><cell>54.6 86.0 89.8 73.2 48.2 87.2 53.2 53.9 61.7</cell><cell>60.9</cell></row><row><cell>DAFormer[19] + CLUDA(Ours)</cell><cell cols="5">87.4 44.8 86.5 47.9 8.7 49.8 44.5</cell><cell>52.7 85.6 89.2 74.4 50.2 86.9 65.3 56.85 57.1</cell><cell>61.7</cell></row><row><cell>HRDA [20]</cell><cell cols="5">85.2 47.7 88.8 49.5 4.8 57.2 65.7</cell><cell>60.9 85.3 92.9 79.4 52.8 89.0 64.7 63.9 64.9</cell><cell>65.8</cell></row><row><cell>HRDA [20] + CLUDA</cell><cell cols="5">87.7 46.9 90.2 49 7.9 59.5 66.9</cell><cell>58.5 88.3 94.6 80.1 57.1 89.8 68.2 65.5 65.8</cell><cell>67.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison with existing UDA methods on SYNTHIA ? Cityscapes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation of weighted and unweighted contrastive loss applied over domain mixed features in DAFormer[19] + CLUDA. The contrastive losses are applied over fused feature map obtained from source and domain mixed images separately. However features obtained from domain mixed images are weighted. All experiments are conducted with fused feature embedding dimension C = 512. All experiments are conducted on GTA?Cityscapes and trained over 40K iterations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of DAFormer[19] + CLUDA with DAFormer[19] on GTA?Cityscapes using different backbone network architectures.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel focal tversky loss function with improved attention u-net for lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabila</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naimul Mefraz</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding robustness of transformers for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10231" to="10241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint contrastive learning with infinite possibilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12638" to="12648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Resunet-a: A deep learning framework for semantic segmentation of remotely sensed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Foivos I Diakogiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Caccetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="94" to="114" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01916</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three ways to improve semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Koring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11130" to="11140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14887</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hrda: Context-aware high-resolution domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13132</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Region-aware contrastive learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16291" to="16301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with directional context-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1205" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Domain adaptation for semantic segmentation via patch-wise contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Zebedin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11056</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bapa-net: Boundary adaptation and prototype alignment for cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8801" to="8811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Source-free domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantically adaptive image-to-image translation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Musto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zinelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01166</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Acdc: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10765" to="10775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Strided u-net model: Retinal vessels segmentation using dice loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toufique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Soomro ; Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoranjan</forename><surname>Hellwich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications (DICTA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5688" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via crossdomain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Positive-negative equal contrastive loss for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfei</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyun</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.01417</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8515" to="8525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7303" to="7313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3024" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cross-region domain adaptation for class-level alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06422</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoren</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08808</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Looking beyond single images for contrastive semantic segmentation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3285" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Brain tumor segmentation based on refined fully convolutional neural networks with a hierarchical dice loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09093</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Contrastive learning for label efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Andrew</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10623" to="10633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pixel contrastive-consistent semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7273" to="7282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improving semantic segmentation via self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14960</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09713</idno>
	</analytic>
	<monogr>
		<title level="m">Jia-Bin Huang, and Tomas Pfister</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
