<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Molecular Graph Convolutions: Moving Beyond Fingerprints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
							<email>kearnes@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
							<email>mccloskey@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
							<email>marcberndl@google.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
							<email>pande@stanford.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Molecular Graph Convolutions: Moving Beyond Fingerprints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Molecular "fingerprints" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make datadriven decisions. We describe molecular graph convolutions, a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph-atoms, bonds, distances, etc.-which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement. Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. Journal of chemical information and modeling, 53 <ref type="formula">(7)</ref>:1563-1575, 2013. . Deep neural nets as a method for quantitative structureactivity relationships. Journal of chemical information and modeling, 55(2):263-274, 2015.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computer-aided drug design requires representations of molecules that can be related to biological activity or other experimental endpoints. These representations encode structural features, physical properties, or activity in other assays <ref type="bibr" target="#b19">[Todeschini and Consonni, 2009;</ref><ref type="bibr">Petrone et al., 2012]</ref>. The recent advent of "deep learning" has enabled the use of very raw representations that are less applicationspecific when building machine learning models <ref type="bibr">[Le-Cun et al., 2015]</ref>. For instance, image recognition models that were once based on complex features extracted from images are now trained exclusively on the pixels themselves-deep architectures can "learn" appropriate representations for input data. Consequently, deep learning systems for drug screening or design should benefit from molecular representations that are as complete and general as possible rather than relying on application-specific features or encodings.</p><p>First-year chemistry students quickly become familiar with a common representation for small molecules: the molecular graph. <ref type="figure" target="#fig_0">Figure 1</ref> gives an example of the molecular graph for ibuprofen, an over-the-counter non-steroidal anti-inflammatory drug. The atoms and bonds between atoms form the nodes and edges, respectively, of the graph. Both atoms and bonds have associated properties, such as atom type and bond order. Although the basic molecular graph representation does not capture the quantum mechanical structure of molecules or necessarily express all of the information that it might suggest to an expert medicinal chemist, its ubiquity in academia and industry makes it a desirable starting point for machine learning on chemical information.</p><p>Here we describe molecular graph convolutions, a deep learning system using a representation of small molecules as undirected graphs of atoms. Graph con-volutions extract meaningful features from simple descriptions of the graph structure-atom and bond properties, and graph distances-to form moleculelevel representations that can be used in place of fingerprint descriptors in conventional machine learning applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The history of molecular representation is extremely diverse <ref type="bibr" target="#b19">[Todeschini and Consonni, 2009</ref>] and a full review is outside the scope of this report. Below we describe examples from several major branches of the field to provide context for our work. Additionally, we review several recent examples of graph-centric approaches in cheminformatics.</p><p>Much of cheminformatics is based on so-called "2D" molecular descriptors that attempt to capture relevant structural features derived from the molecular graph. In general, 2D features are computationally inexpensive and easy to interpret and visualize. One of the most common representations in this class is extended-connectivity fingerprints (ECFP), also referred to as circular or Morgan fingerprints <ref type="bibr">[Rogers and Hahn, 2010]</ref>. Starting at each heavy atom, a "bag of fragments" is constructed by iteratively expanding outward along bonds (usually the algorithm is terminated after 2-3 steps). Each unique fragment is assigned an integer identifier, which is often hashed into a fixed-length representation or "fingerprint". Additional descriptors in this class include decompositions of the molecular graph into subtrees or fixed-length paths <ref type="bibr">[OpenEye GraphSim Toolkit]</ref>, as well as atom pair (AP) descriptors that encode atom types and graph distances (number of intervening bonds) for all pairs of atoms in a molecule <ref type="bibr" target="#b3">[Carhart et al., 1985]</ref>.</p><p>Many representations encode 3D information, with special emphasis on molecular shape and electrostatics as primary drivers of interactions in real-world systems. For example, rapid overlay of chemical structures (ROCS) aligns pairs of pre-generated conformers and calculates shape and chemical ("color") similarity using Gaussian representations of atoms and color features defined by a simple force field <ref type="bibr" target="#b10">[Hawkins et al., 2007]</ref>. ROCS can also be used to generate alignments for calculation of electrostatic field similarity <ref type="bibr">[Muchmore et al., 2006]</ref>. Ultrafast shape recognition (USR) calculates alignment-free 3D similarity by comparing distributions of intramolecular distances <ref type="bibr" target="#b1">[Ballester and Richards, 2007]</ref>.</p><p>The Merck Molecular Activity Challenge <ref type="bibr">[Dahl, 2012]</ref> catalyzed interest in deep neural networks trained on fingerprints and other molecular descrip-tors. In particular, multitask neural networks have produced consistent gains relative to baseline models such as random forest and logistic regression <ref type="bibr" target="#b5">[Dahl et al., 2014;</ref><ref type="bibr">Ma et al., 2015;</ref><ref type="bibr">Mayr et al., 2015;</ref><ref type="bibr">Ramsundar et al., 2015]</ref>.</p><p>Other approaches from both the cheminformatics and the machine learning community directly operate on graphs in a way similar to how we do here. The "molecular graph networks" of Merkwirth and Lengauer <ref type="bibr">[2005]</ref> iteratively update a state variable on each atom with learned weights specific to each atom type-bond type pair. Similarly, <ref type="bibr" target="#b14">Micheli [2009]</ref> presents a more general formulation of the same concept of iterated local information transfer across edges and applies this method to predicting the boiling point of alkanes.</p><p>Scarselli et al. <ref type="bibr">[2009]</ref> similarly defines a local operation on the graph. They demonstrate that a fixed point across all the local functions can be found and calculate fixed point solutions for graph nodes as part of each training step. In another vein, <ref type="bibr">Lusci et al. [2013]</ref> convert undirected molecular graphs to a directed recursive neural net and take an ensemble over multiple conversions.</p><p>Recently, Duvenaud et al. <ref type="bibr">[2015]</ref> presented an architecture trying to accomplish many of the same goals as this work. The architecture was based on generalizing the fingerprint computation such that it can be learned via backpropagation. They demonstrate that this architecture improves predictions of solubility and photovoltaic efficiency but not binding affinity. <ref type="bibr" target="#b2">Bruna et al. [2013]</ref> introduce convolutional deep networks on spectral representations of graphs. However, these methods apply when the graph structure is fixed across examples and only the labeling/features on individual nodes varies.</p><p>Convolutional networks on non-Euclidean manifolds were described by <ref type="bibr">Masci et al. [2015]</ref>. The problem addressed was to describe the shape of the manifold (such as the surface of a human being) in such a way that the shape descriptor of a particular point was invariant to perturbations such as movement and deformation. They also describe an approach for combining local shape descriptors into a global descriptor and demonstrate its use in a shape classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep neural networks</head><p>Neural networks are directed graphs of simulated "neurons". Each neuron has a set of inputs and com-putes an output. The neurons in early neural nets were inspired by biological neurons and computed an affine combination of the inputs followed by a nonlinear activation function. Mathematically, if the inputs are x 1 . . . x N , weights w 1 . . . w N and bias b are parameters, and f is the activation function, the output is</p><formula xml:id="formula_0">f (b + i w i x i )<label>(1)</label></formula><p>Popular activation functions include the sigmoid function (f (z) = 1 1+e ?z ) and rectified linear unit</p><formula xml:id="formula_1">(ReLU) (f (z) = 0 if z ? 0 else z).</formula><p>Any mostly differentiable function can be used as the unit of computation for a neuron and in recent years, many other functions have appeared in published networks, including max and sum.</p><p>Convolution in neural networks refers to using the same parameters (such as the w i in Equation 1) for different neurons that are attached to different parts of the input (or previous neurons). In this way, the same operation is computed for many different subsets of the input.</p><p>At the "top" of the neural network you have node(s) whose output is the value you are trying to predict (e.g. the probability that this molecule binds to a target or the binding affinity). Many output nodes for different tasks can be added and this is commonly done <ref type="bibr">[Ma et al., 2015;</ref><ref type="bibr">Ramsundar et al., 2015]</ref>. In this way, different output tasks can share the computation and model parameters in lower parts of the network before using their own parameters for the final output steps.</p><p>The architecture of a neural network refers to the choice of the number of neurons, the type of computation each one does (including what learnable parameters they have), which parameters are shared across neurons, and how the output of one neuron is connected to the input of another.</p><p>In order to train the network, you first have to choose a loss function describing the penalty for the network producing a set of outputs which differ from the outputs in the training example. For example, for regression problems, the L2 distance between the predicted and actual values is commonly used. The objective of training is then to find a set of parameters for the network that minimizes the loss function. Training is done with the well known technique of back-propagation <ref type="bibr">[Rumelhart et al., 1986]</ref> and stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Desired invariants of a model</head><p>A primary goal of designing a deep learning architecture is to restrict the set of functions that can be learned to ones that match the desired properties from the domain. For example, in image understanding, spatial convolutions force the model to learn functions that are invariant to translation.</p><p>For a deep learning architecture taking a molecular graph as input, some arbitrary choice must be made for the order that the various atoms and bonds are presented to the model. Since that choice is arbitrary, we want:</p><formula xml:id="formula_2">Property 1 (Order invariance).</formula><p>The output of the model should be invariant to the order that the atom and bond information is encoded in the input.</p><p>Note that many current procedures for fingerprinting molecules achieve Property 1. We will now gradually construct an architecture which achieves Property 1 while making available a richer space of learnable parameters.</p><p>The first basic unit of representation is an atom layer which contains an n-dimensional vector associated with each atom. Therefore the atom layer is a 2 dimensional matrix indexed first by atom. Part of the original input will be encoded in such an atom layer and the details of how we construct the original input vector are discussed in Section 3.5. The next basic unit of representation is a pair layer which contains an n-dimensional vector associated with each pair of atoms. Therefore, the pair layer is a 3 dimensional matrix where the first two dimensions are indexed by atom. Note that the pair input can contain information not just about edges but about any arbitrary pair. Notably, we will encode the graph distance (length of shortest path from one atom to the other) in the input pair layer. The order of the atom indexing for the atom and pair layer inputs must be the same.</p><p>We will describe various operations to compute new atom and pair layers with learnable parameters at every step. Notationally, let A x be the value of a particular atom layer x and P y be the value of a particular pair layer y. The inputs that produce those values should be clear from the context. A x a refers to the value of atom a in atom layer x and P y (a,b) refers to the value of pair (a, b) in pair layer y.</p><p>In order to achieve Property 1 for the overall architecture, we need a different type of invariance for each atom and pair layer.</p><p>Property 2 (Atom and pair permutation invariance). The values of an atom layer and pair permute with the original input layer order. More precisely, if the inputs are permuted with a permutation operator Q, then for all layers x, y, A x and P y are permuted with operator Q as well.</p><p>In other words, Property 2 means that from a single atom's (or pair's) perspective, its value in every layer is invariant to the order of the other atoms (or pairs).</p><p>Since molecules are undirected graphs, we will also maintain the following: Property 3 is easy to achieve at the input layer and the operations below will maintain this.</p><p>Properties 2 and 3 make it easy to construct a molecule-level representation from an atom or pair such that the molecule-level representation achieves Property 1 (see Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Invariant-preserving operations</head><p>We now define a series of operations that maintain the above properties.</p><p>Throughout, f represents an arbitrary function and g represents an arbitrary commutative function (g returns the same result regardless of the order the arguments are presented). In this work, f is a learned linear operator with a rectified linear (ReLU) activation function and g is a sum.</p><p>The most trivial operation is to combine one or more layers of the same type by applying the same operation to every atom or pair. Precisely, this means if you have layers x1, x2, . . . , xn and function f , you can compute a new atom layer from the previous atom layer (A ? A) as</p><formula xml:id="formula_3">A y a = f (A x1 a , A x2 a , . . . , A xn a )<label>(2)</label></formula><p>or pair layer from the previous pair layer (P ? P ) as</p><formula xml:id="formula_4">P y a,b = f (P x1 a,b , P x2 a,b , . . . , P xn a,b )<label>(3)</label></formula><p>Since we apply the same function for every atom/pair, we refer to this as a convolution. All the transformations we develop below will have this convolution nature of applying the same operation to every atom/pair, maintaining Property 2. When operating on pairs of atoms, instead of putting all pairs through this function, you could select a subset. In Section 4.3.3 we show experiments for restricting the set of pairs to those that are less than some graph distance away.</p><p>Next, consider an operation that takes a pair layer x and constructs an atom layer y (P ? A). The operation is depicted in <ref type="figure" target="#fig_2">Figure 2</ref>. Formally:</p><formula xml:id="formula_5">A y a = g(f (P x (a,b) ), f (P x (a,c) ), f (P x (a,d) ), ...)<label>(4)</label></formula><p>In other words, take all pairs of which a is a part, run them through f , and combine them with g. Note that Property 3 means we can choose an arbitrary one of P x <ref type="bibr">(a,b)</ref> or P x (b,a) . The most interesting construction is making a pair layer from an atom layer (A ? P ). The operation is graphically depicted in <ref type="figure" target="#fig_1">Figure 3</ref> and formally as</p><formula xml:id="formula_6">P y ab = g(f (A x a , A x b ), f (A x b , A x a ))<label>(5)</label></formula><p>Note that just applying g to A x a and A x b would maintain Properties 2 and 3 but we use this more complex form. While commutative operators (such as max pooling) are common in neural networks, commutative operators with learnable parameters are not common. Therefore, we use f to give learnable parameters while maintaining the desired properties.</p><p>Once we have all the primitive operations on atom and pair layers (A ? A, P ? P , P ? A, A ? P ), we can combine these into one module. We call this the Weave module ( <ref type="figure">Figure 4</ref>) because the atoms and pair layers cross back and forth to each other. The module can be stacked to an arbitrary depth similar to the Inception module that inspired it . Deep neural networks with many layers (e.g. for computer vision) learn progressively more general features-combinations of lower-level features-in a hierarchical manner <ref type="bibr">[LeCun et al., 2015]</ref>. By analogy, successive Weave modules can produce more informative representations of the original input. Additionally, stacked Weave modules with limited maximum atom pair distance progressively incorporate longerrange information at each layer. </p><formula xml:id="formula_7">P y (a,b) g(v 1 ,v 2 ) v 1 f(A x a ,A x b ) v 2 f(A x b ,A x a ) a b A x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Molecule-level features</head><p>The construction of the Weave module maintains Properties 2 and 3. What about overall order invariance (Property 1)? At the end of a stack of Weave modules we are left with an n-dimensional vector associated with every atom and an m-dimensional vector associated with every pair. We need to turn this into a molecule-level representation with some commutative function of these vectors.</p><p>In related work <ref type="bibr">[Merkwirth and Lengauer, 2005;</ref><ref type="bibr" target="#b8">Duvenaud et al., 2015;</ref><ref type="bibr">Lusci et al., 2013]</ref>, a simple unweighted sum is often used to combine orderdependent atom features into order-independent molecule-level features. However, reduction to a single value does not capture the distribution of learned features. We experimented with an alternative approach and created "fuzzy" histograms for each dimension of the feature vector.</p><p>A fuzzy histogram is described by a set of membership functions that are functions with range [0, 1] representing the membership of the point in each histogram bin <ref type="bibr" target="#b23">[Zadeh, 1965]</ref>. A standard histogram has membership functions which are 1 in the bin and 0 everywhere else. For each point, we normalize so that the total contribution to all bins is 1. The value of a bin in the histogram over all points is just the sum of the normalized contributions for all the points. <ref type="figure" target="#fig_4">Figure 5</ref> gives an example of a fuzzy histogram composed of three Gaussian bins. A histogram is constructed for each dimension of the feature vectors and the concatenation of those histograms is the <ref type="figure">Figure 4</ref>: Weave module. This module takes matrices A k and P k (containing atom and pair features, respectively) and combines A ? A, P ? P , P ? A, and A ? P operations to yield a new set of atom and pair features (A k+1 and P k+1 , respectively). The output atom and pair features can be used as input to a subsequent Weave module, which allows these modules to be stacked in series to an arbitrary depth.</p><formula xml:id="formula_8">A k?? (A?A) 0 P k? A k? P k A k P k+1 A k+1 P k?? (A?P) 0 (P?A) 0 (P?P) 0 (A?A) 1 (P?P) 1</formula><p>molecule-level representation.</p><p>In this work we used Gaussian membership functions (which are unnormalized versions of the standard Gaussian PDF) with eleven bins spanning a Gaussian distribution with mean of zero and unit standard deviation, shown in <ref type="figure">Figure F</ref>.1. These bins were chosen somewhat arbitrarily to cover the expected distribution of incoming features and were not optimized further (note that the incoming features were batch normalized; see Section 3.7).</p><p>Throughout this paper, we construct the moleculelevel features only from the top-level atom features and not the pair features. This is to restrict the total number of feature vectors that must be summarized while still providing information about the entire molecule. Note, however, that the initial and intermediate pair features can influence the final atom features through Weave module operations.</p><p>Before the molecule-level featurization, we do one final convolution on the atoms. Since molecule-level featurization can be a major bottleneck in the model, this convolution expands the depth so that each dimension of the atom feature vector contains less information and therefore less information is lost during the molecule-level featurization. On this convolution, we do not use a ReLU activation function to avoid the histogram having many points at zero.</p><p>Once you have a molecule-level representation, this becomes a more standard multitask problem. We follow the common approach <ref type="bibr">[Ramsundar et al., 2015;</ref><ref type="bibr">Ma et al., 2015;</ref><ref type="bibr">Mayr et al., 2015]</ref>  of fully connected layers on top of the molecule-level features followed by standard softmax classification.</p><p>The overall architecture is depicted in <ref type="figure" target="#fig_5">Figure 6</ref>. <ref type="table" target="#tab_0">Table 1</ref> lists hyperparameters and default values for graph convolution models. In models with multiple Weave modules it is conceivable to vary the convolution depths in a module-specific way. However, the models in this work used the same settings for all Weave modules.</p><p>Our current implementation imposes an upper limit on the number of heavy atoms represented in the initial featurization. For molecules that have more than the maximum number of atoms, only a subset of atoms (and therefore atom pairs) are represented in the input encoding. This subset depends on the order in which the atoms are traversed by the featurization code and should be considered arbitrary. In this work we set the maximum number of atoms to 60, and only 814 of the 1 442 713 unique molecules in our datasets (see Section 3.6) exceed this limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Input featurization</head><p>The initial atom and pair features are summarized in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>, respectively. The features are a mix of floating point, integer, and binary values (all encoded as floating point numbers in the network). The feature set is intended to be broad, but not necessarily exhaustive, and we recognize that some features can potentially be derived from or correlated to a subset of the others (e.g. atom hybridization can be determined by inspecting the bonds that atom makes). We performed experiments using a "simple" subset of these features in an effort to understand their relative contributions to learning (Section 4.2), but many other questions about specifics of the input featurization are left to future work.</p><p>All features were generated with RDKit [Landrum, 2014], including Gasteiger atomic partial charges <ref type="bibr" target="#b9">[Gasteiger and Marsili, 1980]</ref>. Although our featurization includes space for hydrogen atoms, we did not use explicit hydrogens in any of our experiments in order to conserve memory and emphasize contributions from heavy atoms.</p><p>Other deep learning applications with more "natural" inputs such as computer vision and speech recognition still require some input engineering; for example, adjusting images to a specific size or scale, or transforming audio into the frequency domain. Likewise, the initial values for the atom and pair layers describe these primitives in terms of properties that are often considered by medicinal chemists and other experts in the field, allowing the network to use or ignore them as needed for the task at hand. One of the purposes of this work is to demonstrate that learning can occur with as little preprocessing as possible. Accordingly, we favor simple descriptors that are more or less "obvious".   Graph distance * For each distance (1-7), whether the shortest path between the atoms in the pair is less than or equal to that number of bonds (binary values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>Same ring Whether the atoms in the pair are in the same ring. 1 12 * Included in the "simple" featurization (see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Datasets</head><p>We used a dataset collection nearly identical to the one described by <ref type="bibr">Ramsundar et al. [2015]</ref> except for some changes to the data processing pipeline (including the duplicate merging process for the Tox21 dataset) and different cross-validation fold divisions. Briefly, there are 259 datasets divided into four groups indicating their source: PubChem BioAssay <ref type="bibr" target="#b22">[Wang et al., 2012]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Model training and evaluation</head><p>Graph convolution and traditional neural network models were implemented with TensorFlow <ref type="bibr" target="#b0">[Abadi et al., 2015]</ref>, an open-source library for machine learning. Models were evaluated by the area under the receiver operating characteristic curve (ROC AUC, or simply AUC) as recommended by Jain and Nicholls <ref type="bibr">[2008]</ref>. We used 5-fold stratified crossvalidation, where each fold-specific model used 60% of the data for training, 20% for validation (early stopping/model selection), and 20% as a test set. Graph convolution models were trained for 10-20 M steps using the Adagrad optimizer <ref type="bibr" target="#b7">[Duchi et al., 2011]</ref> with learning rate 0.003 and batch size 96, with periodic checkpointing. All convolution and fullyconnected layer outputs were batch normalized <ref type="bibr" target="#b11">[Ioffe and Szegedy, 2015]</ref> prior to applying the ReLU nonlinearity. Training was parallelized over 96 CPUs (or 96 GPUs in the case of the W 4 N 2 model) and required several days for each model. Adding additional Weave modules significantly increased training time. However, models trained on smaller datasets (see Section 3.8) trained much faster.</p><p>To establish a baseline, we also trained pyramidal (2000, 100) multitask neural network (PMTNN) <ref type="bibr">[Ramsundar et al., 2015]</ref>, random forest (RF), and logistic regression (LR) models using Morgan fingerprints with radius 2 (essentially equivalent to ECFP4) generated with RDKit [Landrum, 2014]. As a very simple baseline, we also computed Tanimoto similarity to all training set actives and used the maximum similarity score as the active class probability (MaxSim).</p><p>The PMTNN had two hidden layers (with 2000 and 100 units, respectively) with rectified linear activa-tions, and each fold-specific model was trained for 40-50 M steps using the SGD optimizer with batch size 128 and a learning rate of 0.0003, with periodic checkpointing. Additionally, this model used 0.25 dropout <ref type="bibr" target="#b16">[Srivastava et al., 2014]</ref>, initial weight standard deviations of 0.01 and 0.04 and initial biases of 0.5 and 3.0 in the respective hidden layers. This model did not use batch normalization.</p><p>Logistic regression (LR) models were trained with the LogisticRegression class in scikit-learn <ref type="bibr">[Pedregosa et al., 2011]</ref> using the 'lbfgs' solver and a maximum of 10 000 iterations. Values for the regularization strength (C) parameter were chosen by grid search, using the held-out validation set for model selection. Random forest (RF) models were trained using the scikit-learn RandomForestClassifier with 100 trees.</p><p>In graph convolution and PMTNN models, active compounds were weighted in the cost function such that the total active weight equalled the total inactive weight within each dataset (logistic regression and random forest models also used these weights as the sample_weight argument to their fit methods). Furthermore, graph convolution and PMTNN models were evaluated in a task-specific manner by choosing the training checkpoint with the best validation set AUC for each task. We note that some fold-specific models had a small number of tasks were not "converged" in the sense that their validation set AUC scores were still increasing when training was halted, and that the specific tasks that were not converged varied from model to model.</p><p>To statistically compare graph convolution and baseline models, we report three values for each dataset group: (1) median 5-fold mean AUC over all datasets, (2) median difference in per-dataset 5-fold mean AUC (?AUC) relative to the PMTNN baseline, and (3) a 95% Wilson score interval for the sign test statistic relative to the PMTNN baseline. The sign test estimates the probability that a model will achieve a higher 5-fold mean AUC than the PMTNN baseline; models with sign test confidence intervals that do not include 0.5 are considered significantly different in their performance (the median ?AUC can be used as a measure of effect size). To calculate these intervals, we used the proportion_confint function in statsmodels <ref type="bibr" target="#b15">[Seabold and Perktold, 2010</ref>] version 0.6.1 with method='wilson' and alpha=0.05, counting only non-zero differences in the sign test. We do not report values for the DUD-E dataset group since all models achieved &gt; 0.98 median 5-fold mean AUC.</p><p>As a general note, confidence intervals for box plot medians were computed as ?1.57 ? IQR/ ? N <ref type="bibr" target="#b25">[McGill et al., 1978]</ref> and do not necessarily correspond to sign test confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Comparisons to other methods</head><p>In addition to the baseline models described in Section 3.7, there are many other methods that would be interesting to compare to our graph convolution models. In particular, Duvenaud et al. <ref type="bibr">[2015]</ref> described "neural fingerprints" (NFP), a related graph-based method. The original publication describing NFP reported mean squared errors (MSE) on datasets for aqueous solubility, drug efficacy, and photovoltaic efficiency. We trained multitask graph convolution models on these datasets using 5-fold cross-validation (note that the published NFP models were singletask).</p><p>Additionally, we report results on a dataset used to validate the influence relevance voter (IRV) method of <ref type="bibr" target="#b17">Swamidass et al. [2009]</ref>, which is a hybrid of neural networks and k-nearest neighbors. The original publication reported results for two datasets, HIV and DHFR, but the latter was no longer available from its original source. We trained graph convolution models on the HIV dataset using 10-fold stratified crossvalidation. In each cross-validation round, one fold each was used for testing and validation (early stopping), and the remaining folds were used for training. We note that RDKit was only able to process 41 476 of the 42 678 SMILES strings in the HIV dataset. We report performance on this dataset using both ROC AUC and BEDROC <ref type="bibr" target="#b20">[Truchon and Bayly, 2007]</ref> with ? = 20.</p><p>Although we expect our results on these datasets to provide reasonable comparisons to published data, differences in fold assignments and variations in dataset composition due to featurization failures mean that the comparisons are not perfect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proof of concept</head><p>With so many hyperparameters to adjust, we sought to establish a centerpoint from which to investigate specific questions. After several experiments, we settled on a simple model with two Weave modules, a maximum atom pair distance of 2, Gaussian histogram molecule-level reductions, and two fullyconnected layers of size 2000 and 100, respectively. Notationally, we refer to this model as W 2 N 2 . Table 4 shows the performance of the W 2 N 2 model and related models derived from this centerpoint by varying a single hyperparameter. Additionally, <ref type="table" target="#tab_4">Table 4</ref> includes results for several baseline models: MaxSim, logistic regression (LR), random forest (RF), and pyramidal (2000, 100) multitask neural network (PMTNN) models trained on Morgan fingerprints.</p><p>Several graph convolution models achieved performance comparable to the baseline PMTNN on the classification tasks in our dataset collection, which is a remarkable result considering the simplicity of our input representation. For example, the centerpoint W 2 N 2 model is statistically indistinguishable from the PMTNN for the PCBA, MUV, and Tox21 dataset groups (we do not report results for the DUD-E dataset group because all models achieved extremely high median AUC scores). Additionally, many of the graph convolution models with worse performance than the PMTNN (i.e. sign test confidence intervals excluding 0.5) had very small effective differences as measured by median ?AUC.</p><p>As an additional measure of model performance, we also calculated ROC enrichment  scores at the following false positive rates: 1%, 5%, 10%, and 20%. Enrichment scores are reported in Section B and show that graph convolution models generally performed worse than or comparable to the PMTNN. We note that the analysis of model performance and hyperparameter optimization that follows is based only on ROC AUC scores.</p><p>We also trained graph convolution models on some additional datasets in order to compare to the "neural fingerprints" (NFP) of Duvenaud et al. <ref type="bibr">[2015]</ref> and the influence relevance voter (IRV) method of <ref type="bibr" target="#b17">Swamidass et al. [2009]</ref> (see Section 3.8). <ref type="table">Table 5</ref> compares graph convolution models to published results on these datasets under similar cross-validation conditions. Graph convolution results were comparable to published NFP models, with significant improvement on the photovoltaic efficiency task (note that the graph convolution results are from multitask models trained on all three NFP datasets while Duvenaud et al. <ref type="bibr">[2015]</ref> report values for single-task models). The 10-fold mean AUC and BEDROC scores on the HIV dataset were slightly lower than the published IRV values. However, we held out 10% of the data (one fold) in each cross-validation round as a validation set for checkpoint selection, meaning that the graph convolution models were trained with fewer examples than the published IRV models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Input featurization</head><p>As a further proof of concept and to address the importance of the initial featurization, we trained a model using a subset of features that match typical  2D structural diagrams seen in chemistry textbooks: only atom type, bond type, and graph distance are provided to the network. <ref type="figure" target="#fig_6">Figure 7</ref> compares a model trained with this "simple" input featurization to the "full" featurization containing all features from <ref type="table" target="#tab_1">Table 2 and Table 3</ref>. Both featurizations achieve similar median 5-fold mean AUC scores, suggesting that the additional features in the "full" representation are either mostly ignored during training or can be derived from a simpler representation of the molecular graph. Further work is required to understand the importance of individual features, perhaps with datasets that are sensitive to particular components of the input representation (such as hydrogen bonding or formal charge). <ref type="figure" target="#fig_9">Figure 8</ref> gives examples of how the initial atom features for a single molecule (ibuprofen) evolve as they progress through graph convolution Weave modules. The initial atom and pair feature encodings for the "full" featurization are depicted in Panel A. Comparing the initial atom features to their source molecular graph, the aromatic carbons in the central ring are clearly visible (and nearly identical in the featurization). The pair features are more difficult to interpret visually, and mostly encode graph distance.</p><p>As the atom features are transformed by the Weave modules (Panel B), they become more heterogeneous and reflective of their unique chemical environments. "Simple" features behave similarly, beginning with rather sterile initial values and quickly diverging as neighborhood information is included by Weave module operations (Panel C). Comparison of the "full" and "simple" atom features after the second Weave module shows that both featurizations lead to similarly diverse feature distributions. <ref type="figure" target="#fig_0">Figure E.1</ref> and <ref type="figure">Figure E</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameter sensitivity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Number of Weave modules</head><p>In relatively "local" models with limited atom pair distance, successive Weave modules update atom features with information from progressively larger regions of the molecule. This suggests that the number of Weave modules is a critical hyperparameter to optimize, analogous to the number of hidden layers in traditional neural networks. <ref type="figure">Figure 9</ref> compares models with 2-4 Weave modules to a model with a single Weave module. As expected, models with a single Weave layer were outperformed by deeper architectures. For the PCBA and Tox21 datasets, there was not much benefit to using more than two Weave modules ( <ref type="figure" target="#fig_0">Figure D.1)</ref>, but using three Weave modules gave the best median AUC for the MUV datasets (in exchange for significantly increased training time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Alternative feature reductions</head><p>The reduction of atom features from the final Weave module to an order-invariant, molecule-level representation is a major information bottleneck in graph convolution models. In related work, a simple unweighted sum <ref type="bibr" target="#b8">[Duvenaud et al., 2015;</ref><ref type="bibr">Merkwirth and Lengauer, 2005;</ref><ref type="bibr">Lusci et al., 2013]</ref> or root-meansquare (RMS) <ref type="bibr" target="#b6">[Dieleman, March 17, 2015]</ref> reduction is used. Using a consistent base architecture with two Weave modules and a maximum atom pair distance of 2, we compared these traditional reduction strategies with our Gaussian histogram approach. <ref type="figure" target="#fig_0">Figure 10</ref> shows that Gaussian histogram models had consistently improved scores relative to sum reductions. RMS reductions were not as robust as Gaussian histograms in terms of per-dataset differences relative to sum reductions, although RMS and Gaussian histogram reductions had similar distributions of absolute AUC values <ref type="figure" target="#fig_2">(Figure D.2)</ref>. Additionally, RMS reductions achieved a slightly higher median AUC than Gaussian histogram reductions on the MUV datasets. <ref type="table">Table 5</ref>: Comparison of graph convolution to neural fingerprint (NFP) and influence relevance voter (IRV) models. Section 3.8 provides details for datasets and experimental procedures. Note that the NFP comparisons were performed using multitask graph convolution models, and that graph convolution models for the HIV dataset were trained with fewer examples than IRV since one cross-validation fold was used as a held-out validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Dataset</head><p>Metric  <ref type="figure">Figure 9</ref>: Comparison of models with different numbers of Weave modules with a model containing a single Weave module. All models used a maximum atom pair distance of two. The y-axis is cropped to emphasize differences near zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Distance-dependent pair features</head><p>In Weave modules, atoms are informed about their chemical environment by mixing with pair features in the P ? A operation. Recall that during this operation, pair features are combined for pairs that contain a given atom, yielding a new representation for that atom. A critical parameter for this operation is the maximum distance (in bonds) allowed between the atoms of the pairs that are combined. If only adjacent atoms are combined, the resulting atom features will reflect the local chemical environment. As an alternative to increasing the number of Weave modules, longer-range interactions can be captured by increasing the maximum atom pair distance. However, our Gaussian histogram reductions vs. sum reduction. The y-axis reports difference in 5-fold mean AUC relative to sum reduction. All models used two Weave modules and a maximum atom pair distance of two. The y-axis is cropped to emphasize differences near zero.</p><p>implementation of the P ? A operation uses a simple sum to combine pair features, such that a large amount of information (possibly including every pair of atoms in the molecule) is combined in a way that could prevent useful information from being available in later stages of the network. <ref type="figure" target="#fig_0">Figure 11</ref> shows the performance of several models with different maximum pair distances relative to a model that used only adjacent atom pairs (N 1 ). For the PCBA datasets, a maximum distance of 2 (N 2 ) improves performance relative to the N 1 model, and N ? (no maximum distance) is clearly worse. However, the N 1 model achieves the best median AUC score for the MUV and Tox21 datasets <ref type="table" target="#tab_4">(Table 4</ref> and <ref type="figure" target="#fig_1">Figure D.3)</ref>. These results suggest that graph convolution models do not effectively make use of the initial graph distance features to preserve or emphasize distance-dependent information.</p><p>To further investigate the effect of distance information in Weave modules, we experimented with models that use distance-specific weights for operations involving pair features in order to maintain distance information explicitly throughout the network. However, results for these models are preliminary and were not included in this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Graph convolutions are a deep learning architecture for learning directly from undirected graphs. In this work, we emphasize their application to small molecules-undirected graphs of atoms connected by bonds-for virtual screening. Starting from simple descriptions of atoms, bonds between atoms, and pairwise relationships in a molecular graph, we have demonstrated performance that is comparable to state of the art multitask neural networks trained on traditional molecular fingerprint representations, as well as alternative methods including "neural fingerprints" <ref type="bibr" target="#b8">[Duvenaud et al., 2015]</ref> and influence relevance voter <ref type="bibr" target="#b17">[Swamidass et al., 2009]</ref>.</p><p>Our experiments with the adjustable parameters in graph convolution models indicate a relatively minor sensitivity to the number of Weave modules and the maximum distance between atom pairs (at least for our datasets). These results suggest that a model with two Weave modules, a maximum atom pair distance of 2, and Gaussian histogram reductions is a good starting point for further optimization. Remarkably, graph convolution models perform well with a "simple" input featurization containing only atom type, bond type, and graph distancesessentially the information available from looking at <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Flexibility is a highlight of the graph convolution architecture: because we begin with a representation that encodes the complete molecular graph, graph convolution models are free to use any of the available information for the task at hand. In a sense, every possible molecular "fingerprint" is available to the model. Said another way, graph convolutions and other graph-based approaches purposefully blur the distinction between molecular features and predictive models. As has been pointed out elsewhere <ref type="bibr" target="#b8">[Duvenaud et al., 2015]</ref>, the ability to use backpropagation to tune parameters at every stage of the network provides greater representational power than traditional descriptors, which are inflexible in the features they encode from the initial representation. Accordingly, it is not appropriate to think of graph-based methods as alternative descriptors; rather, they should be viewed as fully integrated approaches to virtual screening (although future work could investigate the utility of the learned molecule-level features for additional tasks or other applications such as molecular similarity). Looking forward, graph convolutions (and related graph-based methods; see Section 2) present a "new hill to climb" in computer-aided drug design and cheminformatics. Although our current graph convolution models do not consistently outperform stateof-the-art fingerprint-based models, we emphasize their flexibility and potential for further optimization and development. In particular, we are aware of several specific opportunities for improvement, including (1) additional optimization of model hyperparameters such as Weave module convolution depths;</p><p>(2) fine-tuning of architectural decisions, such as the choice of reduction in the P ? A operation (cur-rently a sum, but perhaps a Gaussian histogram or distance-dependent function); and (3) improvements in memory usage and training performance, such as not handling all pairs of atoms or implementing more efficient versions of Weave module operations. With these and other optimizations, we expect that graph convolutions could exceed the performance of the best available fingerprint-based methods.</p><p>Finally, we note that much (or most) of the information required to represent biological systems and the interactions responsible for small molecule activity is not encapsulated in the molecular graph. Biology takes place in a three-dimensional world, and is sensitive to shape, electrostatics, quantum effects, and other properties that emerge from-but are not necessarily unique to-the molecular graph (see, for example, Nicholls et al. <ref type="bibr">[2010]</ref>). Additionally, most small molecules exhibit 3D conformational flexibility that our graph representation does not even attempt to describe. The extension of deep learning methods (including graph convolutions) to three-dimensional biology is an active area of research (e.g. <ref type="bibr" target="#b21">Wallach et al. [2015]</ref>) that requires special attention to the added complexities of multiple-instance learning in a relatively small-data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Version information</head><p>Submitted to the Journal of Computer-Aided Molecular Design. Comments on arXiv versions: v2: Changed cross-validation scheme to use a heldout validation set and made other changes in response to reviewer comments, such as including comparisons to additional models and adding more background for the methods.</p><p>v3: Added ROC enrichment metrics and changed baseline model training strategy to use sample weights. Added BEDROC comparison to IRV models. Corrected an error in the logistic regression model training protocol and updated the method used to calculate the number of unique molecules in our datasets. Some AUC values changed slightly due to model retraining and/or reevaluation.       MaxSim  </p><formula xml:id="formula_9">MaxSim LR RF PMTNN Simple Sum RMS W 1 N 2 W 2 N 1 W 2 N 2 W 2 N 3 W 2 N 4 W 2 N ? W 3 N 2 W 4 N 2<label>Model</label></formula><formula xml:id="formula_10">MaxSim LR RF Simple Sum RMS W 1 N 2 W 2 N 1 W 2 N 2 W 2 N 3 W 2 N 4 W 2 N ? W 3 N 2 W 4 N 2<label>Model</label></formula><formula xml:id="formula_11">MaxSim LR RF PMTNN Simple Sum RMS W 1 N 2 W 2 N 1 W 2 N 2 W 2 N 3 W 2 N 4 W 2 N ? W 3 N 2 W 4 N 2<label>Model</label></formula><formula xml:id="formula_12">MaxSim LR RF Simple Sum RMS W 1 N 2 W 2 N 1 W 2 N 2 W 2 N 3 W 2 N 4 W 2 N ? W 3 N 2 W 4 N 2<label>Model</label></formula><formula xml:id="formula_13">MaxSim LR RF PMTNN Simple Sum RMS W 1 N 2 W 2 N 1 W 2 N 2 W 2 N 3 W 2 N 4 W 2 N ? W 3 N 2 W 4 N 2<label>Model</label></formula><formula xml:id="formula_14">LR RF Simple Sum RMS W 1 N 2 W 2 N 1 W 2 N 2 W 2 N 3 W 2 N 4 W 2 N ? W 3 N 2 W 4 N 2<label>Model</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix: ROC enrichment</head><p>The following tables report ROC enrichment  scores for baseline and graph convolution models. Each metric was optimized separately using the held-out validation set for each model, such that ROC AUC or ROC enrichment scores at different false positive rates (FPRs) are not necessarily derived from predictions using the same set of model training checkpoints.        <ref type="figure" target="#fig_9">Figure 8</ref> showed the evolution of atom features at different stages of a graph convolution model (after subsequent Weave modules). The following figures show the evolution of atom pair features from the same models, using both the "full" and "simple" input featurization. As in <ref type="figure" target="#fig_9">Figure 8</ref>, the initial pair features describe ibuprofen. Most of the initial featurization describes the graph distance between the atoms in the pair (see <ref type="table" target="#tab_2">Table 3</ref>). There are many blank rows since pairs separated by more than the maximum atom pair distance are masked. Note that only unique pairs are represented (i. <ref type="figure">e. (a, b) but not (b, a)</ref>). As the pair features move through the graph convolution network, it can be seen that similar initial featurizations diverge as a consequence of Weave module operations.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Appendix: Gaussian histogram membership functions</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Molecular graph for ibuprofen. Unmarked vertices represent carbon atoms, and bond order is indicated by the number of lines used for each edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Property 3 (</head><label>3</label><figDesc>Pair order invariance). For all pair layers y, P y (a,b) = P y (b,a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>P ? A operation. P x is a matrix containing features for atom pairs ab, ac, ad, etc. The vi are intermediate values obtained by applying f to features for a given atom pair. Applying g to the intermediate representations for all atom pairs involving a given atom (e.g. a) results in a new atom feature vector for that atom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A ? P operation. A x is a matrix containing features for atoms a, b, etc. The vi are intermediate values obtained by applying f to features for a given pair of atoms concatenated in both possible orderings (ab and ba). Applying g to these intermediate ordered pair features results in an order-independent feature vector for atom pair ab.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Fuzzy histogram with three Gaussian "bins". Each curve represents the membership function for a different bin, indicating the degree to which a point contributes to that bin. The vertical blue line represents an example point which contributes normalized densities of &lt; 0.01, ? 0.25, and ? 0.75 to the bins (from left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Abstract graph convolution architecture. In the current implementation, only the final atom features are used to generate molecule-level features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of models with "simple" and "full" input featurizations. The simple featurization only encodes atom type, bond type, and graph distance. The full featurization includes additional features such as aromaticity and hydrogen bonding propensity (see Section 3.4 for more details). Confidence intervals for box plot medians were computed as ?1.57 ? IQR/ ? N<ref type="bibr" target="#b25">[McGill et al., 1978]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>.2 show similar behavior for pair features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of root-mean-square (RMS) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Graph convolution feature evolution. Atoms or pairs are displayed on the y-axis and the dimensions of the feature vectors are on the x-axis. (A) Conversion of the molecular graph for ibuprofen into atom and (unique) atom pair features. (B) Evolution of atom features after successive Weave modules in a graph convolution model with a W3N2 architecture and depth 50 convolutions in Weave modules. (C) Evolution of "simple" atom features (see Section 4.2) starting from initial encoding and progressing through the Weave modules of a W2N2 architecture. The color bar applies to all panels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of models with different maximum atom pair distances to a model with a maximum pair distance of one (bonded atoms). All models have two Weave modules. The y-axis is cropped to emphasize differences near zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>fold mean AUC vs. PMTNN (b) Difference box plot vs. PMTNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A. 1 :</head><label>1</label><figDesc>Model performance on PCBA datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Full box plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>fold mean AUC vs. PMTNN (b) Difference box plot vs. PMTNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure A. 2 :</head><label>2</label><figDesc>Model performance on MUV datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>fold mean AUC vs. PMTNN (b) Difference box plot vs. PMTNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure A. 3 :</head><label>3</label><figDesc>Model performance on Tox21 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Difference box plot vs. "simple" featurization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure C. 1 :</head><label>1</label><figDesc>Comparison of models with "simple" and "full" input featurizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure D. 1 :</head><label>1</label><figDesc>Comparison of models with different numbers of Weave modules. Difference box plot vs. sum reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure D. 2 :</head><label>2</label><figDesc>Comparison of models with different feature reduction methods. Difference box plot vs. N1 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure D. 3 :</head><label>3</label><figDesc>Comparison of models with different maximum atom pair distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure E. 1 :</head><label>1</label><figDesc>Graph convolution atom pair feature evolution using the "full" featurization in a W3N2 architecture. Unique atom pairs are on the y-axis (one atom pair per row). Initial pair features are shown on the left, with whitespace separating subsequent Weave module outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure E. 2 :</head><label>2</label><figDesc>Graph convolution atom pair feature evolution using the "simple" featurization in a W2N2 architecture. Unique atom pairs are on the y-axis (one atom pair per row). Initial pair features are shown on the left, with whitespace separating subsequent Weave module outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure F. 1 :</head><label>1</label><figDesc>Visualization of the Gaussian membership functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Graph convolution model hyperparameters.</figDesc><table><row><cell>Group</cell><cell>Hyperparameter</cell><cell>Default Value</cell></row><row><cell>Input</cell><cell>Maximum number of atoms per molecule Maximum atom pair graph distance</cell><cell>60 2</cell></row><row><cell></cell><cell>Number of Weave modules</cell><cell>1</cell></row><row><cell></cell><cell>(A ? A)0 convolution depth</cell><cell>50</cell></row><row><cell></cell><cell>(A ? P )0 convolution depth</cell><cell>50</cell></row><row><cell>Weave</cell><cell>(P ? P )0 convolution depth</cell><cell>50</cell></row><row><cell></cell><cell>(P ? A)0 convolution depth</cell><cell>50</cell></row><row><cell></cell><cell>(A ? A)1 convolution depth</cell><cell>50</cell></row><row><cell></cell><cell>(P ? P )1 convolution depth</cell><cell>50</cell></row><row><cell>Reduction</cell><cell>Final atom layer convolution depth Reduction to molecule-level features</cell><cell>128 Gaussian histogram</cell></row><row><cell cols="2">Post-reduction Fully-connected layers (number of units per layer)</cell><cell>2000, 100</cell></row><row><cell></cell><cell>Batch size</cell><cell>96</cell></row><row><cell>Training</cell><cell>Learning rate</cell><cell>0.003</cell></row><row><cell></cell><cell>Optimization method</cell><cell>Adagrad</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Atom features.</figDesc><table><row><cell>Feature</cell><cell>Description</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Atom pair features.</figDesc><table><row><cell>Feature</cell><cell>Description</cell><cell>Size</cell></row><row><cell>Bond type  *</cell><cell>Single, double, triple, or aromatic (one-hot or null).</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Median 5-fold mean AUC values for reported models. Graph convolution models are labeled as WxNy, where x and y denote the number of Weave modules and the maximum atom pair distance, respectively (see the text for descriptions of the simple, sum, and RMS models). All graph convolution models fed into a Pyramidal (2000, 100) MTNN after the molecule-level feature reduction step. MaxSim, logistic regression (LR), random forest (RF), and pyramidal (2000, 100) multitask neural network (PMTNN) baselines used Morgan fingerprints as input. For each model, we report the median ?AUC and the 95% Wilson score interval for a sign test estimating the probability that a given model will outperform the PMTNN baseline (see Section 3.7). Bold values indicate sign test confidence intervals that do not include 0.5.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PCBA (n = 128)</cell><cell></cell><cell cols="2">MUV (n = 17)</cell><cell></cell><cell cols="2">Tox21 (n = 12)</cell></row><row><cell>Model</cell><cell>Median AUC</cell><cell>Median ?AUC</cell><cell>Sign Test 95% CI</cell><cell>Median AUC</cell><cell>Median ?AUC</cell><cell>Sign Test 95% CI</cell><cell>Median AUC</cell><cell>Median ?AUC</cell><cell>Sign Test 95% CI</cell></row><row><cell>MaxSim</cell><cell>0.754</cell><cell cols="2">?0.137 (0.00, 0.04)</cell><cell>0.638</cell><cell cols="2">?0.136 (0.01, 0.27)</cell><cell>0.728</cell><cell cols="2">?0.131 (0.00, 0.24)</cell></row><row><cell>LR</cell><cell>0.838</cell><cell cols="2">?0.059 (0.04, 0.13)</cell><cell>0.736</cell><cell cols="2">?0.070 (0.10, 0.47)</cell><cell>0.789</cell><cell cols="2">?0.073 (0.01, 0.35)</cell></row><row><cell>RF</cell><cell>0.804</cell><cell cols="2">?0.092 (0.02, 0.10)</cell><cell>0.655</cell><cell cols="2">?0.135 (0.01, 0.27)</cell><cell>0.802</cell><cell cols="2">?0.047 (0.01, 0.35)</cell></row><row><cell>PMTNN</cell><cell>0.905</cell><cell></cell><cell></cell><cell>0.869</cell><cell></cell><cell></cell><cell>0.854</cell><cell></cell><cell></cell></row><row><cell>W2N2-simple</cell><cell>0.905</cell><cell cols="2">?0.003 (0.27, 0.44)</cell><cell>0.849</cell><cell>0.012</cell><cell>(0.36, 0.78)</cell><cell>0.866</cell><cell>0.003</cell><cell>(0.39, 0.86)</cell></row><row><cell>W2N2-sum</cell><cell>0.898</cell><cell cols="2">?0.011 (0.16, 0.31)</cell><cell>0.818</cell><cell>?0.014</cell><cell>(0.17, 0.59)</cell><cell>0.848</cell><cell>?0.010</cell><cell>(0.09, 0.53)</cell></row><row><cell>W2N2-RMS</cell><cell>0.902</cell><cell cols="2">?0.007 (0.20, 0.35)</cell><cell>0.851</cell><cell>?0.026</cell><cell>(0.13, 0.53)</cell><cell>0.854</cell><cell cols="2">?0.007 (0.05, 0.45)</cell></row><row><cell>W1N2</cell><cell>0.905</cell><cell cols="2">?0.007 (0.20, 0.35)</cell><cell>0.840</cell><cell>?0.002</cell><cell>(0.26, 0.69)</cell><cell>0.849</cell><cell>?0.009</cell><cell>(0.09, 0.53)</cell></row><row><cell>W2N1</cell><cell>0.908</cell><cell cols="2">?0.003 (0.30, 0.46)</cell><cell>0.858</cell><cell>?0.016</cell><cell>(0.17, 0.59)</cell><cell>0.867</cell><cell>?0.002</cell><cell>(0.19, 0.68)</cell></row><row><cell>W2N2</cell><cell>0.909</cell><cell>0.000</cell><cell>(0.42, 0.59)</cell><cell>0.847</cell><cell>?0.004</cell><cell>(0.22, 0.64)</cell><cell>0.862</cell><cell>0.004</cell><cell>(0.32, 0.81)</cell></row><row><cell>W2N3</cell><cell>0.906</cell><cell>?0.001</cell><cell>(0.38, 0.55)</cell><cell>0.838</cell><cell>?0.013</cell><cell>(0.26, 0.69)</cell><cell>0.861</cell><cell>0.000</cell><cell>(0.25, 0.75)</cell></row><row><cell>W2N4</cell><cell>0.908</cell><cell>?0.001</cell><cell>(0.37, 0.54)</cell><cell>0.836</cell><cell>?0.008</cell><cell>(0.17, 0.59)</cell><cell>0.858</cell><cell>0.001</cell><cell>(0.39, 0.86)</cell></row><row><cell>W2N?</cell><cell>0.897</cell><cell cols="2">?0.008 (0.12, 0.25)</cell><cell>0.841</cell><cell cols="2">?0.025 (0.10, 0.47)</cell><cell>0.846</cell><cell>?0.006</cell><cell>(0.14, 0.61)</cell></row><row><cell>W3N2</cell><cell>0.906</cell><cell>0.000</cell><cell>(0.44, 0.61)</cell><cell>0.875</cell><cell>0.010</cell><cell>(0.31, 0.74)</cell><cell>0.859</cell><cell>0.004</cell><cell>(0.47, 0.91)</cell></row><row><cell>W4N2</cell><cell>0.907</cell><cell>?0.001</cell><cell>(0.33, 0.50)</cell><cell>0.856</cell><cell>?0.007</cell><cell>(0.22, 0.64)</cell><cell>0.862</cell><cell>0.004</cell><cell>(0.32, 0.81)</cell></row></table><note>model Simple Full</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table B.1: Median 5-fold mean ROC enrichment values for reported models at 1% FPR (E 1% ). For each model, we report the median ?E 1% and the 95% Wilson score interval for a sign test estimating the probability that a given model will outperform the PMTNN baseline (see Section 3.7). Median 5-fold mean ROC enrichment values for reported models at 5% FPR (E 5% ). For each model, we report the median ?E 5% and the 95% Wilson score interval for a sign test estimating the probability that a given model will outperform the PMTNN baseline (see Section 3.7). Median 5-fold mean ROC enrichment values for reported models at 10% FPR (E 10% ). For each model, we report the median ?E 10% and the 95% Wilson score interval for a sign test estimating the probability that a given model will outperform the PMTNN baseline (see Section 3.7). Median 5-fold mean ROC enrichment values for reported models at 20% FPR (E 20% ). For each model, we report the median ?E 20% and the 95% Wilson score interval for a sign test estimating the probability that a given model will outperform the PMTNN baseline (see Section 3.7). Bold values indicate sign test confidence intervals that do not include 0.5.For each of the experiments described in Section 4.2, we provide figures showing (a) box plots for absolute 5-fold mean AUC scores for each model and (b) difference box plots showing differences in 5-fold mean AUC scores against a baseline model (without any y-axis cropping).</figDesc><table><row><cell cols="4">C Appendix: Input featurization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Bold values indicate Table B.2: Bold values indicate sign test confidence intervals that do not include 0.5. PCBA (n = 128) MUV (n = 17) Tox21 (n = 12) sign test confidence intervals that do not include 0.5. PCBA (n = 128) MUV (n = 17) Tox21 (n = 12) Table B.3: Bold values indicate sign test confidence intervals that do not include 0.5. PCBA (n = 128) MUV (n = 17) Tox21 (n = 12) Table B.4: PCBA (n = 128) MUV (n = 17) Tox21 (n = 12)</cell></row><row><cell>Model Model Model Model</cell><cell>Median E 1% Median E 5% Median E 10% Median E 20%</cell><cell>Median ?E 1% Median ?E 5% Median ?E 10% Median ?E 20%</cell><cell>Sign Test 95% CI Sign Test 95% CI Sign Test 95% CI Sign Test 95% CI</cell><cell>Median E 1% Median E 5% Median E 10% Median E 20%</cell><cell>Median ?E 1% Median ?E 5% Median ?E 10% Median ?E 20%</cell><cell>Sign Test 95% CI Sign Test 95% CI Sign Test 95% CI Sign Test 95% CI</cell><cell>Median E 1% Median E 5% Median E 10% Median E 20%</cell><cell>Median ?E 1% Median ?E 5% Median ?E 10% Median ?E 20%</cell><cell>Sign Test 95% CI Sign Test 95% CI Sign Test 95% CI Sign Test 95% CI</cell></row><row><cell>MaxSim MaxSim MaxSim MaxSim LR LR LR LR RF RF RF RF PMTNN PMTNN PMTNN PMTNN</cell><cell>24.1 8.5 5.1 3.0 20.2 8.8 5.9 3.6 34.5 10.2 6.0 3.4 43.7 13.5 7.8 4.2</cell><cell>?16.2 ?4.4 ?2.2 ?1.1 ?18.8 ?3.6 ?1.4 ?0.5 ?6.9 ?2.5 ?1.3 ?0.7</cell><cell>(0.04, 0.13) (0.01, 0.08) (0.00, 0.06) (0.00, 0.03) (0.01, 0.08) (0.02, 0.09) (0.01, 0.08) (0.03, 0.11) (0.12, 0.25) (0.06, 0.17) (0.04, 0.14) (0.03, 0.11)</cell><cell>13.3 6.0 3.3 2.2 16.7 6.0 4.7 3.0 23.3 6.0 3.7 2.5 30.0 10.7 6.3 3.8</cell><cell>?3.3 ?3.3 ?2.0 ?1.0 0.0 ?2.0 ?0.7 ?0.5 ?3.3 ?2.0 ?1.0 ?0.7</cell><cell>(0.22, 0.64) (0.03, 0.34) (0.04, 0.38) (0.03, 0.34) (0.28, 0.72) (0.14, 0.56) (0.26, 0.69) (0.18, 0.61) (0.23, 0.67) (0.14, 0.56) (0.13, 0.53) (0.03, 0.36)</cell><cell>12.8 6.7 4.3 2.8 17.8 8.3 5.2 3.2 26.4 9.6 5.8 3.4 28.1 10.3 6.4 3.7</cell><cell>?13.0 ?3.9 ?2.1 ?1.1 ?5.1 ?1.9 ?1.1 ?0.5 ?0.2 ?1.0 ?0.7 ?0.4</cell><cell>(0.00, 0.24) (0.00, 0.24) (0.00, 0.24) (0.00, 0.24) (0.01, 0.35) (0.00, 0.24) (0.01, 0.35) (0.05, 0.45) (0.05, 0.45) (0.05, 0.45) (0.01, 0.35) (0.25, 0.75)</cell></row><row><cell>W2N2-simple W2N2-simple W2N2-simple W2N2-simple W2N2-sum W2N2-sum W2N2-sum W2N2-sum W2N2-RMS W2N2-RMS W2N2-RMS W2N2-RMS W1N2 W1N2 W1N2 W1N2 W2N1 W2N1 W2N1 W2N1 W2N2 W2N2 W2N2 W2N2 W2N3 W2N3 W2N3 W2N3 W2N4 W2N4 W2N4 W2N4 W2N? W2N? W2N? W2N? W3N2 W3N2 W3N2 W3N2 W4N2 W4N2 W4N2 W4N2</cell><cell>42.3 13.4 7.7 4.3 34.5 12.3 7.2 4.2 39.2 12.9 7.5 4.2 38.3 13.0 7.5 4.2 40.9 13.3 7.6 4.3 42.2 13.6 7.7 4.3 42.0 13.3 7.7 4.3 42.0 13.3 7.7 4.3 38.8 12.8 7.4 4.2 42.1 13.6 7.8 4.3 40.6 13.3 7.7 4.3</cell><cell>?1.6 ?0.3 ?0.1 0.0 ?6.5 ?0.9 ?0.4 ?0.1 ?3.5 ?0.7 ?0.2 ?0.1 ?3.6 ?0.5 ?0.2 ?0.1 ?2.2 ?0.4 ?0.1 0.0 ?0.8 ?0.1 0.0 0.0 ?0.9 ?0.2 0.0 0.0 ?0.7 ?0.2 ?0.1 0.0 ?2.7 ?0.5 ?0.3 ?0.1 ?1.0 ?0.1 0.0 0.0 ?1.2 ?0.1 0.0 0.0</cell><cell>(0.15, 0.29) (0.19, 0.34) (0.26, 0.42) (0.30, 0.46) (0.05, 0.15) (0.12, 0.25) (0.12, 0.25) (0.17, 0.31) (0.04, 0.14) (0.12, 0.25) (0.13, 0.26) (0.19, 0.34) (0.05, 0.15) (0.13, 0.27) (0.12, 0.25) (0.19, 0.34) (0.17, 0.31) (0.20, 0.35) (0.21, 0.37) (0.32, 0.49) (0.30, 0.46) (0.30, 0.47) (0.28, 0.44) (0.38, 0.55) (0.18, 0.33) (0.24, 0.40) (0.28, 0.45) (0.35, 0.52) (0.23, 0.39) (0.25, 0.41) (0.25, 0.41) (0.28, 0.45) (0.06, 0.17) (0.06, 0.16) (0.09, 0.20) (0.12, 0.25) (0.19, 0.34) (0.26, 0.43) (0.34, 0.51) (0.37, 0.54) (0.22, 0.38) (0.29, 0.46) (0.29, 0.46) (0.34, 0.51)</cell><cell>30.0 10.0 5.7 3.3 16.7 7.3 5.3 3.3 13.3 8.0 5.3 3.5 20.0 9.3 5.0 3.7 16.7 8.7 6.0 3.5 26.7 10.0 5.7 3.5 26.7 8.7 5.7 3.3 23.3 8.7 5.7 3.3 20.0 8.7 5.0 3.3 26.7 9.3 6.0 3.5 23.3 8.0 5.7 3.7</cell><cell>?3.3 ?1.3 ?0.7 ?0.3 ?13.3 ?2.0 ?0.7 ?0.3 ?6.7 ?2.0 ?1.0 ?0.2 ?3.3 ?2.0 ?1.0 ?0.3 ?6.7 ?0.7 ?0.7 ?0.2 ?3.3 ?1.3 ?0.3 ?0.3 ?3.3 ?1.3 ?0.7 ?0.3 ?6.7 ?1.3 ?0.7 ?0.3 ?3.3 ?1.3 ?1.0 ?0.3 0.0 0.0 ?0.3 ?0.2 ?3.3 ?1.3 ?0.7 ?0.2</cell><cell>(0.14, 0.56) (0.22, 0.64) (0.15, 0.58) (0.10, 0.49) (0.03, 0.36) (0.04, 0.38) (0.13, 0.53) (0.07, 0.43) (0.01, 0.30) (0.06, 0.41) (0.07, 0.45) (0.11, 0.52) (0.08, 0.48) (0.10, 0.49) (0.10, 0.49) (0.14, 0.56) (0.14, 0.56) (0.01, 0.33) (0.11, 0.52) (0.23, 0.67) (0.07, 0.45) (0.10, 0.49) (0.18, 0.61) (0.17, 0.59) (0.10, 0.49) (0.12, 0.55) (0.10, 0.49) (0.26, 0.69) (0.08, 0.48) (0.13, 0.53) (0.13, 0.53) (0.10, 0.47) (0.14, 0.56) (0.03, 0.34) (0.13, 0.53) (0.07, 0.43) (0.25, 0.70) (0.16, 0.61) (0.17, 0.59) (0.23, 0.67) (0.08, 0.48) (0.14, 0.56) (0.13, 0.53) (0.16, 0.61)</cell><cell>24.7 10.1 6.3 3.8 17.2 8.8 5.9 3.7 21.2 9.4 5.9 3.8 22.6 9.9 6.2 3.7 25.6 10.4 6.3 3.9 26.2 10.4 6.2 3.9 25.5 10.5 6.3 3.8 23.5 10.2 6.4 3.8 23.4 10.4 6.3 3.8 24.8 10.4 6.2 3.8 24.8 10.5 6.3 3.8</cell><cell>?1.1 ?0.2 0.0 0.0 ?9.8 ?1.9 ?0.6 ?0.1 ?4.3 ?1.4 ?0.4 ?0.1 ?4.7 ?0.8 ?0.2 0.0 ?2.7 ?0.4 ?0.1 0.0 1.6 0.0 0.0 0.1 2.4 ?0.2 0.1 0.0 ?0.4 ?0.2 0.0 0.0 ?1.1 ?0.2 ?0.1 0.0 0.5 ?0.2 0.0 0.1 ?0.9 0.0 0.1 0.1</cell><cell>(0.19, 0.68) (0.25, 0.75) (0.32, 0.81) (0.19, 0.68) (0.01, 0.35) (0.05, 0.45) (0.09, 0.53) (0.01, 0.35) (0.01, 0.35) (0.05, 0.45) (0.09, 0.53) (0.05, 0.45) (0.09, 0.53) (0.05, 0.45) (0.14, 0.61) (0.09, 0.53) (0.14, 0.61) (0.09, 0.53) (0.25, 0.75) (0.09, 0.53) (0.28, 0.79) (0.25, 0.75) (0.35, 0.85) (0.47, 0.91) (0.19, 0.68) (0.35, 0.85) (0.32, 0.81) (0.39, 0.86) (0.14, 0.61) (0.25, 0.75) (0.25, 0.75) (0.25, 0.75) (0.15, 0.65) (0.09, 0.53) (0.19, 0.68) (0.09, 0.53) (0.14, 0.61) (0.25, 0.75) (0.32, 0.81) (0.32, 0.81) (0.09, 0.53) (0.25, 0.75) (0.32, 0.81) (0.47, 0.91)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>D Appendix: Hyperparameter sensitivity</head><label></label><figDesc>each of the experiments described in Section 4.3, we provide figures showing (a) box plots for absolute 5-fold mean AUC scores for each model and (b) difference box plots showing differences in 5-fold mean AUC scores against a baseline model (without any y-axis cropping). Difference box plot vs. W1 model.</figDesc><table><row><cell cols="5">For D.1 Number of Weave modules</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>W 2</cell></row><row><cell>5-fold mean AUC</cell><cell>0.6 0.7 0.8 0.9</cell><cell></cell><cell></cell><cell>model W 1 W 2</cell><cell>? 5-fold mean AUC vs. W 1</cell><cell>0.05 0.00 0.05 0.10</cell><cell></cell><cell>W 3 W 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>W 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>W 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>pcba</cell><cell>muv</cell><cell>tox</cell><cell></cell><cell>0.10</cell><cell>pcba</cell><cell>muv</cell><cell>tox</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dataset group</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset group</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) Full box plot.</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table F.1: Gaussian membership functions.</figDesc><table><row><cell>Mean</cell><cell>Variance</cell></row><row><cell>?1.645</cell><cell>0.080</cell></row><row><cell>?1.080</cell><cell>0.029</cell></row><row><cell>?0.739</cell><cell>0.018</cell></row><row><cell>?0.468</cell><cell>0.014</cell></row><row><cell>?0.228</cell><cell>0.013</cell></row><row><cell>0.000</cell><cell>0.013</cell></row><row><cell>0.228</cell><cell>0.013</cell></row><row><cell>0.468</cell><cell>0.014</cell></row><row><cell>0.739</cell><cell>0.018</cell></row><row><cell>1.080</cell><cell>0.029</cell></row><row><cell>1.645</cell><cell>0.080</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Bharath Ramsundar, Brian Goldman, and Robert McGibbon for helpful discussion. We also acknowledge Manjunath Kudlur, Derek Murray, and Rajat Monga for assistance with TensorFlow. S.K. was supported by internships at Google Inc. and Vertex Pharmaceuticals Inc. Additionally, we acknowledge use of the Stanford BioX3 cluster supported by NIH S10 Shared Instrumentation Grant 1S10RR02664701. S.K. and V.P. also acknowledge support from from NIH 5U19AI109662-02.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: Model comparison</head><p>The following figures are box plot representations of the data summarized in <ref type="table">Table 4</ref>, organized by dataset group. We provide (a) box plots for absolute 5-fold mean AUC scores for each model and <ref type="formula">(b)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ultrafast shape recognition to search compound databases for similar molecular shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Graham</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational chemistry</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1711" to="1723" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Atom pairs as molecular features in structure-activity studies: definition and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkataraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep learning how I did it: Merck 1st place interview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1231</idno>
		<title level="m">Navdeep Jaitly, and Ruslan Salakhutdinov. Multi-task neural networks for QSAR predictions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Classifying plankton with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<ptr target="http://benanne.github.io/2015" />
		<imprint>
			<date type="published" when="2015-03-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2215" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative partial equalization of orbital electronegativity-a rapid access to atomic charges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marsili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tetrahedron</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="3219" to="3228" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparison of shape-matching and docking as virtual screening tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Skillman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="82" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recommendations for evaluation of computational methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="133" to="139" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">RDKit: Open-source cheminformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<ptr target="http://www.rdkit.org" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A con</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; -Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statsmodels: Econometric and statistical modeling with python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skipper</forename><surname>Seabold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Perktold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Python in Science Conference</title>
		<meeting>the 9th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Influence relevance voting: an accurate and interpretable virtual high throughput screening method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Joshua</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?-Agathe</forename><surname>Azencott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Wan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Gramajo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiou-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="756" to="766" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Molecular Descriptors for Chemoinformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Todeschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viviana</forename><surname>Consonni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating virtual screening methods: good and bad metrics for the ???early recognition??? problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Truchon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher I</forename><surname>Bayly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="488" to="508" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Atomnet: A deep convolutional neural network for bioactivity prediction in structure-based drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhar</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Heifets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02855</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PubChem&apos;s BioAssay database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jewen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tugba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Karapetyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dracheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shoemaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="400" to="412" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy sets. Information and control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="353" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recommendations for evaluation of computational methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="133" to="139" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variations of box plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne A</forename><surname>Tukey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
