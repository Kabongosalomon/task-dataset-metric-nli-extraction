<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Adversarial Domain Adaptation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">MOE</orgName>
								<orgName type="institution" key="instit3">NEL-BDS; TNList</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
							<email>caozhangjie14@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">MOE</orgName>
								<orgName type="institution" key="instit3">NEL-BDS; TNList</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<email>mingsheng@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">MOE</orgName>
								<orgName type="institution" key="instit3">NEL-BDS; TNList</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">MOE</orgName>
								<orgName type="institution" key="instit3">NEL-BDS; TNList</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Adversarial Domain Adaptation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep domain adaptation reveal that adversarial learning can be embedded into deep networks to learn transferable features that reduce distribution discrepancy between the source and target domains. Existing domain adversarial adaptation methods based on single domain discriminator only align the source and target data distributions without exploiting the complex multimode structures. In this paper, we present a multi-adversarial domain adaptation (MADA) approach, which captures multimode structures to enable fine-grained alignment of different data distributions based on multiple domain discriminators. The adaptation can be achieved by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Empirical evidence demonstrates that the proposed model outperforms state of the art methods on standard domain adaptation datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep networks, when trained on large-scale labeled datasets, can learn transferable representations which are generically useful across diverse tasks and application domains <ref type="bibr" target="#b18">Yosinski et al. 2014</ref>). However, due to a phenomenon known as dataset bias or domain shift (Torralba and Efros 2011), predictive models trained with these deep representations on one large dataset do not generalize well to novel datasets and tasks. The typical solution is to further fine-tune these networks on task-specific datasets, however, it is often prohibitively expensive to collect enough labeled data to properly fine-tune the high-capacity deep networks. Hence, there is strong motivation to establishing effective algorithms to reduce the labeling consumption by leveraging readily-available labeled data from a different but related source domain. This promising transfer learning paradigm, however, suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting classification models to target tasks <ref type="bibr" target="#b14">(Pan and Yang 2010)</ref>.</p><p>Existing transfer learning methods assume shared label space and different feature distributions across the source and target domains. These methods bridge different domains by learning domain-invariant feature representations without using target labels, and the classifier learned from source domain can be directly applied to target domain. Recent studies have revealed that deep neural networks can learn more transferable features for domain adaptation <ref type="bibr" target="#b18">Yosinski et al. 2014)</ref>, by disentangling explanatory factors of variations behind domains. The latest advances have been achieved by embedding domain adaptation modules in the pipeline of deep feature learning to extract domain-invariant representations <ref type="bibr" target="#b10">Long et al. 2015;</ref><ref type="bibr" target="#b8">Ganin and Lempitsky 2015;</ref><ref type="bibr" target="#b11">Long et al. 2016;</ref><ref type="bibr" target="#b4">Bousmalis et al. 2016;</ref><ref type="bibr" target="#b12">Long et al. 2017)</ref>.</p><p>Recently, adversarial learning has been successfully embedded into deep networks to learn transferable features to reduce distribution discrepancy between the source and target domains. Domain adversarial adaptation methods (Ganin and Lempitsky 2015;  are among the topperforming deep architectures. These methods mainly align the whole source and target distributions, without considering the complex multimode structures underlying the data distributions. As a result, not only all data from the source and target domains will be confused, but also the discriminative structures could be mixed up, leading to false alignment of the corresponding discriminative structures of different distributions, with intuitive example shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Hence, matching the whole source and target domains as previous methods without exploiting the discriminative structures may not work well for diverse domain adaptation scenarios.</p><p>There are two technical challenges to enabling domain adaptation: (1) enhancing positive transfer by maximally matching the multimode structures underlying data distributions across domains, and (2) alleviating negative transfer by preventing false alignment of modes in different distributions across domains. Motivated by these challenges, we present a multi-adversarial domain adaptation (MADA) approach, which captures multimode structures to enable fine-grained alignment of different data distributions based on multiple domain discriminators. A key improvement over previous methods is the capability to simultaneously promote positive transfer of relevant data and alleviate negative transfer of irrelevant data. The adaptation can be achieved by stochastic gradient descent with the gradients computed by backpropagation in linear-time. Empirical evidence demonstrates that the proposed MADA approach outperforms state of the art methods on standard domain adaptation benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Transfer learning <ref type="bibr" target="#b14">(Pan and Yang 2010)</ref> bridges different domains or tasks to mitigate the burden of manual labeling for machine learning <ref type="bibr" target="#b15">(Pan et al. 2011;</ref><ref type="bibr" target="#b7">Duan, Tsang, and Xu 2012;</ref><ref type="bibr" target="#b19">Zhang et al. 2013;</ref><ref type="bibr" target="#b18">Wang and Schneider 2014)</ref>, computer vision <ref type="bibr" target="#b16">(Saenko et al. 2010;</ref><ref type="bibr" target="#b8">Gong et al. 2012;</ref>   <ref type="bibr" target="#b10">Long et al. 2015;</ref><ref type="bibr" target="#b8">Ganin and Lempitsky 2015;</ref><ref type="bibr" target="#b11">Long et al. 2016;</ref><ref type="bibr" target="#b4">Bousmalis et al. 2016;</ref><ref type="bibr" target="#b12">Long et al. 2017)</ref>, which extends deep convolutional networks (CNNs) to domain adaptation by adding adaptation layers through which the mean embeddings of distributions are matched <ref type="bibr" target="#b10">Long et al. 2015;</ref><ref type="bibr" target="#b11">Long et al. 2016)</ref>, or by adding a subnetwork as domain discriminator while the deep features are learned to confuse the discriminator in a domain-adversarial training paradigm (Ganin and Lempitsky 2015; . While performance was significantly improved, these state of the art methods may be restricted by the fact that the discriminative structures as well as complex multimode structures are not exploited for fine-grained alignment of different distributions.</p><p>Adversarial learning has been explored for generative modeling in Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">(Goodfellow et al. 2014)</ref>. Recently, several difficulties of GANs have been addressed, e.g. ease training <ref type="bibr" target="#b1">(Arjovsky, Chintala, and Bottou 2017;</ref><ref type="bibr" target="#b1">Arjovsky and Bottou 2017)</ref>, avoid mode collapse (Mirza and Osindero 2014; <ref type="bibr" target="#b5">Che et al. 2017;</ref><ref type="bibr" target="#b13">Metz et al. 2017)</ref>. In particular, Generative Multi-Adversarial Network (GMAN) <ref type="bibr" target="#b8">(Durugkar, Gemp, and Mahadevan 2017)</ref> extends GANs to multiple discriminators including formidable adversary and forgiving teacher, which significantly eases model training and enhances distribution matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Adversarial Domain Adaptation</head><p>In unsupervised domain adaptation, we are given a source domain <ref type="figure">Figure 2</ref>: The architecture of the proposed Multi-Adversarial Domain Adaptation (MADA) approach, where f is the extracted deep features,? is the predicted data label, andd is the predicted domain label; G f is the feature extractor, G y and L y are the label predictor and its loss, G k d and L k d are the domain discriminator and its loss; GRL stands for Gradient Reversal Layer. The blue part shows the multiple adversarial networks (each for a class, K in total) crafted in this paper. Best viewed in color.</p><formula xml:id="formula_0">D s = {(x s i , y s i )} ns i=1 of n s labeled examples and y f G d G d y f y f CNN d d x y y f G d GRL @L f @? f back-propagation @L d @? f @L d @? d L d 1 2 K @L y @? y Gf Gy @L y @? f L y 1 2 K</formula><formula xml:id="formula_1">a target domain D t = {x t j } nt j=1 of n t unlabeled examples.</formula><p>The source domain and target domain are sampled from joint distributions P (X s , Y s ) and Q(X t , Y t ) respectively, and note that P = Q. The goal of this paper is to design a deep neural network that enables learning of transfer features f = G f (x) and adaptive classifier y = G y (f ) to reduce the shifts in the joint distributions across domains, such that the target risk Pr (x,y)?q [G y (G f (x)) = y] minimized by jointly minimizing source risk and distribution discrepancy by multiadversarial domain adaptation.</p><p>There are two technical challenges to enabling domain adaptation: (1) enhancing positive transfer by maximally matching the multimode structures underlying data distributions P and Q across domains, and (2) alleviating negative transfer by preventing false alignment of different distribution modes across domains. These two challenges motivate the multi-adversarial domain adaptation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adversarial Network</head><p>Domain adversarial networks have been successfully applied to transfer learning (Ganin and Lempitsky 2015;  by extracting transferable features that can reduce the distribution shift between the source domain and the target domain. The adversarial learning procedure is a two-player game, where the first player is the domain discriminator G d trained to distinguish the source domain from the target domain, and the second player is the feature extractor G f finetuned simultaneously to confuse the domain discriminator.</p><p>To extract domain-invariant features f , the parameters ? f of feature extractor G f are learned by maximizing the loss of domain discriminator G d , while the parameters ? d of domain discriminator G d are learned by minimizing the loss of the domain discriminator. In addition, the loss of label predictor G y is also minimized. The objective of domain adversarial network (Ganin and Lempitsky 2015) is the functional:</p><formula xml:id="formula_2">C 0 (? f , ? y , ? d ) = 1 n s xi?Ds L y (G y (G f (x i )) , y i ) ? ? n xi?(Ds?Dt) L d (G d (G f (x i )) , d i ),</formula><p>(1) where n = n s + n t and ? is a trade-off parameter between the two objectives that shape the features during learning. After training convergence, the parameters? f ,? y ,? d will deliver a saddle point of the functional <ref type="formula">(1)</ref>:</p><formula xml:id="formula_3">(? f ,? y ) = arg min ? f ,?y C 0 (? f , ? y , ? d ) , (? d ) = arg max ? d C 0 (? f , ? y , ? d ) .</formula><p>(2) Domain adversarial networks (Ganin and Lempitsky 2015;  are the top-performing architectures for standard domain adaptation when the distributions of the source domain and target domain can be aligned successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Adversarial Domain Adaptation</head><p>In practical domain adaptation problems, however, the data distributions of the source domain and target domain usually embody complex multimode structures, reflecting either the class boundaries in supervised learning or the cluster boundaries in unsupervised learning. Thus, previous domain adversarial adaptation methods that only match the data distributions without exploiting the multimode structures may be prone to either under transfer or negative transfer. Under transfer may happen when different modes of the distributions cannot be maximally matched. Negative transfer may happen when the corresponding modes of the distributions across domains are falsely aligned. To promote positive transfer and combat negative transfer, we should find a technology to reveal the multimode structures underlying distributions on which multi-adversarial domain adaptation can be performed.</p><p>To match the source and target domains upon the multimode structures underlying data distributions, we notice that the source domain labeled information provides strong signals to reveal the multimode structures. Therefore, we split the domain discriminator G d in Equation <ref type="formula">(1)</ref> into K class-wise domain discriminators G k d , k = 1, . . . , K, each is responsible for matching the source and target domain data associated with class k, as shown in <ref type="figure">Figure 2</ref>. Since target domain data are fully unlabeled, it is not easy to decide which domain discriminator G k d is responsible for each target data point. Fortunately, we observe that the output of the label predictor? i = G y (x i ) to each data point x i is a probability distribution over the label space of K classes. This distribution well characterizes the probability of assigning x i to each of the K classes. Thus, it is a natural idea to use? i as the probability to indicate how much each data point x i should be attended to the K domain discriminators G k d , k = 1, . . . , K. The attention of each point x i to a domain discriminator G k d can be modeled by weighting its features G f (x i ) with probability? k i . Applying this to all K domain discriminators G k d , k = 1, . . . , K yields</p><formula xml:id="formula_4">L d = 1 n K k=1 xi?Ds?Dt L k d G k d ? k i G f (x i ) , d i ,<label>(3)</label></formula><p>where G k d is the k-th domain discriminator while L k d is its cross-entropy loss, and d i is the domain label of point x i . We note that the above strategy shares similar ideas with the attention mechanism.</p><p>Compared with the previous single-discriminator domain adversarial network in Equation <ref type="formula">(1)</ref>, the proposed multiadversarial domain adaptation network enables fine-grained adaptation where each data point x i is matched only by those relevant domain discriminators according to its probabilit? y i . This fine-grained adaptation may introduce three benefits.</p><p>(1) It avoids the hard assignment of each point to only one domain discriminator, which tends to be inaccurate for target domain data.</p><p>(2) It circumvents negative transfer since each point is only aligned to the most relevant classes, while the irrelevant classes are filtered out by the probability and will not be included in the corresponding domain discriminators, hence avoiding false alignment of the discriminative structures in different distributions.</p><p>(3) The multiple domain discriminators are trained with probability-weighted data points? k i G f (x i ), which naturally learn multiple domain discriminators with different parameters ? k d ; these domain discriminators with different parameters promote positive transfer for each instance. Integrating all things together, the objective of the Multi-Adversarial Domain Adaptation (MADA) is</p><formula xml:id="formula_5">C ? f , ? y , ? k d | K k=1 = 1 n s xi?Ds L y (G y (G f (x i )) , y i ) ? ? n K k=1 xi?D L k d G k d ? k i G f (x i ) , d i ,<label>(4)</label></formula><p>where n = n s + n t , D = D s ? D t and ? is a hyper-parameter that trade-offs the two objectives in the unified optimization problem. The optimization problem is to find the parameter? ? f ,? y and? k d (k = 1, 2, ..., K) that jointly satisfy</p><formula xml:id="formula_6">(? f ,? y ) = arg min ? f ,?y C ? f , ? y , ? k d | K k=1 , (? 1 d , ...,? K d ) = arg max ? 1 d ,...,? K d C ? f , ? y , ? k d | K k=1 .<label>(5)</label></formula><p>The multi-adversarial domain adaptation (MADA) model simultaneously enhances positive transfer by maximally matching the multimode structures underlying data distributions across domains, and circumvents negative transfer by avoiding false alignment of the distribution modes across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate the proposed multi-adversarial domain adaptation (MADA) model with state of the art transfer learning and deep learning methods. The codes, datasets and configurations will be available online at github.com/thuml.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Office-31 <ref type="bibr" target="#b16">(Saenko et al. 2010</ref>) is a standard benchmark for visual domain adaptation, comprising 4,652 images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which contain images respectively taken by web camera and digital SLR camera with different environments. We evaluate all methods across three transfer tasks A ? W, D ? W and W ? D, which are widely used by previous deep transfer learning methods <ref type="bibr" target="#b8">Ganin and Lempitsky 2015)</ref>, and another three transfer tasks A ? D, D ? A and W ? A as used in <ref type="bibr" target="#b10">(Long et al. 2015;</ref><ref type="bibr" target="#b11">Long et al. 2016</ref>). ImageCLEF-DA 1 is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which is organized by selecting the 12 common categories shared by the following three public datasets, each is considered as a domain: Caltech-256 (C), ImageNet ILSVRC 2012 (I), and Pascal VOC 2012 (P). The 12 common categories are aeroplane, bike, bird, boat, bottle, bus, car, dog, horse, monitor, motorbike, and people. There are 50 images in each category and 600 images in each domain. We use all domain combinations and build 6 transfer tasks: I ? P, P ? I, I ? C, C ? I, C ? P, and P ? C. Different from the Office-31 dataset where different domains  <ref type="bibr" target="#b11">(Long et al. 2016)</ref>, and Reverse Gradient (RevGrad) (Ganin and Lempitsky 2015). TCA learns a shared feature space by Kernel PCA with linear-MMD penalty. GFK interpolates across an infinite number of intermediate subspaces to bridge the source and target subspaces. For these shallow transfer methods, we adopt SVM as the base classifier. DDC maximizes domain confusion by adding to deep networks a single adaptation layer that is regularized by linear-kernel MMD. DAN learns transferable features by embedding deep features of multiple domain-specific layers to reproducing kernel Hilbert spaces (RKHSs) and matching different distributions optimally using multi-kernel MMD. RTN jointly learns transferable features and adapts different source and target classifiers via deep residual learning <ref type="bibr" target="#b9">(He et al. 2016)</ref>. RevGrad enables domain adversarial learning <ref type="bibr" target="#b8">(Goodfellow et al. 2014)</ref> by adapting a single layer of deep networks, which matches the source and target domains by making them indistinguishable for a domain discriminator.</p><p>We follow standard evaluation protocols for unsupervised domain adaptation <ref type="bibr" target="#b10">(Long et al. 2015;</ref><ref type="bibr" target="#b8">Ganin and Lempitsky 2015)</ref>. For both Office-31 and ImageCLEF-DA datasets, we use all labeled source examples and all unlabeled target examples. We compare the average classification accuracy of each method on three random experiments, and report the standard error of the classification accuracies by different experiments of the same transfer task. For all baseline methods, we either follow their original model selection procedures, or conduct transfer cross-validation <ref type="bibr" target="#b20">(Zhong et al. 2010)</ref> if their model selection strategies are not specified. We also adopt transfer cross-validation <ref type="bibr" target="#b20">(Zhong et al. 2010)</ref> to select parameter ? for the MADA models. Fortunately, our models perform very stably under different parameter values, thus we fix ? = 1 throughout all experiments. For MMD-based methods (TCA, DDC, DAN, and RTN), we use Gaussian kernel with bandwidth set to the median pairwise squared distances on the training data, i.e. median trick <ref type="bibr">(Gretton et al. 2012;</ref><ref type="bibr" target="#b10">Long et al. 2015)</ref>. We examine the influence of deep representations for domain adaptation by exploring AlexNet (Krizhevsky, Sutskever, and Hinton 2012) and ResNet <ref type="bibr" target="#b9">(He et al. 2016)</ref> as base architectures for learning deep representations. For shallow methods, we follow DeCAF ) and use as deep representations the activations of the f c7 (AlexNet) and pool5 (ResNet) layers.</p><p>We implement all deep methods based on the Caffe (Jia et al. 2014) framework, and fine-tune from AlexNet (Krizhevsky, Sutskever, and Hinton 2012) and ResNet <ref type="bibr" target="#b9">(He et al. 2016</ref>) models pre-trained on the ImageNet dataset <ref type="bibr" target="#b15">(Russakovsky et al. 2014)</ref>. We fine-tune all convolutional and pooling layers and train the classifier layer via back propagation. Since the classifier is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We employ the mini-batch stochastic gradient descent (SGD) with momentum of 0.9 and the learning rate strategy implemented in RevGrad (Ganin and Lempitsky 2015): the learning rate is not selected by a grid search due to high computational cost-it is adjusted during SGD using these formulas: ? p = ?0 (1+?p) ? , where p is the training progress linearly changing from 0 to 1, ? 0 = 0.01, ? = 10 and ? = 0.75, which is optimized to promote convergence and low error on source domain. To suppress noisy activations at the early stages of training, instead of fixing parameter ?, we gradually change it by multiplying 2 1+exp(??p) ? 1, where ? = 10 (Ganin and Lempitsky 2015). This progressive training strategy significantly stabilizes parameter sensitivity of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The classification accuracy results on the Office-31 dataset for unsupervised domain adaptation based on AlexNet and ResNet are shown in <ref type="table" target="#tab_1">Table 1</ref>. For fair comparison, the re-   <ref type="bibr" target="#b16">(Saenko et al. 2010)</ref>. The three domains in the ImageCLEF-DA dataset are balanced in each category. As reported in <ref type="table" target="#tab_2">Table 2</ref>, the MADA approach outperforms the comparison methods on most transfer tasks. The encouraging results highlight the importance of multi-adversarial domain adaptation in deep neural networks, and suggest that MADA is able to learn more transferable representations for effective domain adaptation.</p><p>The experimental results reveal several insightful observations.</p><p>(1) Standard deep learning methods (AlexNet and ResNet) either outperform or underperform traditional shallow transfer learning methods (TCA and GFK) using deep features as input. This confirms the current practice that deep networks, even the extremely deep ones (ResNet), can learn abstract feature representations that only reduce but not remove the cross-domain discrepancy <ref type="bibr" target="#b18">(Yosinski et al. 2014)</ref>.</p><p>(2) Deep transfer learning methods substantially outperform both standard deep learning methods and traditional shallow transfer learning methods with deep features as input. This validates that explicitly reducing the cross-domain discrepancy by embedding domain-adaptation modules into deep networks (DDC, DAN, RTN, and RevGrad) can learn more transferable features. (3) MADA substantially outperforms previous methods based on either multilayer adaptation (DAN), semi-supervised adaptation (RTN), and domain adversarial training (RevGrad). Although both MADA and RevGrad (Ganin and Lempitsky 2015) perform domain adversarial adaptation, the improvement from RevGrad to MADA is crucial for domain adaptation: RevGrad matches data distributions across domains without exploiting the complex multimode structures; MADA enables domain adaptation by making the source and target domains indistinguishable multiple domain discriminators, each responsible for matching the source and target data associated with the same class, which can essentially reduce the shift in the data distributions of complex multimode structures.</p><p>Negative transfer is an important technical bottleneck for successful domain adaptation. Negative transfer is more likely to happen when the source domain is substantially larger than the target domain, in which there exist many source data points that are irrelevant to the target domain. To evaluate the robustness against negative transfer, we randomly remove 6 classes from all transfer tasks constructed from the Office-31 dataset. For example, we perform domain adaptation on transfer task A 31 ? W 25, where the source domain A has 31 classes but the target domain W has only 25 classes. In this more general and challenging scenario, we observe from <ref type="table" target="#tab_3">Table 3</ref> that the top-performing adversarial adaptation method, RevGrad, significantly underperforms standard AlexNet on most transfer tasks. This is an evidence of the negative transfer difficulty. The proposed MADA approach significantly exceeds the performance of both AlexNet and RevGrad, and successfully avoids the negative transfer trap. These positive results imply that the multi-adversarial adaptation can alleviate negative transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Feature Visualization: We go deeper into the feature transferability by visualizing in <ref type="figure" target="#fig_1">Figures 3(a</ref>   gories are not well discriminated clearly. The reason is that domain adversarial learning is performed only at the feature layer f cb, while the discriminative information is not taken into account by the domain adversary.</p><p>(2) Under MADA features, not only the source and target domains are made more indistinguishable but also different categories are made more discriminated, which leads to the best adaptation accuracy. This superior results benefit from the integration of discriminative information into multiple domain discriminators, which enables matching of complex multimode structures of the source and target data distributions.</p><p>Sharing Strategies: Besides the proposed multiadversarial strategy, one may consider other sharing strategies for multiple domain discriminators. For example, one can consider sharing all network parameters in the multiple domain discriminators, which is similar to previous domain adversarial adaptation methods with single domain discriminator; or consider sharing only a fraction of the network parameters for more flexibility. To examine different sharing strategies, we compare different variants of MADA: MADAfull, which shares all parameters of the multiple domain discriminator networks; MADA-partial, which shares only the lowest layers of the multiple discriminator networks. The accuracy results of tasks A ? W and A ? D in <ref type="figure" target="#fig_2">Figure 4</ref>(a) reveal that the transfer performance decreases when we share more parameters of multiple discriminators. This confirms our motivation that multiple domain discriminators are nec-essary to establish fine-grained distribution alignment.</p><p>Distribution Discrepancy: The domain adaptation theory <ref type="bibr" target="#b2">(Ben-David et al. 2010;</ref><ref type="bibr" target="#b12">Mansour, Mohri, and Rostamizadeh 2009</ref>) suggests A-distance as a measure of crossdomain discrepancy, which, together with the source risk, will bound the target risk. The proxy A-distance is defined as d A = 2 (1 ? 2 ), where is the generalization error of a classifier (e.g. kernel SVM) trained on the binary task of discriminating source and target. <ref type="figure" target="#fig_2">Figure 4(b)</ref> shows d A on tasks A ? W, W ? D with features of ResNet, RevGrad, and MADA. We observe that d A using MADA features is much smaller than d A using ResNet and RevGrad features, which suggests that MADA features can reduce the cross-domain gap more effectively. As domains W and D are similar, d A of task W ? D is smaller than that of A ? W, which well explains better accuracy of W ? D.</p><p>Convergence Performance: Since MADA involves alternating optimization procedures, we testify the convergence performance with ResNet and RevGrad. <ref type="figure" target="#fig_2">Figure 4(c)</ref> demonstrates the test errors of different methods on task A ? W, which suggests that MADA has similarly stable convergence performance as RevGrad while significantly outperforming RevGrad in the whole process of convergence. Also, the computational complexity of MADA is similar to RevGrad since the multiple domain discriminators only occupy a small fraction of the overall computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper presented a novel multi-adversarial domain adaptation approach to enable effective deep transfer learning. Unlike previous domain adversarial adaptation methods that only match the feature distributions across domains without exploiting the complex multimode structures, the proposed approach further exploits the discriminative structures to enable fine-grained distribution alignment in a multi-adversarial adaptation framework, which can simultaneously promote positive transfer and circumvent negative transfer. Experiments show state of the art results of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The difficulty of domain adaptation: discriminative structures may be mixed up or falsely aligned across domains. As an intuitive example, in this figure, the source class cat is falsely aligned with target class dog, making final classification wrong.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>)-3(d) the network activations of task A ? W (10 classes) learned by RevGrad (the bottleneck layer f cb) and MADA (the bottleneck layer f cb) respectively using t-SNE embeddings (Donahue et al. 2014). The visualization results reveal several interesting observations. (1) Under RevGrad features, the source and target domains are made indistinguishable; however, different cate-The t-SNE visualization of deep features extracted by RevGrad (a)(b) and MADA (c)(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Empirical analysis: (a) Sharing strategies, (b) A-distance, and (c) Convergence performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) on Office-31 for unsupervised domain adaptation (AlexNet and ResNet) Sutskever, and Hinton 2012) 60.6?0.4 95.4?0.2 99.0?0.1 64.2?0.3 45.5?0.5 48.3?0.5 68.8 TCA (Pan et al. 2011) 59.0?0.0 90.2?0.0 88.2?0.0 57.8?0.0 51.6?0.0 47.9?0.0 65.8 GFK (Gong et al. 2012) 58.4?0.0 93.6?0.0 91.0?0.0 58.6?0.0 52.4?0.0 46.1?0.0 66.7 DDC (Tzeng et al. 2014) 61.0?0.5 95.0?0.3 98.5?0.3 64.9?0.4 47.2?0.5 49.4?0.4 69.3 DAN (Long et al. 2015) 68.5?0.3 96.0?0.1 99.0?0.1 66.8?0.2 50.0?0.4 49.8?0.</figDesc><table><row><cell>Method</cell><cell>A ? W</cell><cell>D ? W</cell><cell>W ? D</cell><cell>A ? D</cell><cell>D ? A</cell><cell>W ? A</cell><cell>Avg</cell></row><row><cell cols="8">AlexNet (Krizhevsky, 3 71.7</cell></row><row><cell>RTN (Long et al. 2016)</cell><cell cols="7">73.3?0.2 96.8?0.2 99.6?0.1 71.0?0.2 50.5?0.3 51.0?0.1 73.7</cell></row><row><cell>RevGrad (Ganin and Lempitsky 2015)</cell><cell cols="7">73.0?0.5 96.4?0.3 99.2?0.3 72.3?0.3 52.4?0.4 50.4?0.5 74.1</cell></row><row><cell>MADA</cell><cell cols="7">78.5?0.2 99.8?0.1 100.0?.0 74.1?0.1 56.0?0.2 54.5?0.3 77.1</cell></row><row><cell>ResNet (He et al. 2016)</cell><cell cols="7">68.4?0.2 96.7?0.1 99.3?0.1 68.9?0.2 62.5?0.3 60.7?0.3 76.1</cell></row><row><cell>TCA (Pan et al. 2011)</cell><cell cols="7">74.7?0.0 96.7?0.0 99.6?0.0 76.1?0.0 63.7?0.0 62.9?0.0 79.3</cell></row><row><cell>GFK (Gong et al. 2012)</cell><cell cols="7">74.8?0.0 95.0?0.0 98.2?0.0 76.5?0.0 65.4?0.0 63.0?0.0 78.8</cell></row><row><cell>DDC (Tzeng et al. 2014)</cell><cell cols="7">75.8?0.2 95.0?0.2 98.2?0.1 77.5?0.3 67.4?0.4 64.0?0.5 79.7</cell></row><row><cell>DAN (Long et al. 2015)</cell><cell cols="7">83.8?0.4 96.8?0.2 99.5?0.1 78.4?0.2 66.7?0.3 62.7?0.2 81.3</cell></row><row><cell>RTN (Long et al. 2016)</cell><cell cols="7">84.5?0.2 96.8?0.1 99.4?0.1 77.5?0.3 66.2?0.2 64.8?0.3 81.6</cell></row><row><cell>RevGrad (Ganin and Lempitsky 2015)</cell><cell cols="7">82.0?0.4 96.9?0.2 99.1?0.1 79.7?0.4 68.2?0.4 67.4?0.5 82.2</cell></row><row><cell>MADA</cell><cell cols="7">90.0?0.1 97.4?0.1 99.6?0.1 87.8?0.2 70.3?0.3 66.4?0.3 85.2</cell></row><row><cell cols="2">are of different sizes, the three domains in this dataset are of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>equal size, making it a good alternative dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">We compare the proposed multi-adversarial domain adap-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">tation (MADA) with both shallow and deep transfer learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">methods: Transfer Component Analysis (TCA) (Pan et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2011), Geodesic Flow Kernel (GFK) (Gong et al. 2012),</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Deep Domain Confusion (DDC) (Tzeng et al. 2014), Deep</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Adaptation Network (DAN) (Long et al. 2015), Residual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transfer Network (RTN)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) on ImageCLEF-DA for unsupervised domain adaptation (AlexNet and ResNet) Sutskever, and Hinton 2012) 66.2?0.2 70.0?0.2 84.3?0.2 71.3?0.4 59.3?0.5 84.5?0.3 73.9 DAN (Long et al. 2015) 67.3?0.2 80.5?0.3 87.7?0.3 76.0?0.3 61.6?0.3 88.4?0.2 76.9 RTN (Long et al. 2016) 67.4?0.3 82.3?0.3 89.5?0.4 78.0?0.2 63.0?0.2 90.1?0.1 78.4 RevGrad (Ganin and Lempitsky 2015) 66.5?0.5 81.8?0.4 89.0?0.5 79.8?0.5 63.5?0.4 88.7?0.4 78.2 MADA 68.3?0.3 83.0?0.1 91.0?0.2 80.7?0.2 63.8?0.2 92.2?0.3 79.8</figDesc><table><row><cell>Method</cell><cell>I ? P</cell><cell>P ? I</cell><cell>I ? C</cell><cell>C ? I</cell><cell>C ? P</cell><cell>P ? C</cell><cell>Avg</cell></row><row><cell>AlexNet (Krizhevsky, ResNet (He et al. 2016)</cell><cell cols="7">74.8?0.3 83.9?0.1 91.5?0.3 78.0?0.2 65.5?0.3 91.2?0.3 80.7</cell></row><row><cell>DAN (Long et al. 2015)</cell><cell cols="7">75.0?0.4 86.2?0.2 93.3?0.2 84.1?0.4 69.8?0.4 91.3?0.4 83.3</cell></row><row><cell>RTN (Long et al. 2016)</cell><cell cols="7">75.6?0.3 86.8?0.1 95.3?0.1 86.9?0.3 72.7?0.3 92.2?0.4 84.9</cell></row><row><cell>RevGrad (Ganin and Lempitsky 2015)</cell><cell cols="7">75.0?0.6 86.0?0.3 96.2?0.4 87.0?0.5 74.3?0.5 91.5?0.6 85.0</cell></row><row><cell>MADA</cell><cell cols="7">75.0?0.3 87.9?0.2 96.0?0.3 88.8?0.3 75.2?0.2 92.2?0.3 85.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>and W ? A, where the source and target domains are substantially different, and produce comparable classification accuracies on easy transfer tasks, D ? W and W ? D, where the source and target domains are similar</figDesc><table><row><cell cols="7">: Accuracy (%) on Office-31 for domain adaptation from 31 classes to 25 classes (AlexNet)</cell><cell></cell></row><row><cell>Method</cell><cell>A ? W</cell><cell>D ? W</cell><cell>W ? D</cell><cell>A ? D</cell><cell>D ? A</cell><cell>W ? A</cell><cell>Avg</cell></row><row><cell cols="8">AlexNet (Krizhevsky, Sutskever, and Hinton 2012) 58.2?0.4 95.9?0.2 99.0?0.1 60.4?0.3 49.8?0.5 47.3?0.5 68.4</cell></row><row><cell>RevGrad (Ganin and Lempitsky 2015)</cell><cell cols="7">65.1?0.5 91.7?0.3 97.1?0.3 60.6?0.3 42.1?0.4 42.9?0.5 66.6</cell></row><row><cell>MADA</cell><cell cols="2">70.8?0.2 96.6?0.1</cell><cell>99.5?.0</cell><cell cols="4">69.6?0.1 51.4?0.2 54.2?0.3 73.7</cell></row><row><cell cols="2">sults of DAN (Long et al. 2015), RTN (Long et al. 2016),</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and RevGrad (Ganin and Lempitsky 2015) are directly re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ported from their original papers. MADA outperforms all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">comparison methods on most transfer tasks. It is noteworthy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">that MADA promotes the classification accuracies substan-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">tially on hard transfer tasks, e.g. A ? W, A ? D, D ? A,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://imageclef.org/2014/adaptation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Research and Development Program of China (2016YFB1000701), National Natural Science Foundation of China (61772299,  61325008, 61502265, 61672313)  and Tsinghua National Laboratory (TNList) Key Project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chintala</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bottou ; Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courville</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bousmalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mode regularized generative adversarial networks. ICLR</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
	<note>Natural language processing (almost) from scratch</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsang</forename><surname>Xu ; Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemp</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>Gong et al. 2012</idno>
	</analytic>
	<monogr>
		<title level="m">ICML. [Glorot, Bordes, and Bengio</title>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
		</imprint>
	</monogr>
	<note>NIPS. Gretton et al. 2012] Gretton, A.; Borgwardt, K.; Rasch, M. and Smola, A. 2012. A kernel two-sample test</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
	</analytic>
	<monogr>
		<title level="m">ICML. [Mansour, Mohri, and Rostamizadeh</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>COLT</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
	</analytic>
	<monogr>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Yang ; Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks (TNN)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Russakovsky et al. 2014. ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<idno>abs/1412.3474</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Simultaneous deep transfer across domains and tasks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flexible transfer learning under support and model shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">How transferable are features in deep neural networks? In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross validation framework to choose amongst models and datasets for transfer learning</title>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="547" to="562" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
