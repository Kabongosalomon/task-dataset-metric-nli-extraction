<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MOTION FEATURE AUGMENTED RECURRENT NEURAL NETWORK FOR SKELETON-BASED DYNAMIC HAND GESTURE RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-10">10 Aug 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MOTION FEATURE AUGMENTED RECURRENT NEURAL NETWORK FOR SKELETON-BASED DYNAMIC HAND GESTURE RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-10">10 Aug 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Skeleton</term>
					<term>Dynamic Hand Gesture Recog- nition</term>
					<term>Recurrent Neural Network</term>
					<term>Feature Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dynamic hand gesture recognition has attracted increasing interests because of its importance for human computer interaction. In this paper, we propose a new motion feature augmented recurrent neural network for skeleton-based dynamic hand gesture recognition. Finger motion features are extracted to describe finger movements and global motion features are utilized to represent the global movement of hand skeleton. These motion features are then fed into a bidirectional recurrent neural network (RNN) along with the skeleton sequence, which can augment the motion features for RNN and improve the classification performance. Experiments demonstrate that our proposed method is effective and outperforms start-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Due to its flexibility and expressiveness, hand gesture can provide an efficient and natural way for human computer interaction (HCI). Hand gesture recognition has been researched for decades and has great potentials for applications in sign language recognition, remote control and virtual reality etc <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Dynamic hand gesture recognition aims to understand what a hand sequence conveys. It remains a challenging task due to high intra-class variance because the way of performing a gesture differs from person to person.</p><p>Previous works on dynamic hand gesture recognition usually took RGB images and depth images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> as input <ref type="bibr" target="#b4">[5]</ref>. Some of them used multi-modal input including IR images <ref type="bibr" target="#b5">[6]</ref> or audio stream <ref type="bibr" target="#b6">[7]</ref>. Recent progresses on hand pose estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> have greatly promoted the research on dynamic hand gesture recognition from 3D hand skeleton sequences. Smedt et al. <ref type="bibr" target="#b13">[14]</ref> proposed a skeleton-based approach for dynamic hand gesture recognition and demonstrated its superiority over depth-based approaches. In their approach, a temporal pyramid representation was utilized to <ref type="bibr">This</ref>   <ref type="figure">Fig. 1</ref>. The framework of our proposed method. Finger motion features and global motion features are extracted from the input dynamic hand gesture skeleton sequence. These motion features, along with the skeleton sequence, are fed into a recurrent neural network (RNN) to get the predicted class of input gesture. model temporal information. Shape of connected joints, histogram of hand directions and wrist rotations were used to characterize hand shape and hand movement. However, the amplitude of gesture is not considered in their approach and the temporal pyramid representation may lose some motion information.</p><p>The most important clues for dynamic hand gesture are articulated movements of fingers and the global movements of the hand. In prior works, some sort of joint angle features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> were utilized to describe the hand shape. However, these features are not sufficient enough to characterize the full pose of a hand.</p><p>In this paper, we propose a motion feature augmented RNN for skeleton-based dynamic hand gesture recognition. We extract the angles of bones from the hand skeleton, which is efficient and concise representation of the finger articulated movements. To describe the global movements of the hand, we extract the global rotation and global translation of the hand. A distance adaptive discretization scheme is given to better model the amplitude of the gestures. The finger motion features and global features are fed into a bidirectional RNN along with the skeleton sequence to predict the class of input gesture. Experiments on the publicly-available skeletonbased DHG-14/28 dataset <ref type="bibr" target="#b13">[14]</ref> demonstrate the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED FRAMEWORK</head><p>The framework of our proposed algorithm is shown in <ref type="figure">Figure 1</ref>. A hand skeleton sequence is taken as input and the class of gesture is predicted by RNN. Firstly the global motion features and finger motion features are extracted from the input skeleton sequence. The hand skeleton can be directly and effectively represented by a kinematic hand model whose parameters are the angles of bones, the global translation and global rotation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Therefore, these hand parameters can serve as efficient and discriminating features for dynamic hand gesture recognition. In our approach, theses features with offset and dynamic pose modelling are utilized as the motion features to represent dynamic hand gestures. The details of motion feature extraction will be presented in Section 3.</p><p>We exploit the recurrent neural network (RNN) to model temporal information for its success in temporal sequences recognition tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>. Though RNN can somehow learn features from the input sequences, some information may be absent or weakened, which will hinder the classification performance. Some previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> combined features extracted from deep neural network with hand-crafted features to enhance the discriminability of the features. Inspired by these works, in this paper we utilized a RNN which is augmented by motion features to classify dynamic hand gestures from skeleton sequence. The finger motion features and global features are fed into a RNN along with the skeleton sequence to predict the class of input gesture, which will be discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MOTION FEATURE EXTRACTION</head><p>In this section we will describe how to extract finger motion features H(S) and global motion features G(S) from the input hand skeleton sequence</p><formula xml:id="formula_0">S = {s t } T t=1 , where s t = {x t i , y t i , z t i } J i=1</formula><p>denotes a hand skeleton for frame t, T is the number of frames of this sequence and J is the number of joints for hand skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Motion Feature</head><p>The global motion features (global rotation and global translation) are important for dynamic hand gesture. Typically, the global status of the hand can be represented by the wrist joint, palm joint and metacarpophalangeal (MCP) joints, which is denoted by p t . We use Kabsch algorithm <ref type="bibr" target="#b18">[19]</ref> to infer the global rotation G r and global translation G l , as shown in Equation <ref type="formula" target="#formula_1">(1)</ref>:</p><formula xml:id="formula_1">[G l , G r ] = Kabsch(p t , p 0 )<label>(1)</label></formula><p>where G r = (r x , r y , r z ) represents the rotations along three axis and G l = (?, ?, ?) is the spherical coordinates of global translation. p 0 is a fake palm that centers at (0, 0, 0) and faces the camera.</p><p>The amplitudes of hand gestures differ from person to person for the same gesture. Therefore previous work <ref type="bibr" target="#b13">[14]</ref> ignored the amplitude part ? of global translation. However, sometimes the amplitude is critical for gestures. For example, gesture Grab and gesture Pinch are quite similar except for the amplitude of the gesture. To this end, we propose a distance adaptive discretization(DAD) method to extract global translation amplitude feature, inspired by Distance Adaptive Scheme <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref> which is used for feature selection. The DAD method discretizes ? into M bins using the threshold</p><formula xml:id="formula_2">{? i } M i=1 . A gaussian distribution kernel g(x) is used to generate the thresholds. ?i 0 g(x)dx = i M ? 0 g(x)dx (2)</formula><p>where ? is the standard deviation of the gaussian function. In our experiments, ? = 1.5r palm where r palm is the radius of the palm. The global feature can be written as Equation <ref type="formula" target="#formula_3">(3)</ref>:</p><formula xml:id="formula_3">? t = [? bin , ?, ?, r x , r y , r z ]<label>(3)</label></formula><p>where ? bin is the discrete representation of ? using the thresholds determined by Equation <ref type="formula">(2)</ref>.</p><p>Similarly to previous works <ref type="bibr" target="#b20">[21]</ref>, we use offset pose ? t op and dynamic pose ? t dp to model the finger motion features. The offset pose represents the offset from current pose to the pose of first frame of the gesture sequence. The dynamic pose represents the difference of global features between current frame and several previous frames. There features can enhance the representability of the global motion of the hand and thus can model the temporal information of dynamic hand gesture.</p><formula xml:id="formula_4">? t op = ? t ? ? 1 (4) ? t dp = {? t ? ? t?s |s = 1, 5, 10}<label>(5)</label></formula><p>All above features are concatenated to form the global motion</p><formula xml:id="formula_5">features G t (S) = [? t , ? t op , ? t dp ] for frame t.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Finger Motion Feature</head><p>For many dynamic hand gesture, the movement of fingers are critical because the global movement may be non-significant, especially for fine-grained gestures. We use 20 DoFs (degree of freedoms) to model the finger movement. For the MCP joints, there are 2 DoFs for each joint. For proximal interphalangeal (PIP) and distal interphalangeal (DIP) joints, 1 DoF is used to describe the angle of bone. These parameters can retain rich information for the shape of the hand skeleton. We use IK(?) to denote the inverse kinematics function that derive hand parameters from the original hand skeleton s t .</p><formula xml:id="formula_6">? t = IK(s t )<label>(6)</label></formula><p>Similarly, we use dynamic pose ? t dp and offset pose ? t op to model the finger motion feature. </p><formula xml:id="formula_7">? t op = ? t ? ? 1<label>(7)</label></formula><formula xml:id="formula_8">? t dp = {? t ? ? t?s |s = 1, 5, 10}<label>(8)</label></formula><p>These features are concatenated to form the finger motion features F t (S) = [? t , ? t op , ? t dp ] for frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DYNAMIC HAND GESTURE RECOGNITION</head><p>RNN has shown great successes in human action recognition and hand gesture recognition. Although RNN can learn features for the input data, the representability of the features may be absent in some aspects. To this end, we augment features for RNN by combining the hand-crafted global and finger motion features and the original skeleton. The framework of our proposed method for skeleton-based dynamic hand gesture recognition is shown in <ref type="figure">Figure 1</ref>. The finger motion features and global motion features are extracted from the input skeleton sequence. These motion features and the input skeleton sequence are fed into the RNN. Each branch contains two long short term memory (LSTM) layers and one fully connected (FC) layer. Outputs from three branches are concatenated together, followed by three FC layers and a softmax layer for class prediction. All layers are followed by a dropout layer and FC layers are followed by a ReLU function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>DHG-14/28 <ref type="bibr" target="#b13">[14]</ref> is a public dynamic hand gesture dataset that provides hand gesture sequences with depth images and skeletons. Since our proposed method bases on hand skeleton, we only use the skeleton information of the dataset to conduct our experiments. DHG-14/28 is a challenging dataset since it contains hand gesture from 20 subjects and has 14 gestures with two different finger configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation</head><p>The proposed RNN framework is implemented in Keras <ref type="bibr" target="#b21">[22]</ref>. We use Adam <ref type="bibr" target="#b22">[23]</ref> algorithm with mini-batch of 32 to train the network. The parameters of Adam are set to default setting suggested in <ref type="bibr" target="#b22">[23]</ref>, with learning lr = 0.001, ? 1 = 0.9, ? 2 = 0.999 and ? = 1e ?08 .The network is trained for 100 epochs. In our experiments, M of Equation <ref type="formula">(2)</ref> is set to M = 5. Every skeleton sequence is subtracted by the palm position of the first frame and scaled the amplitude to 1 before fed into third branch in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Self-comparison</head><p>To verify the contributions of our proposed method, we conduct two self-comparison baseline experiments on DHG-14 dataset, which has 14 gesture classes. The first baseline (Motion Features) only takes motion features as input and remove the third branch of the framework shown in <ref type="figure">Figure 1</ref>. The second baseline (Skeleton) only use the skeleton sequences as input. We follow same experimental setup as <ref type="bibr" target="#b13">[14]</ref>, using a leave-one subject-out cross-validation (LOOCV) strategy for all following experiments. The proposed network is trained on data from 19 subjects and tested on the rest one. Therefore, these experiments are repeated 20 times, with different subject being used for testing. Previous work <ref type="bibr" target="#b13">[14]</ref> only reported the average classification accuracy, which is not sufficient to evaluate the performance of the algorithm. In this paper, we report the worst, best and average results of 20 different splitting protocol as well as the standard derivation. The recognition rates of these two baselines are shown in <ref type="table" target="#tab_1">Table 1</ref>. In most cases, Our proposed method outperforms two baselines in terms of worst, best, average accuracy and the stand derivation, which verify the effectiveness of the proposed framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with State-of-the-arts</head><p>We compare our work with state-of-the art method <ref type="bibr" target="#b13">[14]</ref> on DHG-14/28 dataset. The recognition rates of different methods on DHG-14 and DHG-28 dataset are shown in <ref type="table" target="#tab_2">Table 2</ref>. It shows that our proposed method outperforms state-of-the-art work <ref type="bibr" target="#b13">[14]</ref> on DHG-14 dataset in terms of coarse, fine and all gestures. To better illustrate the performance of our proposed algorithm, the confusion matrix of 14 classes is shown in <ref type="figure">Figure 3</ref>. It can be observed that the confusion between gesture Grab and Pinch is severe, due to the high similarity of these two gestures. However, our algorithm does improve the performance of these two gestures compared with those of <ref type="bibr" target="#b13">[14]</ref>. Sh <ref type="formula" target="#formula_1">(1)</ref> Sh <ref type="formula">(2)</ref> G It can be observed that our method promotes the classification accuracy of fine-grained gestures a lot. The improvement of recognition rate of coarse-grained gestures is comparatively little because it's already quite good. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our method is also better than <ref type="bibr" target="#b13">[14]</ref> when considering the more complicated 28-gestures classification task, which demonstrates the effectiveness of our proposed algorithm. The confusion matrix of 28 classes is shown in <ref type="figure">Figure 2</ref>. A metric called Loss of Accuracy when Re- <ref type="table">Table 3</ref>. Comparison of LAFED metric. Method Smedt et al. <ref type="bibr" target="#b13">[14]</ref> Ours LAFED 0.0114 0.0075 moving the Finger Differentiation (LARFD) was proposed in <ref type="bibr" target="#b13">[14]</ref> to evaluate to what degree we can blame the loss of accuracy from 14-gestures to 28-gestures classification on the intra-gesture confusion. The smaller LARFD metric is, the less loss of accuracy is due to intra-gesture confusion. The LARFD metric of different methods is listed in <ref type="table">Table 3</ref>. We can see that our proposed algorithm outperforms <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_9">( 1 ) G ( 2 ) T ( 1 ) T ( 2 ) E ( 1 ) E ( 2 ) P ( 1 ) P ( 2 ) R -C W ( 1 ) R -C W ( 2 ) R -C C W ( 1 ) R -C C W ( 2 ) S -R ( 1 ) S -R ( 2 ) S -L ( 1 ) S -L ( 2 ) S -U ( 1 ) S -U ( 2 ) S -D ( 1 ) S -D ( 2 ) S -X ( 1 ) S -X ( 2 ) S -V ( 1 ) S -V ( 2 ) S -+ ( 1 ) S -+ ( 2 ) S h ( 1 ) S h ( 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This paper proposes an algorithm to augment the motion features for recurrent neural network to recognize skeleton-based dynamic hand gestures. The finger motion features are extracted from the input skeleton sequence to describe the articulated movements of fingers and the global motion features are extracted to represent the global translation and rotation of the hand. The motion features, along with the skeleton sequence, are fed into a RNN to predict the class of input gesture. Experiments on the public DHG-14/28 dataset demonstrate that our proposed method outperforms state-of-the-art methods. Future work may focus on a hierarchical coarse to fine framework to achieve better classification performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>The confusion matrix of the proposed approach for DHG-28. The confusion matrix of the proposed approach for DHG-14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>work was partially supported by National Science Foundation of China (No. 61271390) and State High-Tech R&amp;D Program of China (No. 2015AA016304). We gratefully acknowledge NVIDIA for GPU donation.</figDesc><table><row><cell>Finger Motion Feature</cell><cell>LSTM</cell><cell>LSTM</cell><cell>FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>256</cell><cell>512</cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Global Motion Feature</cell><cell>LSTM 128</cell><cell>LSTM 256</cell><cell>FC 256</cell><cell>FC 512</cell><cell>FC 256</cell><cell>FC</cell><cell>Softmax</cell><cell>Predicted class</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#categories</cell><cell></cell><cell></cell></row><row><cell></cell><cell>LSTM</cell><cell>LSTM</cell><cell>FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>128</cell><cell>256</cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Merge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Skeleton sequence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Corresponding author: wangguijin@tsinghua.edu.cn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Recognition rates (%) of self-comparison experiments on DHG-14 dataset. ? 12.37 97.78 74.44 86.44 ? 7.94 93.57 67.86 77.43 ? 6.82 ? 9.19 97.78 72.22 89.0 ? 7.55 94.29 67.86 84.68 ? 6.67</figDesc><table><row><cell>Method</cell><cell>fine best worst</cell><cell>avg?std</cell><cell>best</cell><cell>coarse worst</cell><cell>avg?std</cell><cell>best</cell><cell>both worst</cell><cell>avg?std</cell></row><row><cell cols="4">Skeleton 61.2 Motion Features 84.0 86.0 42.0 46.0 71.5 ? 11.44 96.67</cell><cell cols="2">64.44 81.94 ? 8.17</cell><cell>90.0</cell><cell>58.57</cell><cell>78.21 ? 7.49</cell></row><row><cell>Ours</cell><cell cols="2">90.0 56.0 76.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of recognition rates (%) on DHG-14/28 dataset.</figDesc><table><row><cell>Method</cell><cell>fine</cell><cell>DHG-14 coarse</cell><cell>both</cell><cell>DHG-28 both</cell></row><row><cell cols="3">Smedt et al. [14] 73.60 88.33</cell><cell>83.07</cell><cell>80.0</cell></row><row><cell>Ours</cell><cell>76.9</cell><cell cols="2">89.0 84.68</cell><cell>80.32</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gesture recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushmita</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinku</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="324" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hand gesture based interactive presentation system utilizing heterogeneous cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobo</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="336" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Static hand gesture recognition based on finger root-center-angle and length weighted mahalanobis distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Photonics Europe. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98970" to="98970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">American sign language alphabet recognition using microsoft kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Leu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2368" to="2377" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4207" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moddrop: Adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1692" to="1706" />
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth estimation for speckle projection system using progressive reliable points growing matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied optics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="516" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-accuracy stereo matching based on adaptive ground control points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1412" to="1423" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth-based hand pose estimation: data, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1868" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3316" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opening the black box: Hierarchical sampling optimization for estimating human hand pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3325" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial attention deep net with partial pso for hierarchical hybrid hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic hand gesture recognition with leap motion controller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1188" to="1192" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Combining convnets with hand-crafted features for action recognition based on an hmm-svm classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00749</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature augmented deep neural networks for segmentation of cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petter</forename><surname>Sajith Kecheril Sadanandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Ranefall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>W?hlby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A solution for the best rotation to relate two sets of vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Kabsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="922" to="923" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parsing the hand in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1241" to="1253" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel hierarchical framework for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="148" to="159" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
