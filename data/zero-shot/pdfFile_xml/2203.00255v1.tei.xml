<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shang</surname></persName>
							<email>chao.shang3@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
							<email>guangtao.wang@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
							<email>peng.qi@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
							<email>jhuangz@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Alexa AI</orgName>
								<address>
									<country>Amazon</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question answering over temporal knowledge graphs (KGs) efficiently uses facts contained in a temporal KG, which records entity relations and when they occur in time, to answer natural language questions (e.g., "Who was the president of the US before Obama?"). These questions often involve three timerelated challenges that previous work fail to adequately address: 1) questions often do not specify exact timestamps of interest (e.g., "Obama" instead of 2000); 2) subtle lexical differences in time relations (e.g., "before" vs "after"); 3) off-the-shelf temporal KG embeddings that previous work builds on ignore the temporal order of timestamps, which is crucial for answering temporal-order related questions. In this paper, we propose a timesensitive question answering (TSQA) framework to tackle these problems. TSQA features a timestamp estimation module to infer the unwritten timestamp from the question. We also employ a time-sensitive KG encoder to inject ordering information into the temporal KG embeddings that TSQA is based on. With the help of techniques to reduce the search space for potential answers, TSQA significantly outperforms the previous state of the art on a new benchmark for question answering over temporal KGs, especially achieving a 32% (absolute) error reduction on complex questions that require multiple steps of reasoning over facts in the temporal KG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal knowledge graphs (KGs) record the relations between entities and the timestamp or time period when such relation hold, e.g., in the form of a quadruple: (Franklin D. Roosevelt, position held, President of USA, <ref type="bibr">[1933,</ref><ref type="bibr">1945]</ref>). This makes them a perfect source of knowledge to answer questions that involve knowledge of when certain events occurred as well as how they are related temporally * Work done at JD AI Research. (see <ref type="figure" target="#fig_0">Figure 1</ref> for an example). Unlike question answering (QA) over non-temporal KGs that is mainly concerned with relational inference, a core challenge in temporal KGQA is correctly identifying the time of reference mentioned explicitly or implicitly in the question, and locating relevant facts by jointly reasoning over relations and timestamps.</p><p>Inspired by work on relational KGQA <ref type="bibr" target="#b18">Saxena et al., 2020)</ref>, where knowledge graph embeddings <ref type="bibr" target="#b1">(Dasgupta et al., 2018;</ref><ref type="bibr" target="#b5">Garc?a-Dur?n et al., 2018;</ref><ref type="bibr" target="#b6">Goel et al., 2020;</ref><ref type="bibr" target="#b15">Lacroix et al., 2020)</ref> learned independently of question answering are used as input to KGQA models, previous work <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> employs temporal KG embeddings to attack the problem of temporal KGQA. Despite its relative success on simple temporal questions that directly queries facts in the KG with one out of the four facts left as the answer (e.g., "When was Franklin D. Roosevelt the President of USA?" or "What position did Franklin D. <ref type="bibr">Roosevelt hold between 1933 and</ref><ref type="bibr">1945?")</ref>, this approach still struggles to handle questions that require multiple steps of relational-temporal reasoning (e.g., the example in <ref type="figure" target="#fig_0">Figure  1</ref>).</p><p>We identify three main challenges that hinder further progress on temporal KGQA. Firstly, complex temporal questions often require inferring the correct point of reference in time, which is not considered by previous work. For instance, to correctly answer the question in <ref type="figure" target="#fig_0">Figure 1</ref>, it is crucial that we first identify that World War II took place between 1939 and 1945, and look for entities with the desired relation with President of USA in the time interval specified by these times. Secondly, unlike entity relations, which are usually expressed in natural language with a handful of content words that correspond well with their recorded relations in KGs (e.g., "What position did ... hold ..." vs the "position held" relation), temporal relations often involve just one or two prepositions (e.g., "before" or "during") and are expressed only implicitly in temporal KGs (e.g., nowhere is it clearly stated that 1931 is earlier than, or before, 1934, by a gap of 3 years). As a result, a small lexical change can drastically alter the temporal relation expressed by the question, and therefore the answer set. Thirdly, previous work on temporal KGQA build on temporal KG embeddings, where each timestamp is assigned a randomly initialized vector representation that is jointly optimized with entity and relation representations to reconstruct quadruples in the KG from embeddings. While sound as a standalone method for encoding knowledge in temporal KGs, this approach does not guarantee that the learned timestamp representations can recover implicit temporal relations like temporal orders or distance, which are crucial for temporal KGQA.</p><p>In this paper, we propose a time-sensitive question answering framework (TSQA) to address these challenges. We first equip the temporal KGQA model with a time estimation module that infers the unstated timestamps from questions as the first step of reasoning, and feed the result into relational inference as a reference timestamp. Even without explicit training data for this module, the explicit factorization of the problem yields significant improvement over previous work on complex questions that require reasoning over multiple temporal quadruples. To improve the sensitivity of our question encoder to time relation words, we also propose auxiliary contrastive losses that contrast the answer prediction and time estimation for questions that differ only by the time relation word (e.g., "before" vs "after"). By leveraging the mutual exclusiveness of answers and the prior knowledge regarding potential time estimates from different time relation words, we observe further improvements in model performance on complex questions. Next, to learn temporal KG embeddings with prior knowledge of temporal order and distance built in, we introduce an auxiliary loss of time-order classification between each pair of timestamp embeddings. As a result, the knowledge in the temporal KG can be distilled into the entity, relation, and timestamp embeddings where the timestamp embeddings can naturally recover order and distance information between the underlying timestamps, thus improving the performance of temporal KGQA where such information is crucial. Finally, we enhance TSQA with KG-based approaches to narrow the search space to speed up model training and inference, as well as reduce the number of false positives in model prediction. As a result, TSQA outperforms the previous state of the art on the CRONQUES-TIONS benchmark <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> by a large margin.</p><p>To summarize, our contributions in this paper are: a) we propose a time-sensitive question answering framework (TSQA) that performs time estimation for complex temporal answers; b) we present contrastive losses that improve model sensitivity to time relation words in the question; c) we propose a time-sensitive temporal KG embedding approach that benefits temporal KGQA; d) with the help of KG-based pruning technique, our TSQA model outperforms the previous state of the art by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Temporal Knowledge Graph Embedding. Knowledge graph embedding learning <ref type="bibr" target="#b0">(Bordes et al., 2013;</ref><ref type="bibr" target="#b27">Yang et al., 2014;</ref><ref type="bibr" target="#b24">Trouillon et al., 2016;</ref><ref type="bibr" target="#b2">Dettmers et al., 2018;</ref><ref type="bibr" target="#b21">Shang et al., 2019;</ref><ref type="bibr" target="#b22">Sun et al., 2019;</ref><ref type="bibr" target="#b9">Ji et al., 2021)</ref> has been an active research area with applications directly in knowledge base completion and relation extractions. Recently, there are several works that extended the static KG embedding models to temporal KGs. <ref type="bibr" target="#b13">Jiang et al. (2016)</ref> first attempt to extend TransE <ref type="bibr" target="#b0">(Bordes et al., 2013)</ref> by adding a timestamp embedding into the score function. Later, Hyte <ref type="bibr" target="#b1">(Dasgupta et al., 2018)</ref> projects each timestamp with a corresponding hyperplane and utilizes the TransE score in each space. <ref type="bibr" target="#b5">Garc?a-Dur?n et al. (2018)</ref> extend TransE and DistMult by utilizing recurrent neural networks to learn time-aware representations of relation types. TCompLEx <ref type="bibr" target="#b15">(Lacroix et al., 2020)</ref> extends the ComplEx with time based on the canonical decomposition of tensors of order 4. Temporal QA on Knowledge Graph. Temporal QA have mostly been studied in the context of reading comprehension. ForecastQA <ref type="bibr" target="#b14">(Jin et al., 2021)</ref> formulates the forecasting problem as a multiplechoice question answering task, where both the articles and questions include the timestamps. The recent released TORQUE <ref type="bibr" target="#b16">(Ning et al., 2020</ref>) is a dataset that explores the temporal ordering relations between events described in a passage of text.</p><p>Another direction is the temporal question answering over knowledge bases (KB) <ref type="bibr">(Jia et al., 2018b,a)</ref>, which retrieves time information from the KB. TempQuestions <ref type="bibr" target="#b10">(Jia et al., 2018a</ref>) is a KGQA dataset specifically aimed at temporal QA. Based on this dataset, <ref type="bibr" target="#b11">Jia et al. (2018b)</ref> design a method that decomposes and rewrites each question into nontemporal sub-question and temporal sub-question. Here the KG used in TempQuestions is based on a subset of FreeBase which is not a temporal KG. Later <ref type="bibr" target="#b12">Jia et al. (2021)</ref> proposes a first end-to-end system (EXAQT) for answering complex temporal questions, which takes advantage of the question-relevant compact subgraphs within the KG, and relational graph convolutional networks <ref type="bibr" target="#b19">(Schlichtkrull et al., 2018)</ref> for predicting the answers. All previous datasets only include a limited number of temporal questions. Recently, a much larger temporal KGQA dataset CRONQUES-TIONS <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> is released, which includes both the temporal questions and the temporal KG with time annotation for all edges. Based on this dataset, the CronKGQA model <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> is presented that exploits recent advances in Temporal KG embeddings and achieves performance superior to all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first give the problem definition of temporal question answering over temporal knowledge graph. Then, we introduce the framework to solve this problem, which integrates time sensitivity into KG embedding and answer inference. Finally, we describe the key modules of our proposed system in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Framework</head><p>QA on Temporal KG aims at finding out the answer from a given temporal KG G = (V, E, R, T ) for a given free-text temporal question Q containing implicit temporal expression, and the answer is either an entity of entity set V or a timestamp of timestamp set T . Here, E ? V ? V is a set of edges, and R is the set of relations. Edge from a quadruple (s, r, [t s , t e ], o) indicates the relation r ? R holds between subject entity s and object entity o during time interval [t s , t e ] (t s &lt; t e and t e/s ? T ). Framework. Our framework resorts to KG embeddings along with pretrained language models to perform temporal KGQA. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architecture which consists of two modules: 1) time-aware TKG encoder; 2) time-sensitive question answer.</p><p>The time-aware TKG encoder extends the existing TKG embedding method by adding an auxiliary time-order learning task to consider the quadruple orders. And the time sensitive QA module first performs neighboring graph extraction to reduce the search space for question answer, then performs joint training for answer/time prediction and timesensitive contrastive learning to enhance the model ability in capturing temporal signals in free-text question. Next, we will introduce these two modules in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Time-aware KG Encoder</head><p>We first briefly review a time-aware KG embedding method based on TCompLEx <ref type="bibr" target="#b15">(Lacroix et al., 2020)</ref> since it has been used in <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> for TKGQA and shows competitive performance. Next, we show that how to perform TCompLEx on temporal KG, then analyze its weakness in TKGQA especially for complex question and further overcome such weakness by introducing an auxiliary time-order learning task in TKG embedding. TCompLEx for TKG. TCompLEx is an extension of ComplEx considering time information, which not only encodes the entity and relation to complex vectors, but also maps each timestamp to a complex vector. To perform TCompLEx over temporal KG in our problem definition, we first reformulate each quadruple to a set of new quadruples by</p><formula xml:id="formula_0">(s, r, [t s , t e ], o) = {(s, r, t, o)|t s ? t ? t e } (1)</formula><p>Let e s , e r , e t , e o ? C d be the complex-value embeddings of s, r, t, o, respectively. Then, TCom- pLEx scores each quadruple (s, r, t, o) by</p><formula xml:id="formula_1">S(s, r, o, t) = Re( e s , e r , e o , e t )<label>(2)</label></formula><p>where Re(.) denotes the real part of a complex vector, and denotes the multi-linear product. Finally, we use a loss function similar to the negative sampling loss for effectively TCompLEx training.</p><formula xml:id="formula_2">L T C = ?log(?(? ? S(s, r, o, t))) ? 1 K K i=1 (log(?(S(s i , r, o i , t i ) ? ?))),<label>(3)</label></formula><p>where ? is a fixed margin, ? is the sigmoid function,</p><formula xml:id="formula_3">(s i , r, o i , t i ) is the i-th negative quadruple.</formula><p>According to the loss function in equation 3, we observe that TCompLEx only cares about whether the quadruple is true or false and ignores the orders of different quadruples occur. However, the time orders are critical to find the correct answer in knowledge graphs. For example, to answer the 'Who is the President of USA before William J. Clinton?", we need not only the two facts (President of USA, Position Held, Ronald Reagan, <ref type="bibr">[1981,</ref><ref type="bibr">1989]</ref>) and (President of USA, Position Held, William J. Clinton, <ref type="bibr">[1993,</ref><ref type="bibr">2001]</ref>), but also the time order of these facts. To overcome such a limit of TCompLEx in TKGQA, we introduce an auxiliary time-order learning task over time-embeddings. Time-order learning in TKG. To keep the time order in embedding spaces, we first sort the timestamps in T by an ascending order and get</p><formula xml:id="formula_4">(t 1 , t 2 , ? ? ? , t |T | ) and t i &lt; t j if 1 ? i &lt; j ? |T |. Let t i = [Re(e t i )</formula><p>, Im(e t i )] ? R 2d be the concatenation the real and imaginary components of embedding e t i of timestamp t i . Inspired by position embedding in <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>, we first initialize the timestamp embedding t i as follows.</p><formula xml:id="formula_5">t i [2k] = sin( i 10000 2k/2d ) t i [2k + 1] = cos( i 10000 2k/2d ) (4) where 0 ? k ? d ? 1.</formula><p>Afterwards, for any pair of timestamps (t i , t j ), we calculate the probability of time order as:</p><formula xml:id="formula_6">p t (i, j) = sigmoid((t 1 ? t 2 ) T W t ),<label>(5)</label></formula><p>where W t ? R 2d represents a parameter vector. Based on the time-order probabilities, we introduce a binary cross-entropy loss as a time-order constraint over timestamp embeddings as follow:</p><formula xml:id="formula_7">L T O = ? ?(i, j) log(p t (i, j)) ? (1 ? ?(i, j)) log(1 ? p t (i, j)),<label>(6)</label></formula><p>where ?(i, j) = 1 if t i &lt; t j else ?(i, j) = 0. Joint-training. A weighted sum of T-CompLEx training loss and time-order constraint is considered as the final objective function for the joint training for TKG embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Time-Sensitive TKG-QA</head><p>In this section, we introduce our time-sensitive question answering module from the following aspects in details: 1) question decomposition which divides the questions as entities and relations described in free-text; 2) entity neighboring subgraph extraction which reduces the search space of candidate timestamps and answer entities; and 3) time-sensitive question answer which explores the time information implied in both KG and questions to help the model find the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Question Decomposition and Encoder</head><p>For each question Q, we first identify all the entities {Ent 1 , Ent 2 , ? ? ? , Ent k } in Q which also appear in KG G, i.e., Ent i ? E (1 ? i ? k). Then, by replacing the entities in question Q with special token [subject] and [object] in order, we obtain an entity-independent temporal relation description in free-text named temporal expressionQ.</p><p>Taking the question "When did Obama hold the position of President of USA?" as an example, by replacing the identified entities "President of USA" and "Obama", we get its temporal expression as "When did subject hold the position of object?".</p><p>Next [CLS] +Q are fed into BERT that outputs [CLS] token embedding as e q ? R d bert , where d bert is the output dimension of BERT, and two kinds of question representations as follows.</p><formula xml:id="formula_8">q r = W r q (?(W e q ))<label>(7)</label></formula><p>q t = W t q (?(W e q )),</p><p>where q r , q t ? R 2d represents the embedding of relation and time implied in question, respectively. W ? R d bert ?2d , W r q , W t q ? R 2d?2d are the parameter matrix, and ? represents the activation function. Finally, to facilitate the calculation with KG embeddings, we reformulate q r , q t in complex space as:</p><formula xml:id="formula_10">q r = q r [0 : d] + ? ?1 ? q r [d : 2d]<label>(9)</label></formula><formula xml:id="formula_11">q t = q t [0 : d] + ? ?1 ? q t [d : 2d]<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Entity Neighbor Graph Extraction</head><p>Let {Ent 1 , Ent 2 , ? ? ? , Ent k } be the k entities extracted from question Q, we first extract the mhop neighboring sub-graph G i for each entity Ent i . Then, by combining these k sub-graphs, we obtain the search graph G q for question Q: G q = ? k i=1 G i . Suppose that E q and T q are the sets of entities and timestamps appearing in G q , respectively, they constitute the search space of time and entity prediction in our TKG-QA method. In training stage, we set the hop number m as the minimum value which results in correct answer entity appearing in G q . In testing stage, we set m as the largest hop number used in training stage. In practice, the size of graph G q in usually much smaller than that of whole graph G. For example, in CronKGQA, the average value of |G q |/|G| is about 3%.</p><p>Entity Neighboring graph extraction aims at reducing the search space of candidate timestamps and answer entities. This results in not only more efficient training procedure, but also performance improvement of question answer because a larger number of candidates usually means a much more difficult learning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Time-Sensitive Question Answering</head><p>For temporal question answer over KG, the interaction of time and answer entity prediction is very important since the time range brings a strong constraint on the search space of answers. However, the existing method <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> usually performs such two predictions independently which results in poor performance especially for complex questions which need to consider multiple facts to get the answer. To overcome this limitation, we directly feed the intermediate time representation t q learned from time estimation to answer prediction to enhance the interaction of these two tasks. Time Estimation. Based on the embeddings e s and e o of subject entity s and object entity o from KG and the time embedding q t from a question, we design the time estimation function F T for learning the time embedding t q as follows: 1</p><formula xml:id="formula_12">t q = F T (e s , q t , e o ) = W t q ([Re( e s , q t , e o ), Im( e s , q t , e o )]),<label>(11)</label></formula><p>where W t q ? R 2d?2d represents the parameter matrix.</p><p>[.] is the concatenation function, Re(.) denotes the real part of a complex vector and Im(.) is the imaginary part.</p><p>After getting the time embedding w.r.t. question t q , for timestamp prediction, the following score function to estimate the score for each timestamp t ? T q as follow:</p><formula xml:id="formula_13">S t = Re( t q , t )<label>(12)</label></formula><p>Entity Prediction. In enhance the interaction between time prediction and answer prediction, we update the embedding of entity w.r.t. question by considering time embedding t q by an entity function F E as follow:</p><formula xml:id="formula_14">e q = F E (e s , q r , t q ) = e s , q r , t q<label>(13)</label></formula><p>Finally, we score the entity e ? E q by:</p><formula xml:id="formula_15">S e = Re( e q , e )<label>(14)</label></formula><p>The answer entity of the question is either timestamp or entity. Let S a be the answer score and thus S a = S t or S e when the answer is timestamp or entity. Suppose C represents the number of candidate answers (i.e., C = |E q | + |T q |), then we can define the probability of i-th candidate answer being true as:</p><formula xml:id="formula_16">P a,i = exp(S a,i ) C j=1 exp(S a,j )) .<label>(15)</label></formula><p>Finally, we train the answer prediction model by minimizing the cross-entropy loss as follow:</p><formula xml:id="formula_17">L answer = ? C i y i log(P a,i ),<label>(16)</label></formula><p>where y i = 1 if the i-th candidate is the true answer, otherwise y i = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Temporal Contrastive Learning</head><p>The temporal question answer system should be sensitive to the temporal relation implied in the question. For example, the answer of "What does happen before a given event?" is quite different from that of "What does happen after a given event?". Existing works on TKG-QA usually resort to pre-trained language models for question understanding. But these models are not sensitive to the difference of temporal expressions in freetext <ref type="bibr" target="#b16">(Ning et al., 2020;</ref><ref type="bibr" target="#b3">Dhingra et al., 2021;</ref><ref type="bibr" target="#b20">Shang et al., 2021;</ref><ref type="bibr" target="#b7">Han et al., 2021)</ref>, and thus prone to wrong predictions. To make the system sensitive to the temporal relation implied in question, we resort to a contrastive learning method: we construct a contrastive question to the original question, then add auxiliary contrastive learning tasks to distinguish the latent temporal representation and prediction results coming from the pair of contrastive questions. Contrastive Question Generation. To generate the contrastive questionQ for the given question Q, we first extract all the temporal words based on large number of questions in temporal question answer dataset, and then build a contrastive word pair dictionary by finding the antonyms. The dictionary consists of D contr = {(first, last), (before, after), (before, during), (during, after), (before, when), (when, after)}. Based on such dictionary, we replace the temporal word in given question Q by its antonym to generate its contrastive questionQ. Contrastive time order learning. For the contrastive question pair Q andQ, we follow the same encoder in Eq. 11 to get the corresponding timeaware embeddings t q and t qc , respectively. Meanwhile, according to the contrastive temporal word pair dictionary, suppose that we pickup the pair (word 1 , word 2 ) ? D contr for contrastive question construction, we can construct a question order label y o : y o = 0 ifQ is achieved by replacing word 1 as word 2 , else y o = 1.</p><p>Afterward, we distinguish the temporal orders implied by word 1 and word 2 by predicting of the order label y o based on t q and t qc as follow:</p><formula xml:id="formula_18">p o = sigmoid((t q ? t qc ) T W o ) (17) L order = ?y o log(p o ) ? (1 ? y o ) log(1 ? p o ),<label>(18)</label></formula><p>where W o ? R 2d represents the parameter vector to be learned. Answer-guided Contrastive Learning. Let S = [s 1 , ? ? ? , s C ],S = [s 1 , ? ? ? , s C ] be the answer scores w.r.t. questions Q and its contrastive ques-tionQ, respectively, where C = |E q | + |T q |. By stacking these two scores together, we get S q = [S;S] ? R 2?C . Then, we can apply softmax over S q along the last dimension and get the probability scores P q = sof tmax(S q ) ? R 2?C and sum(P q [:, i]) = 1 for i = 1, ? ? ? , C. Due to the fact that the answers of question Q are definitely not for questionQ, we construct an answer-guided learning labels as y a = [y 1 , ? ? ? , y C ], where y i = 1 if and only if the i-th candidate is true answer for Q, otherwise y i = 0. Then, we get an answer-guided contrastive loss as follow:</p><formula xml:id="formula_19">L contrast = ? 1 C C j=0 y i log(P q [0, i])<label>(19)</label></formula><p>Joint Training.</p><p>We combine the answer prediction loss and contrastive losses as the final objective function for joint training:</p><formula xml:id="formula_20">Loss = L answer +? o ?L order +? c ?L contrast ,<label>(20)</label></formula><p>where ? o &gt; 0, ? c &gt; 0 are the weight factors to make tradeoffs between different losses.</p><p>In this section, we conduct experiments to assess the effectiveness of our proposed method TSQA for TKG-QA. Our experimental results show that our approach obtains significant improvements over the baseline models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Data. CRONQUESTIONS 2 is the largest known Temporal KGQA dataset consisting of two parts: a KG with temporal annotations, and a set of free-text questions requiring temporal reasoning. This Temporal KG has 125k entities and 328k facts (quadruples), while a set of 410k questions is given. The facts have the time spans in the edge. These time spans or timestamps were discretized to years. This dataset consists of questions that can be categorized into two groups based on their answer type: entity questions where the answer is an entity in the KG, and time questions where the answer is a timestamp. The authors also categorize these questions into "simple reasoning" (including simple entity and simple time subtypes) and "complex reasoning" (including before/after, first/last and time join subtypes). <ref type="table" target="#tab_1">Table 1</ref> provides the number of questions across different categories. Complex questions require complex temporal reasoning which takes advantage of multiple facts and temporal order of these facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics include Hits@1 and Hits@10</head><p>, which is the standard evaluation metrics on CRON-QUESTIONS <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref>.</p><p>Hyper-parameter setting. We train the TSQA models by setting the hyper-parameters as: learning rate = {1e ?4 , 2e ?5 , 1e ?5 }, ? o = {0.5, 1.0, 2.0, 3.0, 5.0} and ? c = {0.5, 1.0, 2.0, 3.0, 5.0}, and pick up 2 https://github.com/apoorvumang/ CronKGQA the best hyper-parameters on dev set by the overall Hits@1 metrics. Our models are implemented by PyTorch and trained using NVIDIA Tesla V100 GPUs. Baselines. We select several recent SOTA TKG-QA models as our baselines as follow:</p><p>? EmbedKGQA <ref type="bibr" target="#b18">(Saxena et al., 2020)</ref> is the first method to use KG embeddings for the multi-hop KGQA task. It uses ComplEx <ref type="bibr" target="#b24">(Trouillon et al., 2016)</ref> embeddings and can only deal with nontemporal KGs and single entity questions.</p><p>? T-EaE-add/replacement <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> are two modifications of KG enhanced language model EaE <ref type="bibr" target="#b4">(F?vry et al., 2020)</ref>, which integrates entity knowledge into a transformer-based language model and has been used for TKG-QA <ref type="bibr" target="#b18">(Saxena et al., 2020)</ref>. T-EaE-add has all grounded entities and time spans marked in the question, and T-EaE-replace replaces the BERT embeddings with the entity/time embeddings instead of adding them with token embeddings.</p><p>? CronKGQA <ref type="bibr" target="#b17">(Saxena et al., 2021)</ref> extends Em-bedKGQA to the temporal QA task, and takes advantage of the temporal KG embeddings to answering temporal questions. This is the current SOTA model on CRONQUESTIONS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Table 2 compares different TKG-QA methods in terms of Hits@1 and Hits@10. From this table, we observe that: 1) our proposed TSQA has achieved state-of-the-art performance in terms of all types of questions on both Hits@1 and Hits@10.</p><p>2) The performance improvement over the SOTA model is significant. TSQA outperforms the SOTA results by more than 82% Hits@1 relative improvement (32% absolute error reduction) on complex questions and 21% Hits@10 relative improvement on simple questions. These results proved the excellent performance of our proposed TSQA on question answering on the temporal knowledge graph, especially for complex temporal reasoning. We also compare our method with baselines in terms of Hits@1 on different subtype questions in <ref type="table" target="#tab_4">Table 3</ref>. From this table, we observe that: on complex questions, our proposed TSQA model outperforms all baseline models significantly. The relative improvement is up to 75%, 94%, 56%, for "before/after", "first/last" and "Time Joint", respectively. The first two kinds of questions are more   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To understand the contributions of the proposed modules in our method, we perform an ablation study by sequentially removing the following components from our proposed TSQA: temporal Contrastive learning (TC), time-aware TKG embeddings (TKE), entity neighboring graph extractor (NG), and time estimation for question answer (TE) in <ref type="table">Table 4</ref>. It is noted that removing TKE means that we replace TKE with T-CompLEx as KG encoder, and removing NG means that we perform QA over the whole knowledge graph. By comparing the two adjacent rows of this table, we can infer the contributions of TC, TKE, NG and TE, respectively: 1) all these modules improve the overall performance in terms of Hits@1, especially for complex questions; 2) by comparing the last two adjacent rows, the proposed time estimation brings significant Hits@1 improvement (14.5%), since this module supplies the latent time embedding which not only enhances the interaction of timestamp estimation and answer estimation but also supplies a good anchor for finding the answer entity, which is very crucial for answering complex questions; 3) entity neighboring graph extraction gets 7.8% Hits@1 improvement over complex questions by comparing rows "TC-TKE" and "TC-TKE-NG", since it significantly narrows down the search space of the candidate answers; 4) by comparing the first three rows, time-aware TKG embedding (TKE) and temporal contrastive learning (TC) further boost the Hits@1 over complex questions. This is because the complex questions usually require the model to capture time ordering information implied in temporal words of the question. And these two modules enhance temporal order learning by adding explicit time-order constraints.  <ref type="table">Table 4</ref>: Results of the ablation study. "-" means to remove a module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a time-sensitive question answering framework (TSQA) over temporal knowledge graphs (KGs). To facilitate the reasoning over temporal and relational facts over multiple facts, we propose a time estimation component to infer the unstated timestamp in the question. To further improve the model's sensitivity to time relation words in the question and facilitate temporal reasoning, we enhance the model with a temporal KG encoder that produces KG embeddings that can recover the implicit temporal order and distance between different timestamps, and with contrastive losses that compare temporally exclusive questions.</p><p>With the help of answer search space pruning from entity neighboring sub-graphs, our TSQA model significantly improves the performance on complex temporal questions that require reasoning over multiple pieces of facts, and outperforms the previous state of the art by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of complex temporal question on a temporal KG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our TSQA model (Left: Time-aware TKG encoder; Right: Time-Sensitive TKG-QA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>CRONQUESTIONS dataset statistics as well as the numbers of questions across different types of reasoning required and answer types.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different TKG-QA models on CRONQUESTIONS dataset.</figDesc><table><row><cell>Question type</cell><cell>Before After</cell><cell>First Last</cell><cell>Time Join</cell><cell>Simple Entity</cell><cell>Simple Time</cell></row><row><cell>EmbedKGQA</cell><cell>0.199</cell><cell cols="2">0.324 0.223</cell><cell>0.421</cell><cell>0.087</cell></row><row><cell>T-EaE-add</cell><cell>0.256</cell><cell cols="2">0.285 0.175</cell><cell>0.296</cell><cell>0.321</cell></row><row><cell>T-EaE-replace</cell><cell>0.256</cell><cell cols="2">0.288 0.168</cell><cell>0.318</cell><cell>0.346</cell></row><row><cell>CronKGQA</cell><cell>0.288</cell><cell cols="2">0.371 0.511</cell><cell>0.988</cell><cell>0.985</cell></row><row><cell>TSQA</cell><cell>0.504</cell><cell cols="2">0.721 0.799</cell><cell>0.988</cell><cell>0.987</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Comparison of different models w.r.t. question</cell></row><row><cell>type in terms of Hits@1.</cell></row><row><cell>challenging as they require a better understand-</cell></row><row><cell>ing of the temporal expressions in question. Our</cell></row><row><cell>method is better in capturing such time-sensitivity</cell></row><row><cell>change in temporal words and thus results in great</cell></row><row><cell>improvement. Moreover, for the simple questions,</cell></row><row><cell>our method still keeps competitive performance</cell></row><row><cell>compared to the SOTA model.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A simple temporal question might contain the timestamp(e.g. 2001). In this case, we set tq as the linear combination of this learned time embedding and the timestamp embedding from KG.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hyte: Hyperplane-based temporally aware knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swayambhu</forename><surname>Shib Sankar Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Nath Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2001" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15110</idno>
		<title level="m">Time-aware language models as temporal knowledge bases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07202</idno>
		<title level="m">Entities as experts: Sparse memory access with entity supervision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning sequence encoders for temporal knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastijan</forename><surname>Duman?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diachronic embedding for temporal knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Econet: Effective continual pretraining of language models for event temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5367" to="5380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tempquestions: A benchmark for temporal question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishiraj</forename><surname>Saha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Str?tgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1057" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tequila: Temporal question answering over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishiraj</forename><surname>Saha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Str?tgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1807" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complex temporal question answering on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumajit</forename><surname>Pramanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishiraj</forename><surname>Saha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="792" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards time-aware knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingsong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Forecastqa: A question answering challenge for event forecasting with temporal text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woojeong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4636" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04926</idno>
		<title level="m">Tensor decompositions for temporal knowledge base completion</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00242</idno>
		<title level="m">Torque: A reading comprehension dataset of temporal ordering questions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01515</idno>
		<title level="m">Question answering over temporal knowledge graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving multi-hop question answering over knowledge graphs using knowledge base embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditay</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4498" to="4507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Open temporal relation extraction for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Conference on Automated Knowledge Base Construction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Orthogonal relation transforms with graph context modeling for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04910</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03526</idno>
		<title level="m">Temp: temporal message passing for temporal knowledge graph completion</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
