<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungil</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
						</author>
						<title level="a" type="main">Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantages, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. We introduce Monotonic Alignment Search (MAS), an internal alignment search algorithm for training Glow-TTS. By leveraging the properties of flows, MAS searches for the most probable monotonic alignment between text and the latent representation of speech. Glow-TTS obtains an order-of-magnitude speedup over the autoregressive TTS model, Tacotron 2, at synthesis with comparable speech quality, requiring only 1.5 seconds to synthesize one minute of speech in end-to-end. We further show that our model can be easily extended to a multi-speaker setting. Our demo page and code are available at public. 12</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text-to-Speech (TTS) is the task to generate speech from text, and deep-learning-based TTS models have succeeded in producing natural speech indistinguishable from human speech. Among neural TTS models, autoregressive models such as Tacotron 2  or Transformer TTS <ref type="bibr" target="#b13">(Li et al., 2019)</ref>, show the state-of-the-art performance. Based on these autoregressive models, there have been many advances in generating diverse speech in terms of modelling different speaking styles or various prosodies <ref type="bibr" target="#b24">Skerry-Ryan et al., 2018;</ref><ref type="bibr" target="#b8">Jia et al., 2018)</ref>.</p><p>Despite the high quality of autoregressive TTS models, there are a few difficulties in deploying end-to-end autoregressive models directly in real-time services. As the synthesizing time of the models grows linearly with the output length, undesirable delay caused by generating long speech can be propagated to the multiple pipelines of TTS systems without designing sophisticated frameworks <ref type="bibr" target="#b14">(Ma et al., 2019)</ref>. In addition, most of the autoregressive models show lack of robustness in some cases <ref type="bibr" target="#b20">(Ren et al., 2019)</ref>. For example, when the input text includes the repeated words, autoregressive TTS models often produce serious attention errors.</p><p>To overcome such limitations of the autoregressive TTS models, parallel TTS models such as FastSpeech have been proposed. These models can synthesize mel-spectrogram significantly faster than the autoregressive TTS models. In addition to the fast sampling, FastSpeech reduces the failure cases for the extremely hard sentences by enforcing its alignment monotonic.</p><p>However, these strengths of parallel TTS models come from the well-aligned attention map between text and speech, which is extracted from their external aligner. Recently proposed parallel models address these challenges by extracting attention maps from the pre-trained autoregressive models. Therefore, the performance of the parallel TTS models critically depends on that of the autoregressive TTS models. Furthermore, since the parallel TTS models assume this alignment to be given during training, they cannot be trained without the external aligners.</p><p>In this work, our goal is to eliminate the necessity of any external aligner from the training procedure of parallel TTS models. Here, we propose Glow-TTS, a flow-based generative model for parallel TTS that can internally learn its own alignment. Glow-TTS is directly trained to maximize the log-likelihood of speech given text, and its sampling process is totally parallel due to the properties of the generative flow. In order to eliminate any dependency on other networks, we introduce Monotonic Alignment Search (MAS), a novel method to search for the most probable monotonic alignment with only text and latent representation of speech. This internal alignment search algorithm simplifies the en-tire training procedure of our parallel TTS models so that it requires only 3 days for training on two GPUs.</p><p>Without any external aligner, our parallel TTS model can generate mel-spectrograms 15.7 times faster than the autoregressive TTS model, Tacotron 2, while maintaining the comparable performance. Glow-TTS also provides diverse speech synthesis, in contrast to other TTS models, which have their stochasticities only in dropout operations. We can control some properties of synthesized samples from Glow-TTS by altering the latent variable of normalizing flows. We further show that our model can be extended to a multi-speaker setting with only a few modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Text-to-Speech (TTS) Models. TTS models are a family of generative models that synthesize speech from text. One subclass of TTS models, including Tacotron 2 , Deep Voice 3  and Transformer TTS <ref type="bibr" target="#b13">(Li et al., 2019)</ref>, generates a mel-spectrogram, a compressed representation of audio, from text. They produce natural speech comparable to the human voice. Another subclass, also known as vocoder, has been developed to transform mel-spectrograms into high-fidelity audio waveform <ref type="bibr" target="#b28">Van Den Oord et al., 2016)</ref> with fast synthesis speed <ref type="bibr" target="#b9">(Kalchbrenner et al., 2018;</ref><ref type="bibr" target="#b29">Van Den Oord et al., 2017;</ref><ref type="bibr" target="#b19">Prenger et al., 2019)</ref>. It has also been studied to enhance expressiveness of TTS models. Auxiliary embedding methods have been proposed to generate diverse speech by controlling some factors such as intonation and rhythm <ref type="bibr" target="#b31">Wang et al., 2018)</ref>, and some studies have aimed at synthesizing speech in the voices of various speakers <ref type="bibr" target="#b4">Gibiansky et al., 2017)</ref>.</p><p>Parallel Decoding Models. There are challenges for sequence-to-sequence (seq2seq) models to decode output sequence in parallel. One of the challenges is the lack of information about how many output tokens have to be generated from each input token. For example, most TTS datasets do not contain the duration value for each phoneme in speech. Another challenge is the difficulty of modelling conditional dependency between output tokens in parallel, which makes parallel models struggle to match the performance of autoregressive models.</p><p>Parallel decoding models have been proposed to address these challenges in various domains. In natural language processing, <ref type="bibr" target="#b5">Gu et al. (2017)</ref> tackle the challenges by using an external aligner to segment output tokens according to each input token. Furthermore, they use sequence-level knowledge distillation <ref type="bibr" target="#b11">(Kim &amp; Rush, 2016)</ref> to reduce the performance gap between the autoregressive teacher network and the parallel model. In TTS, <ref type="bibr" target="#b20">Ren et al. (2019)</ref> similarly extract alignment information from the autore-gressive TTS model, Transformer TTS, as well as utilizing sequence-level knowledge distillation for performance boosting. Another parallel TTS model, ParaNet , utilizes soft attention map from its teacher network. Our Glow-TTS differs from all the previous approaches, which rely on the autoregressive teacher network or the external aligner.</p><p>Flow-based Generative Models. Flow-based generative models have received a lot of attention due to their advantages <ref type="bibr" target="#b6">(Hoogeboom et al., 2019;</ref><ref type="bibr" target="#b3">Durkan et al., 2019;</ref><ref type="bibr" target="#b21">Serr? et al., 2019)</ref>. They can estimate the exact likelihood of the data by applying some invertible transformations. Generative flows are simply trained to maximize this likelihood. In addition to efficient density estimation, the transformations proposed in <ref type="bibr" target="#b1">(Dinh et al., 2014;</ref><ref type="bibr" target="#b12">Kingma &amp; Dhariwal, 2018</ref>) guarantee fast and efficient sampling. <ref type="bibr" target="#b19">Prenger et al. (2019)</ref> and <ref type="bibr" target="#b10">Kim et al. (2018)</ref> introduce these transformations for raw audio speech synthesis to overcome slow sampling speed of an autoregressive vocoder, WaveNet (Van Den <ref type="bibr" target="#b28">Oord et al., 2016)</ref>. Their proposed models, WaveGlow and FloWaveNet, both synthesize raw audio significantly faster than WaveNet. By applying these transformations, Glow-TTS can synthesize a mel-spectrogram given text in parallel.</p><p>In parallel with our work, Flowtron <ref type="bibr" target="#b27">(Valle et al., 2020)</ref> and Flow-TTS <ref type="bibr" target="#b15">(Miao et al., 2020)</ref> are proposed. Flowtron is a flow-based TTS model which exhibits diverse applications of the model such as style transfer and control of speech variation by using flow properties. The main difference from our work is that Flowtron uses autoregressive flows as its objective is not about fast speech synthesis. Flow-TTS is a parallel TTS synthesizer using flow-based decoder and multi-head soft attention with positional encoding, in contrast with our work, which predicts hard monotonic alignments by estimating duration of each input token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Glow-TTS</head><p>In this section, we describe a new type of parallel TTS model, Glow-TTS, which is directly trained to maximize likelihood without any other networks. Glow-TTS achieves the parallel sampling via inverse transformation of the generative flow. In Section 3.1, we formulate the training and inference procedures of our model and these procedures are illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. We present our novel alignment search algorithm in Section 3.2, which removes the necessity of other networks from training Glow-TTS, and all components of Glow-TTS, text encoder, duration predictor, and flow-based decoder, are covered in Section 3.3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Glow-TTS</head><p>In general, normalizing flows for conditional density estimation incorporates a given condition into each flow and maps data into a known prior with the conditional flows. However, Glow-TTS incorporates the text condition into the statistics of the prior distribution rather than into each flow.</p><p>Given a mel-spectrogram x, Glow-TTS transforms the melspectrogram x into the latent variable z with the flow-based decoder f dec : x ? z without any text information, and the latent variable z follows some isotropic Gaussian distribution P Z . Then, text encoder f enc maps the text condition c into the high-level representation of text h, and projects h into the statistics, ? and ?, of Gaussian distribution. Thus, each token of text sequence has its corresponding distribution, and each frame of the latent variable z j follows one of these distributions predicted by the text encoder.</p><p>We define this correspondence between the latent variable and the distribution as an alignment A. Thus, if the latent variable z j follows the predicted distribution of i-th text token N (z j ; ? i , ? i ), then we define A(j) = i. This alignment can be interpreted as a hard attention in sequenceto-sequence modelling. Thus, given an alignment A, we can calculate the exact log-likelihood of the data as follows:</p><formula xml:id="formula_0">log P X (x|c; ?, A) = log P Z (z; c, ?, A) + log det ?f dec (x) ?x (1) log P Z (z; c, ?, A) = T mel j=1 log N (z j ; ? A(j) , ? A(j) ) (2)</formula><p>Since text and speech are monotonically aligned, we assume the alignment A to be monotonically increasing.</p><p>Our goal is to find the parameters ? and the alignment A that maximize the log-likelihood, as in Equation 3. However, it is computationally intractable to find the global solution of Equation 3. To tackle the intractability, we reduce the search space of the parameters ? and the monotonic alignments A by modifying our objective from Equation 3 to Equation <ref type="formula" target="#formula_1">4</ref>.</p><formula xml:id="formula_1">max ?,A L(?, A) = log P X (x|c; A, ?) (3) max ? max A L(?, A) = log P X (x|c; A, ?)<label>(4)</label></formula><p>Thus, training Glow-TTS can be decomposed into two subsequent problems: (i) to search for the most probable alignment A * with the latent representation z and the predicted distributions given the current parameter ? as in Equation 5 and 6, and (ii) to update the current parameters ? to maximize log probability log p X (x|c; ?, A * ) given A * . In practice, we handle these two problems using an iterative approach. At each training step, we first find A * , and then update parameter ? using gradient descent. Our modified objective does not guarantee the global solution of Equation 3, but it still provides a good lower bound of the global solution.</p><formula xml:id="formula_2">A * = arg max A log P X (x|c; A, ?) (5) = arg max A T mel j=1 log N (z j ; ? A(j) , ? A(j) )<label>(6)</label></formula><p>To solve the alignment search problem (i), we introduce a novel alignment search algorithm, Monotonic Alignment (a) An example of alignment, which cover all tokens of text representation h.</p><p>(b) The process of calculating the maximum log-likelihood Q.</p><p>(c) The process of searching for the most probable alignment A * . Search (MAS), described in Section 3.2. Note that Glow-TTS can also be trained as FastSpeech by maximizing L(?, A ext ), where A ext is extracted from an external aligner, but MAS totally removes the necessity of the external aligner from our training procedure.</p><p>In addition to maximizing the log-likelihood, we also train a duration predictor f dec , which predicts how many frames of mel-spectrogram are aligned to each text token. To train the duration predictor, a duration label is needed for each text token. We simply extract this label from the most probable alignment A * , the output of the MAS, though MAS would provide a poor alignment at the beginning of training.</p><p>From the alignment A * , we can count how many speech frames are aligned to each text as in Equation 7 and use the number of frames d j as the duration label for the j-th input token. Given a high-level representation of text h, our duration predictor f dur is learned with the means squared error (MSE) loss as in Equation 8. As with FastSpeech <ref type="bibr" target="#b20">(Ren et al., 2019)</ref>, we train f dur with the duration d j in the logarithmic domain. We also apply stop gradient operator sg[?], which removes the gradient of input in backward pass , to the input of the duration predictor to avoid affecting the maximum likelihood objective. Therefore, our final objective function is Equation <ref type="formula" target="#formula_3">9</ref>.</p><formula xml:id="formula_3">d j = T mel i=1 1 A(i)=j , j = 1, ..., T text (7) L dur (? dur ) = M SE(f dur (sg[h]), d; ? dur ) (8) max ?,? dur L total (?, ? dur ) = L(?, A * ) + L dur (? dur )<label>(9)</label></formula><p>During inference, as shown in <ref type="figure" target="#fig_1">Figure 1b</ref>, Glow-TTS predicts the statistics of prior distributions and the duration of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Monotonic Alignment Search</head><p>Input: latent representation z, the statistics of prior distribution ? , ?, the mel-spectrogram length T mel , the text length T text Output: monotonic alignment A * Initialize Q ?,? ? ??, a cache to store the maximum log-likelihood calculations Compute the first raw</p><formula xml:id="formula_4">Q 1,j ? j k=1 log N (z k ; ? 1 , ? 1 ) for j = 2 to T mel do for i = 2 to min(j, T text ) do Q i,j ? max(Q i?1,j?1 , Q i,j?1 ) + log N (z j ; ? i , ? i ) end for end for Initialize A * (T mel ) ? T text for j = T mel ? 1 to 1 do A * (j) ? arg max i?{A * (j+1)?1,A * (j+1)} Q i,j end for</formula><p>each text token with the text encoder f enc and the duration predictor f dur . We round up these predicted durations to integer and duplicate each distribution by the corresponding duration. This extended distribution is the prior of Glow-TTS during inference. Then, Glow-TTS samples the latent variable z from this prior and synthesizes a mel-spectrogram in parallel by applying inverse transformation of the decoder f ?1 dec to the latent variable z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Monotonic Alignment Search</head><p>As mentioned in Section 3.1, MAS searches for the most probable alignment A * between the latent variable z and the text representation h. As there are numerous alignments to explore, we restrict the properties of them based on our assumption. We suppose that text and speech are monotonically aligned and all tokens of text are covered in the speech. Therefore, we only concern alignments that are monotonic and does not skip any element of h. <ref type="figure" target="#fig_2">Figure 2a</ref> shows one of the possible alignments that we concern.</p><p>We present our alignment search algorithm in Algorithm 1. We first derive recursive solution over partial alignments up to i-th element of h and j-th element of z, then find the most probable alignment A * by using the derivation.</p><p>Let Q i,j be the maximum log-likelihood where h and z are partially given up to i-th and j-th elements, respectively. As h :i should be monotonically covered by our assumption, the alignment z j is aligned to h i and z j?1 is aligned to h i?1 or h i . This means that Q i,j can be calculated based on the possible partial alignments:</p><formula xml:id="formula_5">Q i,j = max A j k=1 log N (z k ; ? A(k) , ? A(k) ) (10) = max(Q i?1,j?1 , Q i,j?1 ) + log N (z j ; ? i , ? i ) (11)</formula><p>This process is illustrated in <ref type="figure" target="#fig_2">Figure 2b</ref>. We iteratively calculate all the values of Q up to Q Ttext,T mel . Note that Q Ttext,T mel is the maximum log-likelihood of all possible monotonic alignments.</p><p>Similarly, the most probable alignment A * can be retrieved by determining which Q value is greater in the recurrence relation, Equation 11. Thus, we backtrack from the end of the alignment, A * (T mel ) = T text , to find all the values of A * <ref type="figure" target="#fig_2">(Figure 2c)</ref>.</p><p>The time complexity of our algorithm is O(T text ? T mel ).</p><p>Even though our method is difficult to parallelize, it runs fast on CPU without need of GPU execution. In our experiments, it spends less than 20ms for each iteration, which amounts to less than 2% of the total training time. Furthermore, we do not need to use MAS during inference, as there is a duration predictor to estimate the duration of each input token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Architecture</head><p>Overall architecture of Glow-TTS is visualized in Appendix A.1. We also list model configurations in Appendix A.2.</p><p>Decoder. The core part of Glow-TTS is the flow-based decoder. In training, we need to efficiently transform a melspectogram into the latent representation for maximum likelihood estimation and our internal alignment search. During inference, it is necessary to invert the prior distribution into the mel-spectrogram distribution efficiently for parallel decoding. Therefore, our decoder is composed of a family of flows that can perform forward and inverse transformation in parallel. Affine coupling layer <ref type="bibr" target="#b1">(Dinh et al., 2014;</ref>, invertible 1x1 convolution, and activation normalization <ref type="bibr" target="#b12">(Kingma &amp; Dhariwal, 2018)</ref> are included in them. Specifically, our decoder is a stack of multiple blocks, each of which consists of activation normalization, invertible 1x1 convolution, and affine coupling layer. We follow affine coupling layer architecture of WaveGlow <ref type="bibr" target="#b19">(Prenger et al., 2019)</ref>, except that we do not use the local conditioning (Van Den <ref type="bibr" target="#b28">Oord et al., 2016)</ref>.</p><p>For computational efficiency, we split 80 channel melspectrogram frames into two halves along the time dimension and group them into one 160 channel feature map, before the decoder operation. We also modify 1x1 convolution to reduce time-consuming calculation of the log determinant of Jacobian of it. Before every 1x1 convolution, we split the feature map into 40 groups along channel dimension and perform 1x1 convolution to them separately.</p><p>Encoder. We follow the encoder structure of Transformer TTS <ref type="bibr" target="#b13">(Li et al., 2019)</ref> with two slight modifications. We remove the positional encoding and add relative position representations <ref type="bibr" target="#b22">(Shaw et al., 2018)</ref> into self-attention modules instead. We also add residual connection to encoder pre-net. To estimate the statistics of the prior distribution, we just append a linear layer at the end of the encoder. Duration predictor is composed of two convolutional layers with ReLU activation, layer normalization and dropout followed by a projection layer. The architecture and configuration of our duration predictor is the same as that of FastSpeech <ref type="bibr" target="#b20">(Ren et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate our proposed methods, we conduct experiments on two different datasets. For single speaker TTS, we train our model on the widely used single female speaker dataset LJSpeech <ref type="bibr" target="#b7">(Ito, 2017)</ref>, which consists of 13100 short audio clips with a total duration of approximately 24 hours. We randomly split the dataset into training set (12500 samples), validation set (100 samples), and test set (500 samples). For multi-speaker TTS, we use the train-clean-100 subset of the LibriTTS corpus <ref type="bibr" target="#b32">(Zen et al., 2019)</ref>, which consists of about 54 hours audio recording of 247 speakers. We first trim the beginning and ending silence of all the audio clips in the data, then filter out all data of which text lengths are larger than 190, and split it into three datasets for training (29181 samples), validation (88 samples), and test (442 samples). Additionally, we collect out-of-distribution text data for robustness test. Similar to <ref type="bibr" target="#b0">(Battenberg et al., 2019)</ref>, we extract 227 utterances from the first two chapters of the book, Harry Potter and the Philosopher's Stone.</p><p>To compare Glow-TTS with an autoregressive TTS model, we set our baseline as Tacotron 2, which is the most widely used, and follows the configuration of <ref type="bibr" target="#b25">(Valle, 2018)</ref>. For all the experiments, we choose phoneme as input text token. We train both Glow-TTS and Tacotron 2 conditioned on the phoneme sequence. We initialize all the parameters except the text embedding layer of our baseline as same as those 3.88 ? 0.08 GLOW-TTS (T = 0.333, MEL + WAVEGLOW) 4.01 ? 0.08 GLOW-TTS (T = 0.500, MEL + WAVEGLOW)</p><p>3.96 ? 0.08 GLOW-TTS (T = 0.667, MEL + WAVEGLOW)</p><p>3.97 ? 0.08 of the pre-trained baseline 3 . We follow the configuration of mel-spectrogram of  for training, and all the generated mel-spectrograms from both models are transformed to raw waveforms by using the pre-trained vocoder, WaveGlow 4 .</p><p>During training, we simply set the variance ? of the learned prior to be a constant 1. Glow-TTS was trained for 240K iterations using the Adam Optimizer with the same learning rate schedule in <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>. This required only 3 days with mixed precision training on 2 NVIDIA V100 GPUs.</p><p>To train Glow-TTS for a multi-speaker setting, we add the speaker embedding and increase all hidden dimensions of text encoder and the decoder. We condition our model on a speaker embedding by applying global conditioning (Van Den <ref type="bibr" target="#b28">Oord et al., 2016)</ref> to all affine coupling layers of the decoder. The rest of the settings are the same as for the single speaker setting. In this case, we trained our Glow-TTS for 480K iterations for convergence.</p><p>3 https://github.com/NVIDIA/tacotron2 4 https://github.com/NVIDIA/waveglow 5. Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Audio Quality</head><p>We measure the mean opinion score (MOS) to compare the quality of all the audios including ground truth (GT), and our synthesized samples via Amazon Mechanical Turk (AMT); the results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Fifty sentences are randomly chosen from our test dataset and used for the evaluation. The quality of the raw audio converted from groundtruth mel-spectrogram (4.19?0.07) is the upper limit of TTS models. We measure the performance of Glow-TTS for various standard deviations (i.e., temperatures) of the prior distribution; the temperature of 0.333 shows the best performance. For any temperature, our Glow-TTS shows comparable performance to the strong autoregressive baseline, Tacotron 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sampling Speed</head><p>We use the test dataset, which has 500 sentences, to measure the sampling speed of TTS models. <ref type="figure" target="#fig_3">Figure 3</ref> demonstrates that the inference time of our parallel TTS model is almost constant at 40ms, regardless of the length, whereas that of Tacotron 2 linearly increases with the length due to the sequential sampling. Based on the inference time for average length of speech, Glow-TTS synthesizes mel-spectrogram 15.7 times faster than Tacotron 2.</p><p>We also measure the total inference time for synthesizing one-minute speech from the text in an end-to-end manner. For this measurement, Glow-TTS synthesizes a melspectrogram that is longer than 5000 frames, and WaveGlow converts the mel-spectrogram into the raw waveform of oneminute speech. The total inference time is only 1.5 seconds to synthesize one-minute speech 5 , and the inference time of Glow-TTS and WaveGlow accounts for 4% and 96% of the total inference time, respectively. That is, the inference time of the Glow-TTS still takes only 55ms to synthesize a very long mel-spectrogram and is negligible compared to that of vocoder, WaveGlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Diversity</head><p>For the sample diversity, most of previous TTS models such as Tacotron 2 or FastSpeech only rely on dropout at inference time. However, as Glow-TTS is a flow-based generative model, it can synthesize a variety of speech given an input text. This is because each latent representation z sampled from an input text is converted to a different mel-spectrogram f ?1 dec (z). We can express this latent representation z ? N (?, T ) with (b) Pitch tracks for the generated speech samples from same sentence with different gaussian noise and same temperature T = 0.667. a noise sampled from the standard Normal distribution, and the mean ? and temperature T of the prior distribution as follows:</p><formula xml:id="formula_6">z = ? + * T<label>(12)</label></formula><p>Thus, we can synthesize diverse speech by varying the and the temperature T. <ref type="figure" target="#fig_4">Figure 4a</ref> demonstrates that by changing the temperature for the same , we can control the pitch of speech while maintaining the trend of the fundamental frequency (F0) contour. In addition, in <ref type="figure" target="#fig_4">Figure 4b</ref>, we show that Glow-TTS can generate various speeches which has a different F0 contour shapes by only altering .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Length Robustness and Controllability</head><p>Length Robustness. To investigate the ability of TTS models to generalize to long texts, we synthesize speech from utterances extracted from the book, Harry Potter and the Philosopher's Stone. The maximum length of the collected data exceeds 800. It is much greater than the maximum length of input characters in LJ dataset, which is less than 200.  We measure the character error rate (CER) of synthesized samples from the out-of-distribution utterances via Google speech recognition API, Google Cloud Speech-To-Text 6 . <ref type="figure" target="#fig_5">Figure 5a</ref> shows a similar result to <ref type="bibr" target="#b0">(Battenberg et al., 2019)</ref>. The CER of Tacotron 2 starts to grow when the length of input characters exceeds about 260. On the other hand, even though our model has not seen such long text during training, it shows robustness to input length.</p><p>In addition to the results of length robustness, we also analysis attention errors on specific sentences. The results are shown in Appendix B.1.</p><p>Length Controllability. As Glow-TTS shares the same duration predictor architecture with FastSpeech, our model is also able to control the speaking rate of the output speech. We multiply a positive scalar value across the predicted duration from the duration predictor. We visualize the result in <ref type="figure" target="#fig_5">Figure 5b</ref>. We multiply different values to the predicted duration as: 1.25, 1.0, 0.75, and 0.5 respectively. As shown in <ref type="figure" target="#fig_5">Figure 5b,</ref>    3.40 ? 0.09 GLOW-TTS (T = 0.500, MEL + WAVEGLOW)</p><p>3.54 ? 0.09 GLOW-TTS (T = 0.667, MEL + WAVEGLOW)</p><p>3.52 ? 0.09 5.5. Multi-speaker TTS Audio Quality. We measure MOS similar to Section 5.1. Fifty sentences are randomly chosen from our test dataset for evaluation. We compare the audio quality with two different settings of GT: ground truth audios and synthesized audios from ground truth mel-spectrograms. The results are presented in <ref type="table" target="#tab_4">Table 2</ref>. The quality of the raw audio converted from ground-truth mel-spectrogram (4.06?0.07) is the upper limit of TTS models. Our model with the best configuration achieves about 3.5 MOS, which demonstrates Glow-TTS can model the diverse speaker styles.</p><p>Speaker Dependent Duration. <ref type="figure" target="#fig_6">Figure 6a</ref> shows the melspectrograms of generated speech from a same sentence with different speaker identities. As the only different input for the duration predictor is speaker embedding, the result demonstrates that our model predicts the duration of each input tokens differently with respect to the speaker identity.</p><p>Voice Conversion. As we do not provide speaker identity into the encoder, the prior distribution is forced to be independent from speaker identity. In other words, Glow-TTS learns to disentangle the latent representation z and the speaker identity. To investigate the degree of disentangle-ment, we transform a ground truth mel-spectrogram into the latent reprersentation with correct speaker identity, then invert the transformation with different speaker identities. The result is presented in <ref type="figure" target="#fig_6">Figure 6b</ref>. It shows converted speeches maintain the similar trend of the fundamental frequency, but with diverse pitch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose Glow-TTS, a new type of parallel TTS model, which provides fast and high quality speech synthesis. Glow-TTS is a flow-based generative model that is directly trained with maximum likelihood estimation and generates a melspectrogram given text in parallel. By introducing our novel alignment search algorithm, Monotonic Alignment Search (MAS), we simplify the whole training procedure of our parallel TTS model so that it requires only 3 days to train.</p><p>In addition to the simple training procedure, we show that Glow-TTS synthesizes mel-spectrograms 15.7x faster than the autoregressive baseline, Tacotron 2, while showing comparable performance. We also demonstrate additional advantages of Glow-TTS, such as controlling the speaking rate or the pitch of synthesized speech, robustness to long utterances, and extensibility to a multi-speaker setting. Thanks to these advantages, we present Glow-TTS as an alternative to existing TTS models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A A.1. Details of Model Architecture</head><p>The detailed encoder architecture is depicted in <ref type="figure">Figure 7</ref>. Some implementation details that we use in the decoder, and the decoder architecture are depicted in <ref type="figure" target="#fig_7">Figure 8</ref>. <ref type="figure">Figure 7</ref>. The encoder architecture of Glow-TTS. The encoder gets text sequence and processes it through encoder pre-net and Transformer encoder. Then, the last projection layer and the duration predictor of the encoder use the hidden representation h to predict statistics of prior distribution and duration, respectively.</p><p>(a) The decoder architecture of Glow-TTS. The decoder gets mel-spectrogram and squeezes it. The, the decoder processes it through a number of flow blocks. Each flow block contains activation normalization layer, affine coupling layer, and invertible 1x1 convolution layer. The decoder reshapes the output to make equal to the input size.</p><p>(b) An illustration of Squeeze and U nSqueeze operations. When squeezing, the channel size doubles up and the number of time steps becomes a half. If the number of time steps is odd, we simply ignore the last element of mel-spectrogram sequence. It corresponds to about 11ms audio, which makes no difference in quality.</p><p>(c) An illustration of our invertible 1x1 convolution. If input channel size is 8 and the number of groups is 2, we share a small 4x4 matrix as a kernel of the invertible 1x1 convolution layer. We split the input into each groups, and perform 1x1 convolution separately. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) An abstract diagram of training procedure.(b) An abstract diagram of inference procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Training and inference procedures of Glow-TTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Illustrations for Monotonic Alignment Search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The inference time comparison for Tacotron 2 and Glow-TTS (yellow: Tacotron2, blue: Glow-TTS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The fundamental frequency (F0) contours of synthesized speech samples from Glow-TTS trained on the LJ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Length Robustness and Controllability</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>The fundamental frequency (F0) contours of synthesized speech samples from Glow-TTS trained on the LibriTTS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>The decoder architecture of Glow-TTS and the implementation details used in the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The Mean Opinion Score (MOS) of single speaker TTS models with 95% confidence intervals.</figDesc><table><row><cell>METHOD</cell><cell>9-SCALE MOS</cell></row><row><cell>GT</cell><cell>4.54 ? 0.06</cell></row><row><cell>GT (MEL + WAVEGLOW)</cell><cell>4.19 ? 0.07</cell></row><row><cell>TACOTRON2 (MEL + WAVEGLOW)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>our model generates mel-spectrograms of lengths. Despite our model have not seen such extremely fast or slow speech during training, the model can control the velocity of the speech without quality degradation.</figDesc><table><row><cell>Frequency [Hz]</cell><cell>100 150 200 250 300 350</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female 1 Female 2 Male 1 Male 2</cell><cell>Frequency [Hz]</cell><cell>100 150 200 250 300 350</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Female 1 Female 2 Male 1 Male 2</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell></cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time [s]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time [s]</cell><cell></cell><cell></cell></row><row><cell cols="7">(a) Pitch tracks for the generated speech samples from same sen-</cell><cell cols="10">(b) Pitch tracks for the voice conversion samples with different</cell></row><row><cell cols="5">tence with different speaker identities.</cell><cell></cell><cell></cell><cell cols="4">speaker identities.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>The</figDesc><table><row><cell cols="2">Mean Opinion Score (MOS) of a multi-speaker TTS</cell></row><row><cell>with 95% confidence intervals.</cell><cell></cell></row><row><cell>METHOD</cell><cell>9-SCALE MOS</cell></row><row><cell>GT</cell><cell>4.29 ? 0.06</cell></row><row><cell>GT (MEL + WAVEGLOW)</cell><cell>4.06 ? 0.07</cell></row><row><cell>GLOW-TTS (T = 0.333, MEL + WAVEGLOW)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We generated a speech sample from our abstract paragraph, which we mention as the one-minute speech. Visit our demo page to listen to it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://cloud.google.com/speech-to-text</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Hyper-parameters</head><p>Hyper-parameters of Glow-TTS are listed in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter</head><p>Glow  We measured attention alignment results using 100 test sentences used in ParaNet . The average length and maximum length of test sentences are 59.65 and 315, respectively. Results are shown in <ref type="table">Table 4</ref>. The results of DeepVoice 3 and ParaNet are taken from  and are not directly comparable due to the difference of grapheme-to-phoneme conversion tools.</p><p>Attention mask ) is a method of computing attention only over a fixed window around target position at inference time. When constraining attention to be monotonic by applying attention mask technique, models make fewer attention errors.</p><p>Tacotron 2, which uses location sensitive attention, also makes little attention errors. Though Glow-TTS perform slightly worse than Tacotron 2 on the test sentences, Glow-TTS does not lose its robustness to extremely long sentences while Tacotron 2 does as we show in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C C.1. Samples</head><p>Our demo page that contains generated samples of Glow-TTS is at https://bit.ly/2LSGPXv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Code</head><p>Our repository that contains model and training code is at https://bit.ly/2LD7O9a.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Locationrelative attention mechanisms for robust long-form speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mariooryad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10288</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Papamakarios</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep voice 2: Multispeaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Non-autoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Emerging convolutions for generative normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11137</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The lj speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transfer learning from speaker verification to multispeaker textto-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4480" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08435</idno>
		<title level="m">Efficient neural audio synthesis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flowavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02155</idno>
		<title level="m">A generative flow for raw audio</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6706" to="6713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Incremental text-to-speech synthesis with prefix-to-prefix framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02750</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flow-tts: A non-autoregressive network for text to speech based on flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7209" to="7213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08459</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Parallel neural text-to-speech. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep voice 3: Scaling text-to-speech with convolutional sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07654</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fastspeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blow: a singlescale hyperconditioned flow for non-parallel raw-audio voice conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Perales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards end-to-end prosody transfer for expressive speech synthesis with tacotron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09047</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tacotron 2</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/tacotron2" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waveglow</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/waveglow" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Flowtron: an autoregressive flow-based generative network for textto-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05957</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10433</idno>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09017</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Libritts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02882</idno>
		<title level="m">A corpus derived from librispeech for text-to-speech</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
