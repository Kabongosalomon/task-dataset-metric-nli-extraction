<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSSNet: Multi-Scale-Stage Network for Single Image Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyeon</forename><surname>Kim</surname></persName>
							<email>kiyeon@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
							<email>s.cho@postech.ac.kr</email>
						</author>
						<title level="a" type="main">MSSNet: Multi-Scale-Stage Network for Single Image Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of traditional single image deblurring methods before deep learning adopt a coarse-to-fine scheme that estimates a sharp image at a coarse scale and progressively refines it at finer scales. While this scheme has also been adopted to several deep learning-based approaches, recently a number of single-scale approaches have been introduced showing superior performance to previous coarse-to-fine approaches both in quality and computation time. In this paper, we revisit the coarse-to-fine scheme, and analyze defects of previous coarse-to-fine approaches that degrade their performance. Based on the analysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep learning-based approach to single image deblurring that adopts our remedies to the defects. Specifically, MSSNet adopts three novel technical components: stage configuration reflecting blur scales, an inter-scale information propagation scheme, and a pixel-shuffle-based multi-scale scheme. Our experiments show that MSSNet achieves the state-of-the-art performance in terms of quality, network size, and computation time.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single image deblurring aims to restore a sharp image from a blurry one caused by camera shake or object motion. As blur severely degrades the image quality and the performance of other tasks such as object detection, deblurring has been extensively studied for decades <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Most of classical single image deblurring approaches before deep learning estimate a blur kernel, which describes how an image has been blurred, and a latent sharp image through alternating optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref>. For computational efficiency and accuracy in estimating a blur kernel and latent sharp image, a coarse-to-fine scheme has been widely adopted by classical approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4]</ref>. The coarse-to-fine scheme estimates a small blur kernel and latent image at a coarse scale and uses them as an initial solution at the next scale. The small sizes of both images and blur at a coarse scale enable computationally efficient estimation. Also, the small blur size at a coarse scale enables more accurate estimation of a blur kernel and latent image. As a result, the coarse-to-fine scheme can quickly provide an accurate initial solution to the next scale, and improve both quality and efficiency of deblurring.</p><p>Thanks to the effectiveness of the coarse-to-fine scheme proven by traditional approaches, it has also been adopted to several deep learning-based single image deblurring approaches, such as DeepDeblur <ref type="bibr" target="#b19">[20]</ref>, SRN <ref type="bibr" target="#b31">[32]</ref>, and PSS-NSC <ref type="bibr" target="#b6">[7]</ref>. These approaches directly restore a latent sharp image from a blurry image without blur kernel estimation. They adopt multi-scale neural network architectures that stack sub-networks for different scales to initially estimate a small-scale latent image and then a large-scale latent image using the small-scale latent image as a guidance. While they do not estimate blur kernels, they share the same motivation with classical approaches: as the image and blur sizes are small at a coarse scale, a deblurred image can be estimated more efficiently and accurately.</p><p>Nonetheless, several deep learning-based single-scale approaches have recently been introduced. Specifically, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> pointed out the expensive computation time of the previous multi-scale approaches and the relatively low contribution of lower scale results on the final deblurring quality, and proposed an alternative single-scale approach named DMPHN. Following Zhang et al., Suin et al. <ref type="bibr" target="#b28">[29]</ref> and Zamir et al. <ref type="bibr" target="#b34">[35]</ref> also proposed hierarchical multi-stage methods based on DMPHN. These approaches show superior performance to previous multi-scale approaches both in quality and computation time, making the traditional coarse-to-fine scheme seem obsolete.</p><p>In this paper, we address the following questions. The motivations of the coarse-to-fine scheme still look valid, but why do the coarse-to-fine approaches perform worse than recent single-scale approaches? What degrades their performance and how can we fix them? To this end, we revisit the coarse-to-fine scheme and analyze the defects of previous coarse-to-fine approaches that degrade their performance but have been overlooked so far.</p><p>Based on the analysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep learning-based deblurring approach that adopts a coarse-to-fine scheme with our remedies to the defects. MSSNet consists of multiple scales and multiple stages at each scale. To remedy the defects of previous coarse-to-fine approaches, MSSNet adopts three novel strategies: stage configuration reflecting blur scales, an inter-scale information propagation scheme, and a pixel-shuffle-based multi-scale scheme. Each strategy is simple and straightforward, resulting in simple architecture for MSSNet. Nonetheless, our experiments show that, once the details are done right, this model can achieve state-of-the-art performance in terms of quality, network size, and computation time. Our simple yet effective architecture can serve as a strong baseline and our strategies can provide a guideline for future deblurring research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional single image deblurring methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref> before deep learning assume blur models that describe how a blurred image is obtained using blur kernels. Unfortunately, they often fail due to their restrictive blur models and the ill-posedness of the problem. To improve deblurring quality, convolutional neural networks (CNNs) have recently been adopted <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26]</ref>. For example, Schuler et al. <ref type="bibr" target="#b25">[26]</ref> and Sun et al. <ref type="bibr" target="#b29">[30]</ref> proposed CNNs that estimate blur kernels and a latent image based on traditional blur models. However, as they still rely on blur models, their performances are limited. To overcome such limitation, deep learning-based methods that directly restore sharp images without blur kernels have been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5]</ref>. These methods can be broadly categorized into single-and multi-scale approaches with respect to their network architectures and training strategies.</p><p>Single-Scale Approaches. Recently, single-scale multi-stage architectures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2]</ref> are gaining popularity. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> proposed DMPHN, the first multi-stage network based on a multipatch approach in single image deblurring. This approach splits an image into multiple disjoint patches and processes each patch independently while gradually merging them in a hierarchical manner through multiple stages. Based on the multi-patch approach, Suin et al. <ref type="bibr" target="#b28">[29]</ref> proposed a dynamic filtering module to remove spatially varying blurs. Zamir et al. <ref type="bibr" target="#b34">[35]</ref> proposed MPRNet, which progressively removes blur by giving supervision at each stage. Chen et al. <ref type="bibr" target="#b1">[2]</ref> introduced half-instance normalization to the multi-stage architecture. Besides multi-stage architectures, Purohit et al. <ref type="bibr" target="#b22">[23]</ref> proposed a deep single-stage architecture based on DenseNet <ref type="bibr" target="#b10">[11]</ref>. However, these singlescale approaches do not use initial solutions estimated from coarse scales, so they are less efficient and accurate as will be shown in Sec. 5.</p><p>Multi-Scale Approaches. Multi-scale approaches are typically based on multi-scale neural network architectures that stack sub-networks in a hierarchical way, and training strategies that train each sub-network to produce deblurred images at different scales. Nah et al. <ref type="bibr" target="#b19">[20]</ref> proposed DeepDeblur, the first end-to-end deep learning-based method that adopts a multi-scale neural network to directly restore a latent sharp image from a blurry image in a coarse-to-fine manner. Each sub-network consists of ResBlocks <ref type="bibr" target="#b7">[8]</ref>, and is trained to produce a deblurred image of its corresponding scale. Tao et al. <ref type="bibr" target="#b31">[32]</ref> presented SRN, which adopts a UNet-based architecture <ref type="bibr" target="#b24">[25]</ref> for each scale. Gao et al. <ref type="bibr" target="#b6">[7]</ref> also proposed a UNet-based multi-scale architecture with a different parameter sharing   <ref type="bibr" target="#b19">[20]</ref>. (b) SRN <ref type="bibr" target="#b31">[32]</ref> and PSS-NSC <ref type="bibr" target="#b6">[7]</ref>. strategy. However, their performance is limited due to the drawbacks of their network architectures as we will discuss in Sec. 3. Besides these approaches, Cho et al. <ref type="bibr" target="#b4">[5]</ref> recently proposed MIMO-UNet, which adopts a single UNet <ref type="bibr" target="#b24">[25]</ref> with multi-scale loss terms. This approach is, however, different from a conventional coarse-to-fine approach as it has a large encoder that processes an input image in a fine-to-coarse manner. Furthermore, as Sec. 5 will show, our MSSNet outperforms MIMO-UNet with much fewer parameters and computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Shortcomings of Previous Coarse-to-Fine Approaches</head><p>This section analyzes architectural defects of previous coarse-to-fine approaches, and discusses our ideas to remedy them. MSSNet with our remedies is presented in Sec. 4. <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates the network architectures of previous coarse-to-fine approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref>. While SRN <ref type="bibr" target="#b31">[32]</ref> adopts additional recurrent connections between consecutive scales to achieve additional performance gain, which is omitted in the figure, the previous coarse-to-fine approaches share essentially the same deblurring process. All the methods first build an image pyramid by downsampling an input blurred image. Then, from the coarsest scale, they estimate a deblurred image from a downsampled blurred image, upsample the deblurred image, and feed it to the sub-network at the next scale. The sub-network at the next scale then estimates a deblurred image from the blurred image at the current scale using the deblurred image from the previous scale as a guidance. All the sub-networks at different scales share the same network architecture. In the following, we analyze the shortcomings of these approaches one by one and present our ideas to address them.</p><p>Network architectures disregarding blur scales. The first shortcoming of the previous approaches is their network architectures that disregard blur scales. Blur spreads a pixel value in a latent image over an area of the blur size. Thus, to restore the pixel value at a certain pixel, it is essential to use receptive fields larger than the blur size to aggregate information spread over the area. Consequently, larger blur sizes require larger receptive fields or deeper neural networks. Likewise, a coarse-to-fine approach needs deeper sub-networks for finer scales. While the previous coarse-to-fine approaches use a deblurred image from the previous scale to deblur the blurred image at the current scale <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref>, large receptive fields are still required for finer scales. In multi-scale approaches, a deblurred image from a lower scale lacks fine details as it is estimated from a downsampled image, and such fine details must be restored from the blurred image at a finer scale. Restoring detail at one pixel inevitably needs to aggregate information spread over an area of the blur size regardless of a result from the previous scale. Thus, it is still more effective to have deeper sub-networks for finer scales as will be shown in our experiments.</p><p>Ineffective information propagation across scales. The previous coarse-to-fine approaches pass the pixel values of a deblurred result from a coarse scale to the next scale <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref>. This causes a significant loss of abundant information encoded in the feature vectors at coarse scales, and eventually degrades the deblurring performance.</p><p>Information loss caused by downsampling. To produce multi-scale input blurred images, the previous approaches build an image pyramid by repeatedly downsampling an input image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref>. Unfortunately, downsampling causes significant information loss. Specifically, a downsampling operation reduces the pixels not only in the input image but also in its deblurred result by 1/4, which severely limits the quality of a guidance to the next scale. To overcome this, in our approach, we present a multi-scale scheme based on the pixel-shuffle <ref type="bibr" target="#b27">[28]</ref> operation that reduces the spatial resolution without information loss.    In this section, we present MSSNet, which is designed based on the analysis in Sec. 3. <ref type="figure" target="#fig_2">Fig. 2</ref> illustrates the architecture of MSSNet. MSSNet is composed of three scales following previous coarse-to-fine approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref>. We denote each scale by S 1 , S 2 , and S 3 from the coarsest to finest scales, respectively. MSSNet takes a single input blurred image B and estimates a deblurred image L in a coarse-to-fine manner. For effective restoration, MSSNet adopts the residual learning scheme, which has been widely adopted in various restoration tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5]</ref>, i.e., MSSNet predicts a residual image R, which is added to the input blurred image B to obtain a deblurred output L = B + R. A detailed architecture of MSSNet can be found in the appendix.</p><p>MSSNet is specifically designed to reflect blur scales, to facilitate effective inter-scale information propagation, and to avoid information loss caused by downsampling. We describe each component of our network in the following.</p><p>Stage Configuration Reflecting Blur Scales. To reflect blur scales, the sub-networks of MSSNet at finer scales are designed to have deeper architectures. Specifically, each scale of MSSNet has one, two and three stages from S 1 to S 3 , respectively, where each stage consists of a single light-weight UNet module <ref type="bibr" target="#b24">[25]</ref>. We denote each UNet module by U j i where i and j are scale and stage indices, respectively. The modules share the same network architecture but have different weights. Each module is trained to produce residual features that can be converted to a residual image and added to a blurred image to produce a deblurred image. More details on the training of MSSNet is explained in Sec. 4.2.  <ref type="figure">Figure 4</ref>: Training of MSSNet. We train every stage to produce a residual image using auxiliary conv and pixel-shuffle layers.</p><p>Inter-Scale Information Propagation. Whereas the existing multi-scale networks deliver an upsampled deblurred image from a coarse scale to the next scale as an initial solution, MSSNet delivers upsampled residual features to facilitate effective information propagation between scales. Specifically, at the end of a coarse scale, residual features are bilinearly upsampled and processed through a 1 ? 1 conv layer. Then, the resulting features are concatenated to the features from a blurred image at the next scale and convolved with 3 ? 3 filters to produce fused features. The fused features are then fed into the UNet modules to produce deblurred residual features at the current scale.</p><p>Pixel-Shuffle-Based Multi-Scale Scheme. To avoid information loss caused by the downsampling operations when producing multi-scale input blurred images, we propose a pixel-shuffle <ref type="bibr" target="#b27">[28]</ref> based multi-scale scheme. Specifically, from the input blurred image B of size W ? H, we generate multi-scale input images as follows. For the finest scale S 3 , we use the input blurred image B. The input image downsampled to a different scale is denoted by</p><formula xml:id="formula_0">B i , where i is a scale index, i.e., B 3 = B, and B 2 is a downsampled version of B of size W/2 ? H/2.</formula><p>For S 2 , we do not use B 2 , but unshuffle B 3 to obtain four images of size W/2 ? H/2. Then, we stack the unshuffled images along the channel direction to generate an input tensor X 2 for S 2 . As B is an RGB image with three color channels, the size of X 2 is W/2 ? H/2 ? 12, so X 2 has the same spatial size as B 2 but still has the same amount of information as B 3 . Then, X 2 is fed into the feature extractor module (E 2 in <ref type="figure" target="#fig_2">Fig. 2</ref>) and processed through the stages at S 2 . Note that, despite X 2 having the same amount of information as B 3 , the computation cost increase for S 2 is relatively small because we use features extracted from X 2 by the feature extractor module. Moreover, thanks to X 2 having richer information than B 2 , the sub-network at S 2 can produce a more accurate result.</p><p>For the coarsest scale S 1 , we first downsample B 3 to obtain B 2 . Then, we apply the same unshuffling process as for S 2 and obtain an input tensor X 1 for S 1 . Another possible choice is to directly unshuffle B 3 and obtain X 1 of W/2 ? H/2 ? 48, but we empirically found that this performs slightly worse. While the pixel-shuffle-based multi-scale architecture can already enhance deblurring quality when trained with conventional loss terms as will be shown in Sec. 5, we propose a pixel-shuffle-based training strategy to minimize information loss and enhance deblurring quality in Sec. 4.2.</p><p>Cross-Stage and Cross-Scale Feature Fusion. MSSNet also adopts the cross-stage feature fusion scheme proposed in <ref type="bibr" target="#b34">[35]</ref>. The cross-stage feature fusion scheme connects network modules in consecutive stages with additional connections (dotted pink lines in <ref type="figure" target="#fig_2">Fig. 2</ref>) to help information flow more effectively between stages. <ref type="figure" target="#fig_4">Fig. 3</ref>(a) describes the cross-stage feature fusion scheme. We refer the readers to <ref type="bibr" target="#b34">[35]</ref> for more details on the cross-stage feature fusion scheme. In addition, we also introduce cross-scale feature fusion (dotted green lines in <ref type="figure" target="#fig_2">Fig. 2</ref>) to facilitate more effective information flow between consecutive scales. The cross-scale feature fusion scheme is described in <ref type="figure" target="#fig_4">Fig. 3</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and Loss Functions</head><p>During training, we guide each stage of MSSNet to produce a deblurred image. We attach auxiliary layers to every stage to produce a deblurred image, except for the last one in S 3 that already has such layers. Specifically, for S 3 , we attach an auxiliary conv layer at the end of U 1 3 and U 2 3 as shown in <ref type="figure">Fig. 4</ref>. The attached conv layers take features from the UNet modules and produce residual images R 1 3 and R 2 3 . Each residual image is then added to B 3 to produce deblurred results L 1 3 and L 2 3 . We also denote the final deblurred result L by L 3 3 .</p><p>For S 1 and S 2 , we use a slightly different training strategy as the sub-networks at S 1 and S 2 take unshuffled images as input. Specifically, at the end of each stage at S 1 and S 2 , we attach a conv layer and a pixel-shuffle layer as shown in <ref type="figure">Fig. 4</ref>. The attached layers at the stages at S 1 and S 2 produce residual images of sizes W/2 ? H/2 and W ? H, respectively. We denote the deblurred results from the auxiliary layers by L j i where i and j are scale and stage indices, respectively. We train MSSNet using two types of loss functions: a content loss L cont and a frequency reconstruction loss L f req . The content loss L cont is defined as:</p><formula xml:id="formula_1">L cont = 1 N 1 L 1 1 ? L gt? 1 + 2 j=1 1 N 2 L j 2 ? L gt 1 + 3 j=1 1 N 3 L j 3 ? L gt 1 ,<label>(1)</label></formula><p>where L gt is the ground-truth blurred image, and L gt? is a downsampled version of L gt . N 1 , N 2 and N 3 are normalization factors, which we set</p><formula xml:id="formula_2">N 1 = W/2 ? H/2 ? 3 and N 2 = N 3 = W ? H ? 3.</formula><p>The frequency reconstruction loss was proposed in <ref type="bibr" target="#b4">[5]</ref> to restore high-frequency details from blurred image by minimizing the difference between blurred image and ground-truth in the frequency domain.</p><p>The frequency reconstruction loss is defined as:</p><formula xml:id="formula_3">L f req = 1 N 1 F(L 1 1 ) ? F(L gt? ) 1 + 2 j=1 1 N 2 F(L j 2 ) ? F(L gt ) 1 (2) + 3 j=1 1 N 3 F(L j 3 ) ? F(L gt ) 1 ,</formula><p>where F is Fourier transform. Finally, our final loss is L total = L cont + ?L f req where ? = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>For evaluation, we trained MSSNet on the GoPro dataset <ref type="bibr" target="#b19">[20]</ref>. For training, we randomly cropped 256 ? 256 patches from blurry and sharp images. Horizontal and vertical flips were randomly applied to cropped patches. We trained our model for 3,000 epochs (396,000 iterations) with batch size 16.</p><p>We used the Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with cosine annealing <ref type="bibr" target="#b18">[19]</ref>. We set the initial learning rate to 2?10 -4 and gradually decreased it to 1?10 -6 . To evaluate the performance of MSSNet on real-world blurred images, we also use the RealBlur dataset <ref type="bibr" target="#b23">[24]</ref>. For evaluation on the RealBlur test set, We trained MSSNet using the GoPro <ref type="bibr" target="#b19">[20]</ref>, BSD-B <ref type="bibr" target="#b23">[24]</ref>, and RealBlur training sets following the RealBlur benchmark <ref type="bibr" target="#b23">[24]</ref>. We trained the model for 100 epochs (397,400 iterations). The other training details are the same as above. The computation times of all models are measured on a PC with an NVIDA GeForce RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Previous Methods</head><p>We compare MSSNet with previous state-of-the-art methods. <ref type="table" target="#tab_0">Table 1</ref> shows a quantitative comparison on the GoPro test set <ref type="bibr" target="#b19">[20]</ref>. All the methods in the comparison were trained with the GoPro training set. Among the compared methods, DeepDeblur <ref type="bibr" target="#b19">[20]</ref>, SRN <ref type="bibr" target="#b31">[32]</ref> and PSS-NSC <ref type="bibr" target="#b6">[7]</ref> are coarse-tofine approaches. MIMO-UNet <ref type="bibr" target="#b4">[5]</ref> is trained using multi-scale loss terms, but not a conventional coarse-to-fine approach as it is based on a single UNet architecture with an encoder that processes an image in a fine-to-coarse manner. MIMO-UNet+ is a variant of MIMO-UNet with more parameters, and MIMO-UNet++ is MIMO-UNet+ with self-ensemble. All the other methods are single-scale approaches.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, recent single-scale approaches tend to perform better than coarse-to-fine approaches except for MIMO-UNet <ref type="bibr" target="#b4">[5]</ref> and its variants. On the other hand, MSSNet clearly outperforms all the other methods in terms of PSNR and SSIM thanks to our remedies. Specifically, MSSNet performs better than MIMO-UNet+ by more than 0.5dB with fewer parameters and fewer computations. Compared to MIMO-UNet++, a self-ensemble version of MIMO-UNet+, MSSNet still outperforms by 0.33dB with a 4? fewer computations. Also, compared to HINet <ref type="bibr" target="#b1">[2]</ref>, MSSNet achieves 0.11dB higher PSNR with 5.7? fewer parameters and fewer computations while slightly slower.  <ref type="bibr" target="#b19">[20]</ref>. The models in blue are coarse-to-fine approaches, while the models in red are single-scale approaches. MIMO-UNet and its variants are based on a single UNet with multi-scale losses <ref type="bibr" target="#b4">[5]</ref>. The computation times of all the methods are measured in the same environment described in Sec. 5.1. The numbers of parameters, MACs, and computation times of RADN <ref type="bibr" target="#b22">[23]</ref> and SAPHN <ref type="bibr" target="#b28">[29]</ref>    <ref type="bibr" target="#b19">[20]</ref>.</p><p>We also include two variants of MSSNet: MSSNet-small and MSSNet-large, in this evaluation. Their detailed architectures are provided in the appendix. Compared to MIMO-UNet and SRN, which have larger model sizes, MSSNet-small achieves a higher PSNR and SSIM with smaller computation time. While SAPHN <ref type="bibr" target="#b28">[29]</ref> achieves similar PSNR and SSIM values to those of MSSNet-small, ours performs much faster according to the computation time reported in their paper. Specifically, the reported computation time of SAPHN measured on a Titan Xp GPU is 0.77 sec., while that of MSSNet-small on the same GPU is 0.19 sec. MSSNet-large has about twice the parameters of MSSNet, which is still 3? fewer than HINet, and its computation time is more than twice shorter than those of MIMO-UNet++ and MPRNet. Nevertheless, it achieves 33.39 dB in PSNR, significantly exceeding all the other methods by a large margin. <ref type="figure" target="#fig_7">Fig. 5</ref> shows a qualitative comparison on the GoPro dataset <ref type="bibr" target="#b19">[20]</ref>. As shown in the figure, our results show clearly restored sharp details while those of the others have remaining blur.</p><p>We also study the generalization ability and performance of MSSNet on real-world blurred images. <ref type="table">Table 2</ref> shows a quantitative evaluation on the RealBlur dataset <ref type="bibr" target="#b23">[24]</ref>, which consists of real-world blurred images. The methods in the upper section in the table are trained on the GoPro dataset <ref type="bibr" target="#b19">[20]</ref>, <ref type="table">Table 2</ref>: Quantitative evaluation on RealBlur <ref type="bibr" target="#b23">[24]</ref>. The models in the upper part of the table are trained on the GoPro dataset <ref type="bibr" target="#b19">[20]</ref> and tested on the RealBlur test sets. The models in the lower part are trained and tested on each of the RealBlur-R and RealBlur-J datasets. MIMO-UNet++ <ref type="bibr" target="#b4">[5]</ref> provides only a model trained on the RealBlur-J dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>RealBlur-R RealBlur-J PSNR SSIM PSNR SSIM Hu et al. <ref type="bibr" target="#b9">[10]</ref> 33.67 0.916 26.41 0.803 DeepDeblur <ref type="bibr" target="#b19">[20]</ref> 32.51 0.841 27.87 0.827 DeblurGAN <ref type="bibr" target="#b13">[14]</ref> 33  <ref type="figure">Figure 6</ref>: Qualitative evaluation on the ReaBlur-J dataset <ref type="bibr" target="#b23">[24]</ref>.</p><p>while those in the lower section are trained on the RealBlur-R and RealBlur-J datasets. Among the methods trained on the GoPro datasets, MSSNet achieves the highest SSIM for the RealBlur-R test set, and the highest PSNR and SSIM for the RealBlur-J test set. Also, among the methods trained on the RealBlur datasets, MSSNet achieves the highest PSNR and SSIM. <ref type="figure">Fig. 6</ref> shows a qualitative comparison on the RealBlur-J dataset <ref type="bibr" target="#b23">[24]</ref>. In all the examples, the results of the other methods show either remaining blur and incorrectly restored details. On the other hand, our results show better restored details. Additional qualitative examples are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study and Analysis</head><p>We validate the effectiveness of the coarse-to-fine approach, and then analyze the effect of each technical component in our model. For analysis, we test several variants of MSSNet. All the models in the analysis are trained and tested on the GoPro training and test sets <ref type="bibr" target="#b19">[20]</ref>, respectively. For ease of analysis, all the variants of MSSNet in the ablation studies use neither the pixel-shuffling scheme nor the cross-stage and cross-scale feature fusion scheme if not otherwise noted.</p><p>Coarse-to-Fine vs Single-Scale. As discussed in Sec. 1, the coarse-to-fine approach can quickly estimate a high-quality initial solution using coarse scales. Specifically, compared to performing a single stage of deblurring at the original scale, performing multiple stages at a coarse scale can be computationally more efficient. Moreover, thanks to the small blur size at a coarse scale, it can estimate a more accurate result, which serves as an initial solution for a finer scale, which leads to a final deblurring result of higher quality.  On the other hand, MSSNet-Multi-Small has fewer parameters for each stage at S 1 and S 2 so that its total number of parameters is similar to that of MSSNet-Single. Its architecture details are in the appendix. The multi-scale models use our pixel-shuffle-based approach, but none of the models use the cross-stage and cross-scale feature fusion schemes. While the multi-scale models have six stages in total, three of them are at coarser scales. As a result, both multi-scale model require smaller amounts of computation than MSSNet-Single as shown in the table.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, 'Initial' and 'Final' indicates the initial and final results of the single-scale and multi-scale models. An initial solution of the single-scale model indicates a deblurring result of the first stage obtained using an auxiliary conv layer, while an initial solution of the multi-scale models indicates a deblurring result of the last stage at S 2 obtained using auxiliary conv and pixel-shuffle layers. We compare these as they serve as initial solutions for the last three stages. As shown in the table, despite its smaller computation cost, MSSNet-Multi produces higher-quality initial and final deblurring results. Also, although MSSNet-Multi-Small has a similar number of parameters and a much smaller computation cost, it still achieves a similar PSNR for the final result to that of MSSNet-Single. This proves the advantage of the coarse-to-fine approach against the single-scale approach.</p><p>Stage Configuration Reflecting Blur Scales. Our first remedy that we adopt into our MSSNet is the stage configuration reflecting blur scales. To verify its effect as a common rule, we conduct two ablation studies using DeepDeblur <ref type="bibr" target="#b19">[20]</ref> and MSSNet. <ref type="table" target="#tab_4">Table 4</ref> compares three variants of DeepDeblur <ref type="bibr" target="#b19">[20]</ref>. D444 and D444L have four residual blocks at each scale, while D246 adopts our stage configuration scheme and has two, four and six residual blocks at S 1 , S 2 and S 3 , respectively. To match the computation cost of D246, we also prepare D444L, which has more channels at each residual block. The table shows that D246 outperforms both of the others in terms of PSNR and SSIM, especially, despite its fewer parameters and a smaller computation cost than those of D444L.</p><p>In the second experiment, we compare two variants of MSSNet in <ref type="table" target="#tab_5">Table 5</ref>. The variants have different numbers of stages at different scales as informed in the table, but share the same network architecture for the UNet modules. The deblurring performance is not only affected by the number of stages, but also by the computation amount and the number of parameters. To isolate the impact of the stage configuration on the deblurring performance from other factors, each of the tested models in this experiment shares the network weights across different stages. In <ref type="table" target="#tab_5">Table 5</ref>, M123 has the same stage configuration as MSSNet. M552 has fewer stages at S 3 but more stages at coarse scales so it requires the same amount of computation. The table shows that M123 clearly outperforms M552, validating our argument on the stage configuration. Additional experiments with different settings, e.g., models without parameter sharing, are provided in the appendix.  Inter-Scale Feature Propagation. In the next ablation study, we verify the effect of our inter-scale feature propagation scheme. In this study, we also investigate how to fuse the solution from a coarse scale with the input to the finer scale. To this end, we compare three variants of MSSNet: MSS(Image,Concat), MSS(Feature,Skip) and MSS(Feature,Concat). MSS(Image,Concat) has auxiliary conv layers at the end of S 1 and S 2 to convert features to residual images. The residual images are added to the input blurred images of the corresponding sizes to produce deblurred results. The deblurred results are then upsampled and concatenated to the blurred images at the next scales. This model corresponds to the previous coarse-to-fine approaches that transfer pixel values from coarse to fine scales. MSS(Feature,Skip) transfers features from coarse to fine scales as done in MSSNet. However, features from coarse scales are not concatenated but added to the features of the blurred images at the next scales. As the sub-networks estimate residual features, adding them to the features of blurred images will produce initial deblurred features at finer scales. MSS(Feature,Concat) uses our inter-scale feature propagation scheme that concatenates features from coarse scales to the features of the blurred images at the next scales. <ref type="table" target="#tab_6">Table 6</ref> compares the performance of the variants. The results confirm that using features instead of pixel values clearly improves the deblurring quality as features provide richer information. The table also shows that MSS(Feature,Concat) performs slightly better than MSS(Feature,Skip), although it requires slightly more parameters, validating our approach.</p><p>Pixel-Shuffle-Based Multi-Scale Scheme. We then verify the effect of our pixel-shuffle-based multi-scale scheme. As discussed in Sec. 4, our pixel-shuffle-based multi-scale scheme consists of pixel-unshuffle layers that generate input tensors, and auxiliary pixel-shuffle layers used only in the training phase. To verify the effect of each component, we compare the performance of three variants of MSSNet: 1) without both pixel-unshuffle and shuffle layers, 2) with only the pixel-unshuffle layers, and 3) with both layers in <ref type="table" target="#tab_7">Table 7</ref>. The first model takes downsampled images as input as done in previous coarse-to-fine approaches, and its sub-networks at S 1 and S 2 are trained to produce intermediate results of the corresponding sizes. The second model takes tensors generated by pixel-unshuffling layers as input, but its sub-networks in S 1 and S 2 are trained in the same manner as the first model. The third model corresponds to our approach.</p><p>As <ref type="table" target="#tab_7">Table 7</ref> shows, introducing the pixel-unshuffling and shuffling layers introduces a negligible increase in the number of parameters. On the other hand, the pixel-unshuffling layers clearly improve the deblurring quality as they provide richer information than downsampling. Also, the auxiliary pixel-shuffling layers further improve the deblurring quality as they enable higher-quality supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we analyzed the defects of previous deep learning-based coarse-to-fine approaches to single image deblurring. Based on our analysis, we proposed MSSNet, a novel coarse-to-fine approach with our remedies to the defects. MSSNet adopts stage configuration reflecting blur scales, inter-scale feature propagation, and pixel-shuffle-based multi-scale network architecture. The experiment results prove the effectiveness of our novel technical components and show that our method is superior compared to the previous state-of-the art methods in regard to the accuracy, computation time, and network size. Limitations and Future Work. While MSSNet achieves the state-of-the-art performance, it still fails on many real-world blurred images especially with large blur as other methods. Extending MSSNet for handling large blur can be an interesting future work. Also, improving the computational efficiency and reducing the model size to deploy deblurring on mobile devices can be another interesting direction. As a future work, we also plan to examine the performance of MSSNet on other types of image degradation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Network Architectures</head><p>The detailed architectures of MSSNet, MSSNet-small and MSSNet-large are shown in <ref type="table" target="#tab_3">Tables A2, A3</ref> and A4. <ref type="table" target="#tab_0">Table A1</ref> defines symbols used in the tables. In this section, CSFF represents both crossstage <ref type="bibr" target="#b34">[35]</ref> and cross-scale feature fusion. The architecture of UNet is based on that of MPRNet <ref type="bibr" target="#b34">[35]</ref> without channel attention.</p><p>Our Models. For MSSNet, the channel sizes x, y and z of UNet are set to 54, 96 and 138, respectively. For MSSNet-small, they are set to 20, 60 and 100, respectively. For MSSNet-large, they are set to 80, 130 and 180, respectively.</p><p>Models in the ablation study <ref type="table" target="#tab_3">(Table 3</ref>. in the paper). For the models in the ablation study, CSFF is not used (i.e., set to false) in all of the scales in <ref type="table" target="#tab_4">Table A4</ref>. For MSSNet-Multi, the channel sizes x, y and z are set to 20, 60 and 100, respectively in all of the UNet modules. In MSSNet-Multi-Small, the channel sizes of UNet are different according to the scales. Specifically, x, y and z are set to 20, 36 and 52, respectively, in U 1 1 , U 1 2 and U 2 2 , and set to 20, 60 and 100, respectively, in U 1 3 , U 2 3 and U 3 3 .   </p><formula xml:id="formula_4">S 1 Input bi-down B 3 -3 --0.5 - B 2 unshuffle B 2 -12 --0.5 - X 1 E 1 conv X 1 3 x 1 1 - - conv 1 Res conv 1 3 x 1 1 - - f eat 1 UNet U 1 1 f eat 1 -----false U 1 1 E, U 1 1 D S 2 S 2 Input unshuffle B 3 -12 --0.5 - X 2 E 2 conv X 2 3 x 1 1 - - conv 2 Res conv 2 3 x 1 1 - - f eat 2 Fusion bi-up U 1 1 D[1] -x --2 - S 1 sol conv S 1 sol 1 x 1 0 - - S 1 sol concat f eat 2 , S 1 sol -2x --- - cat 12 conv cat 12 3 x 1 1 - - f usion 12 S 1 Auxiliary conv U 1 1 D[1] 3 12 1 1 - - rfeat shuffle rfeat -3 --2 - R 1 1 sum R 1 1 , B 2 -3 --- - L 1 1 = R 1 1 + B 2 S 2 conv U 1 2 D[1] 3 12 1 1 - - rfeat shuffle rfeat -3 --2 - R 1 2 sum R 1 2 , B 3 -3 --- - L 1 2 = R 1 2 + B 3 conv U 2 2 D[1] 3 12 1 1 - - rfeat shuffle rfeat -3 --2 - R 2 2 sum R 2 2 , B 3 -3 --- - L 2 2 = R 2 2 + B 3 S 3 conv U 1 3 D[1] 3 3 1 1 - - R 1 3 sum R 1 3 , B 3 -3 --- - L 1 3 = R 1 3 + B 3 conv U 2 3 D[1] 3 3 1 1 - - R 2 3 sum R 2 3 , B 3 -3 --- - L 2 3 = R 2 3 + B 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MSSNet with Parameter Sharing</head><p>In this section, we verify the effectiveness of our architecture with parameter sharing. To this end, we build a variant of MSSNet with parameter sharing, which we refer to as MSSNet-WS. MSSNet-WS has the same network architecture as MSSNet, but shares its parameters across all the stages and scales. We compare its performance with previous state-of-the-art methods using parameter sharing in <ref type="table" target="#tab_5">Table A5</ref>. SRN <ref type="bibr" target="#b31">[32]</ref> and PSS-NSC <ref type="bibr" target="#b6">[7]</ref> are multi-scale methods, while MT-RNN <ref type="bibr" target="#b21">[22]</ref> is a single-scale multi-stage method. All of them use parameter sharing. As the table shows, MSSNet-WS clearly outperforms all the others in terms of PSNR and SSIM with a comparable number of parameters, much fewer computation amounts, and faster computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Experiment on the Stage Configuration</head><p>We provide additional experiments on the stage configuration reflecting blur scales. In the first experiment, we compare different variants of MSSNet fixing the number of stages and scales and the number of parameters. In <ref type="table" target="#tab_6">Table A6</ref>, M321 has three, two and one stages for S 1 , S 2 and S 3 , respectively, while M222 has two stages for all the scales. M123 has the same stage configuration as MSSNet, i.e., it has one, two and three stages for S 1 , S 2 and S 3 , respectively. M222 represents the previous coarse-to-fine approaches using the same sub-networks for all scales, while M123 represents our approach. The three models use neither the pixel-shuffling scheme nor the cross-stage and cross-scale feature fusion scheme. The models do not use parameter sharing either. The table shows that M123 performs the best, followed by M222. This result again proves the validity of our stage configuration scheme reflecting blur scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Qualitative Comparisons</head><p>In this section, we present additional qualitative comparisons on the GoPro test set 1 <ref type="bibr" target="#b19">[20]</ref>, and the RealBlur-J and -R test sets 2 <ref type="bibr" target="#b23">[24]</ref>. Figs. A1, A2, A3 and A4 show comparisons on the GoPro test set with SRN <ref type="bibr" target="#b31">[32]</ref>, PSS-NSC <ref type="bibr" target="#b6">[7]</ref>, SDNet4 <ref type="bibr" target="#b35">[36]</ref>, MTRNN <ref type="bibr" target="#b21">[22]</ref>, MPRNet <ref type="bibr" target="#b34">[35]</ref>, MIMO-UNet++ <ref type="bibr" target="#b4">[5]</ref> and HINet <ref type="bibr" target="#b1">[2]</ref>. Figs. A5, A6, A7 and A8 show comparisons on the RealBlur-J test set. In these comparisons, we compare our model trained with the RealBlur-J training set with SRN <ref type="bibr" target="#b31">[32]</ref>, MPRNet <ref type="bibr" target="#b34">[35]</ref> and MIMO-UNet++ <ref type="bibr" target="#b4">[5]</ref>, which provide models pre-trained with the RealBlur-J training set. Figs. A9, A10, A11 and A12 show comparisons on the RealBlur-R test set. In these comparisons, we also compare our model trained with the RealBlur-R training set with SRN <ref type="bibr" target="#b31">[32]</ref>, DeblurGAN-v2 <ref type="bibr" target="#b14">[15]</ref> and MPRNet <ref type="bibr" target="#b34">[35]</ref>, which provide models pre-trained with the RealBlur-R training set. For visualization, we applied gamma correction to the resulting images in the figures where we set gamma to 2.2. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Previous coarse-to-fine architectures. (a) DeepDeblur</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture of MSSNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>:</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Cross-stage and cross-scale feature fusion schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative evaluation on the GoPro dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A1 :Figure A2 :Figure A3 :Figure A4 :Figure A5 :Figure A6 :Figure A7 :Figure A8 :Figure A9 :Figure A10 :Figure A11 :Figure A12 :</head><label>A1A2A3A4A5A6A7A8A9A10A11A12</label><figDesc>Additional qualitative comparison on the GoPro dataset (1)<ref type="bibr" target="#b19">[20]</ref>. Additional qualitative comparison on the GoPro dataset (2)<ref type="bibr" target="#b19">[20]</ref>. Additional qualitative comparison on the GoPro dataset (3)<ref type="bibr" target="#b19">[20]</ref>. Additional qualitative comparison on the GoPro dataset (4)<ref type="bibr" target="#b19">[20]</ref>. Additional qualitative comparison on the RealBlur-J dataset (1)<ref type="bibr" target="#b23">[24]</ref>. Additional qualitative comparison on the RealBlur-J dataset (2)<ref type="bibr" target="#b23">[24]</ref>. Additional qualitative comparison on the RealBlur-J dataset (3)<ref type="bibr" target="#b23">[24]</ref>. Additional qualitative comparison on the RealBlur-J dataset (4)<ref type="bibr" target="#b23">[24]</ref>. Additional qualitative comparison on the RealBlur-R dataset (1)<ref type="bibr" target="#b23">[24]</ref>. Additional qualitative comparison on the RealBlur-R dataset (2)<ref type="bibr" target="#b23">[24]</ref>. Additional qualitative comparison on the RealBlur-R dataset (3)<ref type="bibr" target="#b23">[24]</ref>. Additional qualitative comparison on the RealBlur-R dataset (4)<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation on the GoPro test dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>are unavailable as their source codes are not publicly released yet.</figDesc><table><row><cell>Models</cell><cell cols="6">PSNR (dB) SSIM Param (M) MACs (G) Time (s)</cell></row><row><cell>DeepDeblur [20]</cell><cell>29.08</cell><cell cols="2">0.914</cell><cell>11.72</cell><cell>4729</cell><cell>1.290</cell></row><row><cell>DMPHN [36]</cell><cell>30.25</cell><cell cols="2">0.935</cell><cell>7.23</cell><cell>1100</cell><cell>0.137</cell></row><row><cell>SRN [32]</cell><cell>30.26</cell><cell cols="2">0.934</cell><cell>8.06</cell><cell>20134</cell><cell>0.736</cell></row><row><cell>PSS-NSC [7]</cell><cell>30.92</cell><cell cols="2">0.942</cell><cell>2.84</cell><cell>3255</cell><cell>0.316</cell></row><row><cell>MT-RNN [22]</cell><cell>31.15</cell><cell cols="2">0.945</cell><cell>2.6</cell><cell>2315</cell><cell>0.323</cell></row><row><cell>SDNet4 [36]</cell><cell>31.20</cell><cell cols="2">0.945</cell><cell>21.7</cell><cell>3301</cell><cell>0.414</cell></row><row><cell>MIMO-UNet [5]</cell><cell>31.73</cell><cell cols="2">0.951</cell><cell>6.8</cell><cell>944</cell><cell>0.133</cell></row><row><cell>RADN [23]</cell><cell>31.76</cell><cell cols="2">0.953</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>SAPHN [29]</cell><cell>32.02</cell><cell cols="2">0.953</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>MSSNet-small (Ours)</cell><cell>32.02</cell><cell cols="2">0.953</cell><cell>6.75</cell><cell>634</cell><cell>0.104</cell></row><row><cell>MIMO-UNet+ [5]</cell><cell>32.45</cell><cell cols="2">0.957</cell><cell>16.1</cell><cell>2171</cell><cell>0.290</cell></row><row><cell>MPRNet [35]</cell><cell>32.66</cell><cell cols="2">0.959</cell><cell>20.1</cell><cell>10927</cell><cell>1.023</cell></row><row><cell>MIMO-UNet++ [5]</cell><cell>32.68</cell><cell cols="2">0.959</cell><cell>16.1</cell><cell>8683</cell><cell>1.169</cell></row><row><cell>HINet [2]</cell><cell>32.90</cell><cell cols="2">0.960</cell><cell>88.67</cell><cell>2401</cell><cell>0.247</cell></row><row><cell>MSSNet (Ours)</cell><cell>33.01</cell><cell cols="2">0.961</cell><cell>15.59</cell><cell>2159</cell><cell>0.255</cell></row><row><cell>MSSNet-large (Ours)</cell><cell>33.39</cell><cell cols="2">0.964</cell><cell>28.15</cell><cell>4235</cell><cell>0.457</cell></row><row><cell></cell><cell cols="2">Blurred</cell><cell cols="2">Ground-truth</cell><cell>SRN</cell><cell>PSS-NSC</cell><cell>SDNet4</cell></row><row><cell></cell><cell cols="2">MTRNN</cell><cell></cell><cell cols="2">MPRNet MIMO-UNet++</cell><cell>HINet</cell><cell>MSSNet</cell></row><row><cell></cell><cell cols="2">Blurred</cell><cell cols="2">Ground-truth</cell><cell>SRN</cell><cell>PSS-NSC</cell><cell>SDNet4</cell></row><row><cell></cell><cell cols="2">MTRNN</cell><cell></cell><cell>MPRNet</cell><cell>MIMO-UNet++</cell><cell>HINet</cell><cell>MSSNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison among a single-scale architecture with four stages and our multiscale architectures. MSSNet-Single is a single-scale architecture, while MSSNet-Multi and MSSNet-Multi-Small are multi-scale architectures. 'Initial' and 'Final' are the initial and final results of each architecture, respectively.</figDesc><table><row><cell></cell><cell cols="3">MSSNet-Single MSSNet-Multi-Small MSSNet-Multi</cell></row><row><cell>PSNR (Initial / Final)</cell><cell>29.11 / 31.59</cell><cell>29.51 / 31.58</cell><cell>30.09 / 31.75</cell></row><row><cell cols="2">Params (M) / MACs (G) 4.39 / 660.69</cell><cell>4.38 / 574.82</cell><cell>6.61 / 621.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the stage configuration using variants of DeepDeblur<ref type="bibr" target="#b19">[20]</ref>.</figDesc><table><row><cell># ResBlocks</cell><cell></cell><cell></cell></row><row><cell cols="3">Models S 1 S 2 S 3 PSNR SSIM Params (M) MACs (G)</cell></row><row><cell>D444 4 4 4 27.07 0.8269</cell><cell>2.5</cell><cell>1009.3</cell></row><row><cell>D444L 4 4 4 27.26 0.8315</cell><cell>3.42</cell><cell>1382.3</cell></row><row><cell>D246 2 4 6 27.38 0.8324</cell><cell>2.5</cell><cell>1363.5</cell></row></table><note>To verify this, in Table 3, we compare three variants of MSSNet. MSSNet-Single is a single-scale model with four stages at the original scale. MSSNet-Multi and MSSNet-Multi-Small are multi-scale models with the same number of scales and stages as MSSNet. MSSNet-Single and MSSNet-Multi has the same number of parameters for each stage.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the stage configuration using variants of MSSNet.</figDesc><table><row><cell># Stages</cell><cell></cell><cell></cell></row><row><cell cols="3">Models S 1 S 2 S 3 PSNR SSIM Params (M) MACs (G)</cell></row><row><cell>M123 1 2 3 29.58 0.925</cell><cell>1.18</cell><cell>521.33</cell></row><row><cell>M552 5 5 2 29.27 0.920</cell><cell>1.18</cell><cell>521.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the scale information propagation.</figDesc><table><row><cell>Model</cell><cell cols="3">PSNR SSIM Params (M) MACs (G)</cell></row><row><cell cols="2">MSS(Image,Concat) 31.42 0.947</cell><cell>6.59</cell><cell>613.1</cell></row><row><cell>MSS(Feature,Skip)</cell><cell>31.52 0.948</cell><cell>6.59</cell><cell>621.8</cell></row><row><cell cols="2">MSS(Feature,Concat) 31.54 0.949</cell><cell>6.61</cell><cell>621.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on the pixel-shuffle-based multi-scale approach. PUS: pixel-unshuffle. PS: pixel-shuffle.</figDesc><table><row><cell cols="3">PUS PS PSNR SSIM Params (M) MACs (G)</cell></row><row><cell>31.54 0.949</cell><cell>6.61</cell><cell>621.1</cell></row><row><cell>31.67 0.950</cell><cell>6.61</cell><cell>621.6</cell></row><row><cell>31.75 0.951</cell><cell>6.61</cell><cell>621.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A1 :</head><label>A1</label><figDesc>Descriptions of the symbols in Tables A2, A3 and A4.</figDesc><table><row><cell>Symbols</cell><cell>Description</cell></row><row><cell>type</cell><cell>Layer type</cell></row><row><cell>input</cell><cell>Input feature name</cell></row><row><cell>k</cell><cell>Filter size of a conv layer (k ? k)</cell></row><row><cell>c</cell><cell>Output channel size of a layer</cell></row><row><cell>s</cell><cell>Stride</cell></row><row><cell>p</cell><cell>Padding size</cell></row><row><cell>r</cell><cell>Upsampling or downsampling ratio</cell></row><row><cell>CSFF</cell><cell>Whether to use cross-stage or scale feature fusion, or not</cell></row><row><cell>output</cell><cell>Output feature name</cell></row><row><cell>conv</cell><cell>Convolution layer (bias = False)</cell></row><row><cell>PRelu</cell><cell>Parametric ReLU layer</cell></row><row><cell>sum</cell><cell>Element-wise summation</cell></row><row><cell>bi-down</cell><cell>Bilinear downsampling</cell></row><row><cell>bi-up</cell><cell>Bilinear upsampling</cell></row><row><cell>concat</cell><cell>Concatenation</cell></row><row><cell>unshuffle</cell><cell>Pixel-unshuffle</cell></row><row><cell>shuffle</cell><cell>Pixel-shuffle</cell></row><row><cell>U j i</cell><cell>UNet module (j-th stage, i-th scale)</cell></row><row><cell>Res</cell><cell>Residual block</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A2 :</head><label>A2</label><figDesc>Detailed architecture of a residual block (Res). The channel size n is a variable that is set differently at different locations in the MSSNet. Refer toTables A3 and A4. sum feat, conv 2 -n --out = feat + conv 2</figDesc><table><row><cell>type</cell><cell>input</cell><cell>k c s p</cell><cell>output</cell></row><row><cell>conv</cell><cell>feat</cell><cell>3 n 1 1</cell><cell>conv 1</cell></row><row><cell>PRelu</cell><cell>conv 1</cell><cell>-n --</cell><cell>act</cell></row><row><cell>conv</cell><cell>act</cell><cell>3 n 1 1</cell><cell>conv 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A3 :</head><label>A3</label><figDesc>Detailed architecture of UNet in each stage at each scale (U 1 1 UNet module takes an input feature tensor denoted by f eat. The layers in the CSFF sub-block are used only when CSFF inTable A4is true. In such a case, a UNet module utilizes additional features from the previous stage or scale, [p el 1 , p el 2 , p el 3 ] and [p dl 1 , p dl 2 , p dl 3 ] where p el i and p dl i correspond to el i and dl i in the previous stage or scale. For U 1 1 , U 1 2 and U 1 3 , f eat is set to f eat 1 , f usion 12 and f usion<ref type="bibr" target="#b22">23</ref> inTable A4, respectively. Except for them, f eat is set to p dl 1 . , pc el 1 , pc dl 1 -x ---el 1 = el 1 + pc el 1 + pc dl 1 , pc el 2 , pc dl 2 -y ---el 2 = el 2 + pc el 2 + pc dl 2 , pc el 3 , pc dl 3 -z ---el 3 = el 3 + pc el 3 + pc dl 3 , up dl 3 -y ---up dl 3 = skip el 2 + up dl 3 , up dl 2 -x ---up dl 2 = skip el 1 + up dl 2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>, U 1 2 , U 2 2 , U 1 3 , U 2 3 , and U 3 3 ).</cell></row><row><cell cols="2">A Network sub-blocks type</cell><cell>input</cell><cell>k c s p r</cell><cell>output</cell></row><row><cell>Encoder level 1</cell><cell>Res Res</cell><cell>feat res 1</cell><cell>3 x 1 1 -3 x 1 1 -</cell><cell>res 1 el 1</cell></row><row><cell></cell><cell>conv</cell><cell>p el 1</cell><cell>1 x 1 0 -</cell><cell>pc el 1</cell></row><row><cell>CSFF</cell><cell>conv</cell><cell>p dl 1</cell><cell>1 x 1 0 -</cell><cell>pc dl 1</cell></row><row><cell cols="2">sum bi-down el 1 Down conv</cell><cell>el 1 down el 1</cell><cell>-x --0.5 1 y 1 0 -</cell><cell>down el 1 down el 1</cell></row><row><cell>Encoder level 2</cell><cell>Res Res</cell><cell>down el 1 res 2</cell><cell>3 y 1 1 -3 y 1 1 -</cell><cell>res 2 el 2</cell></row><row><cell></cell><cell>conv</cell><cell>p el 2</cell><cell>1 y 1 0 -</cell><cell>pc el 2</cell></row><row><cell>CSFF</cell><cell>conv</cell><cell>p dl 2</cell><cell>1 y 1 0 -</cell><cell>pc dl 2</cell></row><row><cell cols="2">sum bi-down el 2 Down conv</cell><cell>el 2 down el 2</cell><cell>-y --0.5 1 z 1 0 -</cell><cell>down el 2 down el 2</cell></row><row><cell>Encoder level 3</cell><cell>Res Res</cell><cell>down el 2 res 3</cell><cell>3 z 1 1 -3 z 1 1 -</cell><cell>res 3 el 3</cell></row><row><cell></cell><cell>conv</cell><cell>p el 3</cell><cell>1 z 1 0 -</cell><cell>pc el 3</cell></row><row><cell>CSFF</cell><cell>conv</cell><cell>p dl 3</cell><cell>1 z 1 0 -</cell><cell>pc dl 3</cell></row><row><cell cols="2">sum Res el 3 Decoder level 3 Res</cell><cell>el 3 res 4</cell><cell>3 z 1 1 -3 z 1 1 -</cell><cell>res 4 dl 3</cell></row><row><cell>Up</cell><cell>bi-up conv</cell><cell>dl 3 up dl 3</cell><cell>-z --2 1 y 1 0 -</cell><cell>up dl 3 up dl 3</cell></row><row><cell cols="3">Res sum Res skip el 2 Decoder level 2 el 2 Skip up dl 3 Res res 5</cell><cell>3 y 1 1 -3 y 1 1 -3 y 1 1 -</cell><cell>skip el 2 res 5 dl 2</cell></row><row><cell>Up</cell><cell>bi-up conv</cell><cell>dl 2 up dl 2</cell><cell>-y --2 1 x 1 0 -</cell><cell>up dl 2 up dl 2</cell></row><row><cell cols="3">Res sum Res skip el 1 Decoder level 1 el 1 Skip up dl 2 Res res 6</cell><cell>3 x 1 1 -3 x 1 1 -3 x 1 1 -</cell><cell>skip el 1 res 6 dl 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A4 :</head><label>A4</label><figDesc>Detailed architecture of MSSNet. Auxiliary layers are only used for training. The subblocks E 1 , E 2 and E 3 correspond to E 1 , E 2 and E 3 inFig. 2, respectively. While U 1 1 takes only one input tensor f eat 1 , the other UNet modules take three input tensors as they use CSFF. The first input of each UNet module corresponds to f eat inTable A3, while the second and third input values correspond to {p el i } and {p dl i } inTable A3, respectively. The output of each UNet module consists of the features from its encoders and decoders, corresponding to {el i } and {dl i } inTable A3, respectively. For example, U 1 1 E and U 1 1 D are the sets of features from the encoder and decoder of U 1 1 , respectively. U 1 1 D[1] is the first feature tensor of U 1 1 D, i.e., dl 1 of U 1 1 .</figDesc><table><row><cell>Scale Sub-blocks</cell><cell>type</cell><cell>input</cell><cell>k c s p r CSFF</cell><cell>output</cell></row><row><cell>S 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A5 :</head><label>A5</label><figDesc>Quantitative evaluation of parameter sharing-based methods on the GoPro test set<ref type="bibr" target="#b19">[20]</ref>. The models in blue are coarse-to-fine approaches, while the model in red are single-scale approach.</figDesc><table><row><cell>Models</cell><cell cols="5">PSNR (dB) SSIM Param (M) MACs (G) Time (s)</cell></row><row><cell>SRN [32]</cell><cell>30.26</cell><cell>0.934</cell><cell>8.06</cell><cell>20134</cell><cell>0.736</cell></row><row><cell>PSS-NSC [7]</cell><cell>30.92</cell><cell>0.942</cell><cell>2.84</cell><cell>3255</cell><cell>0.316</cell></row><row><cell>MT-RNN [22]</cell><cell>31.15</cell><cell>0.945</cell><cell>2.60</cell><cell>2315</cell><cell>0.323</cell></row><row><cell>MSSNet-WS (Ours)</cell><cell>31.83</cell><cell>0.950</cell><cell>2.85</cell><cell>2057</cell><cell>0.238</cell></row><row><cell cols="6">Table A6: Analysis on the stage configuration using variants of MSSNet without parameter sharing.</cell></row><row><cell cols="2"># Stages</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Models S 1 S 2 S 3 PSNR SSIM Params (M) MACs (G)</cell></row><row><cell cols="3">M321 3 2 1 31.04 0.943</cell><cell>6.61</cell><cell cols="2">305.14</cell></row><row><cell cols="3">M222 2 2 2 31.35 0.947</cell><cell>6.61</cell><cell cols="2">463.14</cell></row><row><cell cols="3">M123 1 2 3 31.54 0.949</cell><cell>6.61</cell><cell cols="2">621.14</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://seungjunnah.github.io/Datasets/gopro (CC BY 4.0) 2 https://github.com/rimchang/RealBlur (CC BY 4.0)</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hinet: Half instance normalization network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengpeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Asia 2009 papers</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convergence analysis of map based blur kernel estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4808" to="4816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo-Won</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Pyo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2006 Papers</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3848" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for direct text deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Hradi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Zemc?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>?roubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deblurring low-light images with light streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3382" to="3389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8878" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2599" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1964" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient marginal likelihood optimization in blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2657" to="2664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1628" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Un</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se Young</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="327" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Region-adaptive dense network for efficient motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11882" to="11889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-world blur dataset for learning and benchmarking deblurring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haeyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jucheol</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="184" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm transactions on graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatially-attentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3606" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Edge-based blur kernel estimation using patch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1107" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14821" to="14831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2521" to="2529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
