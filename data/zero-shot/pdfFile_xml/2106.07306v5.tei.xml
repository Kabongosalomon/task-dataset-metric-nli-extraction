<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONSTRAINING LINEAR-CHAIN CRFS TO REGULAR LANGUAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-02">2 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Papay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pad?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CONSTRAINING LINEAR-CHAIN CRFS TO REGULAR LANGUAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-02">2 Mar 2022</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge in structured prediction is to represent the interdependencies within output structures. When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn local dependencies in the output. However, the CRF's Markov assumption makes it impossible for CRFs to represent distributions with nonlocal dependencies, and standard CRFs are unable to respect nonlocal constraints of the data (such as global arity constraints on output labels). We present a generalization of CRFs that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language L. The resulting regular-constrained CRF (RegCCRF) has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in L. Notably, RegC-CRFs can incorporate their constraints during training, while related models only enforce constraints during decoding. We prove that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice. Finally, we demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF into a deep neural model for semantic role labeling, exceeding state-of-the-art results on a standard dataset.</p><p>Published as a conference paper at ICLR 2022 are techniques for relaxing the Markov assumption, but both come with drawbacks in performance and expressiveness.</p><p>In this work, we propose a new method to tractably relax the Markov assumption in CRFs. Specifically, we show how to constrain the output of a CRF to a regular language, such that the resulting regular-constrained CRF (RegCCRF) is guaranteed to output label sequences from that language. Since regular languages can encode long-distance dependencies between the symbols in their strings, RegCCRFs provide a simple model for structured prediction guaranteed to respect these constraints. The domain knowledge specifying these constraints is defined via regular expressions, a straightforward, well understood formalism. We show that our method is distinct from approaches that enforce constraints at decoding time, and that it better approximates the true data distribution. We evaluate our model empirically as the output layer of a neural network and attain state-of-the-art performance for semantic role labeling <ref type="bibr" target="#b39">(Weischedel et al., 2011;</ref><ref type="bibr" target="#b32">Pradhan et al., 2012)</ref>. Our PyTorch RegCCRF implementation can be used as a drop-in replacement for standard CRFs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Structured prediction is a field of machine learning where outputs are expected to obey some predefined discrete structure. Instances of structured prediction with various output structures occur in many applications, including computer vision (e.g., scene graph generation <ref type="bibr" target="#b16">(Johnson et al., 2015)</ref> with graph-structured output), natural language processing (e.g., linguistic parsing <ref type="bibr" target="#b28">(Niculae et al., 2018)</ref> with tree-structured output, relation extraction <ref type="bibr" target="#b35">(Roth &amp; Yih, 2004)</ref> with tuple-structured output) or modeling the spatial structure of physical entities and processes <ref type="bibr" target="#b15">(Jiang, 2020)</ref>.</p><p>A key difficulty faced by all models is to tractably model interdependencies between different parts of the output. As output spaces tend to be combinatorially large, special techniques, approximations, and independence assumptions must be used to work with the associated probability distributions. Considerable research has investigated specific structures for which such approaches make machine learning tractable. For instance, when outputs are trees over a fixed set of nodes, maximal arborescence algorithms allow for exact inference <ref type="bibr" target="#b5">(Chu, 1965;</ref><ref type="bibr">Edmonds, 1967)</ref>; when outputs are graphstructured, loopy belief propagation can provide approximate inference <ref type="bibr" target="#b27">(Murphy et al., 1999)</ref>.</p><p>If the output forms a linear sequence, a Markov assumption is often made: model outputs depend only on their immediate neighbors, but not (directly) on more distant ones. A common model that uses this assumption is the linear-chain conditional random field (CRF) <ref type="bibr" target="#b20">(Lafferty et al., 2001)</ref>, which has found ubiquitous use for sequence labeling tasks, including part-of-speech tagging <ref type="bibr" target="#b9">(Gimpel et al., 2011)</ref> and named entity recognition <ref type="bibr" target="#b21">(Lample et al., 2016)</ref>. This model became popular with the use of hand-crafted feature vectors in the 2000s, and is nowadays commonly used as an output layer in neural networks to encourage the learning of structural properties of the output sequence. The Markov assumption makes training tractable, but also limits the CRF's expressive power, which can hamper performance, especially for long sequences <ref type="bibr" target="#b36">(Scheible et al., 2016)</ref>. Semi-Markov CRFs <ref type="bibr" target="#b36">(Sarawagi &amp; Cohen, 2004)</ref> and skip-chain CRFs <ref type="bibr" target="#b37">(Sutton &amp; McCallum, 2004)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We identify three areas of structured prediction that are relevant to our work: constrained decoding, which can enforce output constraints at decoding time, techniques for weakening the Markov assumption of CRFs to learn long-distance dependencies, and weight-learning in finite-state transducers.</p><p>Constrained decoding. A common approach to enforcing constraints in model output is constrained decoding: Models are trained in a standard fashion, and decoding ensures that the model output satisfies the constraints. This almost always corresponds to finding or approximating a version of the model's distribution conditionalized on the output obeying the specified constraints. This approach is useful if constraints are not available at training time, such as in the interactive information extraction task of <ref type="bibr" target="#b19">Kristjansson et al. (2004)</ref>. They present constrained conditional random fields, which can enforce that particular tokens are or are not assigned particular labels (positive and negative constraints, respectively). Formally, our work is a strict generalization of this approach, as position-wise constraints can be formulated as a regular language, but regular languages go beyond position-wise constraints. Other studies treat decoding with constraints as a search problem, searching for the most probable decoding path which satisfies all constraints. For example, <ref type="bibr" target="#b12">He et al. (2017)</ref> train a neural network to predict token-wise output probabilities for semantic role labeling following the BIO label-alphabet <ref type="bibr" target="#b33">(Ramshaw &amp; Marcus, 1999)</ref>, and then use exact A* search to ensure that the output forms a valid BIO sequence and that particular task-specific constraints are satisfied. For autoregressive models, constrained beam search <ref type="bibr" target="#b13">(Hokamp &amp; Liu, 2017;</ref><ref type="bibr" target="#b0">Anderson et al., 2017;</ref><ref type="bibr" target="#b11">Hasler et al., 2018)</ref> can enforce regular-language constraints during search. We further discuss constrained decoding as it relates to RegCCRFs in Section 5.</p><p>Markov relaxations. While our approach can relax the Markov assumption of CRFs through nonlocal hard constraints, another strand of work has developed models which can directly learn nonlocal dependencies in CRFs: Semi-Markov CRFs <ref type="bibr" target="#b36">(Sarawagi &amp; Cohen, 2004)</ref> relax the Markov property to the semi-Markov property. In this setting, CRFs are tasked with segmentation, where individual segments may depend only on their immediate neighbors, but model behavior within a particular segment need not be Markovian. As such, semi-Markov CRFs are capable of capturing nonlocal dependencies between output variables, but only to a range of one segment and inside of a segment. Skip-chain CRFs <ref type="bibr" target="#b37">(Sutton &amp; McCallum, 2004)</ref> change the expressiveness of CRFs by relaxing the assumption that only the linear structure of the input matters: they add explicit dependencies between distant nodes in an otherwise linear-chain CRF. These dependencies are picked based on particular properties, e.g., input variables of the same value or which share other properties. In doing so, they add loops to the model's factor graph, which makes exact training and inference intractable, and leads to the use of approximation techniques such as loopy belief propagation and Gibbs sampling.</p><p>Weight learning for finite-state transducers. While our approach focuses on the task of constraining the CRF distribution to a known regular language, a related task is that of learning a weighted regular language from data. This task is usually formalized as learning the weights of a weighted finite-state transducer (FST), as in e.g. <ref type="bibr" target="#b8">Eisner (2002)</ref> with directly parameterized weights and <ref type="bibr" target="#b34">Rastogi et al. (2016)</ref> with weights parameterized by a neural network. Despite the difference in task-setting, this task is quite similar to ours in the formal sense, and in fact our proposal can be viewed as a particularly well-behaved special case of FST weight learning for an appropriately chosen transducer architecture and parameterization. We discuss this connection further in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES AND NOTATION</head><p>As our construction involves finite-state automata and conditional random fields, we define these here and specify the notation we will use in the remainder of this work.</p><p>Finite-state automata. All automata are taken to be nondeterministic finite-state automata (NFAs) without epsilon transitions. Let such an NFA be defined as a 5-tuple (?, Q, q 1 , F, E), where ? = {a 1 , a 2 , ..., a |?| } is a finite alphabet of symbols, Q = {q 1 , q 2 , ..., q |Q| } is a finite set of states, q 1 ? Q is the sole starting state, F ? Q is a set of accepting states, and E ? Q ? ? ? Q is a set of directed, symbol-labeled edges between states. The edges define the NFA's transition function ? : Q ? ? ? 2 Q , with r ? ?(q, a) ? (q, a, r) ? E. An automaton is said to accept a string s ? ? * iff there exists a contiguous path of edges from q 1 to some accepting state whose edge labels are exactly the symbols of s. The language defined by an automaton is the set of all such accepted strings. A language is regular if and only if it is the language of some NFA.</p><p>Linear-chain conditional random fields. Linear-chain conditional random fields (CRFs) <ref type="bibr" target="#b20">Lafferty et al. (2001)</ref> are a sequence labeling model. Parameterized by ?, they use a global exponential model to represent the conditional distribution over label sequences y = y 1 , y 2 , ..., y t conditioned on input sequences x = x 1 , x 2 , ..., x t : P ? (y | x) ? exp j f j ? (x, y), with individual observations x i coming from some observation space X, and outputs y i coming from some finite alphabet Y . In this work, we use CRFs for sequence labeling problems, but the dataset labels do not correspond directly to the CRF's outputs y i . In order to avoid ambiguity, and since the term "state" already has a meaning for NFAs, we call y the CRF's tag sequence, and each y i a tag. The terms label sequence and label will thus unambiguously refer to the original dataset labels.</p><p>Each f j ? is a potential function of x and y, parameterized by ?. Importantly, in a linear-chain CRF, these potential functions are limited to two kinds: The transition function g ? (y i , y i+1 ) assigns a potential to each pair (y i , y i+1 ) of adjacent tags in y, and the emission function h ? (y i | x, i) assigns a potential to each possible output tag y i given the observation sequence x and its position i. With these, the distribution defined by a CRF is</p><formula xml:id="formula_0">P ? (y | x) ? exp t?1 i=1 g ? (y i , y i+1 ) + t i=1 h ? (x, y i , i) .</formula><p>(1)</p><p>Limiting our potential functions in this way imposes a Markov assumption on our model, as potential functions can only depend on a single tag or a single pair of adjacent tags. This makes learning and inference tractable: the forward algorithm <ref type="bibr" target="#b17">(Jurafsky &amp; Martin, 2009</ref>) can calculate negative loglikelihood (NLL) loss during training, and the Viterbi algorithm <ref type="bibr" target="#b38">(Viterbi, 1967;</ref><ref type="bibr" target="#b17">Jurafsky &amp; Martin, 2009</ref>) can be used for inference. Both are linear in t, and quadratic in |Y | in both time and space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REGULAR-CONSTRAINED CRFS</head><p>Given a regular language L, we would like to constrain a CRF to L. We formalize this notion of constraint with conditional probabilities -a CRF constrained to L is described by a (further) conditionalized version of that CRF's distribution P ? (y | x), conditioned on the event that the tag sequence y is in L. We write this distribution as</p><formula xml:id="formula_1">P ? (y | x, L) = ? ? P ? (y | x) if y ? L 0 otherwise,<label>(2)</label></formula><p>with ? ? 1 defined as ? ?1 = y?L P ? (y | x). In order to utilize this distribution for machine learning, we need to be able to compute NLL losses and perform MAP inference. As discussed in Section 3, both of these are efficiently computable for CRFs. Thus, if we can construct a separate CRF whose output distribution can be interpreted as P (y | x, L), both of these operations will be available. We do this in the next section.</p><formula xml:id="formula_2">q1 start q2 q3 q4 O B I O O B B B I O q1 B ? ? q2 q2 O ? ? q4 q4 B ? ? q3 q3 I ? ? q3 x h?(x, B, 1) h?(x, O, 2) h?(x, B, 3) h?(x, I, 4) g?(B, O) g?(O, B) g?(B, I) Y ? = q 1 O ? ? q 1 , q 1 B ? ? q 2 , q 2 I ? ? q 2 , q 2 B ? ? q 3 , q 2 O ? ? q 4 , q 3 O ? ? q 1 , q 3 B ? ? q 2 , q 3 I ? ? q 3 , q 4 B ? ? q 3 , q 4 O ? ? q 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CONSTRUCTION</head><p>Let M := (?, Q, F, E) be an NFA that describes L. We assume that M is unambiguous -i.e., every string in L is accepted by exactly one path through M . As every NFA can be transformed into an equivalent unambiguous NFA <ref type="bibr" target="#b25">(Mohri, 2012)</ref>, this assumption involves no loss of generality. Our plan is to represent P ? (y | x, L) by constructing a separate CRF with a distinct tag set, whose output sequences can be interpreted directly as paths through M . As M is unambiguous, each label sequence in L corresponds to exactly one such path. We parameterize this auxiliary CRF identically to our original CRF -that is, with label-wise (not tag-wise) transition and emission functions. Thus, for all parameterizations ?, both distributions P ? (y | x) and P ? (y | x, L) are well defined.</p><p>There are many ways to construct such a CRF. As CRF training and inference are quadratic in the size of the tag set Y , we would prefer a construction which minimizes |Y |. However, for clarity, we will first present a conceptually simple construction, and discuss approaches to reduce |Y | in Section 4.2. We start with our original CRF, parameterized by ?, with tag set Y = ?, transition function g ? , and emission function h ? , describing the probability distribution P ? (y | x), y ? ? * . From this, we construct a new CRF, also parameterized by the same ?, but with tag set Y ? , transition function g ? ? , and emission function h ? ? . This auxiliary CRF describes the distribution P ? ? (y ? | x) (with y ? ? Y ? * ), which we will be able to interpret as P ? (y | x, L). The construction is as follows:</p><formula xml:id="formula_3">Y ? = E (3) g ? ? ((q, a, r), (q ? , a ? , r ? )) = g ? (a, a ? ) if r = q ? ?? otherwise (4) h ? ? (x, (q, a, r), i) = ? ? ? ?? if i = 1, q = q 1 ?? if i = t, r ? F h ? (x, a, i) otherwise.<label>(5)</label></formula><p>This means that the tags of our new CRF are the edges of M , the transition function assigns zero probability to transitions between edges which do not pass through a shared NFA state, and the emission function assigns zero probability to tag sequences which do not begin at the starting state or end at an accepting state. Apart from these constraints, the transition and emission functions depend only on edge labels, and not on the edges themselves, and agree with the standard CRF's g ? and h ? when no constraints are violated.</p><p>As M is unambiguous, every tag sequence y corresponds to a single path through M , representable as an edge sequence ? = ? 1 , ? 2 , ..., ? t , ? i ? E. Since this path is a tag sequence for our auxiliary CRF, we can directly calculate the auxiliary CRF's P ? ? (? | x). From the construction of g ? ? and h ? ? , this must be equal to P ? (y | x, L), as it is proportional to P ? (y | x) for y ? L, and zero (or, more correctly, undefined) otherwise. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this construction with a concrete example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TIME AND SPACE EFFICIENCY</head><p>As the Viterbi and forward algorithms are quadratic in |Y |, very large tag sets can lead to performance issues, possibly making training or inference intractible in extreme cases. Thus, we would like to characterize under which conditions a RegCCRF can be used tractibly, and identify techniques for improving performance. As Y corresponds to the edges of M , we would like to select our unambiguous automaton M to have as few edges as possible. For arbitrary languages, this problem is NP-complete <ref type="bibr" target="#b14">(Jiang &amp; Ravikumar, 1991)</ref>, and, assuming P = NP, is not even efficiently approximable <ref type="bibr" target="#b10">(Gruber &amp; Holzer, 2007)</ref>. Nonetheless, for many common classes of languages, there exist approaches to obtain a tractably small automaton.</p><p>One straightforward method is to construct M directly from a short unambiguous regular expression. Br?ggemann-Klein &amp; Wood (1992) present a simple algorithm for constructing an unambiguous automaton from an unambiguous regular expression, with |Q| linear in the length of the expression. Using this method to construct M , the time-and space-complexity of Viterbi are polynomial in the length of our regular expression, with a worst-case of quartic complexity when the connectivity graph of M is dense.</p><p>For many other tasks, a reasonable approach is to leverage domain knowledge about the constraints to manually construct a small unambiguous automaton. For example, if the constraints require that a particular label occurs exactly n times in the output sequence, an automaton could be constructed manually to count ocurrences of that label. Multiple constraints of this type can then be composed via automaton union and intersection.</p><p>Without making changes to M , we can also reduce the size of |Y | by adjusting our construction. Instead of making each edge of M a tag, we can adopt equivalence classes of edges. Reminiscent of standard NFA minimization, we define (q, a, r) ? (q ? , a ? , r ? ) ? (q, a) = (q ? , a ? ). When constructing our CRF, whenever a transition would have been allowed between two edges, we allow a transition between their corresponding equivalence classes. We do the same to determine which classes are allowed as initial or final tags. As each equivalence class corresponds (non-uniquely) to a single symbol a, we can translate between tag sequences and strings of L just as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">INTERPRETATION AS A WEIGHTED FINITE-STATE TRANSDUCER</head><p>While we present our model as a variation of a standard CRF which enforces regular-language constraints, an alternate characterization is as a weighted finite-state transducer with the transducer topology and weight parameterization chosen so as to yield the distribution P ? (y | x, L). In order to accommodate CRF transition weights, such an approach involves weight-learning in an auxiliary automaton whose edges correspond to edge-pairs in M -we give a full construction in Appendix C.</p><p>This interpretation enables direct comparison to studies on weight learning in finite-state transducers, such as <ref type="bibr" target="#b34">Rastogi et al. (2016)</ref>. While RegCCRFs can be viewed as special cases of neural-weighted FSTs, they inherit a number of useful properties from CRFs not possessed by neural-weighted automata in general. Firstly, as |y| is necessarily equal to |x|, the partition function y?L P ? (y | x, L) is guaranteed to be finite, and P ? (y | x, L) is a well-defined probability distribution for all ?, which is not true for weighted transducers in general, which may admit paths with unbounded lengths and weights. Secondly, as M is assumed to be unambiguous, string probabilities correspond exactly to path probabilities, allowing for exact MAP inference with the Viterbi algorithm. In contrast, finding the most probable string in the highly ambiguous automata used when learning edge weights for an unknown language is NP-Hard <ref type="bibr" target="#b4">(Casacuberta &amp; de la Higuera, 1999)</ref>, necessitating approximation methods such as crunching <ref type="bibr" target="#b24">(May &amp; Knight, 2006)</ref>. Finally, as each RegCCRF can be expressed as a CRF with a particular parameterization, the convexity guarantees of standard CRFs carry over, in that the loss is convex with respect to emission and transition scores. In contrast, training losses in general weighted finite-state transducers are usually nonconvex <ref type="bibr" target="#b34">(Rastogi et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONSTRAINED TRAINING VS. CONSTRAINED DECODING</head><p>Our construction suggests two possible use cases for a RegCCRF: constrained decoding, where a CRF is trained unconstrained, and the learned weights are then used in a RegCCRF at decoding time, and constrained training, where a RegCCRF is both trained and decoded with constraints. In this section, we compare between these two approaches and a standard, unconstrained CRF. We assume a machine learning setting where we have access to samples from some data distribution P (x, y), with each x ? X * , and each y of matching length in some regular language L ? ? * . We wish to model the conditional distribution P (y | x) with either a CRF or a RegCCRF, by way of maximizing the model's (log) likelihood given the data distribution.</p><p>The unconstrained CRF corresponds to a CRF that has been trained, without constraints, on data from P (x, y), and is used directly for inference: It makes no use of the language L. The model's output distribution is P ?u (y | x), with parameter vector ? u minimizing the NLL objective:</p><formula xml:id="formula_4">? u = arg min ? E x,y? P [? ln P ? (y | x)]<label>(6)</label></formula><p>In constrained decoding, a CRF is trained unconstrained, but its weights are used in a RegCCRF at decoding time. The output distribution of such a model is P ?u (y | x, L). It is parameterized by the same parameter vector ? u as the unconstrained CRF, as the training procedure is identical, but the output distribution is conditioned on y ? L.</p><p>Constrained training involves directly optimizing a RegCCRF's output distribution, avoiding any asymmetry between training and decoding time. In this case, the output distribution of the model is</p><formula xml:id="formula_5">P ?c (y | x, L), where ? c = arg min ? E x,y? P [? ln P ? (y | x, L)]<label>(7)</label></formula><p>is the parameter vector which minimizes the NLL of the RegCCRF's constrained distribution.</p><p>These three approaches form a hierarchy in terms of their ability to match the data distribution: L unconstrained ? L constrained decoding ? L constrained training , with each L corresponding to the negative loglikelihood assigned by each model to the data; see Appendix B for a proof. This suggests we should prefer the constrained training regimen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SYNTHETIC DATA EXPERIMENTS</head><p>While constrained training cannot underperform constrained decoding, the conditions where it is strictly better depend on exactly how the transition and emission functions are parameterized, and are not easily stated in general terms. We now empirically show two simple experiments on synthetic data where the two are not equivalent.</p><p>The procedure is similar for both experiments. We specify a regular language L, an observation alphabet X, and a joint data distribution P (x, y) over observation sequences in X * and label sequences in L. We then train two models, one with a RegCCRF, parameterized by ? c , and one with an unconstrained CRF, parameterized by ? u . For each model, we initialize parameters randomly, then use stochastic gradient descent to minimize NLL with P . We directly generate samples from P to use as training data. After optimizing ? c and ? u , construct a RegCCRF with ? u for use as a constrained-decoding model, and we compare the constrained-training distribution P ?c (y | x, L) with the constrained-decoding distribution P ?u (y | x, L).</p><p>We use a simple architecture for our models, with both the transition functions g ? and emission functions h ? represented as parameter matrices. We list training hyperparameters in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ARBITRARILY LARGE DIFFERENCES IN LIKELIHOOD</head><p>We would like to demonstrate that, when comparing constrained training to constrained decoding in terms of likelihood, constrained training can outperform constrained decoding by an arbitrary margin. We choose L = (ac) * | (bc) * to make conditional independence particularly relevant -as every even-indexed label is c, an unconstrained CRF must model odd-indexed labels independently,  <ref type="figure">Figure 2</ref>: Model output probabilities, and NLL losses, plotted against sequence length k. As k increases, constrained decoding becomes a progressively worse approximation for the data distribution, while constrained training is consistently able to match the data distribution.</p><p>while a constrained CRF can use its constraints to account for nonlocal dependencies. For simplicity, we hold the input sequence constant, with X = {o}.</p><p>Our approach is to construct sequences of various lengths. For k ? N, we let our data distribution be</p><formula xml:id="formula_6">P (o 2k , (ac) k ) = 3 4 and P (o 2k , (bc) k ) = 1 4 .<label>(8)</label></formula><p>As the marginal distributions for odd-indexed characters are not independent, an unconstrained CRF cannot exactly represent the distribution P . We train and evaluate individual models for each sequence length k. <ref type="figure">Figure 2</ref> plots model probabilities and NLL losses for various k. We see that, regardless of k, P ?c (y | x, L) is able to match P (y | x) almost exactly, with only small deviations due to sampling noise in SGD. On the other hand, as sequence length increases, P ?u (y | x, L) becomes progressively "lop-sided", assigning almost all of its probability mass to the string (ac) k . This behavior is reflected in the models' likelihoods -constrained training stays at near-constant likelihood for all k, while the negative log-likelihood of constrained decoding grows linearly with k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">DIFFERENCES IN MAP INFERENCE</head><p>We show here that constrained training and constrained decoding can disagree about which label sequence they deem most likely. Furthermore, in this case, MAP inference agrees with the data distribution's mode for constrained training, but not for constrained decoding. To do this, we construct a fixed-length output language L = acd | bcd | bce, where an unconstrained CRF is limited by the Markov property to predict y's prefix and suffix independently, and choose a data distribution which violates this independence assumption. We select our data distribution, P (ooo, acd) = 0.4 and P (ooo, bcd) = 0.3 and P (ooo, bce) = 0.3,</p><p>to be close to uniform, but with one label sequence holding the slight majority, and we ensure that the majority label sequence is not the label sequence with both the majority prefix and the majority suffix (i.e. bcd). As before, we hold the observation sequence as a constant (ooo). We train a constrained and an unconstrained CRF to convergence, and compare P ?u (y | x, L) to P ?c (y | x, L). <ref type="table" target="#tab_0">Table 1</ref> shows P ?u (y | x, L) and P ?c (y | x, L) as they compare to P (y | x). We find that, while the mode of P (y | x) is acd, with probability of 0.4, the mode of constrained decoding distribution P ?u (y | x, L) is bcd, the string with the majority prefix and the majority suffix, to which the model assigns a probability of 0.48. Conversely, the constrained training distribution P ?c (y | x, L) matches the data distribution almost exactly, and predicts the correct mode. Task. As a final experiment, we apply our RegCCRF to the NLP task of semantic role labeling (SRL) in the popular PropBank framework <ref type="bibr" target="#b30">(Palmer et al., 2005)</ref>. In line with previous work, we adopt the known-predicate setting, where events are given and the task is to mark token spans as (types of) event participants. PropBank assumes 7 semantic core roles (ARG0 through ARG5 plus ARGA) plus 21 non-core roles for modifiers such as times or locations. For example, in [ ARG0 Peter] saw [ ARG1 Paul] [ ARGM-TMP yesterday], the argument labels inform us who does the seeing (ARG0), who is seen (ARG1), and when the event took place (ARGM-TMP). In addition, role spans may be labeled as continuations of previous role spans, or as references to another role span in the sentence. SRL can be framed naturally as a sequence labeling task <ref type="bibr" target="#b12">(He et al., 2017</ref>). However, the task comes with a number of hard constraints that are not automatically satisfied by standard CRFs, namely: (1) Each core role may occur at most once per event;</p><p>(2) for continuations, the span type must occur previously in the sentence; (3) for references, the span type must occur elsewhere in the sentence.</p><p>Data. In line with previous work <ref type="bibr" target="#b29">(Ouchi et al., 2018)</ref>, we work with the OntoNotes corpus as used in the CoNLL 2012 shared task 1 <ref type="bibr" target="#b39">(Weischedel et al., 2011;</ref><ref type="bibr" target="#b32">Pradhan et al., 2012)</ref>, whose training set comprises 66 roles (7 core roles, 21 non-core roles, 19 continuation types, and 19 reference types).</p><p>RegCCRF Models. To encode the three constraints listed above in a RegCCRF, we define a regular language describing valid BIO sequences <ref type="bibr" target="#b33">(Ramshaw &amp; Marcus, 1999)</ref> over the 66 roles. A minimal unambiguous NFA for this language has more than 2 2 ? 3 19 states, which is too large to run the Viterbi algorithm on our hardware. However, as many labels are very rare, we can shrink our automaton by discarding them at the cost of imperfect recall. We achieve further reductions in size by ignoring constraints on reference roles, treating them identically to non-core roles. Our final automaton recognizes 5 core role types (ARG0 through ARG4), 17 non-core / reference roles, and one continuation role type (for ARG1). This automaton has 672 states, yielding a RegCCRF with 2592 tags. A description of our procedure for constructing this automaton can be found in Appendix D.</p><p>Our model architecture is given by this RegCCRF, with emission scores provided by a linear projection of the output of a pretrained RoBERTa network <ref type="bibr" target="#b23">Liu et al. (2019)</ref>. In order to provide the model with event information, the given predicates are prefixed by a special reserved token in the input sequence. RoBERTa parameters are fine-tuned during the learning of transition scores and projection weights. We perform experiments with both constrained training and constrained decoding settings -we will refer to these as ConstrTr and ConstrDec respectively. A full description of the training procedure, including training times, is provided in Appendix A. As RegCCRF loss is only finite for label sequences in L, we must ensure that our training data do not violate our constraints. We discard some roles, as described above, by simply removing the offending labels from the training data. In six cases, training instances directly conflict with the constraints specified -all cases involve continuation roles missing a valid preceding role. We discard these instances for ConstrTr.</p><p>CRF Baselines. As baseline models, we use the same architecture, but with a standard CRF replacing the RegCCRF. Since we are not limited by GPU memory for CRFs, we are optionally able to include all role types present in the training set, using the complete training set. We present two CRF baseline models: CRF-full, which is trained on all role-types from the training set, and CRF- reduced, which includes the same subset of roles as the RegCCRF models. For CRF-reduced, we use the same learned weights as for CD, but we decode without constraints.</p><p>Results and analysis. We evaluate our models on the evaluation partition, and measure performance using F 1 score for exact span matches. For comparability with prior work, we use the evaluation script 2 for the CoNLL-2005 shared task <ref type="bibr" target="#b3">(Carreras &amp; M?rquez, 2005)</ref>. These results, averaged over five trials, are presented in <ref type="table" target="#tab_1">Table 2</ref>. Excepting CRF-reduced, all of our models outperform the existing state-of-the-art ensemble model <ref type="bibr" target="#b29">Ouchi et al. (2018)</ref>. We ascribe this improvement over the existing literature to our use of RoBERTa -prior work in SRL relies on ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, which tends to underperform transformer-based models on downstream tasks <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>.</p><p>Of our models, ConstrTr significantly 3 outperforms the others in F 1 score and yields a new SOTA for SRL on OntoNotes, in line with expectations from theoretical analysis and on synthetic data. Our other three models show trade-offs between precision and recall, wherein CRF-full outperforms the other two in recall and underperforms them in precision. This is not surprising, as CRF-full is the only model capable of predicting rare role types. The other models, which use the reduced tag sets, have a theoretical maximum of 99% recall. Interestingly, when comparing between the three models that use the reduced tag sets, we find a statistically significant interaction between the constraint setting and model recall, but not between constraints and model precision: ConstrTr has significantly higher recall than ConstrDec, which in turn significantly beats CRF-reduced in recall, but there are no statistically significant differences between these models' precisions.</p><p>For our unconstrained models, CRF-full and CRF-reduced, the constraints specified in our automaton are violated in 0.81% and 0.84% of all output sequences respectively. 4 While this number is small, it should not be interpreted to mean that constraints are useless for almost all instances -as shown in Section 6.2, constraints during training can affect MAP inference even when none of the alternatives violate constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>We have presented a method to constrain the output of CRFs to a regular language. Our construction permits constraints to be used at training or prediction time; both theoretically and empirically, training-time constraining better captures the data distribution. Conceptually, our approach constitutes a novel bridge between constrained CRFs and neural-weighted FSTs.</p><p>Future work could target enhancing the model's expressibility, either by allowing constraints to depend explicitly on the input as regular relations, or by investigating non-binary constraints, i.e., regular language-based constraints with learnable weights. Additionally, regular language induction (e.g. <ref type="bibr" target="#b7">Dunay et al. (1994)</ref>; <ref type="bibr" target="#b1">Bartoli et al. (2016)</ref>) could be used to learn languages automatically, reducing manual specification and identifying non-obvious constraints. Another avenue for continuing research lies in identifying further applications for RegCCRFs. The NLP task of relation extraction could be a fruitful target -RegCCRFs offer a mechanism to make the proposal of a relation conditional on the presence of the right number and type of arguments. While our construction cannot be lifted directly to context-free languages due to the unbounded state space of the corresponding pushdown automata, context-free language can be approximated by regular languages <ref type="bibr" target="#b26">(Mohri &amp; Nederhof, 2001)</ref>. On this basis, for example, a RegCCRF backed by a regular language describing trees of a limited depth could also be applied to tasks with context-free constraints.</p><p>To encourage the use of RegCCRFs, we provide an implementation as a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>The research in this paper is fundamental in the sense that it enables machine learning models to better represent data and limit the search space at inference and learning time. It therefore does not in and of itself represent additional ethical risks on top of the previous work we build upon.  <ref type="table" target="#tab_3">Table 3</ref>. Full code for all experiments, along with training logs, are also included in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 CRFS</head><p>For all CRFs and RegCCRFs, transition potentials were initialized randomly from a normal distribution with mean zero and standard deviation 0.1. No CRFs or RegCCRFs employed special start-or end-transitions -that is, we did not insert any additional beginning-of-sequence or end-of-sequence tags for the Viterbi or forward algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SYNTHETIC DATA EXPERIMENTS -TRAINING PROCEDURE</head><p>For both synthetic data experiments, the emission potentials were represented explicitly for each position as trainable parameters -since the observation sequence was constant in all experiments, these did not depend on x.</p><p>Parameters were initialized randomly using PyTorch default initialization, and optimized using stochastic gradient descent. To ensure fast convergence to a stable distribution, we employed learning rate decay -learning rate was initially set to 1.0, and reduced by 10% every 100 training steps.</p><p>We trained all models for a total of 5000 steps with a batch size of 50. All models were trained on CPUs. For the experiment described in Section 6.1, we trained separate models for each k -the total training time for this experiment was approximately 35 minutes. The experiment described in Section 6.2 completed training in approximately 30 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SEMANTIC ROLE LABELING -TRAINING PROCEDURE</head><p>In the semantic role labeling (SRL) experiments, we incorporated a pretrained RoBERTa network <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> -the implementation and weights for this model were obtained using the roberta-base model from the Hugging Face transformers library <ref type="bibr" target="#b40">(Wolf et al., 2020)</ref>. RoBERTa embeddings were projected down to transmission scores using a linear layer with a bias -projection weights and biases were initialized using the PyTorch default initialization.</p><p>Input tokens were sub-tokenized using RoBERTa's tokenizer. The marked predicate in each sentence was prefixed by a special &lt;unk&gt; token. During training, for efficiency reasons, we excluded all sentences with 120 or more subtokens -this amounted to 0.23% of all training instances. We nonetheless predicted on all instances, regardless of length.</p><p>We optimized models using the Adam optimizer <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 10 ?5 . We fine-tune RoBERTa parameters and learn the projection and RegCCRF weights jointly. For performance reasons, batch size was set to 2, but we utilized gradient accumulation over groups of 4 batches to simulate a batch size of 8.</p><p>We utilized early stopping to avoid overfitting. Every 5000 training steps, we approximated our model's F 1 score against a subset of the provided development partition, using a simplified reimplementation of the official evaluation script. Each time we exceeded the previous best F 1 score for a model, we saved all model weights to disk. After 50 such evaluations with no improvement, we terminated training, and used the last saved copy of model weights for final evaluation.</p><p>We performed all SRL experiments on GeForce GTX 1080 Ti GPUs. Each experiment used a single GPU. Training took an average of 88 hours for RegCCRF models with constrained training, 23 hours for RegCCRF with constrained decoding, and 24 hours for CRF baseline models. Inference on the complete test set took an average of 18 minutes 55 seconds for CT and CD, and an average of 55 seconds for CRF-full and CRF-reduced. All training logs with timestamps are included in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF CONSTRAINED TRAINING INEQUALITY</head><p>In this appendix, we prove the inequality presented in section 5: when compared by NLL against the data distribution, L unconstrained ? L constrained decoding ? L constrained training , with each L corresponding to that model's negative log-likelihood. We first prove the left side of this inequality, comparing an unconstrained CRF to constrained decoding, and then prove the right side, comparing constrained decoding to constrained training. We use the notation introduced in Section 5. Theorem 1. For arbitrary ?:</p><formula xml:id="formula_8">E x,y? P [? ln P ? (y | x)] ? E x,y? P [? ln P ? (y | x, L)]</formula><p>Here we compare the distributions P ? (y | x) and P ? (y | x, L). We wish to demonstrate that P ? (y | x) can never achieve lower NLL than P ? (y | x, L), and that the two distributions achieve identical NLL only when P ? (y | x) = P ? (y | x, L) i.e. when constraints have no effect. Of note, this proof is valid for all parameterizations ?, and not just for ? u .</p><p>Proof. Since every y in P is in L,</p><formula xml:id="formula_9">P ? (y | x, L) = ? ? P ? (y | x),<label>(10)</label></formula><p>with ? ? 1. Thus, the NLL of the regular-constrained CRF is</p><formula xml:id="formula_10">E x,y? P [? ln P ? (y | x, L)] = E x,y? P [? ln P ? (y | x)] ? ln ?.<label>(11)</label></formula><p>This differs from the NLL of the unconstrained CRF only by the term ? ln ?. As ? ? 1, the regularconstrained CRF's NLL is less than or equal to that of the unconstrained CRF, with equality only when ? = 1 and therefore P ? (y | x) = P ? (y | x, L).</p><formula xml:id="formula_11">Theorem 2. E x,y? P [? ln P ?u (y | x, L)] ? E x,y? P [? ln P ?c (y | x, L)]</formula><p>In this case, we compare the distributions P ?u (y | x, L) and P ?c (y | x, L). We will demonstrate that the former cannot achieve a lower NLL against the data distribution than the latter.</p><p>Proof. This follows directly from our definitions, as we define ? c to minimize the NLL of P ? (y | x, L) against the data distribution. Thus, P ?u (y | x, L) could never yield a lower NLL than P ?c (y | x, L), as that would contradict our definition of ? c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CONSTRUCTION AS WEIGHTED FST</head><p>In this appendix, we present a construction of the RegCCRF as a weighted finite-state transducer with weight sharing. We do this by first specifying the transducer topology used, and then specifying how edge weights are parameterized in terms of ?. The resulting transducer yields an identical distribution to that of the CRF-based construction, P ? (y | x, L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 TRANSDUCER TOPOLOGY</head><p>Starting from L, we defineL to be the regular language of bigram sequences for the strings in L, i.e., L = (s 1 , s 2 ), (s 2 , s 3 ), ..., (s |s?1| , s |s| ), (s |s| , $) | s ? L ,</p><p>with $ acting as a end-of-string symbol. We letM be an unambiguous FSA for the languageL, and choose to interpret this automaton as a finite-state transducer by stipulating that all edges should accept any symbol in the input language (but only one symbol per transition, and without allowing epsilon transitions). This unweighted transducer will be used as the topology for our weighted finitestate transducer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 EDGE WEIGHTS</head><p>In line with <ref type="bibr" target="#b34">Rastogi et al. (2016)</ref>, we would like to assign weights to the edges of our transducerM with a neural network. In order to obtain the same distribution as from our CRF-based construction, these weights must be parameterized in terms of our transition function g ? and emission function h ? . For each edge inM , the weight depends only on the emitted bigram, the input sequence, and the index of the current input symbol -the weight does not depend on the FST states. For a symbol bigram (a, b), input sequence x, and index i, the edge weight is equal to</p><formula xml:id="formula_13">W a,b = g ? (a, b) + h ? (x, a, i) b = $ h ? (x, a, i) otherwise .<label>(13)</label></formula><p>Each string in L corresponds bijectively to exactly one bigram sequence inL, which corresponds bijectively to exactly one accepting path inM -this path's weight (in the Log semiring) is equal to the unscaled probability produced by our CRF construction, and so the weighted FST, interpreted as a probability distribution, yields the distribution P ? (y | x, L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D AUTOMATON CONSTRUCTION FOR SEMANTIC ROLE LABELING</head><p>In this appendix, we describe how we generate the automaton architecture for our semantic role labeling experiments. While our experiments used 5 core-roles, 17 non-core roles, and one continuation role, we discuss here a generalized setting with arbitrary sets of core, noncore, and continuations of core roles.</p><p>Algorithm 1 provides pseudocode for our construction. The core idea is to use subsets of core roles as NFA states, so that we can keep track of which core roles have already ocurred. Additional states are used in order to ensure all strings are valid BIO sequences.</p><p>Data: Sets R core , R noncore , and R continuation , of core, noncore, and continuation roles, respectively Result: A finite-state automaton M = (?, Q, q 1 , F, E), parameterized as described in Section 3 ? ? {OUTSIDE} ? ({BEGIN, INSIDE} ? (R core ? R noncore ? R continuation )); Q ? ?; </p><formula xml:id="formula_14">q 1 ? ?; F ? ?; E ? ?; for p ? 2 Rcore do Q ? Q ? {p}; F ? F ? {p}; E ? E ? {(p,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example for a RegCCRF, showing NFA and unrolled factor graph. L describes the language (O | BI * O * BI * ) * , the language of valid BIO sequences for an even number of spans. We would like to calculate P ? (y | x, L) for y = B, O, B, I . We show an unambiguous automaton M for L (left), and a factor graph (right) for the auxiliary CRF computing P ? (y ? | x), where y ? ? Y ? * corresponds to the sole accepting path of y through M (marked).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>OUTSIDE, p)}; for r ? R noncore do s ? (r, p);Q ? Q ? {s}; E ? E ? {(p, (BEGIN, r), p), (p, (BEGIN, r), s)}; E ? E ? {(r, (INSIDE, r), s), (r, (INSIDE, r), p)}; end for r ? R continuation do if The core role corresponding to r is in p then s ? (r, p); Q ? Q ? {s}; E ? E ? {(p, (BEGIN, r), p), (p, (BEGIN, r), s)}; E ? E ? {(r, (INSIDE, r), s), (r, (INSIDE, r), p)}; end end for r ? (R core \ p) do s ? (r, p); t ? p ? {r}; Q ? Q ? {s}; E ? E ? {(p,(BEGIN, r), s), (p, (BEGIN, r), t)}; E ? E ? {(s, (INSIDE, r), s), (s, (INSIDE, r), t)}; end end return (?, Q, q 1 , F, E) Algorithm 1: Construction of an FSA from given sets of core, noncore, and continuation roles. To represent BIO labels, we use tuples of the form (BEGIN, &lt;roleType&gt;) for B labels, tuples of the form (INSIDE, &lt;roleType&gt;) for I labels, and the symbol OUTSIDE for the sole O label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Output distributions for constrained decoding (P ?u (y | x, L)) and constrained training (P ?c (y | x, L)), compared to the data distribution P (y | x). Constrained decoding cannot learn the data distribution exactly, and yields a mode which disagrees with that of the data distribution.</figDesc><table><row><cell>y</cell><cell cols="3">P (y | x) P ?u (y | x, L) P ?c (y | x, L)</cell></row><row><cell>acd</cell><cell>0.4</cell><cell>0.32</cell><cell>0.40</cell></row><row><cell>bcd</cell><cell>0.3</cell><cell>0.48</cell><cell>0.30</cell></row><row><cell>bce</cell><cell>0.3</cell><cell>0.20</cell><cell>0.30</cell></row><row><cell cols="4">7 REAL-WORLD DATA EXPERIMENT: SEMANTIC ROLE LABELING</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results from our experiments (averaged over 5 trials), along with selected reported results from recent literature. We rank of our models by precision, recall, and F 1 score -rankings differ if and only if the difference is significant at p &lt; 0.05 (two-tailed), as measured by a permutation test.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">Precision (rank) Recall (rank)</cell><cell>F 1</cell><cell>(rank)</cell></row><row><cell>Our experiments</cell><cell>RoBERTa + CRF (CRF-full) RoBERTa + CRF (CRF-reduced) RoBERTa + RegCCRF (ConstrDec)</cell><cell>86.82 (2) 87.33 (1) 87.28 (1)</cell><cell>87.73 (1) 85.95 (3) 87.13 (2)</cell><cell cols="2">87.27 (2) 86.63 (3) 87.20 (2)</cell></row><row><cell></cell><cell>RoBERTa + RegCCRF (ConstrTr)</cell><cell>87.22 (1)</cell><cell>87.79 (1)</cell><cell cols="2">87.51 (1)</cell></row><row><cell>Results from literature</cell><cell>He et al. (2017) Ouchi et al. (2018) Ouchi et al. (2018) (ensemble) Li et al. (2019)</cell><cell>-87.1 88.5 85.7</cell><cell>-85.3 85.5 86.3</cell><cell cols="2">85.5 86.2 87.0 86.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Python scripts for reproducing all experiments presented in the paper, detailed descriptions of our datasets and preprocessing steps, and training logs. Furthermore, Appendix A lists all model hyperparameters, details the preprocessing steps taken for our experiments, and specifies the hardware used for our experiments along with average training and inference times for each experiment.</figDesc><table><row><cell>Python library under the</cell></row><row><cell>Apache 2.0 license which can be used as a drop-in replacement for standard CRFs in PyTorch. 5</cell></row><row><cell>ACKNOWLEDGMENTS</cell></row><row><cell>This work is supported by IBM Research AI through the IBM AI Horizons Network.</cell></row><row><cell>REPRODUCIBILITY STATEMENT</cell></row><row><cell>To ensure reproducibility, we have released all code for the RegCCRF model as an open-source</cell></row><row><cell>Python 3 library under the Apache 2.0 license, which is included in the supplementary materials.</cell></row><row><cell>Additionally, we include</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of hyperparameters for our models and experiments.</figDesc><table><row><cell>CRFs</cell><cell>Transition score initialization</cell><cell>N (0, 0.1)</cell></row><row><cell></cell><cell>Emission score initialization</cell><cell>PyTorch default</cell></row><row><cell></cell><cell>Optimizer</cell><cell>SGD</cell></row><row><cell>Synthetic data</cell><cell>Training iterations</cell><cell>5000</cell></row><row><cell>experiments</cell><cell>Batch size</cell><cell>50</cell></row><row><cell></cell><cell>Initial learning rate</cell><cell>1.0</cell></row><row><cell></cell><cell>Learning rate decay</cell><cell>10% every 100 steps</cell></row><row><cell></cell><cell>RoBERTa weights</cell><cell>roberta-base</cell></row><row><cell></cell><cell>Projection weight and bias initialization</cell><cell>PyTorch default</cell></row><row><cell>SRL experiments</cell><cell>Optimizer Learning rate</cell><cell>Adam 10 ?5</cell></row><row><cell></cell><cell>Batch size</cell><cell>2</cell></row><row><cell></cell><cell>Gradient accumulation</cell><cell>4 batches</cell></row><row><cell cols="2">A EXPERIMENTAL DESIGN</cell><cell></cell></row><row><cell cols="3">This appendix details the training procedures and hyperparameter choices for our experiments.</cell></row><row><cell>These are summarized in</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As downloaded from https://catalog.ldc.upenn.edu/LDC2013T19, and preprocessed according to https://cemantix.org/data/ontonotes.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As available from https://www.cs.upc.edu/?srlconll/soft.html. 3 All significance results are at the p &lt; 0.05 level (two-tailed), as measured by a permutation test over the five trials of each model.4  For CRF-full, we only count violations of constraints for those roles that our automaton accounts for.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Available at www.ims.uni-stuttgart.de/en/research/resources/tools/regccrf/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Guided open vocabulary image captioning with constrained beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1098</idno>
		<ptr target="https://www.aclweb.org/anthology/D17-1098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="936" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inference of regular expressions for text extraction from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">De</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Medvet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabiano</forename><surname>Tarlao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2016.2515587</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1217" to="1230" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deterministic regular languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Br?ggemann-Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derick</forename><surname>Wood</surname></persName>
		</author>
		<idno>Hei- delberg. ISBN 978-3-540-46775-5</idno>
	</analytic>
	<monogr>
		<title level="m">STACS 92</title>
		<editor>Alain Finkel and Matthias Jantzen</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>M?rquez</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W05-0620" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimal linguistic decoding is a difficult computational problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De La Higuera</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-8655(99</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="1999-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regular language induction with genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Dunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Petry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Buckles</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICEC.1994.349918</idno>
		<ptr target="https://nvlpubs.nist.gov/nistpubs/jres/71B/jresv71Bn4p233_A1b.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence</title>
		<meeting>the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
	<note>Jack Edmonds. Optimal branchings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parameter estimation for probabilistic finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno>doi: 10.3115/ 1073083.1073085</idno>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="2" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P11-2008.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inapproximability of nondeterministic state and transition complexity assuming P = NP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Holzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Developments in Language Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural machine translation decoding with terminology constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2081</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-2081" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="506" to="512" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and what&apos;s next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1044</idno>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lexically constrained decoding for sequence generation using grid beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1141</idno>
		<ptr target="https://www.aclweb.org/anthology/P17-1141" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1535" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimal NFA problems are hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bala</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="629" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial structured prediction models: Applications, challenges, and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2975584</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="38714" to="38727" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298990</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech and Language Processing</title>
		<imprint>
			<publisher>Prentice-Hall, Inc., USA</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">0131873210</biblScope>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive information extraction with constrained conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trausti</forename><surname>Kristjansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
		<ptr target="https://www.aclweb.org/anthology/N16-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dependency or span, end-to-end uniform semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016730</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/4645" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6730" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<ptr target="https://arxiv.org/abs/1907.11692" />
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A better n-best list: Practical determinization of weighted finite tree automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N06-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A disambiguation algorithm for finite automata and functional transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="265" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regular approximation of context-free grammars through transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark-Jan</forename><surname>Nederhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robustness in language and speech technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
	<note>ISBN 1558606149</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparsemap: Differentiable sparse structured inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3799" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A span selection model for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1191</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1191" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="1630" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
		<idno>doi: 10.1162/ 0891201053630264</idno>
		<ptr target="https://www.aclweb.org/anthology/J05-1004" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W12-4501" />
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-017-2390-9_10</idno>
		<ptr target="https://doi.org/10.1007/978-94-017-2390-9_10" />
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing Using Very Large Corpora</title>
		<meeting><address><addrLine>Netherlands, Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="157" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weighting finite-state transductions with neural context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1076</idno>
		<ptr target="https://aclanthology.org/N16-1076" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="623" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W04-2401" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighth Conference on Computational Natural Language Learning<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-05-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>HLT-NAACL 2004</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-markov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W Cohen ; Christian</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pad?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1164</idno>
		<ptr target="https://aclanthology.org/P16-1164" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany, Au</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1736" to="1745" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collective segmentation and labeling of distant entities in information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML 2004 Workshop on Statistical Relational Learning</title>
		<meeting>the ICML 2004 Workshop on Statistical Relational Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.1967.1054010</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ontonotes: A large training corpus for enhanced processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Belvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
