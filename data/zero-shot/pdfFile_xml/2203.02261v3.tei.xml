<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class-Aware Contrastive Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fan-yang20@mails.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guannan</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Southern University of Science</orgName>
								<address>
									<addrLine>Technolog 4 CATL</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<email>zhangwei@catl.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Long</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Class-Aware Contrastive Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pseudo-label-based semi-supervised learning (SSL) has achieved great success on raw data utilization. However, its training procedure suffers from confirmation bias due to the noise contained in self-generated artificial labels. Moreover, the model's judgment becomes noisier in real-world applications with extensive out-of-distribution data. To address this issue, we propose a general method named Class-aware Contrastive Semi-Supervised Learning (CCSSL), which is a drop-in helper to improve the pseudo-label quality and enhance the model's robustness in the real-world setting. Rather than treating real-world data as a union set, our method separately handles reliable in-distribution data with class-wise clustering for blending into downstream tasks and noisy out-of-distribution data with image-wise contrastive for better generalization. Furthermore, by applying target re-weighting, we successfully emphasize clean label learning and simultaneously reduce noisy label learning. Despite its simplicity, our proposed CCSSL has significant performance improvements over the state-of-the-art SSL methods on the standard datasets CIFAR100 <ref type="bibr" target="#b17">[18]</ref> and STL10 [8]. On the real-world dataset Semi-iNat 2021 [27], we improve FixMatch [25] by 9.80% and CoMatch [19] by 3.18%. Code is available https://github.com/TencentYoutuResearch/Classification-SemiCLS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Raw data utilization is becoming a research focal point for its high accessibility and high affordability. The definition of raw data is the union set of in-distribution data (known classes and balanced distribution) and out-ofdistribution data <ref type="bibr" target="#b32">[32]</ref> (unknown classes or unbalanced distribution), as shown in <ref type="figure">Fig. 1</ref>. For in-distribution datasets, * Equal contribution. ? Corresponding author.  <ref type="figure">Figure 1</ref>. Intuition graph for class-aware contrastive semisupervised learning. (a) represents real-world unlabeled data containing in-distribution and out-of-distribution data (unbalanced distribution or unknown classes). Unlike pseudo-label-based semi-supervised learning in (b), which wrongly clusters noisy outof-distribution data, CCSSL in (c) reduces noise by image-level contrastive learning on out-of-distribution data while maintaining the class-aware clustering ability for in-distribution data.</p><p>semi-supervised learning (SSL) has achieved excellent performance with the help of pseudo labeling <ref type="bibr" target="#b24">[25]</ref>  <ref type="bibr" target="#b2">[3]</ref> [2] <ref type="bibr" target="#b12">[13]</ref>. The primary process for the pseudo-label based SSL is iterative training: 1) creating self-generated pseudo labels on raw data, 2) training on pseudo labels, 3) repeating 1 and 2. The underlying assumption for training on pseudo labels is that the distribution of the labeled data is close to the unlabeled, and the unlabeled dataset does not contain any novel categories. This assumption often does not hold in the real-world applications with extensive out-ofdistribution data, which contains unbalanced distribution or unknown classes. SSL's training and labeling loop collapses in the real-world because of the enormous noise introduced by pseudo labeling on out-of-distribution data. The confirmation bias <ref type="bibr" target="#b0">[1]</ref> induced by noisy pseudo labels deteriorates SSL performance by a large margin <ref type="bibr" target="#b26">[26]</ref>. SSL has used many techniques to alleviate the confirmation bias and achieved state-of-the-art results on standard indistribution benchmarks, like CIFAR <ref type="bibr" target="#b17">[18]</ref> and STL10 <ref type="bibr" target="#b7">[8]</ref>. Some methods achieve this goal by using the model's selfcorrecting capability. In <ref type="bibr" target="#b24">[25]</ref>  <ref type="bibr" target="#b2">[3]</ref> [2] <ref type="bibr" target="#b24">[25]</ref>, a high confidence threshold is used to filter incorrectly pseudo-labeled data. <ref type="bibr" target="#b34">[34]</ref> [20] <ref type="bibr" target="#b29">[29]</ref> use a self-ema teacher to generate pseudo labels with the assumption that the weighted averaged model produces more stable and less noisy predictions. Some methods <ref type="bibr" target="#b28">[28]</ref>  <ref type="bibr" target="#b31">[31]</ref> try to narrow the distribution gap between raw data and labeled data by predictions' uncertainty. However, justifying a model's prediction by its output without introducing other information still suffers from confirmation bias <ref type="bibr" target="#b20">[21]</ref>. Especially on the real-world data with unbalanced distribution or unknown classes, the effect of a model's self-correcting is facing a huge challenge. By explicitly introducing contrastive information, our CCSSL alleviates the confirmation bias in the feature space and shows great de-noise ability on the real-world data.</p><p>Another trend is using prior or posterior information to co-rectify a model's predictions. Both <ref type="bibr" target="#b10">[11]</ref> [5] use a pretrained model as a universal prior information. In <ref type="bibr" target="#b20">[21]</ref>, by knowledge embedding graph, semantic information is used to regularize feature learning. Although the above methods are proved helpful, fixed prior is hard to be blended into downstream tasks, resulting in inferior performance in practice. <ref type="bibr" target="#b18">[19]</ref> utilizes posterior information by constructing a prediction graph and introducing contrastive learning from self-supervised learning (Self-SL). However, directly combining image-level feature repulsion interferes with SSL's class-clustering ability. To solve this conflict, in our proposed CCSSL, a class-aware contrastive module is specifically explored for reliable in-distribution data clustering and noisy out-of-distribution data contrasting to better integrate with downstream classification tasks.</p><p>To this end, we propose a class-aware contrastive semisupervised learning (CCSSL) method. CCSSL consists of a semi-supervised module and a class-aware contrastive module. Any end-to-end pseudo-label-based SSL can replace the semi-supervised module to benefit from the classaware contrastive module's confirmation bias alleviation ability. Unlike self-correcting methods that try to alleviate noise by model's own output, CCSSL regularizes the training process by introducing information in the feature space. As shown in the <ref type="figure">Fig. 1</ref>, CCSSL constructs the feature space with high dimensional vectors from two strong augmented views. Rather than directly combining contrastive learning, CCSSL uses class-aware clustering on indistribution data to maintain SSL's clustering ability and image-level contrasting on out-of-distribution data for noise alleviation. Furthermore, to reduce the implicit noise introduced from the class-aware clustering, we incorporate a target re-weighting module that emphasizes learning on high probable in-distribution samples and reduces the effect of uncertain noisy samples. We found that with the help of the re-weighting module, the prediction confidence of learned models is robust enough for roughly discriminating in-distribution and out-of-distribution data.</p><p>Our contributions can be described in three folds:</p><p>? We propose a novel SSL learning method CCSSL, which takes advantage of both SSL's effective learning and Self-SL's noise alleviation ability by class-wise clustering on in-distribution data and image-wise contrasting on out-of-distribution data. ? CCSSL is a drop-in helper that can improve upon any end-to-end pseudo-label-based SSL method for confirmation bias alleviation and faster convergence. CC-SSL's noise alleviation capability makes SSL methods more practical in the real-world setting. ? We thoroughly analyzed the effect of CCSSL with various state-of-the-art SSL methods on both in-distribution data and real-world data. By simply combining CCSSL, current state-of-the-art SSL methods can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we describe the recent trends of deep learning based SSL and contrastive based Self-SL. Then we analyze specific methods aiming at using on the real-world setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-Supervised Learning</head><p>Deep learning based SSL mainly consists of pseudolabeling and consistency regularization. Pseudo-labelbased methods utilize unlabeled data by training on selfgenerated predictions. The self-training process validates the model itself and can also be named as selfconfirming <ref type="bibr" target="#b8">[9]</ref>. However, using pseudo labels suffers confirmation bias <ref type="bibr" target="#b0">[1]</ref> because it is easy to overfit incorrect predictions during training. Both <ref type="bibr" target="#b24">[25]</ref> [3] use high confidence predictions to filter noisy data. <ref type="bibr" target="#b14">[15]</ref> makes a detour by label propagation to spread labeled data distribution on unlabeled data. Consistency based methods <ref type="bibr" target="#b15">[16]</ref> [29] <ref type="bibr" target="#b24">[25]</ref> aim to produce consistent predictions on different image views based on the manifold assumption that different views of the same image should lie on the same point in the high dimensional space. <ref type="bibr" target="#b24">[25]</ref> [30] create different views of an image by strong augmentations to simulate image perturbations of an object. Lately, pseudo-label-based consistency training <ref type="bibr" target="#b24">[25]</ref> [3] <ref type="bibr" target="#b1">[2]</ref>, which use a weak augmentation for creating pseudo labels and a strong augmentation for consistency training, has dominated and achieved excellent per- <ref type="figure">Figure 2</ref>. Framework of our proposed CCSSL. Given a batch of unlabeled images, the weakly augmented views will go through a semisupervised module that can be borrowed from any pseudo-label-based SSL to generate model predictions. With pseudo labels, we make a supervised contrastive matrix with only class-level information. Then, the class-aware contrastive matrix is formed by image-level contrasting on out-of-distribution data to reduce confirmation bias. By applying a re-weighting module, we focus learning on clean data and get the final target matrix. Besides, feature affinity matrix is made by two strong augmented views. Class-aware contrastive module for unlabeled data is formulated by minimizing cross-entropy between the affinity matrix and the target matrix. formance gain. However, the research on the real-world setting with extensive out-of-distribution data has been less studied. With evaluating in-distribution data and real-world data, we show CCSSL's ability of alleviating confirmation bias in practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-Supervised Learning</head><p>Self-SL has a long story for learning a universal representation without supervision. <ref type="bibr" target="#b11">[12]</ref> [14] build the base for contrastive learning by proposing and refining the noise contrastive estimation (NCE). Based on NCE, <ref type="bibr" target="#b21">[22]</ref> proposes infoNCE which inspires learning representations to discriminate different views of a sample among other samples. Moco <ref type="bibr" target="#b10">[11]</ref> and simCLR <ref type="bibr" target="#b4">[5]</ref> train a siamses network <ref type="bibr" target="#b5">[6]</ref> to learn image-wise feature by pulling similar (positive) samples and pushing apart different (negative) samples <ref type="bibr" target="#b6">[7]</ref>. Inspired by contrastive learning, we formulate positive pairs and negative pairs in CCSSL to regularize feature training. However, directly combining contrastive learning ignores the internal conflict between image-level feature repulsion and SSL's class-level feature clustering. By incorporating class-level information, CCSSL can seamlessly blend into downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Real-World Unlabeled Data Learning</head><p>Real-world unlabeled data usually contains out-ofdistribution data with unbalanced class distribution or unknown classes. Current pseudo-label-based SSL suffers from confirmation bias by incorrect pseudo-labeling on similar unknown classes or blurry images from out-ofdistribution data, resulting in worse performance than supervised baseline <ref type="bibr" target="#b26">[26]</ref>. Open set classification is a subfield of real-world unlabeled data learning focusing on novel categories. <ref type="bibr" target="#b9">[10]</ref> keeps tracking the effect of the supervised learning model to prevent performance hazards. By incorporating deep-construction-based representation learning, <ref type="bibr" target="#b33">[33]</ref> learns to distinguish unknowns from knowns. Open set classification methods are proven to be effective on handwriting datasets or self-constructed datasets but seldom generalize on real-world data. <ref type="bibr" target="#b26">[26]</ref> conducts a systematic evaluation of several recently proposed SSL techniques on a real-world setting by two challenging datasets and shows that recent SSL's performance pales on realworld datasets in comparison to in-distribution standard datasets. Therefore, this paper further explores the realworld setting and narrows the gap between research and real-world applications by confirmation bias alleviation for out-of-distribution data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>In the setting of a semi-supervised classification task, let X = {(x i , y i ) : i ? (1, ..., B)} be a batch of labeled data , where x i is the ith image and y i represents its one-hot label, and U = {u i : i ? (1, ..., ?B)} be a batch of unlabeled data , where ? is a hyperparameter that determines the relative ratio of X and U. In the real world, unlabeled data U contains in-distribution data U in and out-of-distribution data U out , which makes pseudo labels more susceptible to confirmation bias <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework</head><p>Our proposed CCSSL consists of a replaceable semisupervised module and a class-aware contrastive module, as in <ref type="figure">Fig. 2</ref>. Our method focuses on introducing classaware contrastive information to alleviate confirmation bias for semi-supervised module especially in real-world setting. Any pseudo-label-based SSL can be used as the semisupervised module to enjoy the benefit. The notations are as follows:</p><p>Data augmentation provides different views of a single image. For a labeled image x i , a weak augmentation is used for supervised learning. For an unlabeled image u i , we apply a weak augmentation Aug w (?) and two strong augmentations Aug s (?). The Aug w and one Aug s are sent to semisupervise module, while all Aug s are used for class-aware contrastive learning.</p><p>Encoder F (?) is used to extract representation r = F (Aug(x)) for a given input x.</p><p>Semi-Supervised module can be replaced by any pseudo-label based semi-supervised learning method. The module generates pseudo labels during training and outputs the final prediction at inference time by a classification head P cls (?). We use FixMatch <ref type="bibr" target="#b24">[25]</ref> in the method for simplicity.</p><p>Class-Aware contrastive module constructs an embedding space by mapping the high-dimensional representation r to a low-dimensional embedding z through the projection head P proj (?). We propose the class-aware contrastive learning in the embedding space to improve the robustness for out-of-distribution data and compatibleness with downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semi-Supervised Module</head><p>Following the general form of pseudo-label-based SSL, the module contains a supervised loss L x and an unsupervised loss L u . L x is the same as <ref type="bibr" target="#b24">[25]</ref>. L u relies on pseudo labelsq i = arg max(p i ) from the model's prediction p i = P cls (Aug w (u i )) on the weak view of image u i . Only pseudo labels with high confidence q ? t, where q = max(p), will be retained for consistency training. H means cross entropy:</p><formula xml:id="formula_0">L u = 1 ?B ?B i=1 1(max(p i ) ? t)H(q i , P cls (Aug s (u i )).</formula><p>(1) However, the manual set threshold t is difficult to guarantee the accuracy of pseudo labelsq, especially on real-world setting with out-of-distribution data. The phenomenon of overfitting on self-generated labels is called confirmation bias <ref type="bibr" target="#b0">[1]</ref>, and it can hardly be alleviated by the model itself during training. Therefore, we propose a novel class-aware contrastive learning method, which introduces unbiased information in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Class-Aware Contrastive Module</head><p>In <ref type="figure">Fig. 2</ref>, we seamlessly integrate clustering and contrasting in the feature space and apply re-weighting for attentive training. Given unlabeled images in a batch, we separate images into two parts based on the likelihood of in-distribution and out-of-distribution, in which T push is used as the threshold for separation from the model's prediction confidence. For highly possible in-distribution data max(p) &gt; T push , we apply class-aware clustering in the feature space to seamlessly blend into downstream tasks. For out-of-distribution data max(p) &lt; T push , we follow contrastive learning mechanism <ref type="bibr" target="#b4">[5]</ref> by taking a image's views as positive samples and others as negative. Then we apply foreground probability re-weighting for each sample to focus learning on high confidence clean data. Next, we will show how to formulate the method step by step.</p><p>Contrastive learning aims to learn a universal prior information for downstream tasks. As in <ref type="bibr" target="#b10">[11]</ref> [5], we randomly sample a minibatch of N images and each image u i has two strong augmentations Aug s which are used to extract features r through encoder F (?). Then, r is mapped to obtain a square normalized low-dimensional embedding z by the projection head. Finally, we get an embedding's affinity matrix S ? R 2N ?2N by the dot product of embeddings Z = {z i : i = 1, ..., 2N } with a temperature factor ? , as follow:</p><formula xml:id="formula_1">s ij = exp(z i ? z j /? ),<label>(2)</label></formula><p>Then, a constrastive matrix W con ? R 2N ?2N for loss calculation with S is formulated as:</p><formula xml:id="formula_2">w con ij = ? ? ? ? ? ? ? 1 if j = i, 1 if z i and z j are from the same image ? s views, 0 otherwise.</formula><p>(3) The self-supervised contrastive loss InfoNCE <ref type="bibr" target="#b21">[22]</ref> takes the following format:</p><formula xml:id="formula_3">L InfoNCE = ? 2N i=1 log exp(z i ? z * i /? ) 2N j=1 1 (j? =i) exp(z i ? z j /? ) ,<label>(4)</label></formula><p>where z * i is from the other Aug s of the same image as z i . The ideal solution for contrastive learning is to push each image far from others and pull same image views closer in the feature space. The property is effective to alleviate noise on the out-of-distribution data. However, it is not compatible with the classification task which requires to cluster images at the class level. To solve the conflict between contrastive learning and downstream tasks, we formulate the class-aware contrastive module to take advantages of both Self-SL's generalizability and SSL's efficient training.</p><p>Class-Aware Contrastive learning considers classaware information for in-distribution data while alleviating noise for out-of-distribution data. Inspired by <ref type="bibr" target="#b16">[17]</ref>, we consider the embeddings z from the same category as positive pairs instead of the same image with the help of the pseudolabelsq from the semi-supervised module to form supervised contrastive matrix W scon in <ref type="figure">Fig. 2</ref>. However, classwise clustering by pseudo-labels is unsafe to use because a model's prediction may contain a large amount of noises especially for out-of-distribution data, as explained in Sec. 1. Therefore, we introduce contrastive learning for noise regularization. We use T push as the threshold for clustering and contrasting in the feature space, and then W scon is tranformed to class-aware contrastive matrix W clacon as shown in <ref type="figure">Fig. 2</ref>. For model's prediction p &gt; T push , we assume the images have a high probability to be in-distribution data U in and should be pulled closer with the same class. For p &lt; T push , our formula degenerates to contrastive learning where only Aug s of the same image are positive pairs. Each element w clacon ij in W clacon has the following format:</p><formula xml:id="formula_4">w clacon ij = ? ? ? ? ? ? ? 1 if j = i, 1</formula><p>if z i and z j are from same category, and both q i and q j &gt; T push , 0 otherwise,</p><p>where i and j are indices of z for augmented sample. q i denotes the pseudo label confidence score of the ith augmented views. Although class-aware contrastive is theoretically functional, directly using T push to judge the cleanness of data is problematic since images just above the T push might be out-of-distribution data too. To solve this issue, we propose a foreground re-weighting module to reduce the noise effect on out-of-distribution data.</p><p>Foreground Re-weighting is used to further emphasize training on in-distribution data with high confidence and weaken the potential bias brought from out-of-distribution data. We formulate the final target matrix W target by re-weight W clacon with the foreground probability from model's prediction on Aug w . Simply by multiplying confidence q of two samples, we get a re-weighting factor w re = q i ? q j , where i and j represent the indices of row and column in W target , respectively. Each element w target ij of W target is defined as follows:</p><formula xml:id="formula_6">w target ij = q i ? q j ? w clacon ij if i ? = j, w clacon ij otherwise.<label>(6)</label></formula><p>The loss of the class-aware contrastive module L c is formulated by minimizing the cross-entropy between S in Eq. (2) and W target . The loss of L c can be defined as:</p><formula xml:id="formula_7">L c = H(S, W target ) = 2N i=1 1 1 + |P (i)| L c,i ,<label>(7)</label></formula><p>where H denotes cross entropy and P (i) represents indices of the views from other images of the same class with confidence p &gt; T push . |P (i)| denotes its cardinality and |P (i)| + 1 represents all positive pairs. L c,i as below:</p><formula xml:id="formula_8">L c,i = ? log exp(z i ? z * i /? ) 2N j=1 1 j? =i exp(z i ? z j /? ) ? p?P (i) w ip ? log exp(z i ? z p /? ) 2N j=1 1 j? =i exp(z i ? z j /? ) ,<label>(8)</label></formula><p>where z * i is the embedding of Aug s rather than z i from the same image. We calcualte the final total loss Eq. (9) using a weighted sum of supervised loss L x , semi-supervised loss L u and class-aware contrastive loss L c . L x and L u are from Sec. 3.3. ? u and ? c are the weights for semisupervised loss and class-aware contrastive loss, respectively.</p><formula xml:id="formula_9">L = L x + ? u L u + ? c L c<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Applications</head><p>CCSSL method can be used with any pseudo-label-based SSL to alleviate confirmation bias in the training process. Applying CCSSL to SSL methods is simple. We applied the CCSSL to the mainstream SSL methods, like MixMatch <ref type="bibr" target="#b2">[3]</ref>, FixMatch <ref type="bibr" target="#b24">[25]</ref> and CoMatch <ref type="bibr" target="#b18">[19]</ref>. MixMatch and Fix-Match are based on pseudo-labels and have a strong augmentation Aug s for unlabeled data learning. We create another Aug s for class-aware contrastive module, which trains parallelly with the original network. CoMatch also incorporates contrastive learning, but no class-wise clustering is involved. Since CoMatch already has two Aug s , we directly use them for class-aware contrastive module. Our experiments in Sec. 4 show CCSSL not only improves the performance for SSL methods but also speeds up the training process.  <ref type="table">Table 2</ref>. Accuracy for real-world dataset -Semi-iNat 2021. We evaluated two scenarios: random starting with serious noise and pre-trained starting with less noise. In the noisy random starting scenario, CCSSL achieves significant performance gain by noise alleviation. In the noiseless pretrained starting scenario, CCSSL achieves marginal improvement because noise has been reduced by the pretrained model in the beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first evaluate the effectiveness of CC-SSL on in-distribution datasets, which may also contain blur out-of-distribution cases. Then, we show CCSSL's ability of confirmations bias alleviation on real-world data. Last, a qualitative comparison is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We assume that the harder the dataset, the higher noise it has, so we conduct experiments on the following different noise level datasets.</p><p>CIFAR10 <ref type="bibr" target="#b17">[18]</ref> is a relatively easy-level in-distribution dataset consisting of 60K 32x32 colour images in 10 classes, with 6K images per class. There are 50K training images and 10K test images. We conduct three different experiments on 40, 250, 4000 random selected images in the class balanced way, following <ref type="bibr" target="#b24">[25]</ref>, as in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Origin</head><p>FixMatch CCSSL CoMatch CCSSL (FixMatch) (CoMatch) <ref type="table">Table 3</ref>. Qualitative comparison on FixMatch and Comatch with/without CCSSL by Grad-CAM <ref type="bibr" target="#b23">[24]</ref>. With CCSSL's de-noise effect, an SSL method focuses more on the foreground objects. CCSSL methods find all ants for the ant image and capture the correct position on the leaf image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STL10</head><p>[8] is a relatively medium-level in-distribution dataset comprises 10 classes. The training set has 5K images (10 predefined folds), 100k unlabeled images containing similar distribution with training images and 800 test images per class. All images are 96x96 pixels. We conduct experiments on all folds following <ref type="bibr" target="#b24">[25]</ref>, as shown in Tab. 1 CIFAR100 [18] is a relatively hard-level in-distribution dataset comprises 100 classes. The training set contains 50K images, while test contains 10K images. All images have a fixed resolution of 32x32. We conduct three different experiments on 400, 2500, 10000 random selected images in a class-balanced way, following <ref type="bibr" target="#b24">[25]</ref>, as shown in Tab. 1.</p><p>Semi-iNat 2021 <ref type="bibr" target="#b27">[27]</ref> is a real-world dataset designed to expose some of the challenges encountered in a realistic setting, such as significant class imbalance, domain mismatch between the labeled and unlabeled data, and large unknown classes. The labeled training and val part each has 9721 and 4050 images. The unlabeled training part has 313248 images. Experiments in Tab. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">In-distribution Data Evaluation</head><p>For CIFAR10, we set ? c = 0.2 with T push = 0 due to low noise level of the dataset and no out-of-distribution data. For high noise level STL10 and CIFAR100, we set ? c = 1.0 with T push = 0. We train 512 epochs for all experiments because of fast convergence speed. For other hyperparameters, optimizer and learning rate schedule, our setting is the same as FixMatch <ref type="bibr" target="#b24">[25]</ref>. We compare CCSSL with MixMatch <ref type="bibr" target="#b2">[3]</ref>, ReMixMatch <ref type="bibr" target="#b1">[2]</ref>, FixMatch <ref type="bibr" target="#b24">[25]</ref>, SS-WPL <ref type="bibr" target="#b28">[28]</ref>, LaplaceNet <ref type="bibr" target="#b22">[23]</ref> and Comatch <ref type="bibr" target="#b18">[19]</ref>. Since the last three SSL methods can be viewed as an extension of FixMatch, we directly apply CCSSL on FixMatch for a fair comparison.</p><p>In Tab. 1, We find that the performance of CCSSL is higher when the task is noisy and more confirmation bias involved. On CIFAR100 and STL10, the noisiest indistribution datasets we use, we achieve the best performance only by adding CCSSL on FixMatch. CCSSL is also on par with state-of-the-art methods on CIFAR10. On CI-FAR100, we achieve +10.04%/ +3.99%/ +3.26% over origin FixMatch and +3.08%/ +2.22%/ +1.54% over the best of all compared SSL methods for 400, 2500, and 10000 labels respectively. The performance gain decline can be explained by the confirmation bias level decrease with more training data. For the medium hard STL10, we achieve +14.62% over origin FixMatch and only +0.2% over the best of all compared SSL methods. CCSSL's performance improvement decreases on the easier dataset with fewer categories. On CIFAR10, we achieve +4.64/ -0.07%/ -0.2% over FixMatch and -2.26%/ -0.23%/ -0.2% over the best of all compared SSL methods. With the decrease of noise level, our CCSSL's performance improvement pales compared with other variations of SSL methods tuned for indistribution datasets but is still helpful for FixMatch. Another thing that needs to be noticed is that when the noise level is the least (CIFAR10 with 4000 labels), FixMatch achieves the best performance by the simplest structurewhich means that when the data is clear and enough, the structure of SSL becomes less critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Realistic Evaluation</head><p>On Semi-iNat 2021, we simulate two situations with different noise levels -(high: training from scratch, low: training from pre-trained model), as shown in Tab. 2. The results show that CCSSL can improve SSL in both situations but more noise more improvement.</p><p>For Semi-iNat 2021, we use an image size of 224x224, resnet50 backbone, projection head with a dimension of 64 like in <ref type="bibr" target="#b10">[11]</ref>, and training on 4 V100 with batchsize 64 for labeled data and batchsize 448 with ? = 7 for unlabeled data. For FixMatch <ref type="bibr" target="#b24">[25]</ref>, pseudo label thresh ? = 0.8 achieves the best results, but 0.6 when combining with our method. For MixMatch <ref type="bibr" target="#b2">[3]</ref> and CoMatch <ref type="bibr" target="#b18">[19]</ref>, we directly follow their papers' setting. We set ? c = 2.0 with T push = 0.9 due to high noise level on out-of-distribution data. When training from Self-SL pre-trained models, we freeze the first three blocks of resnet50 because of the forgetting problem. Other settings are the same as <ref type="bibr" target="#b26">[26]</ref>.</p><p>In Tab. 2 training from scratch, state-of-the-art SSL methods only have on-par performance with the supervised baseline. Compared to the performance on in-distribution datasets, the noise in pseudo-labels harms SSL methods by a large margin. With class-aware contrastive module, we are able to alleviate confirmation bias on out-of-distribution data while maintaining downstream tasks' clustering ability. We improve state-of-the-art SSL by a large margin when using with CCSSL (+2.7%/ +9.8%/ +3.1% on MixMatch, FixMatch and CoMatch separately). The experiments show that CCSSL can be a big helper for pseudo-label-based SSL to reduce the negative impact from noisy predictions when training from scratch on the real-world dataset.</p><p>In Tab. 2, training from MoCo Semi-iNat pre-trained model simulates the situation where SSL starts with less noise <ref type="bibr" target="#b3">[4]</ref>. CCSSL only improves +1.05%/ +0.91%/ on Fix-Match and CoMatch. The result shows that when a model starts with less noise, our proposed CCSSL can also reduce noise introduced in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Evaluation</head><p>In Tab. 3, we show the qualitative results on FixMatch and CoMatch with/without CCSSL. With CCSSL's de-noise effect, an SSL method focuses more on the foreground objects. CCSSL methods find all ants for the ant image and capture the correct position on the leaf image. We choose FixMatch and CoMatch for visualization because the two are the best on Semi-iNat among SSL methods we tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Convergence Speed</head><p>In <ref type="figure" target="#fig_1">Fig. 3</ref>, we study the convergence speed using CC-SSL on MixMatch and FixMatch. We observe that CCSSL  <ref type="table">Table 5</ref>. Different ratios of unlabeled data on Semi-iNat with different degrees of class-aware (T push ). When T push = 0 (no contrastive involved), smaller ratios achieves better results because of noise in unlabeled data. After setting T push = 0.9 to regularize noise, larger ratio achieves better results. methods achieve the best performance with much fewer training iterations. This phenomenon is the same when training on clean pseudo-labels <ref type="bibr" target="#b18">[19]</ref> because of the noise alleviation effect of CCSSL. More specifically, better pseudo labels in the early phase helps better guide the learning process and translate to faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Contrastive &amp; Class-Aware &amp; Re-weighting</head><p>In Tab. 4, we investigate three main techniques of CC-SSL, including contrastive learning, class-aware, and reweighting, based on FixMatch. We observe that all components are helpful. However, directly using contrastive learning deteriorates the performance on in-distribution dataset CIFAR100 while largely benefiting the real-world dataset Semi-iNat 2021 with extensive out-of-distribution data. CCSSL found the equilibrium point for contrastive and clustering by class-aware contrastive and re-weighting, and improved FixMatch by a large margin on both indistribution and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ratio of Unlabeled Data</head><p>As shown in Tab. 5, we experiment on different values of ?, which defines the ratio of labeled and unlabeled data in a batch. We observe that when only using class-aware clustering without contrastive regularization (T push = 0), CCSSL achieves the best result with a small ratio of unla-  <ref type="table">Table 6</ref>. Experiments on different class-aware thresholds. Our algorithm is robust with varying T push from 0.6 to 0.9. Semi-iNat has a larger noise level and needs larger T push which proves CCSSL's ability for noise alleviation.</p><p>beled data (? = 5). However, after we increase the effect of contrastive learning by changing T push = 0.9 (sample conficence &lt; 0.9 will do contrastive learning), the larger ratio has better performance (? = 7). Less noisy dataset like CIFAR100 also benefits from large unlabeled data ratio <ref type="bibr" target="#b20">[21]</ref>. This phenomenon can be explained by the higher noise amount introduced from self-generated labels with larger ratio of unlabeled data. With CCSSL's noise alleviation, SSL methods achieve the best performance with a larger unlabeled ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Threshold for Class-Aware and Contrastive</head><p>We found that our algorithm is robust with varying the threshold T push from 0.6 to 0.9. However, the optimal threshold for class-aware and contrastive learning is inconsistent on different noise level datasets. For high noise level Semi-iNat with unbalanced distribution and unknown classes, T push = 0.9 achieves the best performance. For low noise level CIFAR100, T push = 0.4 achieves the best performance. The result proves CCSSL's noise-reducing ability by needing to regularize more on noisy datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>Potential harmful training fluctuation does exist in our experiments: performance on Semi-iNat fluctuates more than on CIFAR100 in the <ref type="figure" target="#fig_1">Fig. 3</ref>. The main reason lies in the lower quality of pseudo-labels for high noise-level data. Increasing the weight for the class-aware contrastive module or training with different seeds to average the effect of fluctuation can alleviate the problem. During our experiments, we fixed the weight and seed for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed CCSSL, a general confirmation bias alleviation method for pseudo-label-based SSL methods. CCSSL reduces noise by constructing another feature space instead of using a model's output. By class-aware contrastive module, we apply image-level contrastive on out-of-distribution data for noise alleviation and class-level clustering on indistribution data for blending into downstream tasks. We have conducted extensive experiments over different noise level datasets, including in-distribution datasets and a realworld dataset, to demonstrate the effectiveness of CCSSL. With simply replacing the semi-supervised module, we improve the state-of-the-art SSL <ref type="bibr" target="#b24">[25]</ref>  <ref type="bibr" target="#b2">[3]</ref> [19] by a large margin. With the effort above, CCSSL is proved to be helpful in making SSL more practical in the real world.</p><p>Potential negative societal impact CCSSL is a fundamental technology that can generate robust semi-supervised classification models in the real world. Maliciously using a classification model to negatively impact society is possible.   <ref type="figure">Figure 5</ref>. Confusion Matrix with/without CCSSL. STL10 is used for both because CIFAR10 is easy and CIFAR100 has too many categories to be visualized.</p><p>A. What is the intuition of using contrastive in SSL? Are there some proofs other than the final performance?</p><p>The intuition of CCSSL is to end-to-end reduce noise on the feature level by contrastive learning while maintaining model's clustering ability.</p><p>(1) Noise reduction by a projection head. The noise reduction is on the feature level and not directly on pseudo labels. Qualitatively, <ref type="bibr">[?]</ref> verified that the uniformity of embeddings from the projection head is essential to learn general separable features. As in <ref type="figure" target="#fig_2">Fig. 4</ref>, the features are more separable with CCSSL. Quantitatively, the model's confusion on similar categories is halved with CCSSL, as in <ref type="figure">Fig. 5</ref>. With CCSSL, the model can be more resilient to misclassification and thus has less noise on pseudo-labels. So if the model starts with a good pretrained model with good features, like MoCo, CCSSL can only benefit the training process and less helpful.</p><p>(2) Quantitative analysis for pseudo-labels. As in Tab. 7, the model's pseudo label quality is better with CCSSL on many data settings. The more noise (less labels), the more salient of improvements. We cannot evaluate pseudo-label accuracy on Semi-iNat 2021 and STL10 because labels are not provided. We will provide more evaluation results on our public github page https://github.com/TencentYoutuResearch/Classification-SemiCLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of method formulation</head><p>(1) Relationship with infoNCE. Our method improves upon the infoNCE, so we provide S, W con and L infoNCE as references and has not directly used them in the final equation for simplicity.</p><p>(2) Cross Entropy of L c . The cross-entropy between S and W target for L c in Eq. (9) means that we take columns as classes and rows as samples to calculate the soft crossentropy between two matrices. Soft means the targets have been re-weighted.</p><p>(3) Meaning of P i . z p is the high dimensional feature of sample p from P i , which represents the sample set with the same pseudo-label of sample i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of training</head><p>The training strategy is generally the same as FixMatch for a fair comparison.</p><p>(1) Warm-up Peroid. There is no warm-up period, and CCSSL can help a SSL model from a noisy starting. Because the confidence is low in the beginning, the contrastive part of CCSSL is triggered and helps the model learn a general representation instead of over-fitting on the training data. After the model learns a few epochs and can differen-tiate in-distribution and out-of-distribution data, the clustering part of CCSSL will then be triggered with less noise.</p><p>(2) Augmentations. The augmentations of supervised and semi-supervised branches are the same as FixMatch, and the strong augmentation of CCSSL branch is the same as MoCo for a fair comparison.</p><p>(3) Backbone. The model size is the same as FixMatch during inference and the backbone follows the convention with wid-resnet 28-2, 28-8 for CIFAR10 and CIFAR100, resnet18 for STL10, resnet50 for Semi-iNat.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Real-World Data With In-Distribution and Out-of-Distribution Data (b) Pseudo-Label-Based SSL (c) Class-Aware Contrastive SSL (ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Convergence speed and performance difference on Mix-Match and FixMatch with/without CCSSL. As shown in the figure, CCSSL methods achieves better performance with much fewer training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>TSNE of high dimensional features with/without CC-SSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>39?1.32 60.06?0.37 71.69?0.33 52.46?11.5 88.95?0.86 93.58?0.10 38.02?8.29 ReMixMatch [2] 55.72?2.06 72.57?0.31 76.97?0.56 80.90?9.64 94.56?0.05 95.28?0.13 11?2.34 71.63?0.35 79.14?0.36 93.09?1.39 95.09?0.33 95.44?0.20 79.80?0.38 FixMatch [19] 51.15?1.75 71.71?0.11 77.40?0.12 86.19?3.37 94.93?0.65 95.74?0.05 65.38?0.42 CCSSL(FixMatch) 61.19?1.65 75.7?0.63 80.68?0.16 90.83?2.78 94.86?0.55 95.54?0.20 80.01?1.39 Table 1. Top-1 Accuracy for in-distribution datasets including CIFAR100, CIFAR10, and STL10. On high noise-level datasets CIFAR100 and STL10, we achieve the best performance by simply adding CCSSL to Fixmatch. On the easier dataset CIFAR10 with less noise, CCSSL only provides marginal performance gain. '-' means not self-implemented.</figDesc><table><row><cell>Method</cell><cell></cell><cell>400</cell><cell>CIFAR100 2500</cell><cell>10000</cell><cell>40</cell><cell>CIFAR10 250</cell><cell>4000</cell><cell>STL10</cell></row><row><cell>Mixmatch [3]</cell><cell></cell><cell cols="7">32.-</cell></row><row><cell>SSWPL [28]</cell><cell></cell><cell>-</cell><cell cols="2">73.48?0.45 79.12?0.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">LaplaceNet [23]</cell><cell>-</cell><cell cols="2">68.36?0.02 73.40?0.23</cell><cell>-</cell><cell>-</cell><cell>95.35?0.07</cell><cell>-</cell></row><row><cell cols="4">CoMatch [19] 58.Method Semi-iNat 2021 From Scratch From MoCo Pretrain Top1 Top5 Top1 Top5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised</cell><cell cols="3">19.09 35.85 34.96 57.11</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MixMatch [3] 16.89 30.83 +CCSSL 19.65 35.09 FixMatch [25] 21.41 37.65 40.3 60.05 ----+CCSSL 31.21 52.25 41.28 64.3 CoMatch [19] 20.94 38.96 38.94 61.85 +CCSSL 24.12 43.23 39.85 63.68</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>'Class-Aware Contrastive Semi-Supervised Learning' Supplementary for FAQs Best pseudo-label accuracy for various data setttings.</figDesc><table><row><cell>Best Pseudo</cell><cell>CIFAR100</cell><cell>CIFAR10</cell></row><row><cell>Accuracy</cell><cell>400 2500 10000 40</cell><cell>250 4000</cell></row><row><cell cols="3">FixMatch +CCSSL 69.79 85.46 92.08 96.01 96.51 97.59 62.45 83.82 92.01 94.81 96.09 97.79</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merriam-Webster Dictionary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merriam-Webster</surname></persName>
		</author>
		<ptr target="http://www.mw.com/home.htm" />
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Safe deep semi-supervised learning for unseen-class unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan-Zhe</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple: Similar pseudo label exploitation for semisupervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15099" to="15108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of nonnormalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondrej Chum</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="10759" to="10768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comatch: Semi-supervised learning with contrastive graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">All labels are not created equal: Enhancing semi-supervision via label grouping and co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samitha</forename><surname>Islam Nassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haffari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Laplacenet: A hybrid energy-neural model for deep semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Sellars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelica</forename><forename type="middle">I</forename><surname>Aviles-Rivero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carola-Bibiane</forename><surname>Sch?nlieb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04527</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<pubPlace>Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A realistic evaluation of semi-supervised learning for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The semi-supervised inaturalist challenge at the fgvc8 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised wasserstein pseudo-labeling for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariborz</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sobhan</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Humble teachers teach better students for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3132" to="3141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data-uncertainty guided multi-phase learning for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4568" to="4577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Why stable learning works? a theory of covariate shift generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Classificationreconstruction learning for open-set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Yoshihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rei</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Naemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4016" to="4025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Instant-teaching: An end-to-end semi-supervised object detection framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
