<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Br?dermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MPI for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Besides standard cameras, autonomous vehicles typically include multiple additional sensors, such as lidars and radars, which help acquire richer information for perceiving the content of the driving scene. While several recent works focus on fusing certain pairs of sensors-such as camera and lidar or camera and radar-by using architectural components specific to the examined setting, a generic and modular sensor fusion architecture is missing from the literature. In this work, we focus on 2D object detection, a fundamental high-level task which is defined on the 2D image domain, and propose HRFuser, a multi-resolution sensor fusion architecture that scales straightforwardly to an arbitrary number of input modalities. The design of HRFuser is based on state-of-the-art high-resolution networks for image-only dense prediction and incorporates a novel multi-window cross-attention block as the means to perform fusion of multiple modalities at multiple resolutions. Even though cameras alone provide very informative features for 2D detection, we demonstrate via extensive experiments on the nuScenes and Seeing Through Fog datasets that our model effectively leverages complementary features from additional modalities, substantially improving upon camera-only performance and consistently outperforming state-of-the-art fusion methods for 2D detection both in normal and adverse conditions. The source code is publicly available at https://github.com/timbroed/HRFuser.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High-level visual perception is vital for the deployment of autonomous vehicles and robots. The primary sensors for such agents to perceive the surrounding scene are cameras, as they provide rich texture information at very high spatial resolution. This enables perception algorithms to achieve high accuracy in tasks central to recognition, such as 2D object detection and semantic segmentation.</p><p>However, the quality of images captured by cameras degrades severely in adverse visual conditions, e.g. with poor illumination (night), low visibility (fog) or intense backscatter and noise (rain, snow). Moreover, cameras output 2D readings, which do not explicitly capture depth or other geometric attributes of the scene. Complementary characteristics to cameras, such as robustness to adverse conditions or explicit range and velocity measurements, are provided by other sensors, notably lidars, radars, and gated cameras <ref type="bibr" target="#b1">[2]</ref>. Thanks to developments in sensor technology (e.g. quasi-solid-state lidar scanners <ref type="bibr" target="#b74">[75]</ref>), these types of sensors are gradually becoming cheaper and thus more commonly used in practice in automated driving. Exploiting all available measurements from the sensor suite of an autonomous car is of utmost importance for accurate perception under all possible conditions. As a result, fusing information from various sensors is a necessary step in practical visual perception systems, for which redundancy is a must.</p><p>Most of the sensor fusion works for visual perception have focused on 3D tasks, such as 3D object detection, in which the range measurements of lidars and/or radars provide a very informative feature. Thus, these works have examined the utility of adding information from cameras on top of the ranging sensors. By contrast, we focus on the 2D image domain, which is also fully relevant for perception  <ref type="figure">Figure 1</ref>: An instantiation of the overall architecture of our HRFuser backbone for the case where two additional sensors besides the camera are available. Feature maps are colored according to the sensor branch to which they belong. For brevity, we only show the backbone of the network and not the detection head. Transf.: transformer, Conv.: convolution, MWCA: multi-window cross-attention.</p><p>Best viewed on a screen and zoomed in.</p><p>in autonomous vehicles. In particular, we consider the fundamental recognition task of 2D object detection. For image-based methods, the 2D setting is far more explored in the literature compared to 3D settings, and very strong models have been presented in this context <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref>. In this setting, we aim to show that properly leveraging information from additional sensors besides the camera can substantially improve detection performance, even when starting from very strong state-of-the-art image-only baselines. We pursuit this goal by building a modular architecture which treats the camera as the primary modality and adaptively fuses features from an arbitrary number of additional sensors in a repeatable manner.</p><p>Our network, which we name HRFuser, consists of a multi-resolution multi-sensor fusion architecture for 2D object detection. The structure of HRFuser is based on the paradigm of HRNet <ref type="bibr" target="#b75">[76]</ref> and HRFormer <ref type="bibr" target="#b87">[88]</ref>, two networks for camera-only dense prediction. Both preserve high-resolution representations throughout their layers. HRFuser constitutes an adapted version of this architectural paradigm which handles an arbitrary number of additional modalities besides the camera. In particular, in HRFuser we create a separate branch for each modality and process representations in each branch in parallel. Each additional sensor besides the camera adds limited complexity to the network, as the respective branches only include feature maps at one-namely the highest-resolution. Only the camera branch progressively constructs additional lower resolutions. Fusion happens at multiple levels and for all resolutions of the camera branch. This helps to aggregate context not only from the camera features but also from features corresponding to the additional sensors. We implement fusion via a novel multi-window cross-attention (MWCA) block. This block performs an attention-based fusion of the camera with each additional sensor individually, over multiple non-overlapping spatial windows. It aggregates the outputs via addition over sensors and concatenation over windows.</p><p>Our architecture is generic, as it handles all additional sensors in the same way, except for basic pre-processing. Thus, it scales straightforwardly to an arbitrary number of sensors. This allows to leverage multiple sensors, such as a lidar, radar, and gated camera, without the need to create specialized architectural components dedicated to each individual sensor. HRFuser inherits the benefits of HRNet and HRFormer associated to processing camera features at multiple resolutions while preserving a high resolution representation, allowing aggregation of global context without loss of fine spatial details. On top of that, HRFuser exploits high-resolution features from additional sensors, by paying attention only to the subset of these features relevant for 2D detection and filtering out the others.</p><p>We conduct a thorough experimental evaluation of our network for 2D object detection on two major autonomous driving datasets, the large-scale nuScenes and the adverse-condition-oriented Seeing Through Fog (STF). A camera is used as the primary modality, and a lidar, radar and gated camera as additional modalities. HRFuser substantially outperforms state-of-the-art camera-only networks which are heavily engineered for dense prediction tasks, as well as state-of-the-art sensor fusion methods for 2D detection, and this on both sets. Notably, our network is versatile, as it can easily be adapted for sensor fusion on various datasets with different sets of available sensors. Detailed ablation studies evidence the benefit of our carefully designed multi-window cross-attention fusion block compared to basic fusion strategies. They also demonstrate the improvement induced by adding more sensors to our network, showing that the latter succeeds in leveraging the complementary information provided by these sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object detection methods output either 2D or 3D bounding boxes of the objects of the input scene.</p><p>A popular line of work on 2D detection consists in the region-based CNN (R-CNN) framework <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58]</ref>, which employs a two-stage pipeline that first generates object proposals and then predicts the final boxes from the proposals. An alternative approach is single-stage detection <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, which is typically faster but less accurate. Recent approaches alleviate the need for anchor boxes <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b70">71]</ref>, improve the efficiency of the detectors <ref type="bibr" target="#b68">[69]</ref>, explore the usage of keypoints in formulating the box predictions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b90">91]</ref>, and adapt networks to adverse conditions such as fog <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b59">60]</ref>. HRNet <ref type="bibr" target="#b75">[76]</ref> constitutes a CNN backbone for detection and other dense prediction tasks that preserves a high resolution for intermediate representations, while aggregating global context via parallel lower-resolution branches. Very recently, HRFormer <ref type="bibr" target="#b87">[88]</ref> has extended this idea by replacing most convolutional blocks of HRNet with transformer blocks, which facilitate context aggregation via attending to features from any location of the input. Our HRFuser follows the architecture of HRNet and HRFormer with parallel branches of different resolutions, but it upgrades this architecture with a multi-modal transformer-based fusion block, which allows us to adaptively fuse information from additional modalities-encoded by parallel branches-in the detection pipeline. While 2D detection usually works with images, 3D detection methods mostly operate on point clouds, with the associated challenge of how to best represent these sparse 3D inputs so that they can be consumed by CNNs. This has been addressed via voxels <ref type="bibr" target="#b91">[92]</ref>, vertical pillars <ref type="bibr" target="#b32">[33]</ref>, and combinations of voxels and points <ref type="bibr" target="#b62">[63]</ref>. Recent methods enhance existing detectors by implementing a point-based second detection stage <ref type="bibr" target="#b34">[35]</ref>, employ transformer modules to attend to relevant local and global features of point clouds <ref type="bibr" target="#b48">[49]</ref>, and augment point clouds using physically-based simulation to adapt to fog <ref type="bibr" target="#b25">[26]</ref> and snowfall <ref type="bibr" target="#b26">[27]</ref>. Although these works on 3D detection are related to ours as they also use lidar point clouds, our focus is rather on 2D detection and how to improve it by using point clouds from lidar and radar as extra modalities besides the camera.</p><p>Sensor fusion for object detection is the primary application of sensor fusion in visual perception, although other tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b76">77]</ref> have also been studied. For a comprehensive overview of related work, we refer the reader to <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b85">86]</ref>. The KITTI dataset <ref type="bibr" target="#b23">[24]</ref> has catalyzed research in this area by providing recordings of driving scenes with multiple sensors, notably a lidar and a camera, along with object annotations. Successors of KITTI include nuScenes <ref type="bibr" target="#b3">[4]</ref>, Waymo Open <ref type="bibr" target="#b67">[68]</ref> and Argoverse <ref type="bibr" target="#b5">[6]</ref>. Notably, nuScenes also includes radar readings, which are important in adverse-weather scenarios. Such scenarios are explicitly covered in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61]</ref>. Based primarily on the aforementioned sets, several sensor fusion works have been presented, most of which focus on improving lidar-based 3D detection by fusing information from the camera. This category of works range from early (low-level) fusion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90]</ref>, which directly combines the raw lidar data with raw image data or image features, and mid-level fusion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89]</ref>, which combines lidar features with image-space features, to late fusion <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b58">59]</ref>, which fuses the detection results from lidar and camera, asymmetric fusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b89">90]</ref>, which fuses the object-level representations from one modality with data-level or feature-level representations from the other, and weak fusion <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b79">80]</ref>, which utilizes one modality to provide guidance to the other. 3D detection is also addressed by fusing radar and lidar <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b84">85]</ref> or radar and camera <ref type="bibr" target="#b46">[47]</ref>. Fewer sensor fusion methods address 2D detection and a lot of them are based only on radar and camera sensors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b83">84]</ref>. Methods which improve image-based 2D detection by fusing information only from lidar include <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44]</ref>. Very few previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">52]</ref> fuse all three modalities, i.e. camera, lidar, and radar, for detection. We argue that using all three modalities is relevant, as they provide complementary characteristics which are essential for detection, and we propose a modular fusion architecture for 2D detection that easily scales to an arbitrary number of modalities. Another feature of our network is the fusion at multiple levels and resolutions, which has also been applied in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. Different from these methods, our approach maintains high-resolution representations for the camera branch throughout the network in parallel with lower-resolution representations, which allows to better preserve details in the shape of the objects while also exploiting global context for classification. Transformers <ref type="bibr" target="#b71">[72]</ref> gained popularity in computer vision with the vision transformer <ref type="bibr" target="#b17">[18]</ref>. The more recent Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b77">[78]</ref> introduces a spatial reduction attention reducing memory footprint. PVTv2 <ref type="bibr" target="#b78">[79]</ref> improves upon PVT by using a linear-complexity attention module. Attention is vital when handling multiple modalities, as in our sensor fusion setting. Two recent works <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b76">77]</ref> use transformer blocks specifically for sensor fusion, similarly to our method. Different from these works, our transformer-based network handles several modalities instead of only two and fuses them at multiple resolutions, combining both global and local features of the input scene more effectively. Moreover, our MWCA fusion block allows to apply attention at a high resolution by operating on separate spatial windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HRFuser</head><p>The proposed HRFuser network builds upon the seminal HRNet <ref type="bibr" target="#b75">[76]</ref> and its more recent transformerbased variation, HRFormer <ref type="bibr" target="#b87">[88]</ref>. In particular, we extend the regime of repeated multi-resolution fusion that is employed in the aforementioned networks to work with multiple modalities. To this end, we expand the HRFormer backbone with one additional high-resolution branch for each additional input modality besides the camera. These additional, or secondary, modalities are fused repeatedly at multiple resolutions into the branch of the primary modality, i.e. the camera. <ref type="figure">Fig. 1</ref> illustrates the general architecture of the multi-sensor fusion backbone of HRFuser. The branches of additional sensors are equivalent to the first 3 stages of the high-resolution stream from HRFormer. Whereas the design of the camera branch follows the full HRFormer, except for the novel MWCA fusion block we insert, which is illustrated in <ref type="figure">Fig. 2</ref>. The MWCA fusion block is inserted between the multi-resolution fusion module used in HRNet and HRFormer and the subsequent HRFormer block, allowing the features from the secondary modalities to be fused into the camera branch. All secondary branches include feature maps at a single, high resolution, while in the camera branch we introduce lower resolutions as we proceed to later stages, progressively aggregating context. Before applying our MWCA fusion blocks we add 3?3 strided convolutions to match the high-resolution secondary modalities to the lower resolution stream of the primary modality, similar to the HRNets multi-resolution fusion.</p><p>Fusing multiple modalities in such an asymmetric way provides scalability to our method, as the extra branches only have few feature channels. We can include an arbitrary number of modalities by simply adding an extra secondary branch for each new modality and fusing it in parallel into the camera branch. We demonstrate this possibility in Section 4 by applying HRFuser to the STF dataset <ref type="bibr" target="#b1">[2]</ref> and utilizing a gated camera as fourth sensor, besides the more common lidar and radar sensors.</p><p>The HRFuser backbone illustrated in <ref type="figure">Fig. 1</ref> is followed by a neck which forms a feature pyramid by concatenating the up-sampled outputs of all streams <ref type="bibr" target="#b75">[76]</ref>. This neck is in turn followed by a Cascade R-CNN head <ref type="bibr" target="#b4">[5]</ref>, following the widely used two-stage detector architecture. Cascade R-CNN introduces a sequence of detectors trained with increasing Intersection over Union (IoU) thresholds setting a strong baseline for any given backbone.</p><p>Multi-window cross-attention. We propose a novel multi-window cross-attention (MWCA) block to fuse multiple modalities in parallel by applying multi-head cross-attention (CA) on multiple non-overlapping local windows. MWCA extends the local-window self-attention proposed by Yuan et al. <ref type="bibr" target="#b87">[88]</ref> to the context of multi-modal fusion. In particular, it limits the spatial extent of the attention to small windows, thereby reducing the computational cost of each attention operation and allowing to apply this operation to high-resolution feature maps. For each window, this results in K 2 tokens with dimensionality D, depending on the number of channels of the stream we fuse into. Compared to self-attention, CA fuses two modalities by applying attention with queries from the primary modality ? and keys and values from the secondary modality ?.</p><p>More formally, we partition the input feature map X of our primary modality ? into a grid of p nonoverlapping spatial windows:</p><formula xml:id="formula_0">X ? Split ??? {X ? 1 , X ? 2 , . . . , X ? P }. The exact same partition is done for the feature maps Y ? of all secondary modalities ? ? {1, . . . , M }: Y ? Split ??? {Y ? 1 , Y ? 2 , . . . , Y ? P }.</formula><p>All input feature maps are vectorized across the spacial dimensions and have the same shape X, Y ? R N ?D , where N denotes the total number of spatial positions and D denotes the number of channels, and each window is of size K?K.</p><p>A local transformer applies parallel CA to each corresponding set of windows independently. Parallel CA on the set of p-th windows is formulated as follows:</p><formula xml:id="formula_1">MultiHead(X ? p , Y ? p ) = Concat[head(X ? p , Y ? p ) 1 , . . . , head(X ? p , Y ? p ) H ] ? R K 2 ?D , (1) head(X ? p , Y ? p ) h = Softmax (X ? p W h,? q )(Y ? p W h,? k ) T D /H Y ? p W h,? v ? R K 2 ? D H ,<label>(2)</label></formula><formula xml:id="formula_2">X p = X ? p + M ?=1 Y ? p + MultiHead(X ? p , Y ? p )W ? o ? R K 2 ?D ,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Attention</head><p>Cross-Attention ? ? ? ? <ref type="figure">Figure 3</ref>: Our parallel crossattention block for the case where two additional sensors besides the camera are used. ? denotes the primary modality (camera) and ? denotes the secondary modalities.</p><formula xml:id="formula_3">where W ? o ? R D?D and W h,? q , W h,? k , W h,? v ? R D? D H for h ? {1, .</formula><p>. . , H} are weight matrices implemented by trainable linear projections. H denotes the number of heads and X p denotes the output of the parallel CA for the set of p-th windows.</p><p>We arrange the outputs from all P sets of windows back into a single feature map to get the final output of MWCA, X MWCA : <ref type="figure">Fig. 2</ref> illustrates how we split up the input maps for each modality into non-overlapping windows and apply parallel CA across modalities within each window independently, before merging the resulting outputs back into a single feature map. Whereby <ref type="figure">Fig. 3</ref> illustrates parallel CA in more detail. To allow information exchange between the non-overlapping windows, we add a feed-forward network including 3?3 depth-wise convolution similar to the HRFormer block <ref type="bibr" target="#b87">[88]</ref>.</p><formula xml:id="formula_4">X 1 , X 2 , . . . , X P Merge ? ?? ? X MWCA .<label>(4)</label></formula><p>Other architectural features. Before feeding inputs to HRFuser, we project all secondary modalities onto the image plane of the camera, using perspective projection as proposed in <ref type="bibr" target="#b92">[93]</ref>. This yields an exact spatial correspondence between the input feature maps of different modalities, ensuring consistency among corresponding windows from different modalities in MWCA. All branches start with a CNN reducing the resolution by a factor of 4, followed by 4 stages consisting of multiple identical blocks. For all branches, we use basic bottleneck blocks to build the first stage <ref type="bibr" target="#b75">[76]</ref> and transformer blocks to build all subsequent stages and streams <ref type="bibr" target="#b87">[88]</ref>. The parameters of each MWCA transformer (H, D) are equal to the parameters of the subsequent transformer blocks of the respective stream. We include additional implementation details on different versions of HRFuser in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We organize this section as follows. We first present our experimental setup for multi-sensor 2D object detection, i.e. the implementation details for our method on the two examined datasets, nuScenes <ref type="bibr" target="#b3">[4]</ref> and STF <ref type="bibr" target="#b1">[2]</ref>. We then compare our method to the state of the art in multi-sensor fusion and conduct detailed ablation studies on the fusion mechanism and the utility of including additional sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>In all our experiments, we use a two-stage HRFuser network for 2D detection. The backbone of the network is structured as per Section 3 and its outputs are used to feed a Cascade R-CNN <ref type="bibr" target="#b4">[5]</ref> head which serves as the second stage of the network. We test a tiny (T), small (S) and base (B) version of HRFuser and implement them using the mmdetection framework <ref type="bibr" target="#b8">[9]</ref>.</p><p>HRFuser is trained on nuScenes for 12 epochs on batches of size 12 using AdamW <ref type="bibr" target="#b40">[41]</ref> with a base learning rate of 0.0001, weight decay of 0.01 and betas of 0.9 and 0.999. We apply a learning rate warm-up for 500 iterations with a ratio of 0.001 and reduce the learning rate by a factor of 10 at epochs 8 and 11. The training settings are the same for STF, except that the total epochs are 60, the base learning rate is 0.001 and the learning rate reduction is done at epochs 40 and 50. To accelerate learning of features from the less rich modalities such as radar, we randomly set inputs to zero during training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref> with a chance of 20% for nuScenes and 50% for STF.</p><p>NuScenes <ref type="bibr" target="#b3">[4]</ref> is a large-scale dataset (1.4M images) providing 3D data and annotations of a full autonomous vehicle sensor suite including 6 cameras, 1 lidar and 5 radars. We use the 360-degree data of all 6 cameras. We follow <ref type="bibr" target="#b47">[48]</ref> for basic sensor pre-processing, creating radar images with range, radar cross-section (RCS) and velocity over ground, and lidar images with range, intensity and height. Compared to <ref type="bibr" target="#b47">[48]</ref>, we do not accumulate radar data across time or filter them in any way. Unless otherwise stated, we use a subset of 10 nuScenes object classes following the mmdet3d <ref type="bibr" target="#b13">[14]</ref> framework: car, truck, trailer, bus, construction vehicle, bicycle, motorcycle, pedestrian, traffic cone, and barrier. Similar to <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48]</ref>, we project the 3D annotations onto the image plane, keeping only boxes that are at least 40% visible. We train on the official training set and evaluate on the validation set, due to the lack of a public test set. Evaluation uses the 2D COCO evaluation metrics <ref type="bibr" target="#b37">[38]</ref>.</p><p>STF [2] is a multi-modal driving dataset with 100k 2D and 3D bounding boxes. The dataset provides camera images, lidar and radar points, and gated camera images, captured under a variety of normal and adverse weather conditions. The gated camera in STF captures images in the NIR band at 808nm with a time-synchronized flood-lit flash laser source. Following the standard dataset splits in <ref type="bibr" target="#b1">[2]</ref>, we train only on clear-weather data and use adverse-condition data only for evaluation. We follow <ref type="bibr" target="#b1">[2]</ref> for basic sensor pre-processing, obtaining 1248?360 images with depth, intensity and height for lidar and depth and velocity over ground for radar. Note that radar is missing the RCS channel, since this is not published with the rest of STF. We train on the common KITTI classes car, pedestrian, and cyclist, and evaluate only on car using the KITTI evaluation framework <ref type="bibr" target="#b23">[24]</ref>, similar to <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to the State of the Art</head><p>We compare multiple versions of HRFuser to state-of-the-art camera-only and multi-modal methods on nuScenes in <ref type="table" target="#tab_1">Table 1</ref>. All versions of HRFuser, based either on HRFormer or HRNet, outperform substantially all camera-only models. In particular, the fully-fledged HRFuser-B improves AP by 5.0% compared to HRFormer-B and demonstrates analogous improvements in all other metrics. Moreover, all versions of HRFuser beat the radar-camera fusion method of <ref type="bibr" target="#b45">[46]</ref> by a large margin  on the standard nuScenes split, showcasing the advantage of leveraging multiple complementary sensors-including lidar-with a single, modular architecture as ours over just using radar and camera. A substantial performance gain is also observed over CRF-Net <ref type="bibr" target="#b47">[48]</ref> on the nuScenes split that this work employs and using only the front camera for evaluation.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we compare our HRFuser-T to the fusion method of <ref type="bibr" target="#b1">[2]</ref> on STF. Our model clearly outperforms <ref type="bibr" target="#b1">[2]</ref> across all weather conditions, showing in particular significant improvements in the cases of light fog and dense fog, in which it beats [2] by 1.6% and 1.5% on the "hard" setting, respectively. This finding showcases the ability of our model to generalize well to previously unseen, adverse conditions, which degrade the quality of the readings for some of the sensors, such as the camera and the lidar, by properly attending to the features from the sensors that are more robust to these conditions, such as the radar and the gated camera.</p><p>Fusing multiple modalities does not only allow to build more robust features, but also helps against overfitting. This is demonstrated in <ref type="table" target="#tab_1">Table 1</ref>, where HRFormer-T outperforms the significantly larger HRFormer-B by 0.5% in AP, but HRFuser-B outperforms the smaller HRFuser-T by 0.5% in AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Fusion mechanism. <ref type="table" target="#tab_3">Table 3</ref> presents an ablation study on nuScenes regarding the fusion mechanism which is used in HRFuser, in order to verify the benefit of our MWCA fusion block. The reference is the camera-only HRFormer-T baseline. Early fusion-which utilizes a concatenated 8-channel input without any additional changes to HRFormer-T-achieves only a slight 1.2% improvement in AP over the camera-only HRFormer-T. Using our proposed multi-resolution fusion design for our HRFuser, but with a simplified addition-based fusion block instead of MWCA, already yields a large 4.3% improvement in AP over the camera-only baseline. Replacing addition with our proposed MWCA further improves performance consistently across all metrics, showcasing the utility of attention-based fusion for detection. Limiting the fusion to only the high-resolution stream of the camera branch yields a 1.0% reduction in AP, highlighting the importance of multi-resolution fusion. We compare our MWCA to an alternative attention mechanism via the state-of-the-art transformer PVTv2 <ref type="bibr" target="#b78">[79]</ref>, adapted for cross-attention (PVTv2-CA). For implementation details we refer the reader to Appendix C. Our MWCA fusion outperforms PVTv2-CA and the linear version PVTv2-Li-CA  Modalities. <ref type="table" target="#tab_5">Table 5</ref> investigates the contribution of each modality by training HRFuser-T with different subsets of input modalities. Adding radar to HRFuser-T yields an improvement of 1.4% over using only images. The improvement is larger (4.7%) when adding lidar, and is maximized (5.0%) when combining all 3 sensors, showing the ability of our MWCA fusion to attend to the useful part of extra modalities while ignoring noisy content in them. This is in contrast to HRFormer-T (Early Fusion), where the combination of camera and lidar performs 0.5% better than combining all 3 modalities, but still only 1.7% better than the camera-only baseline. We examine the effect of different input modalities on STF in <ref type="table" target="#tab_4">Table 4</ref>. A combination of all four modalities yields the overall best performance, except for the case of dense fog, where a combination of camera, radar and gated camera performs best. This is in line with the findings of <ref type="bibr" target="#b1">[2]</ref> and is due to the severe impact of fog on the lidar, as the laser pulse has to travel to the object and back, which squares the attenuation due to the presence of fog. By contrast, radar and gated camera are more robust to fog. Note that all models in this experiment are trained solely on clearweather data, so the effect of fog on lidar which is present in the foggy test sets is unseen to them. Thus, the network cannot learn how to deal with this adverse condition, as it did with the radar noise on nuScenes. Another finding is that adding the gated camera on top of lidar and radar provides a consistent improvement across conditions, evidencing the informativeness of the high-resolution features from this sensor, which is generally robust to adverse conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4 presents detection results on nuScenes of the best-performing HRFormer (HRFormer-T) and</head><p>HRFuser <ref type="figure">(HRFuser-B)</ref>. HRFuser detects the partially occluded pedestrian in the first example, which is missed by HRFormer. The second example demonstrates a failure case where both methods do not identify the cars in the background. The last example displays a very difficult dark input which  provides minimal queues from the camera. Despite this, HRFuser correctly detects both cars of the scene, showcasing its ability to leverage additional complementary sensors for object detection.</p><p>The respective qualitative results on STF in <ref type="figure" target="#fig_2">Fig. 5</ref> demonstrate that our proposed method is significantly more resilient to adverse conditions which are not encountered during training than a strong camera-only model. Using multiple modalities allows HRFuser to extract features that incorporate information from additional modalities and are thus more robust to changes in weather or illumination and resulting changes in the appearance of objects. The camera-only HRFormer struggles particularly for detecting objects at a large distance. This can be attributed to the cumulative effect of atmospheric phenomena such as fog and snow on the appearance of objects as their distance from the camera increases. The relatively good performance of HRFuser demonstrates its greater generalization capability thanks to learning robust fused features from multiple modalities. Note e.g. the correct detection by HRFuser of the two distant cars which are obscured by fog in the second example, which are both missed by HRFormer. Of course, HRFuser misses few distant objects in the other two examples, but still performs significantly better than HRFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed HRFuser, a multi-modal, multi-resolution and multi-level fusion architecture. In particular, we have extended the camera-only HRNet and HRFormer models to multiple modalities by introducing additional high-resolution branches for the extra modalities besides the camera. HRFuser repeatedly fuses the extra modalities into the multi-resolution camera branch with a novel transformer that applies multi-window cross attention and enables efficient learning of multi-modal features without requiring very large datasets. We have evaluated HRFuser on nuScenes and STF and demonstrated its state-of-the-art performance in 2D object detection across a wide range of scenes and conditions. Our architecture is generic and scales straightforwardly to an arbitrary number of sensors, thus being of particular relevance for practical multi-modal settings in autonomous cars and robots, which usually involve a diverse set of sensors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Details on the Experimental Setup</head><p>NuScenes <ref type="bibr" target="#b3">[4]</ref>. Similar to <ref type="bibr" target="#b47">[48]</ref>, we resize the recorded 1600?900 images to 640?360 and project the radar points as 3m-high pillars onto the image plane. This creates a 640?360 projected radar image with 3 channels: range, radar cross-section (RCS) and velocity over ground. Compared to <ref type="bibr" target="#b47">[48]</ref>, we do not accumulate radar data across time or filter them in any way. The lidar points are projected onto the image plane, yielding a 640?360 image with 3 channels: range, intensity and height. Example inputs are displayed in <ref type="figure" target="#fig_3">Fig. 6</ref>. All input channels are normalized over the entire dataset. We follow the mmdet3d <ref type="bibr" target="#b13">[14]</ref> framework and use a set of 10 classes for training and evaluation, which are defined based on the original nuScenes classes as shown in <ref type="table" target="#tab_8">Table 7</ref>. We run the training of HRFuser-T on 4 Nvidia RTX 2080 TI GPUs with a batch size of 12.</p><p>Seeing Through Fog <ref type="bibr" target="#b1">[2]</ref> (STF). The dataset provides 1920?1024 camera images, lidar and radar points, and 1280?720 gated camera images, captured under a variety of normal and adverse weather conditions. We process the inputs in the same way as <ref type="bibr" target="#b1">[2]</ref>. The camera is cropped to a 1248?360 window around the center of the gated camera. The image from the gated camera is transformed into the image plane of the camera using a homography mapping as in <ref type="bibr" target="#b1">[2]</ref>. We also crop the annotated 2D bounding boxes to the aforementioned 1248?360 window, discarding boxes for which more than 90% of the original box area lies out of the crop. The gated camera is cropped to the same window. The strongest lidar return and radar are projected onto the image plane. As the RCS data are not publicly available, we use only 2 radar channels: depth and velocity over ground. However, RCS data were used for training the method of <ref type="bibr" target="#b1">[2]</ref>, so comparing <ref type="bibr" target="#b1">[2]</ref> to our method is not fully fair. Example inputs are displayed in <ref type="figure">Fig. 7</ref>. We train on 3 classes defined as shown in <ref type="table" target="#tab_9">Table 8</ref>, and evaluate only on the car class, using the KITTI evaluation framework <ref type="bibr" target="#b23">[24]</ref>. We run the training of HRFuser-T on 4 Nvidia Titan RTX GPUs with a batch size of 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Details on Ablations</head><p>PVTv2 adaptations. We create the alternative attention mechanisms PVTv2-CA and PVTv2-Li-CA which are presented in <ref type="table" target="#tab_4">Table 4</ref>      PVTv2 <ref type="bibr" target="#b78">[79]</ref>. We adapt its spatial reduction attention module for cross-attention by entering the query from our primary branch and the key and value from our secondary branch, and pass this into their proposed convolutional feed-forward module employing depth-wise convolution. In contrast to PVTv2, we do not incorporate the overlapping patch embedding, in order to keep the spacial dimensions of the feature maps unchanged. No pre-training is applied when training PVTv2, which is also the case for all other presented methods.</p><p>Parallel cross-attention skip connection. To examine the benefit of the skip connections for the secondary modalities-which are involved in our parallel CA block as shown in <ref type="figure">Fig. 3</ref> of the main paper-we remove these skip connections (blue and orange in <ref type="figure">Fig. 3</ref>) and observe in <ref type="table" target="#tab_10">Table 9</ref> a drop of 0.6% in AP relative to our default MWCA. This finding indicates that skip connections from all modalities are beneficial for cross-attention, as it allows the network to attend to details without having to learn the identity function.</p><p>D HRFuser with an HRNet-based Backbone <ref type="table" target="#tab_1">Table 1</ref> of the main paper also includes HRFuser-w18 (HRNet), a variant of HRFuser built upon HRNetV2-w18 <ref type="bibr" target="#b75">[76]</ref>. In this variant, we keep the same transformer-based MWCA fusion mechanism with the same parameters as for the default, HRFormer-based HRFuser. However, the camera branch of this variant, in which our MWCA fusion blocks are inserted, resembles HRNetV2p-w18 and follows the HRNet architecture using "Basic" blocks introduced in <ref type="bibr" target="#b75">[76]</ref>. The secondary modality branches we introduce follow analogously the design of the highest-resolution branch of HRNetV2p-w18. <ref type="table" target="#tab_1">Table 1</ref> of the main paper shows a 4.3% improvement in AP of our HRFuser-w18 over the camera-only HRNetV2-w18, which demonstrates the generality of the components introduced in HRFuser, as they benefit various dense prediction networks such as HRNet and HRFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Comparison to CRF-Net Using Only Radar</head><p>In <ref type="table" target="#tab_1">Table 10</ref>, we compare on nuScenes the CRF-Net <ref type="bibr" target="#b47">[48]</ref> to a version of HRFuser which only uses radar besides the camera, i.e., omitting lidar. This comparison serves in investigating whether HRFuser can leverage information from the radar better than the competing state-of-the-art CRF-Net, which focuses explicitly on the radar modality. Indeed, HRFuser-T radar yields a 4.9% improvement in AP over CRF-Net, which verifies that HRFuser achieves state-of-the-art results on nuScenes even when only considering radar as a secondary modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Qualitative Results</head><p>We show further qualitative results for nuScenes in <ref type="figure" target="#fig_4">Fig. 8</ref> with exemplary failure cases in <ref type="figure" target="#fig_5">Fig.9</ref> and for STF in <ref type="figure" target="#fig_6">Fig. 10</ref> with exemplary failure cases in <ref type="figure" target="#fig_7">Fig.11</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 2 :</head><label>22</label><figDesc>Our multi-window cross-attention (MWCA) fusion block, consisting of (a) our MWCA and a subsequent feed-forward network. Inputs to the parallel cross-attention blocks are colored according to the sensor they come from. DW conv.: depth-wise convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative detection results on nuScenes. From left to right: image with ground-truth annotation, prediction of HRFormer, prediction of HRFuser. Best viewed on a screen at full zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative detection results on STF. From left to right: image with ground-truth annotation, prediction of HRFormer, prediction of HRFuser. Best viewed on a screen at full zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Example inputs to HRFuser from nuScenes. From left to right: RGB image, projected lidar points, projected radar points. The radar and lidar projections are highlighted and enlarged for better visualization. Best viewed on a screen at full zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Further qualitative detection results on nuScenes. From left to right: image with groundtruth annotation, prediction of HRFormer, prediction of HRFuser. Best viewed on a screen at full zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Further qualitative detection results on nuScenes with exemplary failure cases of HRFuser. From left to right: image with ground-truth annotation, prediction of HRFormer, prediction of HRFuser. Best viewed on a screen at full zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Further qualitative detection results on STF. From left to right: image with ground-truth annotation, prediction of HRFormer, prediction of HRFuser. Best viewed on a screen at full zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Further qualitative detection results on STF with exemplary failure cases of HRFuser. From left to right: image with ground-truth annotation, prediction of HRFormer, prediction of HRFuser. Best viewed on a screen at full zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of 2D detection methods on nuScenes evaluated on 6 classes: car, truck, bus, bicycle, motorcycle and pedestrian. The first group of rows shows results on the standard nuScenes validation set. The second group of rows shows results using the split from<ref type="bibr" target="#b47">[48]</ref> for training and evaluation. All entries including the word "HRFuser" constitute versions of our method and state the camera-only model upon which they build in parentheses. All HR* methods use a Cascade R-CNN head and have undergone hyper-parameter tuning. C: camera, R: radar, L: lidar, (*): results taken directly from the respective paper.</figDesc><table><row><cell>Method</cell><cell>Modalities</cell><cell>AP</cell><cell>AP0.5</cell><cell>AP0.75</cell><cell>APm</cell><cell>AP l</cell><cell>AR</cell></row><row><cell>HRNetV2p-w18 [76]</cell><cell>C</cell><cell>32.4</cell><cell>56.6</cell><cell>33.5</cell><cell>21.0</cell><cell>43.7</cell><cell>43.4</cell></row><row><cell>HRFormer-T [88]</cell><cell>C</cell><cell>34.3</cell><cell>59.6</cell><cell>35.6</cell><cell>23.2</cell><cell>45.5</cell><cell>43.9</cell></row><row><cell>HRFormer-B [88]</cell><cell>C</cell><cell>33.8</cell><cell>59.4</cell><cell>34.6</cell><cell>22.4</cell><cell>45.1</cell><cell>43.1</cell></row><row><cell>Radar-Camera Fusion[46]*</cell><cell>CR</cell><cell>35.6</cell><cell>60.5</cell><cell>37.4</cell><cell>-</cell><cell>-</cell><cell>42.1</cell></row><row><cell>HRFuser-w18 (HRNet)</cell><cell>CRL</cell><cell>36.7</cell><cell>63.1</cell><cell>38.1</cell><cell>24.9</cell><cell>48.6</cell><cell>47.0</cell></row><row><cell>HRFuser-T (HRFormer)</cell><cell>CRL</cell><cell>38.3</cell><cell>65.3</cell><cell>40.1</cell><cell>26.8</cell><cell>49.9</cell><cell>48.3</cell></row><row><cell>HRFuser-S (HRFormer)</cell><cell>CRL</cell><cell>38.5</cell><cell>65.6</cell><cell>40.2</cell><cell>27.2</cell><cell>49.9</cell><cell>48.1</cell></row><row><cell>HRFuser-B (HRFormer)</cell><cell>CRL</cell><cell>38.8</cell><cell>66.0</cell><cell>41.0</cell><cell>26.9</cell><cell>50.7</cell><cell>48.6</cell></row><row><cell>CRF-Net [48]</cell><cell>CR</cell><cell>27.0</cell><cell>42.7</cell><cell>29.0</cell><cell>22.7</cell><cell>35.6</cell><cell>31.3</cell></row><row><cell>HRFuser-T (HRFormer)</cell><cell>CRL</cell><cell>34.6</cell><cell>62.0</cell><cell>34.7</cell><cell>26.0</cell><cell>48.5</cell><cell>45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Weather</cell><cell>clear</cell><cell>light fog</cell><cell>dense fog</cell><cell>snow/rain</cell></row><row><cell>Difficulty</cell><cell>easy mod.</cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparison of 2D detection methods on the STF test sets. HRFuser-T builds upon HRFormer. (*): results taken directly from the respective paper.hard easy mod. hard easy mod. hard easy mod. hard Deep Entropy Fusion [2]* 89.84 85.57 79.46 90.54 87.99 84.90 87.68 81.49 76.69 88.99 83.71 77.85 HRFuser-T 90.15 87.10 79.48 90.60 89.34 86.50 87.93 80.27 78.21 90.05 85.35 78.09</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablations of fusion strategies on nuScenes. Early: early fusion by direct concatenation of all 3 input modalities, Addition: fusion at multiple levels and resolutions between secondary branches and camera branch via an addition block, MWCA: our MWCA fusion block, MWCA onlyHighRes : our fusion block included only in the highest resolution of the camera branch.</figDesc><table><row><cell>Method (Fusion Type)</cell><cell>AP</cell><cell>AP0.5</cell><cell>AP0.75</cell><cell>APm</cell><cell>AP l</cell><cell>AR</cell></row><row><cell>HRFormer-T</cell><cell>26.5</cell><cell>49.9</cell><cell>25.3</cell><cell>18.2</cell><cell>37.0</cell><cell>26.8</cell></row><row><cell>HRFormer-T (Early)</cell><cell>27.7</cell><cell>51.6</cell><cell>26.5</cell><cell>18.4</cell><cell>38.8</cell><cell>38.9</cell></row><row><cell>HRFuser-T (Addition)</cell><cell>30.8</cell><cell>56.4</cell><cell>30.5</cell><cell>22.0</cell><cell>41.9</cell><cell>42.0</cell></row><row><cell>HRFuser-T (MWCAonlyHighRes)</cell><cell>30.5</cell><cell>56.1</cell><cell>29.7</cell><cell>21.8</cell><cell>41.4</cell><cell>41.5</cell></row><row><cell>HRFuser-T (MWCA)</cell><cell>31.5</cell><cell>57.4</cell><cell>31.1</cell><cell>22.7</cell><cell>42.5</cell><cell>42.3</cell></row><row><cell>HRFuser-T (PVTv2-CA [79])</cell><cell>29.8</cell><cell>54.3</cell><cell>29.4</cell><cell>20.1</cell><cell>41.3</cell><cell>40.9</cell></row><row><cell>HRFuser-T (PVTv2-Li-CA [79])</cell><cell>29.5</cell><cell>54.2</cell><cell>28.6</cell><cell>19.9</cell><cell>41.0</cell><cell>40.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablations of input modalities for HRFuser-T on the STF test sets. Results are in AP. 79.81 62.48 53.68 80.84 63.07 62.08 71.84 62.69 54.05 78.68 61.19 52.72 ? ? 89.91 85.16 78.68 90.47 88.44 80.55 87.39 78.32 71.13 89.21 79.88 76.19 ? 89.88 85.17 78.64 90.46 87.87 80.51 88.10 80.11 72.01 89.40 80.02 76.11 ? 90.14 87.18 79.44 90.62 89.17 80.95 88.56 80.33 72.21 90.09 85.32 78.09 ? 89.87 85.13 78.55 90.64 88.37 80.52 88.97 80.86 78.64 89.85 80.33 76.54 90.15 87.10 79.48 90.60 89.34 86.50 87.93 80.27 78.21 90.05 85.35 78.09 by 1.7% and 2.0% respectively, demonstrating the advantage of our local MWCA attention, when trained on a mid-sized dataset such as nuScenes.</figDesc><table><row><cell>Lidar</cell><cell></cell><cell>Gated</cell><cell>clear</cell><cell>light fog</cell><cell>dense fog</cell><cell>snow/rain</cell></row><row><cell>RGB</cell><cell>Radar</cell><cell></cell><cell cols="4">easy mod. hard easy mod. hard easy mod. hard easy mod. hard</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablations of input modalities on nuScenes. Results are in AP. EF: early fusion, C: camera, R: radar, L: lidar.</figDesc><table><row><cell cols="3">Modalities HRFuser-T HRFormer-T(EF)</cell></row><row><cell>C</cell><cell>26.5</cell><cell>26.5</cell></row><row><cell>CR</cell><cell>27.9</cell><cell>25.7</cell></row><row><cell>CL</cell><cell>31.2</cell><cell>28.2</cell></row><row><cell>CRL</cell><cell>31.5</cell><cell>27.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Parameters of the tiny (T), small (S), and base (B) versions of HRFuser. D s denotes the number of channels and H s the number of heads, with s ? {1, . . . , 4} denoting the corresponding stream. For the camera branch ?, the values are displayed as: (D 1 , D 2 , D 3 , D 4 ) and (H 1 , H 2 , H 3 , H 4 ). The secondary branches ? only have one stream: D 1 and H 1 .</figDesc><table><row><cell>Model</cell><cell>Branch</cell><cell>#channels (Ds)</cell><cell>#heads (Hs)</cell></row><row><cell>HRFuser-T</cell><cell>?</cell><cell>(18, 36, 72, 144)</cell><cell>(1, 2, 4, 8)</cell></row><row><cell></cell><cell>?</cell><cell>18</cell><cell>1</cell></row><row><cell>HRFuser-S</cell><cell>?</cell><cell>(32, 64, 128, 256)</cell><cell>(1, 2, 4, 8)</cell></row><row><cell></cell><cell>?</cell><cell>32</cell><cell>1</cell></row><row><cell>HRFuser-B</cell><cell>?</cell><cell>(78, 156, 312, 624)</cell><cell>(2, 4, 8, 16)</cell></row><row><cell></cell><cell>?</cell><cell>78</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>of the main paper by adapting the state-of-the-art transformer</figDesc><table><row><cell>NuScenes Class</cell><cell>Mapped Class</cell></row><row><cell>vehicle.car</cell><cell>car</cell></row><row><cell>vehicle.truck</cell><cell>truck</cell></row><row><cell>vehicle.trailer</cell><cell>trailer</cell></row><row><cell>vehicle.bus.bendy</cell><cell>bus</cell></row><row><cell>vehicle.bus.rigid</cell><cell>bus</cell></row><row><cell>vehicle.construction</cell><cell>construction_vehicle</cell></row><row><cell>vehicle.bicycle</cell><cell>bicycle</cell></row><row><cell>vehicle.motorcycle</cell><cell>motorcycle</cell></row><row><cell>human.pedestrian.child</cell><cell>pedestrian</cell></row><row><cell>human.pedestrian.adult</cell><cell>pedestrian</cell></row><row><cell>human.pedestrian.construction_worker</cell><cell>pedestrian</cell></row><row><cell>human.pedestrian.police_officer</cell><cell>pedestrian</cell></row><row><cell>movable_object.trafficcone</cell><cell>traffic_cone</cell></row><row><cell>movable_object.barrier</cell><cell>barrier</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Mapping from original nuScenes classes to our default set of training and evaluation classes [14].Figure 7: Example inputs to HRFuser from STF. From left to right: RGB image, warped gated camera image, projected lidar points, projected radar points. The radar and lidar projections are highlighted and enlarged for better visualization. Best viewed on a screen at full zoom.</figDesc><table><row><cell>STF Class</cell><cell>Mapped Class</cell></row><row><cell>PassengerCar</cell><cell>Car</cell></row><row><cell>Pedestrian</cell><cell>Pedestrian</cell></row><row><cell>RidableVehicle</cell><cell>Cyclist</cell></row><row><cell>LargeVehicle</cell><cell>DontCare</cell></row><row><cell>Vehicle</cell><cell>DontCare</cell></row><row><cell>DontCare</cell><cell>DontCare</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Mapping from original STF classes to the set of classes we use for training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Additional ablation of MWCA on nuScenes. MWCA: our MWCA fusion block, MWCA w/o skip : our fusion block without skip connections for secondary modalities in the parallel CA block.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP0.5</cell><cell>AP0.75</cell><cell>APm</cell><cell>AP l</cell><cell>AR</cell></row><row><cell>HRFuser-T (MWCAw/o skip)</cell><cell>30.9</cell><cell>56.6</cell><cell>30.0</cell><cell>22.1</cell><cell>41.9</cell><cell>41.9</cell></row><row><cell>HRFuser-T (MWCA)</cell><cell>31.5</cell><cell>57.4</cell><cell>31.1</cell><cell>22.7</cell><cell>42.5</cell><cell>42.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Additional comparison on nuScenes of CRF-Net<ref type="bibr" target="#b47">[48]</ref> against a radar-only HRFuser, both evaluated on 6 classes: car, truck, bus, bicycle, motorcycle and pedestrian, using the split from<ref type="bibr" target="#b47">[48]</ref> for training and evaluation. C: camera, R: radar.</figDesc><table><row><cell>Method</cell><cell>Modalities</cell><cell>AP</cell><cell>AP0.5</cell><cell>AP0.75</cell><cell>APm</cell><cell>AP l</cell><cell>AR</cell></row><row><cell>CRF-Net [48]</cell><cell>CR</cell><cell>27.0</cell><cell>42.7</cell><cell>29.0</cell><cell>22.7</cell><cell>35.6</cell><cell>31.3</cell></row><row><cell>HRFuser-Tradar (HRFormer)</cell><cell>CR</cell><cell>31.9</cell><cell>58.2</cell><cell>31.6</cell><cell>23.9</cell><cell>45.2</cell><cell>42.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Architectural Details</head><p>The camera branch follows HRFormer <ref type="bibr" target="#b87">[88]</ref>, where each stage introduces an additional stream with halved resolution. The single-resolution branches of the secondary modalities are equivalent to the high-resolution stream from HRFormer. The network starts with a CNN reducing the resolution by four with respect to the input, followed by 4 stages consisting of multiple identical blocks. For all branches, we use basic bottleneck blocks to build the first stage <ref type="bibr" target="#b75">[76]</ref> and transformer blocks to build all subsequent stages and streams <ref type="bibr" target="#b87">[88]</ref>. A transformer block consists of a local-window self-attention on 7 ? 7 windows followed by an feed-forward network with 3 ? 3 depth-wise convolution and an expansion ratio of 4. The additional parameters of the transformer blocks for different versions of HRFuser are displayed in <ref type="table">Table 6</ref>, where D s and H s apply to all blocks and MWCA modules within a given stream.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal vehicle detection: fusing 3D-LIDAR and color camera data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Asvadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiano</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urbano</forename><forename type="middle">J</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeing through fog without seeing fog: Deep multimodal sensor fusion in unseen adverse weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bijelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Mannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-RCNN: Joint object detection and pose estimation using 3D object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Flohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">nuScenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial attention fusion for obstacle detection using MmWave radar and vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">RoIFusion: 3D object detection from LiDAR and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Luca Zanotti Fragonara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsourdos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="51710" to="51721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptive faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-aware unsupervised deep lidar-stereo fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust small object detection on the water surface through fusion of camera and millimeter wave radar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MMDetection3D: OpenMMLab next-generation platform for general 3D object detection</title>
		<ptr target="https://github.com/open-mmlab/mmdetection3d" />
	</analytic>
	<monogr>
		<title level="m">MMDetection3D Contributors</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast boosting based detection using scale invariant multimodal multiresolution filtered features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Daniel Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MLOD: A multi-view 3D object detection based on robust feature fusion method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modality-buffet for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Dorka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SEG-VoxelNet for 3D vehicle detection from RGB and LiDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A general pipeline for 3D detection of vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CenterNet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Leveraging uncertainties for deep multi-modal object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Haase-Sch?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudius</forename><surname>Gl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1341" to="1360" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fog simulation on real LiDAR point clouds for 3D object detection in adverse weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LiDAR snowfall simulation for robust 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bijelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-modal sensor fusion for auto driving perception: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2202.02703</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EPNet: Enhancing point features with image semantics for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint 3D proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving 3D object detection for pedestrians with virtual multi-view synthesis orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LiDAR R-CNN: An efficient and universal 3D object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scanet: Spatial-channel attention network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haihua</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The Oxford RobotCar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multimodal CNN pedestrian classification: A study on combining LIDAR and camera data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gledson</forename><surname>Melotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiano</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Nuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urbano</forename><forename type="middle">J C</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">R</forename><surname>Faria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sensor fusion for joint 3D object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Charland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darshan</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Radar-camera sensor fusion for joint object detection and distance estimation in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
		<idno>abs/2009.08428</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CenterFusion: Center-based radar and camera fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A deep learning-based radar and camera sensor fusion architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Nobis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Geisslinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Lienkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor Data Fusion: Trends, Solutions, Applications (SDF)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3D object detection with Pointformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuran</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CLOCs: Camera-lidar object candidates fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayder</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Canadian adverse driving conditions dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Pitropov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danson</forename><forename type="middle">Evan</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rebello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Low latency and low-level sensor fusion for automotive use-cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Pollach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Schiegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Frustum PointNets for 3D object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust multimodal vehicle detection in foggy weather using complementary lidar and radar signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep end-to-end 3D person detection from camera and lidar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Jargot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dariu</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ACDC: The Adverse Conditions Dataset with Correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Real-time hybrid multi-sensor fusion framework for perception in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theja</forename><surname>Babak Shahian Jahromi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabri</forename><surname>Tulabandhula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">20</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PV-RCNN: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">RoarNet: A robust 3D object detection based on region approximation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwook</forename><surname>Paul Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MilliEye: A lightweight mmWave radar and camera fusion system for robust object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Internet-of-Things Design and Implementation</title>
		<meeting>the International Conference on Internet-of-Things Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">MVX-Net: Multimodal voxelnet for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">LiDAR guided small obstacle segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aasheesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kamireddypalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K Madhava</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Transferable semi-supervised 3D object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siang</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">PointPainting: Sequential fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">PointAugmenting: Cross-modal augmentation for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">MEMS mirrors for LiDAR: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingkang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<publisher>Micromachines</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Radar ghost target detection via multimodal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Giebenhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Anklam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="7758" to="7765" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pvt v2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Frustum ConvNet: Sliding frustums to aggregate local point-wise features for amodal 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Fast and accurate 3D object detection for lidar-camera-based autonomous vehicles using one shared voxel-based backbone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Hua</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang-Hyun</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="22080" to="22089" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">PI-RCNN: An efficient multi-sensor 3D object detector with point-based attentive cont-conv fusion module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">PointFusion: Deep sensor fusion for 3D bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Radar + RGB fusion for robust object detection in autonomous vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritu</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Vierling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Berns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">RadarNet: Exploiting radar for robust perception of dynamic objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Sensor and sensor fusion technology in autonomous vehicles: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>De Jong Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Velasco-Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">3D-CVF: Generating joint camera and LiDAR features using cross-view spatial feature fusion for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Hyeok</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yecheol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Won</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">HRFormer: High-resolution transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2110.09408</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">MAFF-Net: Filter false positive for 3D vehicle detection with multi-modal adaptive feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno>abs/2009.10945</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">3D object detection using scale invariant and feature reweighting networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruolan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno>abs/1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Objects as points. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Perception-aware multi-sensor fusion for 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16280" to="16290" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
