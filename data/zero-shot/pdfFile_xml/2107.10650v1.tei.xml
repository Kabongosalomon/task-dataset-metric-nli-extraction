<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Hak</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AKASA South San Francisco</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ganapathi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AKASA South San Francisco</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Proceedings of Machine Learning Research</title>
						<imprint>
							<biblScope unit="volume">149</biblScope>
							<biblScope unit="page" from="1" to="12"/>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
					<note>Machine Learning for Healthcare Editor: Editor&apos;s name</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prediction of medical codes from clinical notes is both a practical and essential need for every healthcare delivery organization within current medical systems. Automating annotation will save significant time and excessive effort spent by human coders today. However, the biggest challenge is directly identifying appropriate medical codes out of several thousands of high-dimensional codes from unstructured free-text clinical notes. In the past three years, with Convolutional Neural Networks (CNN) and Long Short-Term Memory (LTSM) networks, there have been vast improvements in tackling the most challenging benchmark of the MIMIC-III-full-label inpatient clinical notes dataset. This progress raises the fundamental question of how far automated machine learning (ML) systems are from human coders' working performance. We assessed the baseline of human coders' performance on the same subsampled testing set. We also present our Read, Attend, and Code (RAC) model for learning the medical code assignment mappings. By connecting convolved embeddings with self-attention and code-title guided attention modules, combined with sentence permutation-based data augmentations and stochastic weight averaging training, RAC establishes a new state of the art (SOTA), considerably outperforming the current best Macro-F1 by 18.7%, and reaches past the human-level coding baseline. This new milestone marks a meaningful step toward fully autonomous medical coding (AMC) in machines reaching parity with human coders' performance in medical code prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic clinical coding (ACC) and emerging AMC are technologies that use natural language processing (NLP) to generate diagnosis and procedure medical codes from clinical notes automatically. A human coder or health care provider scans the medical documentation in electronic health records (EHR), identifying essential information and annotating codes for that particular treatment or service with the current ACC engine. With a wide range of medical services and providers (primary care clinics, specialty clinics, emergency departments, mother-baby units, outpatient and inpatient units, etc.), the complexity of human coders' tasks increases as the medical industry advances, while productivity standards decrease as charts take more time to review. A typical performance for an inpatient coder is 2.5 charts per hour because the review includes capturing historical diagnoses, lab results, radiology results, and several notes from different providers seen during his or her hospital stay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizable Insights about Machine Learning in the Context of Healthcare</head><p>To better understand human coders' coding performance baseline, we have hired two professional coders to code the same MIMIC-III inpatient clinical notes testing set that the ML systems were also tested on. We found that the human coders' inter-agreement rates are not as high as believed, and the RAC model-based ML system exceeds the human coding baseline by a large margin. As far as we know, this is the first paper that benchmarks human coders' performance on the MIMIC-III dataset by estimating human coders' ability to agree with one another. Our work makes the three main contributions below:</p><p>? First, the human coding baseline is estimated via a primitive internal website. Given the reference diagnosis and procedure codes that Beth Israel Deaconess Medical Center (BIDMC) assigned, we have two coders independently code the different set of notes and evaluate where their total annotations have differed from the references.</p><p>? Second, a new RAC architecture that can process unstructured medical notes and attend to text areas annotating medical codes is developed. The main building blocks of RAC are built upon self-attention and code-title guided attention modules that work on sets of sentence vectors. In principle, we look at the medical codes prediction problem as a set-to-set assignment learning problem from the set of input sentence vectors to the set of code labels and employ the problem's unique permutation equivariant property 1 in the design.</p><p>? Third, based on the MIMIC-III dataset, we demonstrate the RAC model's effectiveness in the most challenging full codes prediction testing set from inpatient clinical notes. The RAC model wins over all the previously reported SOTA results considerably. Compared to models that integrate more priors like CNNs &amp; LSTMs, self-attentionbased models require more data to generalize well. Hence, to train with the same sized dataset, we utilize the sentence permutation-based data augmentation and stochastic weighted average training <ref type="bibr" target="#b7">(Izmailov et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automatic Clinical Coding: Automatic coding of clinical notes can be treated as a multilabel classification problem. The clinical coding problem has received lots of attention in the last few years from this perspective, and it was demonstrated that CNN and LSTM based deep learning models perform better than conventional machine learning models <ref type="bibr" target="#b2">(Baumel et al., 2017;</ref><ref type="bibr" target="#b17">Shi et al., 2017;</ref><ref type="bibr" target="#b13">Mullenbach et al., 2018;</ref><ref type="bibr" target="#b12">Li et al., 2019;</ref><ref type="bibr" target="#b6">Huang et al., 2019;</ref><ref type="bibr" target="#b20">Umair et al., 2019;</ref><ref type="bibr" target="#b25">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020;</ref><ref type="bibr" target="#b22">Vu et al., 2020)</ref>; lately, the works have expanded to non-English clinical notes <ref type="bibr" target="#b1">(Azam et al., 2019;</ref><ref type="bibr">Wang et al., 2020;</ref><ref type="bibr" target="#b16">Reys et al., 2020)</ref>. However, each approach has its disadvantages. The CNN based model requires a stack of many CNN layers to "see" the whole input, and while the LSTM based method is useful at overcoming this, it is not easily parallelizable in training and inference. Clinical Transformers: Converting the free-text in clinical notes into a representation that can easily be used remains one of the prime NLP challenges. With the boom of the Transformer based models, specialized domain versions are trained from scratch or adapted to domains and tasks (see <ref type="bibr" target="#b10">Lee et al., 2020;</ref><ref type="bibr" target="#b14">Peng et al., 2019;</ref><ref type="bibr" target="#b0">Alsentzer et al., 2019;</ref><ref type="bibr" target="#b5">Gu et al., 2020;</ref><ref type="bibr" target="#b26">Zhang et al., 2020)</ref>. However, successfully applying Transformer models to the medical codes prediction problem remains an open challenge as reported in <ref type="bibr">(Li and Yu, 2020;</ref><ref type="bibr" target="#b8">Ji et al., 2020)</ref>, primarily due to three reasons: first, free-text clinical notes are unstructured, riddled with spelling errors, and consist of language particular to the medical domain. Second, the output codes space is large, and there is a severe long-tail sparsity issue with over 68,000 codes in the new ICD-10-CM system <ref type="bibr">(CDC, 2015)</ref>, for example. Third, the standard Transformer-based models' potential disadvantage in handling longer length of text input than CNN and LSTM based models, which presumably can handle texts with an arbitrary length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Human-Level Coding Baselines</head><p>Figure 1: Our web interface for collecting human coders' assignment of a set of ICD-9 codes to inpatient discharge summaries, described in Section 3. The left column lists the set of ICD-9 codes, and each discharge summary is displayed in the right panel.</p><p>Note that an entire set of 4,075 codes are too long to show in the screenshot captured. There is a scroll bar next to the ICD-9 codes column that the coder can scroll to click on the appropriate code. Alternatively, the coders can use the find shortcut (e.g., Ctrl + F) and type the code to save time from scrolling to click on the appropriate code. All discharge summaries are sampled from openaccess, de-identified MIMIC-III patients' data treated in intensive care and is not subject to the HIPAA Privacy Rule restrictions on sharing protected health information <ref type="bibr" target="#b9">(Johnson et al., 2016)</ref>.</p><p>Evaluation Design: To assess the human coders coding baselines, we hired two professional CPC certified coders with more than five years of hospital coding, including ICD-9 coding experience. We asked them to assign a set of ICD-9 codes (from a shortlisted 4,075 <ref type="table">Table 1</ref>: Medical code annotation results (in %) by human coders compared to our RAC model-based ML system's prediction results on 508 random subsamples from the MIMIC-III-full-label testing set. Note that we evaluate the concordance of code assignments between the annotations and the human references in the testing set, as described in Section 3, and the ML system achieves 3.9 times better Micro-Jaccard similarity than human coders. codes) to a total of 508 inpatient discharge summaries randomly sub-sampled from the MIMIC-III testing set for about 30 hours over a week. The website was built using the Label Studio <ref type="bibr" target="#b19">(Tkachenko et al., 2020)</ref> for this task, and each coder is required to log in with their accounts to the website to get started. The coders are given the same discharge summary as was provided to our ML system and were asked to select (not to type) the best possible set of codes in the list for the displayed discharge summaries (see <ref type="figure">Figure 1</ref> for more details). This environment makes the tasks easier than paper coding, though less convenient than a full-fledged computer-assisted coding system. The intention is to provide the same possible conditions as for the ML system. Additionally, all coders are instructed to code as normally as possible without a time limit to spend on each chart and to use whatever resources they usually use while adhering to coding guidelines. They were asked to code continuously with as few interruptions and distractions as possible, not to communicate with other coders, and not to skip notes. Baseline Results: We calculated the similarity between the human coders we hired and the human coders that the BIDMC hired, available as a human reference in the MIMIC-III dataset, to estimate the human-coders inter-agreement as a human-coding baseline 2 . We also reported how much predicted annotations from our RAC model-based ML system agree with the same MIMIC-III reference to benchmark the ML system performance against the established human-level coding baseline. We summarized the measured human-coders interagreement rates in <ref type="table">Table 1</ref> in terms of the macro and micro Jaccard similarity, precision, and recall scores and compared them with our RAC-model based automated ML system. We found that estimated inter-coder agreement rates (i.e., human coding baseline) were not high as we initially thought and are far exceeded by the ML system's 3.9 times higher rate in Micro-Jaccard similarity. We are not here to claim that we reached super-human</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jaccard Similarity Precision</head><formula xml:id="formula_0">x 1 x 2 x nx Convolved Embedding Module E x,1 E x,2 E x,nx</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Module</head><p>Reader</p><formula xml:id="formula_1">U x,1 U x,2 U x,nx</formula><p>Code-title Guided Attention Module accuracy as there is room to improve the baseline; instead, this result highlights that our RAC-model based automated ML system is advanced enough to handle complex medical notes to help code more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RAC Model</head><p>In this section, we describe an end-to-end RAC model to solve the medical code prediction problem. Let C be the set of medical codes of size n y , C T be the set of concatenated long and short titles of all n y codes, x = (x 1 , ..., x nx ) T be the document with n x tokens, and y = (y 1 , ..., y ny ) T be a medical codes prediction vector. Our RAC model has two submodules: a reader and a coder. The reader is a self-attention module that takes a tokenized clinical note x as input, and the coder is a code-title guided attention module that predicts each medical code's likelihood vector y. <ref type="figure" target="#fig_0">Figure 2</ref> visualizes RAC architecture with these components, and we provide more details in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Read</head><p>Convolved Embedding Module: The Read operation first converts each token into a token embedding of dimension d by using an embedding layer followed by two one-dimensional CNN layers. Unlike prior works <ref type="bibr" target="#b13">(Mullenbach et al., 2018;</ref><ref type="bibr" target="#b25">Xie et al., 2019;</ref><ref type="bibr">Li and Yu, 2020;</ref><ref type="bibr" target="#b8">Ji et al., 2020)</ref>, we use CNN layers as an efficient preprocessing mechanism to capture an embedding for a group of nearby tokens (like n-grams), which is informative in the next self-attention module for expressing local dependencies in clinical notes. We let E x ? R nx?d be the convolved embedding matrix corresponding to the n x tokens in the input document.</p><p>Since the next self-attention module does not need to "know" the embeddings' orders, we do not need to concatenate with positional encoding, unlike the language modelings. We pre-train the Word2Vec Skip-gram model on all the notes in the training set using Gensim <ref type="bibr">(?eh??ek and Sojka, 2010)</ref>, with an embedding size of d = 300, a window size of 5, and a minimum word frequency count of 10 for 5 epochs. The pre-trained weights are then loaded into the embedding layer with a maximum input length of n x = 4096 tokens. We stack two CNN layers with d filters, a kernel size of 10, and a tanh activation function on top of the embedding layer. We apply dropout to the module output with a rate of 0.1.</p><p>Self-Attention Module: In our model, the self-attention module (SAM) is a stack of four identical layers. Each layer is a stack of single-head self-attention and feed-forward layers interleaved with residual connections and layer normalization similar to <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref>. In particular, for the convolved embeddings E x , the dot product attention is computed as follows:</p><formula xml:id="formula_2">Attn(E x ) = LN E x + Softmax (E x W q )(E x W k ) T ? d (E x W v ) .</formula><p>Here W q , W k , W v ? R d?d represent the projection matrices associated with the query, key, and value, respectively and LN represents the layer normalization. Subsequently, the layer's output takes the following form:</p><formula xml:id="formula_3">LN (Attn(E x ) + ?(Attn(E x )W 1 )W 2 ) , where W 1 ? R d?d f f , W 2 ? R d f f ?d ,</formula><p>and ? is a ReLU activation function. We use d f f = 1024 and and apply dropout to each sub-layer's output before it is added to the sub-layer input and normalized with a rate of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attend and Code</head><p>Code-title Embedding Module: We now illustrate the AttendAndCode function to effectively handle the large code output space's extreme long-tail sparsity. The majority of medical codes are rare, "tail" codes. For example, 60% of ICD codes appear less than 10 times in the MIMIC-III dataset. Moreover, there are roughly 16 codes on average in MIMIC-III discharge summaries, so 99.8% are zeros in y. To handle this extreme sparsity of the large code output space, unlike prior sub-optimal attempts <ref type="bibr" target="#b13">(Mullenbach et al., 2018;</ref><ref type="bibr" target="#b23">Wang et al., 2018;</ref><ref type="bibr" target="#b25">Xie et al., 2019)</ref>, we utilize the code titles (i.e., output labels) information in terms of queries for the next attention module. Code titles corresponding with medical codes are defined by ICD coding systems. Such code titles include, for example, "Intestinal infection due to Clostridium difficile (008.45)" and "Sinoatrial node dysfunction (427.81)".</p><p>We use the definition tables of the diagnoses and procedure codes, concatenate long and short titles together for all n y codes, and build C T first. By tokenizing C T with n t tokens, we have a title matrix T where T ? R ny?nt . From T input, the module extracts a code-title embedding of dimension d by using an embedding layer followed by a single CNN layer and Global Max Pooling layer. We let E t ? R ny?d be the extracted code-title embedding matrix. In the model, each concatenated code title is padded to n t = 36 tokens, the same pre-trained Word2Vec Skip-gram model weights that the reader used are loaded to initialize the embedding layer, and a single CNN layer with d filters, kernel size 10, and tanh activation function are used.</p><p>Code-title Guided Attention Module: This function computes code-level attention over the reader output to attend to different parts for each code. We explicitly use E t as a query matrix to guide where to attend from the reader output. Specifically, the approach leads to the following attention mechanism:</p><formula xml:id="formula_4">V x = Softmax E t U T x ? d U x ,</formula><p>where U x SAM(E x ) and V x ? R ny?d . Fundamental improvement from the per-label attention initially introduced in <ref type="bibr" target="#b13">(Mullenbach et al., 2018)</ref> is that E t is no longer randomly initialized. It is straightforward to show that queries close in Euclidean space have similar attention scores, and this property is very desirable for learning inter-relations between less frequent codes and the text effectively. Using E t as a query in computing attention scores has actually resulted in large performance gains, as clearly seen by comparing the first and second row in the RAC Models section of <ref type="table" target="#tab_1">Table 2</ref>. This reveals that using learned semantic patterns of code titles in query leads to improving the RAC model's quality, particularly for the "tail" codes.</p><p>With attended V x , finally, the module produces a code likelihood vector y as</p><formula xml:id="formula_5">y = ?(V x W 3 ),</formula><p>where W 3 ? R d?1 and ? is the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learn the Entire Model</head><p>Sentence Permutation: Unlike computer vision tasks, data augmentation should be done carefully in the NLP application due to the text's structure. By taking into account the problem's nature of permutation equivariance, we rely on a simple sentence permutation method. Provided that notes in the training set contain multiple sentences, they are shuffled in a random order to generate a new train sample with the same label. For our model training, we use the 3-fold augmentation to increase the training set size three times and conclude that it is beneficial, as seen by comparing the second and third rows in the RAC Models section of <ref type="table" target="#tab_1">Table 2</ref>. One hindsight here is that the second row shows the RAC model's clear wins over the best prior baselines across the board without providing the augmented data. In other words, this indicates that the performance improvements of the RAC model are not simply the effects of the sentence permutation. Stochastic Weighted Averaging (SWA): With the augmented train data ready, the entire RAC model is trained on the medical codes prediction task to maximize the loglikelihood of n y binary classifiers. Rather than using traditional ensembling techniques to combine multiple models to make an averaged final prediction, we apply the SWA approach. Using SWA, we store the running average of model weights during training, so prediction is faster than conventional ensemble methods. In our training, SWA is averaged every 5 epochs from the first epoch, which provides positive gains, as noted by comparing the last two rows in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Codes Prediction by Machines</head><p>MIMIC-III Dataset: The MIMIC-III Dataset (MIMIC v1.4) is a freely accessible medical database containing de-identified medical data of over 40,000 patients staying in the Beth Israel Deaconess Medical Center between 2001 and 2012. For this study, we extract the discharge summaries and the corresponding ICD-9 codes 3 . We perform the same data processing, and data split as stated in <ref type="bibr" target="#b13">(Mullenbach et al., 2018)</ref> for a direct comparison 3. There are two reasons that the ICD-9 codes are used in the experimental studies. First, the publicly accessible MIMIC-III dataset used for the study was collected between 2001 and 2012 before the ICD-10 adoption. Second, the MIMIC-III dataset has been used as a standard benchmark in the prior studies, making meaningful head-to-head comparisons with our work. Additionally, we believe the proposed RAC model is not limited to the ICD-9 system. It is anticipated to work for a more complex and sparse environment (like ICD-10 and forthcoming ICD-11 codesets) with the help of employing code-title guided attention. The advantage of employing the RAC model (vs. prior arts) will be even more pronounced.</p><p>with prior works. This processing results in 47,724 samples for training, 1,632 and 3,373 samples for validation and testing. One can refer to <ref type="table" target="#tab_1">Table 2</ref> in <ref type="bibr" target="#b13">(Mullenbach et al., 2018)</ref> for more dataset statistics.</p><p>Training Details: For this study, we use an Adam optimizer with a learning rate of 8e-5 and pick a batch size of 16 on 4 Nvidia T4 GPUs. All model training runs are carried with the early stopping after the Precision@8 on the validation set does not improve for 3 epochs. The RAC model typically converges within 10 epochs. The model at the epoch of the highest Precision@8 is evaluated on the testing set. Precision@n corresponds to the number of ground truth labels among the n top-scored outputs, and we made this choice because Precision@8 is informative in a production scenario of providing a fixed number of codes to review.</p><p>Performance Results: We predict a total of 8,921 unique ICD-9 codes (composed of 6,918 diagnosis and 2,003 procedure codes) available in the MIMIC-III-full-label dataset. A smaller subset of code prediction tasks (e.g., TOP-10 or TOP-50 frequent codes) is not considered because it is less challenging and has limited value in our production scenario. We summarized all the previously reported SOTAs values in <ref type="table" target="#tab_1">Table 2</ref> in terms of the macro and micro average AUC, F1, and Precision@n scores to compare with prior works. Note that the RAC model's number can be further optimized for each metric. <ref type="table" target="#tab_1">Table 2</ref> shows the RAC models' superior performance over the past SOTA models across the board. What stands out is that the RAC model show very sharp improvements, 18.7% at Macro-F1 and 3.0% at Macro-AUC. These advancements imply that the RAC model learns a better mapping from the clinical text input to the infrequent codes since macro-metrics places more weight on the uncommon codes. Comparing the last four rows in <ref type="table" target="#tab_1">Table 2</ref> highlights each component's contributions to the overall performance gains in the RAC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we present for the first time a human-coding baseline for medical code prediction on the subsampled MIMIC-III-full-label inpatient clinical notes testing set task. We have developed an attention-based RAC model that sets the new SOTA records, and the resulting RAC model outperforms the human-coding baseline to a great extent on the same task. The performance improvements can be attributed to effectively learning the common embedding space between the clinical note and medical codes by utilizing attention mechanisms that efficiently address the severe long-tail sparsity issues. This achievement is one step forward to the bigger vision of a fully autonomous ML coding system that autonomously codes the medical charts without input from human coders.</p><p>Limitations: The current evaluation has the disadvantage of taking only discharge summaries out of the entire inpatient medical chart. We could turn to advanced models to handle more complex inpatient charts containing various service records. We have not discussed how much the RAC model's accurate prediction performance positively impacts the coding professionals and the healthcare delivery organizations. Conceivably, the computed attention scores can be visualized to inform the notes' relevant portions to understand the predictions. We intend to evaluate how much this visualized insight along with the presentation of likely medical codes would help coding teams not waste their time and resources as a future study.</p><p>Ethics and Broader Impact: An automated ML system for medical code prediction, first of all, intends to streamline the medical coding workflow, reduce human coders' backlog by increasing productivity, and help human coders navigate through extended and complex charts quickly while reducing coding errors <ref type="bibr" target="#b4">(Crawford, 2013)</ref>. Suppose relevant diagnoses and procedures are annotated with the appropriate codes, human coders can review the record much more quickly and validate the correct codes. Secondly, the automated system intends to reduce the administrative burden on providers, who could instead focus on delivering care rather than learning the nuances of coding. It also helps maximize revenue by capitalizing on the level of specificity noted in the documentation. This will impact many corners of the medical claims billing process by improving coding accuracy and consistency and reducing denials and compliance risk. Moreover, better-automated software can further enhance clinical documentation, making the overall picture of its quality better, eventually redirecting the wasted healthcare costs to more meaningful purposes <ref type="bibr" target="#b18">(Shrank et al., 2019)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Graphical visualization of Read, Attend, and Code (RAC) model architecture with the reader and coder modules, as described in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Medical codes prediction results (in %) by ML systems on the MIMIC-III-fulllabel testing set. The bold value shows the best (and highest) SOTA value for each column metric, and underlined numbers indicate the previous SOTA result. The proposed RAC model outperforms all the previously reported SOTA models and achieves new SOTA milestones across all the evaluation metrics. Note that the RAC model has shown an 18.7% relative performance increase in Macro-F1 over the prior best SOTA model. Results for the baseline models are taken from<ref type="bibr" target="#b13">(Mullenbach et al., 2018)</ref>.</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell>F1</cell><cell></cell><cell cols="2">Precision@n</cell></row><row><cell></cell><cell cols="4">Macro Micro Macro Micro 5</cell><cell>8</cell><cell>15</cell></row><row><cell>Baseline Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic Regression</cell><cell>56.1 93.7</cell><cell>1.1</cell><cell>27.2</cell><cell></cell><cell>54.2 41.1</cell></row><row><cell>SVM</cell><cell></cell><cell></cell><cell>44.1</cell><cell></cell></row><row><cell>CNN</cell><cell>80.6 96.9</cell><cell>4.2</cell><cell>41.9</cell><cell></cell><cell>58.1 44.3</cell></row><row><cell>Bi-GRU</cell><cell>82.2 97.1</cell><cell>3.8</cell><cell>41.7</cell><cell></cell><cell>58.5 44.5</cell></row><row><cell>CNN-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAML (Mullenbach et al., 2018)</cell><cell>89.5 98.6</cell><cell>8.8</cell><cell>53.9</cell><cell></cell><cell>70.9 56.1</cell></row><row><cell cols="2">DR-CAML (Mullenbach et al., 2018) 89.7 98.5</cell><cell>8.6</cell><cell>52.9</cell><cell></cell><cell>69.0 54.8</cell></row><row><cell>MSATT-KG (Xie et al., 2019)</cell><cell>91.0 99.2</cell><cell>9.0</cell><cell>55.3</cell><cell></cell><cell>72.8 58.1</cell></row><row><cell>MultiResCNN (Li and Yu, 2020)</cell><cell>91.0 98.6</cell><cell>8.5</cell><cell>55.2</cell><cell></cell><cell>73.4 58.4</cell></row><row><cell>LSTM-based Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LAAT (Vu et al., 2020)</cell><cell>91.9 98.8</cell><cell>9.9</cell><cell cols="3">57.5 81.3 73.8 59.1</cell></row><row><cell>JointLAAT (Vu et al., 2020)</cell><cell>92.1 98.8</cell><cell>10.7</cell><cell>57.5</cell><cell cols="2">80.6 73.5 59.0</cell></row><row><cell>RAC Models (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RAC -SWA -SentPerm -CodeTitles 92.4 98.9</cell><cell>11.0</cell><cell>57.4</cell><cell cols="2">81.5 74.2 58.8</cell></row><row><cell>RAC -SWA -SentPerm</cell><cell>94.9 99.2</cell><cell>11.4</cell><cell>58.0</cell><cell cols="2">82.3 74.9 59.5</cell></row><row><cell>RAC -SWA</cell><cell>94.8 99.2</cell><cell>12.6</cell><cell>58.2</cell><cell cols="2">82.6 74.9 59.8</cell></row><row><cell>RAC</cell><cell>94.8 99.2</cell><cell cols="4">12.7 58.6 82.9 75.4 60.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. We measured the human coding baseline by not comparing the agreement of the two certified coders we hired. Instead, we compared the coders we hired with the BIDMC coders. The BIDMC coders' data is available as a reference label in the MIMIC-III dataset. Of course, neither is an absolute golden truth for inherent ambiguity in the human coding process. Alternatively, we could have two coders independently code, and the notes where they differed are coded independently by a third coder to collect golden reference. We were short on time to pursue this direction when preparing for submission.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>During this work, the discussions and actual human coding baseline projects held with Amy Raymond and Grant Messick proved stimulating and helpful. The author also would like to thank Jesse Swidler, Peng Su, and Shan Huang for considerable infrastructure support. The suggestions of Heidi Lim to improve the figures and website visualizations are also gratefully acknowledged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Publicly available clinical BERT embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B A</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clinical Natural Language Processing (ClinicalNLP) Workshop at NAACL 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascadenet: An LSTM based deep learning model for automated ICD-10 coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Sheikh Shams Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi Chandra</forename><surname>Pagidimarri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasivajjala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information and Communication (FICC 2019)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Michael Elhadad, and No&apos;emie Elhadad. Multi-label classification of patient notes a case study on ICD code assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jumana</forename><surname>Nassour-Kassis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Classification of diseases, functioning, and disability. National Center for Health Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Centers for Disease Control and Prevention</publisher>
		</imprint>
		<respStmt>
			<orgName>CDC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Truth about computer-assisted coding: A consultant, HIM professional, and vendor weigh in on the real CAC impact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Health Information Management Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="24" to="27" />
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep learning for ICD-9 code assignment using MIMIC-III clinical notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinmiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Osorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke Wicent</forename><surname>Sy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="141" to="153" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI 2018)</title>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI 2018)<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dilated convolutional attention network for medical code assignment from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Clinical Natural Language Processing Workshop</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ICD coding from clinical text using multi-filter residual convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020)</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated ICD-9 coding via a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1193" to="1202" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explainable prediction of medical codes from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1101" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioNLP 2019: 18th ACL Workshop on Biomedical Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radim?eh??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
	<note>ELRA</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">D</forename><surname>Reys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saulo</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcia</forename><forename type="middle">M</forename><surname>De Souza E S?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilherme</forename><forename type="middle">A C</forename><surname>Salgado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01515</idno>
		<title level="m">Predicting multiple ICD-10 codes from brazilian-portuguese clinical notes</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04075</idno>
		<title level="m">Towards automated ICD coding using deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Waste in the US health care system: Estimated costs and potential for savings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><forename type="middle">L</forename><surname>Shrank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Rogstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Network</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1501" to="1509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Label Studio: Data labeling software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tkachenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Malyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Shevchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Holmanyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Liubimov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>URL https</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multi-channel convolutional neural network for ICD coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Umair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sawar</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Aftab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE 2019)</title>
		<meeting>the 2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE 2019)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1178" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A label attention model for ICD coding from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint embedding of words and labels for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2321" to="2331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using deep learning for automatic ICD-10 classification from free-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ssu-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu-Cheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Yun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zong-Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufang</forename><surname>Chungy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EHR coding with multiscale feature attention and structured knowledge graph propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiancheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM 2019)</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management (CIKM 2019)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="649" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BERT-XML: Large scale automated ICD coding using BERT pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachariah</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingshu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Clinical Natural Language Processing Workshop</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
