<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rawal</forename><surname>Khirodkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visesh</forename><surname>Chari</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Lab 126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Lab 126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Amazon Lab 126</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A key assumption of top-down human pose estimation approaches is their expectation of having a single person/instance present in the input bounding box. This often leads to failures in crowded scenes with occlusions. We propose a novel solution to overcome the limitations of this fundamental assumption. Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose instances within a given bounding box. We introduce a Multi-Instance Modulation Block (MIMB) that can adaptively modulate channel-wise feature responses for each instance and is parameter efficient. We demonstrate the efficacy of our approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically, we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using ground truth bounding boxes for inference, MIP-Net achieves an improvement of 0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets compared to HRNet. Interestingly, when fewer, high confidence bounding boxes are used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet maintains a relatively stable performance (drop of 1 AP) for the same inputs. arXiv:2101.11223v3 [cs.CV] 28 Oct 2021 Recently, challenging datasets such as OCHuman [48] and CrowdPose [23] containing heavy occlusion have been proposed to capture these biases. These datasets demonstrate the failures of the state-of-art models under severe occlusions (Section 4.3). MIPNet shows a significant improvement in performance under such challenging conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation aims at localizing 2D human anatomical keypoints (e.g., elbow, wrist, etc.) in a given image. Current human pose estimation methods can be categorized as top-down or bottom-up methods. Top-down methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> take as input an image region within a bounding box, generally the output of a human detector, and reduce the problem to the simpler task of single human pose estimation. Bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, in contrast, start by independently localizing keypoints in the entire image, followed by grouping them into 2D human pose instances.</p><p>The single human assumption made by top-down approaches limits the inference to a single configuration of human joints (a single instance) that can best explain the input. Top-down pose estimation approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> are currently the best performers on datasets such as COCO <ref type="bibr" target="#b24">[25]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref>. However, when presented with inputs containing multiple humans like crowded or occluded instances, topdown methods are forced to select a single plausible configuration per human detection. In such cases, top-down methods may erroneously identify pose landmarks corresponding to the occluder (person in the front). See, for example, <ref type="figure" target="#fig_0">Fig. 1 (Middle)</ref>. Therefore, on datasets such as CrowdPose <ref type="bibr" target="#b22">[23]</ref> and OCHuman <ref type="bibr" target="#b47">[48]</ref>, which have a relatively higher proportion of occluded instances <ref type="table">(Table 1)</ref>, the performance of top-down methods suffer due to the single person assumption <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>In this paper, we rethink the architecture for top-down 2D pose estimators by predicting multiple pose instances for the input bounding box. The key idea of our proposed architecture is to allow the model to predict more than one pose instance for each bounding box. We demonstrate that this conceptual change improves the performance of top-Input Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HRNet</head><p>MIPNet, ? = 0 MIPNet, ? = 1 <ref type="figure">Figure 2</ref>: Heatmap predictions for a few keypoints from HRNet vs MIPNet. HRNet only focuses on the foreground person. MIPNet enables prediction of the multiple instances from the same input bounding box by varying ? during inference.</p><p>down methods, especially for images with crowding and heavy occlusion. A na?ve approach to predict multiple instances per bounding box would be to add multiple prediction heads to an existing top-down network with a shared feature-extraction backbone. However, such an approach fails to learn different features corresponding to the various instances. A brute-force approach would then be to replicate the feature-extraction backbone, though at a cost of an N -fold increase in parameters, for N instances. In contrast, our approach enables predicting multiple instances for any existing top-down architecture with a small increase in the number of parameters (&lt; 3%) and inference time (&lt; 9ms, 16%). Technically, our approach can handle N &gt; 2 instances. However, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>, number of examples with 3+ annotated pose instances per ground truth bounding box in existing datasets is extremely small. Thus, similar to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b47">48]</ref>, we primarily focus on the dominant occlusion scenario involving two persons. To enable efficient training and inference of multiple instances in a given bounding box, we propose a novel Multi-Instance Modulation Block (MIMB). MIMB modulates the feature tensors based on a scalar instance-selector, ?, and allows the network to index on one of the N instances ( <ref type="figure">Fig. 2</ref>). MIMB can be incorporated in any existing feature-extraction backbone, with a relatively simple (&lt; 15 lines) code change (refer supplemental). At inference, for a given bounding box, we vary the instance-selector ? to generate multiple pose predictions ( <ref type="figure" target="#fig_1">Fig. 3</ref>).</p><p>Since top-down approaches rely on the output from an object detector, they typically process a large number of bounding box hypotheses. For example, HRNet <ref type="bibr" target="#b39">[40]</ref> uses more than 100K bounding boxes from Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> to predict 2D pose for ? 6000 persons in the COCO val dataset. Many of these bounding boxes overlap and majority have low detection scores (&lt; 0.4). This also adversely impacts the inference time, which increases linearly with the number of input bounding boxes. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, using fewer, high confidence bounding boxes degrades the performance of HRNet from 37.8 to 32.8 AP on OCHuman, a degradation of 5 AP in performance. In contrast, MIPNet is robust and maintains a relatively stable performance for the same inputs (drop of 1 AP). Intuitively, our method can predict the 2D pose instance corresponding to a mis-detected ? MIPNet allows predicting multiple pose instances for a given bounding box efficiently by modulating feature responses for each instance independently.</p><p>? The ability to predict multiple instances makes MIPNet resilient to bounding box confidence and allows it to deal with missing bounding boxes with minimal impact on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Biased benchmarks: Most human pose estimation benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> do not uniformly represent possible poses and occlusions in the real world. Popular datasets such as COCO <ref type="bibr" target="#b24">[25]</ref> and MPII <ref type="bibr" target="#b1">[2]</ref> have less than 3% annotations with crowding at IoU of 0.3 <ref type="bibr" target="#b34">[35]</ref>. More than 86% of annotations in COCO <ref type="bibr" target="#b24">[25]</ref> have 5 or more keypoints visible <ref type="bibr" target="#b37">[38]</ref>. These biases have seeped into our state-of-theart data driven deep learning models <ref type="bibr" target="#b44">[45]</ref>, not only in the form of poor generalization to "in-the-tail" data but surprisingly in critical design decisions for network architectures. Top-down methods: Top-down methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> detect the keypoints of a single person within a bounding box. These bounding boxes are usually generated by an object detector <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. As topdown methods can normalize all the persons to approximately the same scale by cropping and resizing the bounding boxes, they are generally less sensitive to scale variations in images. Thus, state-of-the-art performances on various human pose estimation benchmarks are mostly achieved by top-down methods <ref type="bibr" target="#b39">[40]</ref> in contrast to bottom-up methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>. However, these methods inherently assume a single person (instance) in the detection window and often fail under occlusions in multi-person cases. It is the ambiguity of the implicit bounding-box level representation that leads to this failure. MIPNet resolves this issue by predicting multiple instances within a single bounding box.</p><p>Occluded pose estimation: Many methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9]</ref> have made good progress in occluded person detection. Recent methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19]</ref> have also focused on occluded pose estimation. <ref type="bibr" target="#b22">[23]</ref> uses a top-down model to make a multi-peak prediction and joint peaks are then grouped into persons using a graph model. <ref type="bibr" target="#b47">[48]</ref> uses instance segmentation for occlusion reasoning. <ref type="bibr" target="#b34">[35]</ref> use a graph neural network to refine pose proposals from a top-down model. <ref type="bibr" target="#b18">[19]</ref> is a bottom-up method which uses a differentiable hierarchical graph grouping for joint association. In contrast, our approach is much simpler and does not require initial pose estimates, grouping or solving for joint association.</p><p>Lastly, in machine learning, many models have been trained to behave differently depending on a conditional input <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref>. Instead of training multiple models, our approach enables training a single network for predicting multiple outputs on the same input. Rather than duplicating the feature backbone, our novel MIMB block leads to a parameter efficient design. Our multi-instance pose network is fully supervised and not related to multiple in-stance learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref>, which is a form of weakly-supervised learning paradigm where training instances are arranged in sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Human pose estimation aims to detect the locations of K keypoints from an input image x ? R H?W ?3 . Most top-down methods transform this problem to estimating K heatmaps, where each heatmap indicates the probability of the corresponding keypoint at any spatial location. Similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref> we define a convolutional pose estimator, P , for human keypoint detection. The bounding box at training and inference is scaled to H ?W and is provided as an input to P . Let y ? R H ?W ?K denote the K heatmaps corresponding to the ground truth keypoints for a given input x. The pose estimator transforms input x to a single set of predicted heatmaps,? ? R H ?W ?K , such that? = P (x). P is trained to minimize the mean squared loss L = MSE(y,?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Multi-Instance Pose Network</head><p>We propose to modify the top-down pose estimator P to predict multiple instances as follows. Our pose estimator P predicts N instances,? 0 , . . . ,? N ?1 for an input x. This is achieved by conditioning the network P on a scalar instanceselector ?, 0 ? ? ? N ? 1. P accepts both x and ? as input</p><formula xml:id="formula_0">and predicts? i = P (x, ? = i), where i ? {0, 1, . . . , N ?1}.</formula><p>Let B 0 denote the ground truth bounding box used to crop the input x. Let B i , i ? {1, ..n ? 1}, denote additional n ? 1 ground truth bounding boxes which overlap B 0 , such that at least k = 3 keypoints from B i fall within B 0 . Thus, B 0 , . . . , B n?1 represents the bounding boxes for n ground truth pose instances present in x. We denote the ground truth heatmaps corresponding to these n instances by y 0 , . . . y n?1 .</p><p>To define a loss, we need to assign the predicted pose instances to the ground truth heatmaps. The primary instanc? y 0 = P (x, ? = 0) is assigned to y 0 , the pose instance corresponding to B 0 . The next N ? 1 instances are assigned to the remaining ground truth heatmaps ordered according to the distance of their corresponding bounding box from B 0 . We train the network P to minimize the loss L =</p><formula xml:id="formula_1">1 N N ?1 i=0 L i , where, L i = MSE(y i , P (x, ? = i)), ? 0 ? i &lt; min(n, N ), MSE(y 0 , P (x, ? = i)), ? min(n, N ) ? i &lt; N.<label>(1)</label></formula><p>When n &lt;= N , the available n ground truth pose instances are used to compute the loss for n predictions, and the loss for residual N ? n instances is computed using y 0 . For example, when n = 1 and N = 2, both the predictions are encouraged to predict the heatmaps corresponding to the single ground truth instance present in x. In contrast, when n &gt; N , only N ground truth pose instances (closest to B 0 ) are used to compute the loss.</p><p>In our experience, employing other heuristics such as not propagating the loss, i.e., don't care for residual instances resulted in less stable training. Additionally, a don't care based training scheme for residual instances resulted in significantly higher false positives, especially as we do not know the number of valid person instances per input at runtime. During inference, we vary ? to extract different pose predictions from the same input x as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Instance Modulation Block</head><p>In this section, we describe the Multi-Instance Modulation Block (MIMB) that can be easily introduced in any existing feature extraction backbone. The MIMBs allow a top-down pose estimator P to predict multiple instances from an input image x. Using MIMBs, P can now accept both x and the instance-selector ? as inputs. The design of MIMB is inspired by the squeeze excite block of <ref type="bibr" target="#b13">[14]</ref>. Let X ? R P ?Q?C be an intermediate feature map with C channels, such that X = [x 1 , x 2 , . . . , x C ]. We use an instance-selector ? to modulate the channel-wise activations of the output of the excite module as shown in <ref type="figure" target="#fig_1">Fig. 3 (Right)</ref>.</p><p>The key insight of our design is that we can use the same set of convolutional filters to dynamically cater to different instances in the input. Compared to a brute force approach of replicating the feature backbone or assigning a fixed number of channels per instance, our design is parameter efficient.</p><p>Let F sq , F ex , F em denote the squeeze, excite, and embed operations, respectively, within MIMB. We represent ? as the one hot representation of scalar ?. The feature map X is transformed to X = [x 1 , x 2 , . . . , x C ] as follows, into a channel descriptor using global average pooling. F ex allows modeling for channel-wise interactions on the output of F sq . F ex is implemented as a two layer, fully-connected, neural network. Following the output of the excite module, we modulate the channel-wise activations using the embedding of ? from another simple neural network F em . F em has a similar design to F ex . During inference, we vary the instance-selector ? from 0 to N ?1 to get N predictions and then apply OKS-NMS <ref type="bibr" target="#b39">[40]</ref> after merging all predictions. Please refer supplemental for details. <ref type="figure">Figure 2</ref> visualizes the predicted heatmaps from HRNet and MIPNet (using N = 2). Note that HRNet only outputs the heatmap corresponding to the foreground person while MIPNet predicts heatmaps for both persons using different values of ? at inference.</p><formula xml:id="formula_2">s c = F sq (x c ),<label>(2)</label></formula><formula xml:id="formula_3">e = F ex (s),<label>(3)</label></formula><formula xml:id="formula_4">v = F em (?),<label>(4)</label></formula><formula xml:id="formula_5">x c = (v c ? e c )x c ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate MIPNet on three datasets: Common-Objects in Context-COCO <ref type="bibr" target="#b24">[25]</ref>, CrowdPose <ref type="bibr" target="#b22">[23]</ref> and Occluded Humans-OCHuman <ref type="bibr" target="#b47">[48]</ref>. These datasets represent varying degrees of occlusion/crowding (see <ref type="table">Table 1</ref>) and help illustrate the benefits of predicting multiple instances in top-down methods. We report standard metrics such as AP, AP 50 , AP 75 , AP M , AP L , AR, AP easy , AP med and AP hard at various Object Keypoint Similarity as defined in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref>. We report results using ground truth bounding boxes as well as bounding boxes obtained via YOLO <ref type="bibr" target="#b35">[36]</ref> and Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> detectors.</p><p>We base MIPNet on recent state-of-the-art top-down architectures, namely, SimpleBaseline <ref type="bibr" target="#b43">[44]</ref> and HRNet <ref type="bibr" target="#b39">[40]</ref>. When comparing with HRNet, MIPNet employs a similar feature extraction backbone and adds MIMBs' at the output of the convolutional blocks at the end of stages 3 and 4 <ref type="bibr" target="#b39">[40]</ref>. For comparisons with SimpleBaseline <ref type="bibr" target="#b43">[44]</ref>, two MIMB's are added to the last two ResNet blocks in the encoder.</p><p>Number of instances N : Trivially, N = 1 is equivalent to baseline top-down methods. By design, MIPNet supports predicting multiple instances. Empirically, on average we observed a small improvement of 0.3 AP, 0.5 AP using N = 3 and N = 4 on top of N = 2 respectively on the datasets. This is consistent with the fact that most datasets have very few examples with three or more ground-truth pose instances   <ref type="bibr" target="#b43">[44]</ref>. #Params are only of the pose estimation network, excluding bounding box computation.</p><p>per bounding box <ref type="figure" target="#fig_2">(Fig. 4)</ref>. However, N = 2 provides a substantial improvement over N = 1 baseline as shown in our experiments. Note that since the MIMBs are added to the last few stages in our experiments, the increase in inference time due to predicting N = 2 instances is small <ref type="table" target="#tab_4">(Table 3)</ref>. For bigger HRNet-48 network with input resolution of 384? 288, inference time increases by 8.2ms (16.7%). For smaller HRNet-32 network, increase in run-time is 4.7ms (11.9%). This is significantly better than replicating the backbone for each instance, which would lead to a 2x increase in inference time for N = 2. Please refer supplemental for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">COCO Dataset</head><p>Dataset: COCO contains 64K images and 270K persons labeled with 17 keypoints. For training we use the train set (57K images, 150K persons) and for evaluation we use the val (5K images, 6.3K persons) and the test-dev set (20K images). The input bounding box is extended in either height or width to obtain a fixed aspect ratio of 4 : 3. The detection box is then cropped from the image and is resized to a fixed size of either 256 ? 192 or 384?288, depending on the experiment. Following <ref type="bibr" target="#b28">[29]</ref>, we use data augmentation with random rotation ([?45 ? , 45 ? ]), random scale ([0.65, 1.35]), flipping, and half-body crops. Following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>, we use flipping and heatmap offset during inference.</p><p>Results: <ref type="table" target="#tab_2">Table 2</ref>   50 backbone, MIPNet improves the SimpleBaseline results by 0.9 AP for smaller input size and 1.2 AP for larger input size. Comparing with HRNet, MIPNet shows an improvement ranging from 0.7 to 1.1 AP on various architectures and input sizes. Note that MIPNet results in &lt; 3% increase in parameters compared to the baselines.</p><p>When using bounding boxes obtained from a person detector, as expected, MIPNet performs comparably to SBL and HRNet when using the same backbone ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CrowdPose Dataset</head><p>Dataset: CrowdPose contains 20K images and 80K persons labeled with 14 keypoints. CrowdPose has more crowded scenes as compared to COCO, but the index of crowding is less compared to the OCHuman <ref type="bibr" target="#b47">[48]</ref>. For training, we use the train set (10K images, 35.4K persons) and for evaluation we use the val set (2K images, 8K persons) and test set (8K images, 29K persons).</p><p>Results: <ref type="table" target="#tab_4">Table 3</ref> compares the performance of MIPNet with HRNet when evaluated using ground-truth bounding boxes. MIPNet outperforms HRNet with improvements in AP ranging from 0.9 to 1.5 across different input sizes. As shown in <ref type="table" target="#tab_5">Table 5</ref>, when evaluated using person detector bounding boxes, MIPNet improves SBL by 7.3 AP on the test set with an increase of less than 25 ms in inference time. For completeness, we also trained and evaluated HR-Net on CrowdPose. MIPNet outperforms HRNet by 0.7 AP on the test set and 0.8 AP on the val set. MIPNet achieves state-of-the-art performance of 70.0 AP comparable to the two-stage method OPECNet <ref type="bibr" target="#b34">[35]</ref> which refines initial pose estimates from AlphaPose+ <ref type="bibr" target="#b34">[35]</ref>. We report additional results in the supplemental.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">OCHuman Dataset</head><p>Dataset: OCHuman is focused on heavily occluded humans. It contains 4731 images and 8110 persons labeled with 17 keypoints. In OCHuman, on an average 67% of the bounding box area has overlap with other bounding boxes <ref type="bibr" target="#b47">[48]</ref>, compared to only 0.8% for COCO. Additionally, the number of examples with occlusion IoU &gt; 0.5 is 68% for OCHuman, compared to 1% for COCO <ref type="table">(Table 1)</ref>. This makes the OCHuman dataset complex and challenging  Similar to <ref type="bibr" target="#b47">[48]</ref>, we use the train set of COCO for training. Note that we do not train on the OCHuman train set. For evaluation, we use the val set (2, 500 images, 4, 313 persons) and the test set (2, 231 images, 3, 819 persons).</p><p>Results: <ref type="table" target="#tab_7">Table 4</ref> compares the performance of MIPNet with SimpleBaseline and HRNet on OCHuman when evaluated with ground truth bounding boxes on the val set. MIPNet significantly outperforms SimpleBaseline with improvements in AP ranging from 7.7 to 10.5, across various architectures and input sizes. Similarly, for HRNet the performance gains between 7.7 to 9.4 AP are observed.</p><p>Current state-of-the-art results on OCHuman are reported by HGG <ref type="bibr" target="#b18">[19]</ref> (bottom-up method, multi-scale testing) as shown in <ref type="table" target="#tab_5">Table 5</ref>. In addition, we also evaluated MIPNet using person detector boxes on OCHuman with same backbones as baselines for a fair comparison. MIPNet with ResNet101 backbone and YOLO bounding boxes outperforms OPEC-Net by 5.9 AP on the test set. When using Faster R-CNN bounding boxes, MIPNet outperforms HRNet and HGG by 5.3 AP and 6.5 AP, respectively, on the test set. The improvements are significant and to the best of our knowledge, this is the first time a top-down method has outperformed the state-of-the-art bottom-up method using multi-scale testing on OCHuman. <ref type="figure">Figure 8</ref> shows qualitative results on several examples from OCHuman, highlighting the effectivness of MIPNet in recovering multiple poses under challenging conditions.</p><p>Robustness to Human Detector Outputs: The performance of top-down methods is often gated by the quality of human detection outputs. We analyze the robustness of HRNet and MIPNet with varying detector confidence on OCHuman in <ref type="figure" target="#fig_4">Fig. 5</ref>  bounding boxes) to 32.8 AP (6644 bounding boxes), when the detector confidence is varied from 0 to 0.9. Since HRNet is only able to provide a single output per bounding box, the average precision drops corresponding to misdetections on the occluded persons. In contrast, MIPNet maintains a relatively stable performance (drop of 1 AP) as shown in <ref type="figure" target="#fig_4">Fig. 5</ref> for the same inputs. Since MIPNet can predict multiple instances, it can recover pose configurations for occluded persons despite misdetection of their corresponding bounding boxes. This is a desirable property afforded by the proposed MIPNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head><p>Comparison to Two-Heads baseline: We compare MIP-Net against the Two-Heads baseline which has a primary head (? = 0) and a secondary head (? = 1) in <ref type="table" target="#tab_11">Table 6</ref>. To analyze the effect of head capacity in multi-instance prediction, we create two baselines: Two-Heads (light), and Two-Heads (heavy). MIPNet consistently outperforms the Two-Heads baseline on the OCHuman dataset. Please refer supplemental for more details. Visualization with continuous ?: MIPNet's ability to predict multiple instances provides a useful tool to visualize how predictions can dynamically switch between various pose configurations. After training MIPNet using an one-hot representation of ?, during inference, we use a soft representation of [?, 1 ? ?] as instance-selector for the MIPNet. <ref type="figure">Fig. 6</ref> shows how the predicted keypoints gradually shift from the foreground person to the other pose instance within the bounding box, as ? is varied from 0 to 1.  and HRNet on the val sets using HRNet-W32 backbone with 256 ? 192 input resolution and ground-truth bounding boxes. <ref type="figure">Figure 6</ref>: As ? is varied from 0 to 1 during inference, the keypoints (in blue) gradually shift from the foreground person to the other pose instance within the bounding box. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Top-down 2D pose estimation methods make the key assumption of a single person within the input bounding box. While these methods have shown impressive results, the single person assumption limits their ability to perform well in crowded scenes with occlusions. Our proposed Multi-Instance Pose Network, MIPNet, enables top-down methods to predict multiple instances for a given input. MIPNet is efficient in terms of the number of additional network parameters and is stable with respect to the quality of the input bounding boxes. MIPNet achieves state-of-art results on challenging datasets with significant crowding and occlusions. We believe that the concept of predicting multiple instances is an important conceptual change and will inspire a new research direction for top-down methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*Figure 1 :</head><label>1</label><figDesc>Work done during an internship at Amazon ? Now at Waymo 2D pose estimation networks often fail in presence of heavy occlusion. (Left) Bounding boxes corresponding to two persons. (Middle) For both bounding boxes, HRNet predicts the pose for the front person and misses the occluded person. (Right) MIP-Net allows multiple instances for each bounding box and recovers the pose of the occluded person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(Left) MIPNet is trained to predict the i th instance from an input x by conditioning the network using ? = i, ? i = 0, . . . , N ? 1.(Middle) During inference, we obtain the N pose predictions by varying ?. (Right) MIMB uses squeeze, excitation and embed modules that enables ? to modulate the feature responses for each instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>s.t. s = [s 1 , . . . , s C ], v = [v 1 , . . . , v C ] and e = [e 1 , . . . , e C ]. F sq squeezes the global spatial information m b e r o f B B s ( % ) N u m b e r o f P e r s o n s i n B B ( I o U &gt; 0 . 5 ) C O C O C r o w d P o s e O C H u m a n Percentage of examples with 1, 2 and 3+ pose instances per ground truth bounding box in various datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>i m u m B o u n d i n g B o x C o n f i d e n c e H R N e t M I P N e t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Unlike HRNet, MIPNet maintains a stable performance as a function of detector confidence for selecting input bounding boxes. Results are shown using HRNet-W48-384 ? 288 evaluated on OCHuman val set. for human pose estimation under occlusion. The single person assumption made by existing top-down methods is not entirely applicable to examples in this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>MIPNet fails in some cases with significant scale difference between multiple persons in the bounding box.Limitations: In some cases, MIPNet can fail due to large difference in the scale of the various pose instances in a given bounding box, as shown inFigure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>MIPNet improves performance on COCO val set across various architectures and input sizes (using ground-truth bounding boxes).R-@ and H-@ stands for ResNet-@ and HRNet-W@ respectively. ? and denotes input resolution of 256 ? 192 and 384 ? 288 respectively. SBL refers to SimpleBaseline</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>compares the performance of MIPNet with SimpleBaseline (denoted as SBL) and HRNet using ground truth bounding boxes. MIPNet outperforms the baseline across various backbones and input sizes. Using ResNet-Arch Latency AP AP 50 AP 75 AP easy AP med AP hard HRNet-32 ? 27.5 ms 70.0 91.0 76.3 78.8 70.3 61.7 MIPNet ? 30.9 ms 71.2 91.9 77.4 78.8 71.5 63.8 HRNet-48 ? 33.8 ms 71.3 91.1 77.5 80.5 71.4 62.5 MIPNet ? 39.6 ms 72.8 92.0 79.2 80.6 73.1 65.2 HRNet-32 39.4 ms 71.6 91.1 77.7 80.4 72.1 62.6 MIPNet 44.1 ms 73.0 91.8 79.3 80.7 73.3 65.5 HRNet-48 49.1 ms 72.8 92.1 78.7 81.3 73.3 64.0 MIPNet 57.3 ms 73.7 91.9 80.0 80.7 74.1 66.5</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>MIPNet outperforms HRNet on CrowdPose val set. ? and denote input resolution of 256 ? 192 and 384 ? 288, respectively. Average GPU latency is reported with batch size 24.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>). Un-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on OCHuman val set with ground-truth bounding box evaluation after training on COCO train set. ? and denotes input resolution of 256 ? 192 and 384 ? 288 respectively. R-@ and H-@ stands for ResNet-@ and HRNet-W@ respectively. SBL refers to SimpleBaseline<ref type="bibr" target="#b43">[44]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>. As expected, HRNet performance degrades as low confidence bounding boxes are filtered out, leading to missed detections on occluded persons. Specifically, HRNet performance degrades from 37.8 AP (30637</figDesc><table><row><cell>Method</cell><cell cols="6">COCO val test val test val test CrowdPose OCHuman</cell></row><row><cell cols="6">Comparison with Top Down Methods, ResNet101 + YOLO-v3</cell><cell></cell></row><row><cell>MaskRCNN [13]</cell><cell>-</cell><cell>64.8</cell><cell>-</cell><cell>57.2</cell><cell>-</cell><cell>20.2</cell></row><row><cell>AlphaPose [23]</cell><cell>-</cell><cell>70.1</cell><cell>-</cell><cell>61.0</cell><cell>-</cell><cell>-</cell></row><row><cell>JC-SPPE [23]</cell><cell>-</cell><cell>70.9</cell><cell>-</cell><cell>66.0</cell><cell>-</cell><cell>-</cell></row><row><cell>AlphaPose+ [35]</cell><cell>-</cell><cell>72.2</cell><cell>-</cell><cell>68.5</cell><cell>-</cell><cell>27.5</cell></row><row><cell>OPEC-Net [35]</cell><cell>-</cell><cell>73.9</cell><cell cols="2">-70.6</cell><cell>-</cell><cell>29.1</cell></row><row><cell>SBL [44]</cell><cell>-</cell><cell>73.7</cell><cell>-</cell><cell>60.8</cell><cell>-</cell><cell>24.1</cell></row><row><cell>MIPNet (Ours)</cell><cell cols="6">72.7 74.2 63.4 68.1 32.8 35.0</cell></row><row><cell cols="7">Comparison with Top Down Methods, HRNet-W48-384 + Faster R-CNN</cell></row><row><cell>HRNet [40]</cell><cell cols="6">76.3 75.5 68.0 69.3 37.8 37.2</cell></row><row><cell>MIPNet (Ours)</cell><cell cols="6">76.3 75.7 68.8 70.0 42.0 42.5</cell></row><row><cell cols="6">Comparison with Bottom Up Methods, Multi-scale [?2, ?1, ?0.5]</cell><cell></cell></row><row><cell>AE [29]</cell><cell cols="3">? 65.5 ?</cell><cell cols="3">? 40.0 32.8</cell></row><row><cell>HghrHRNet [8]</cell><cell cols="2">67.1 70.5</cell><cell>-</cell><cell>67.6</cell><cell>-</cell><cell>-</cell></row><row><cell>HghrHRNet+UDP [15]</cell><cell cols="2">? 70.5</cell><cell>-</cell><cell>68.2</cell><cell>-</cell><cell>-</cell></row><row><cell>HGG [19]</cell><cell cols="2">68.3 67.6</cell><cell>-</cell><cell>-</cell><cell cols="2">41.8 36.0</cell></row><row><cell cols="7">MIPNet (Ours, Top Down) 76.3 75.7 68.8 70.0 42.0 42.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods using bounding boxes from a human detector on various datasets. Other numbers are reported from the respective publications.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Method #Params COCO OCHuman AP AP 50 AP 75 AP AP 50 AP 75 HRNet 28.5M 76.5 93.5 83.7 63.1 79.4 69.0 Two-Heads (light) 28.6M 76.7 93.4 84.0 64.0 78.7 71.2 Two-Heads (heavy) 48.9M 77.1 94.1 85.5 69.8 84.5 74.9 MIPNet 28.6M 77.6 94.4 85.3 72.5 89.2 79.4</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparison with the Two-Heads baseline (light, heavy)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">corr abs/1611.08050. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple instance learning: A survey of problem characteristics and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Andr?</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghyslain</forename><surname>Gagnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="453" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10357</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detection in crowded scenes: One proposal, multiple predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuangeng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12214" to="12223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">You only train once: Loss-conditional training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5700" to="5709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A coarsefine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="10863" to="10872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attentive single-tasking of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
	<note>Ilija Radosavovic, and Iasonas Kokkinos</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multi-pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3198" to="3205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bb</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Peeking into occluded joints: A novel framework for crowd pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingteng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10506</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Supplementary materials for the iccv 2017 paper: Benchmarking and error diagnosis in multi-instance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detection and tracking of occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="69" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">No bias left behind: Covariate shift adaptation for discriminative 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Raptis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="674" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Review of multi-instance learning and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>School of Computer Science Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pose2seg: Detection free human instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
