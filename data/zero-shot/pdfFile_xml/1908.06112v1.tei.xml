<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetric Cross Entropy for Robust Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Zaiyi</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Cainiao</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Symmetric Cross Entropy for Robust Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes ("easy" classes), but more surprisingly, it also suffers from significant under learning on some other classes ("hard" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance. * Equal contribution. ? Correspondence to: Yisen Wang (eewangyisen@gmail.com) and Xingjun Ma (xingjun.ma@unimelb.edu.au).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern deep neural networks (DNNs) are often highly complex models that have hundreds of layers and millions of trainable parameters, requiring large-scale datasets with clean label annotations such as ImageNet <ref type="bibr" target="#b1">[2]</ref> for proper training. However, labeling large-scale datasets is a costly and error-prone process, and even high-quality datasets are likely to contain noisy (incorrect) labels. Therefore, training accurate DNNs in the presence of noisy labels has become a task of great practical importance in deep learning.</p><p>Recently, several works have studied the dynamics of DNN learning with noisy labels. Zhang et.al <ref type="bibr" target="#b27">[28]</ref> argued that DNNs exhibit memorization effects whereby they first memorize the training data for clean labels and then subsequently memorize data for the noisy labels. Similar findings are also reported in <ref type="bibr" target="#b0">[1]</ref> that DNNs first learn clean and easy patterns and eventually memorize the wrongly assigned labels. Further evidence is provided in <ref type="bibr" target="#b12">[13]</ref> that DNNs first learn simple representations via subspace dimensionality compression and then overfit to noisy labels via subspace dimensionality expansion. Different findings are reported in <ref type="bibr" target="#b18">[19]</ref>, where DNNs with a specific activation function (i.e., tanh) undergo an initial label fitting phase then a subsequent representation compression phase where the overfitting starts. Despite these important findings, a complete understanding of DNN learning behavior, particularly their learning process for noisy labels, remains an open question.</p><p>In this paper, we provide further insights into the learning procedure of DNNs by investigating the learning dynamics across classes. While Cross Entropy (CE) loss is the most commonly used loss for training DNNs, we have found that DNN learning with CE can be class-biased: some classes ("easy" classes) are easy to learn and converge faster than other classes ("hard" classes). As shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, even when labels are clean, the class-wise test accuracy spans a wide range during the entire training process. As further shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, this phenomenon is amplified when training labels are noisy: whilst easy classes (e.g., class 6) already overfit to noisy labels, hard classes (e.g., class 3) still suffer from significant under learning (class accuracy significantly lower than clean label setting). Specifically, class 3 (bottom curve) only has an accuracy of ?60% at the end, considerably less than the &gt;90% accuracy of class 6 (top curve). Label Smoothing Regularization (LSR) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17]</ref> is a widely known technique to ease overfitting issues, as shown in <ref type="figure" target="#fig_0">Figure 1c</ref>, which still exhibits significant under learning on hard classes. Comparing the overall test accuracy (solid red curve) in <ref type="figure" target="#fig_0">Figure 1</ref>, a low test accuracy (under learning) on hard classes is a barrier to high overall accuracy. This is a different finding from previous belief that poor performance is simply caused by overfitting to noisy labels. We also visualize the learned representations for the noisy label case in <ref type="figure" target="#fig_1">Figure 2b</ref>: some clusters are learned comparably well to those learned with clean labels <ref type="figure" target="#fig_1">(Figure 2a</ref>), while some other clusters do not have clear separated boundaries.</p><p>Intuitively, CE requires an extra term to improve its learning on hard classes, and more importantly, this term needs to be tolerant to label noise. Inspired by the symmetric KL-divergence, we propose such a noise tolerant term, namely Reverse Cross Entropy (RCE), which combined with CE forms the basis of the approach Symmetric cross entropy Learning (SL). SL not only promotes sufficient learning (class accuracy close to clean label setting) of hard classes, but also improves the robustness of DNNs to noisy labels. As a preview of this, we can inspect the improved learning curves of class-wise test accuracy and representations in <ref type="figure" target="#fig_0">Figure 1d</ref> and 2c. Under the same 40% noise setting, the variation of class-wise test accuracy has been narrowed by SL to 20% with 95% the highest and 75% the lowest <ref type="figure" target="#fig_0">(Figure 1d</ref>), and the learned representations are of better quality with more separated clusters <ref type="figure" target="#fig_1">(Figure 2c</ref>), both of which are very close to the clean settings.</p><p>Compared to existing approaches that often involve architectural or non-trivial algorithmic modifications, SL is extremely simple to use. It requires minimal intervention to the training process and thus can be straightforwardly incorporated into existing models to further enhance their performance. In summary, our main contributions are:</p><p>? We provide insights into the class-biased learning procedure of DNNs with CE loss and find that the under learning problem of hard classes is a key bottleneck for learning with noisy labels.</p><p>? We propose a Symmetric Learning (SL) approach, to simultaneously address the hard class under learning problem and the noisy label overfitting problem of CE. We provide both theoretical analysis and empirical understanding of SL.</p><p>? We empirically demonstrate that SL can achieve better robustness than state-of-the-art methods, and can be also easily incorporated into existing methods to significantly improve their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Different approaches have been proposed to train accurate DNNs with noisy labels, and they can be roughly divided into three categories: 1) label correction methods, 2) loss correction methods, and 3) refined training strategies.</p><p>The idea of label correction is to improve the quality of the raw labels. One common approach is to correct noisy labels to their true labels via a clean label inference step using complex noise models characterized by directed graphical models <ref type="bibr" target="#b25">[26]</ref>, conditional random fields <ref type="bibr" target="#b22">[23]</ref>, neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> or knowledge graphs <ref type="bibr" target="#b11">[12]</ref>. These methods require the support from extra clean data or an expensive detection process to estimate the noise model.</p><p>Loss correction methods modify the loss function for robustness to noisy labels. One approach is to model the noise transition matrix that defines the probability of one class changed to another class <ref type="bibr" target="#b4">[5]</ref>. Backward <ref type="bibr" target="#b15">[16]</ref> and Forward <ref type="bibr" target="#b15">[16]</ref> are two such correction methods that use the noise transition matrix to modify the loss function. However, the ground-truth noise transition matrix is not always available in practice, and it is also difficult to obtain accurate estimation <ref type="bibr" target="#b4">[5]</ref>. Work in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref> augments the correction architecture by adding a linear layer on top of the neural network. Bootstrap <ref type="bibr" target="#b17">[18]</ref> uses a combination of raw labels and their predicted labels. There is also research that defines noise robust loss functions, such as Mean Absolute Error (MAE) <ref type="bibr" target="#b2">[3]</ref>, but a challenge is that training a network with MAE is slow due to gradient saturation. Generalized Cross Entropy (GCE) loss <ref type="bibr" target="#b28">[29]</ref> applies a Box-Cox transformation to probabilities (power law function of probability with exponent q) and can behave like a weighted MAE. Label Smoothing Regularization (LSR) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17]</ref> is another technique using soft labels in place of one-hot labels to alleviate overfitting to noisy labels.</p><p>Refined training strategies design new learning paradigms for noisy labels. MentorNet <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> supervises the training of a StudentNet by a learned sample weighting scheme in favor of probably correct labels. Decoupling training strategy <ref type="bibr" target="#b14">[15]</ref> trains two networks simultaneously, and parameters are updated when their predictions disagree.</p><p>Co-teaching <ref type="bibr" target="#b5">[6]</ref> maintains two networks simultaneously during training, with one network learning from the other network's most confident samples. These studies all require training of an auxiliary network for sample weighting or learning supervision. D2L <ref type="bibr" target="#b12">[13]</ref> uses subspace dimensionality adapted labels for learning, paired with a training process monitor. The iterative learning framework <ref type="bibr" target="#b24">[25]</ref> iteratively detects and isolates noisy samples during the learning process. The joint optimization framework <ref type="bibr" target="#b21">[22]</ref> updates DNN parameters and labels alternately. These methods either rely on complex interventions into the learning process, which may be challenging to adapt and tune, or are sensitive to hyperparameters like the number of training epochs and learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakness of Cross Entropy</head><p>We begin by analyzing the Cross Entropy (CE) and its limitations for learning with noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><formula xml:id="formula_0">Given a K-class dataset D = {(x, y) (i) } n i=1</formula><p>, with x ? X ? R d denoting a sample in the d-dimensional input space and y ? Y = {1, ? ? ? , K} its associated label. For each sample x, a classifier f (x) computes its probability of each label k ? {1, ? ? ? , K}: p(k|x) = e z k K j=1 e z j , where z j are the logits. We denote the ground-truth distribution over labels for sample x by q(k|x), and K k=1 q(k|x) = 1. Consider the case of a single ground-truth label y, then q(y|x) = 1 and q(k|x) = 0 for all k = y. The cross entropy loss for sample x is:</p><formula xml:id="formula_1">ce = ? K k=1 q(k|x) log p(k|x).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weakness of CE under Noisy Labels</head><p>We now highlight some weaknesses of CE for DNN learning with noisy labels, based on empirical evidence on CIFAR-10 dataset [9] (10 classes of natural images). To generate noisy labels, we randomly flip a correct label to one of the other 9 incorrect labels uniformly (e.g., symmetric label noise), and refer to the portion of incorrect labels as the noise rate. The network used here is an 8-layer convolutional neural network (CNN). Detailed experimental settings can be found in Section 5.1.</p><p>We first explore in more detail the class-biased phenomenon shown in <ref type="figure" target="#fig_0">Figure 1a</ref> and 1b, focusing on three distinct learning stages: early (the 10-th epoch), middle (the  <ref type="figure">Figure  3a</ref>). However, for noisy labels, the class-wise test accuracy varies significantly across different classes, even at the later stage (the green curve in <ref type="figure">Figure 3b</ref>). In particular, the network struggles to learn hard classes (e.g., class 2/3) with up to a 20% gap to the clean setting, whereas some easy classes (e.g., class 1/6) are better learned and already start overfitting to noisy labels (accuracy drops from epoch 50 to 100). It appears that the under learning of hard classes is a major cause for the overall performance degradation, due to the fact that the accuracy drop caused by overfitting is relatively small. We further investigate the reason behind the under learning of CE on hard classes from the perspective of representations. Due to their high similarities in representations to some other classes (see the red cluster for class 3 in <ref type="figure" target="#fig_1">Figure  2a</ref>), the predictions for hard class examples are likely to assign a relatively large probability to those similar classes. Under the noisy label scenario, class 3 has become even more scattered into other classes (red cluster in <ref type="figure" target="#fig_1">Figure 2b</ref>). As a consequence, no visible cluster was learned by CE, even though there are still 60% correct labels in this scenario. Further delving into these 60% clean portion of class 3 samples, we show, in <ref type="figure">Figure 4a</ref>, their prediction confidence output of the neural network. Although the confidence at class 3 is the highest, it is only around 0.5, while for the other classes, the confidence is around 0.05 or 0.1 which is actually a relatively high value and an indication of insufficient learning of class 3 even on the clean labeled part. Another evidence of under learning can be obtained from <ref type="figure">Figure 4b</ref>, where hard classes (e.g., class 2/3) have fewer true positive samples throughout intermediate stages of learning.</p><p>Clearly, CE by itself is not sufficient for learning of hard classes, especially under the noisy label scenario. We note that this finding sheds new insights into DNN learning behavior under label noise, and differs from previous belief that DNNs overfit to all classes in general <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. In the next section, we propose a symmetric learning approach that can address both the hard class under learning and noisy label overfitting problems of CE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Symmetric Cross Entropy Learning</head><p>In this section, we propose Symmetric cross entropy Learning (SL), an approach that strikes a balance between sufficient learning and robustness to noisy labels. We also provide theoretical analysis about the formulation and behavior of SL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Definition</head><p>Given two distributions q and p, the relation between the cross entropy (denoted as H(q, p)) and the KL-divergence (denoted as KL(q p)) is:</p><formula xml:id="formula_2">KL(q p) = H(q, p) ? H(q),<label>(2)</label></formula><p>where H(q) is the entropy of q. In the context of classification, q = q(k|x) is the ground truth class distribution conditioned on sample x, whilst p = p(k|x) is the predicted distribution over labels by the classifier f . From the perspective of KL-divergence, classification is to learn a prediction distribution p(k|x) that is close to the ground truth distribution q(k|x), which is to minimize the KL-divergence KL(q p) between the two distributions * . In information theory, given a true distribution q and its approximation p, KL(q p) measures the penalty on encoding samples from q using code optimized for p (penalty in the number of extra bits required). In the context of noisy labels, we know that q(k|x) does not represent the true class distribution, instead p(k|x) can reflect the true distribution to a certain extent. Thus, in addition to taking q(k|x) as the ground truth, we also need to consider the other direction of KL-divergence, that is KL(p||q), to punish coding samples * In practice, the H(q(k|x)) term is a constant for a given class distribution and therefore omitted from Eq. (2) giving the CE loss in Eq. (1). that come from p(k|x) when using a code for q(k|x). The symmetric KL-divergence is:</p><formula xml:id="formula_3">SKL = KL(q||p) + KL(p||q).<label>(3)</label></formula><p>Transferring this symmetric idea from KL-divergence to cross entropy gives us the Symmetric Cross Entropy (SCE):</p><formula xml:id="formula_4">SCE = CE + RCE = H(q, p) + H(p, q),<label>(4)</label></formula><p>where RCE = H(p, q) is the reverse version of H(q, p), namely, Reverse Cross Entropy. The RCE loss for a sample x is:</p><formula xml:id="formula_5">rce = ? K k=1 p(k|x) log q(k|x).<label>(5)</label></formula><p>The sample-wise SCE loss can then be defined as:</p><formula xml:id="formula_6">sce = ce + rce .<label>(6)</label></formula><p>While the RCE term is noise tolerant as will be proved in Section 4.2, the CE term is not robust to label noise <ref type="bibr" target="#b2">[3]</ref>. However, CE is useful for achieving good convergence <ref type="bibr" target="#b28">[29]</ref>, which will be verified empirically in Section 5. Towards more effective and robust learning, we propose a flexible symmetric learning framework with the use of two decoupled hyperparameters (e.g., ? and ?), with ? on the overfitting issue of CE while ? for flexible exploration on the robustness of RCE. Formally, the SL loss is:</p><formula xml:id="formula_7">sl = ? ce + ? rce .<label>(7)</label></formula><p>As the ground truth distribution q(k|x) is now inside of the logarithm in rce , this could cause computational problem when labels are one-hot: zero values inside the logarithm. To solve this issue, we define log 0 = A (where A &lt; 0 is some constant), which shortly will be proved useful for the robustness of rce in Theorem 1. This technique is similar to the clipping operation implemented by most deep learning frameworks. Compared with another option label smoothing technique, our approach introduces less bias into the model (negligible bias (in the view of training) at finite number of points like q(k|x) = 0 but no bias at q(k|x) = 1). Note that, the effect of ? on RCE can be reflected by different settings of A (refer to Eq. (4.3)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Theoretical Analysis</head><p>Robustness analysis: In the following, we will prove that the RCE loss rce is robust to label noise following <ref type="bibr" target="#b2">[3]</ref>. We denote the noisy label of x as?, in contrast to its true label y. Given any classifier f and loss function rce , we define the risk of f under clean labels as R(f ) = E x,y rce , and the risk under label noise rate ? as R ? (f ) = E x,? rce . Let f * and f * ? be the global minimizers of R(f ) and R ? (f ) respectively. Risk minimization under a given loss function is noise robust if f * ? has the same probability of misclassification as that of f * on noise free data. When the above is satisfied we also say that the loss function is noise-tolerant. Theorem 1. In a multi-class classification problem, rce is noise tolerant under symmetric or uniform label noise if noise rate ? &lt; 1 ? 1 K . And, if R(f * ) = 0, rce is also noise tolerant under asymmetric or class-dependent label noise when noise rate ? yk &lt; 1 ? ? y with k =y ? yk = ? y .</p><p>Proof. For symmetric noise:</p><formula xml:id="formula_8">R ? (f ) = E x,? rce = E x E y|x E? |x,y rce = E x E y|x (1 ? ?) rce + ? K ? 1 K k =y rce = (1 ? ?)R(f ) + ? K ? 1 ( K k=1 rce ? R(f )) = R(f ) 1 ? ?K K ? 1 ? A?,</formula><p>where the last equality holds due to K k=1 rce = ?(K ? 1)A following Eq. (5) and definition of log 0 = A. Thus,</p><formula xml:id="formula_9">R ? (f * ) ? R ? (f ) = (1 ? ?K K ? 1 )(R(f * ) ? R(f )) ? 0 because ? &lt; 1 ? 1 K and f * is a global minimizer of R(f )</formula><p>. This proves f * is also the global minimizer of risk R ? (f ), that is, rce is noise tolerate.</p><p>Similarly, we can prove the case for asymmetric noise, please refer Appendix A for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient analysis:</head><p>We next derive the gradients of a simplified SL with ?, ? = 1 to give a rough idea of how its learning process differs from that of CE ? . For brevity, we denote p k , q k as abbreviations for p(k|x) and q(k|x). Consider the case of a single true label, the gradient of the sample-wise RCE loss with respect to the logits z j can be derived as:</p><formula xml:id="formula_10">? rce ?z j = ? K k=1 ?p k ?z j log q k ,<label>(8)</label></formula><p>where ?p k ?zj can be further derived based on whether k = j:</p><formula xml:id="formula_11">?p k ?z j = p k (1 ? p k ), k = j ?p j p k , k = j.<label>(9)</label></formula><p>According to Eq. (9) and the ground-truth distribution for the case of one single label (e.g., q y = 1, and q k = 0 for k = y), the gradients of SL can be derived as: where A is the smoothed/clipped replacement of log 0. Note that the gradient of sample-wise CE loss ce is:</p><formula xml:id="formula_12">? sl ?z j = ? ce ?zj ? (Ap 2 j ? Ap j ), q j = q y = 1 ? ce ?zj + (?Ap j p y ), q j = 0,<label>(10)</label></formula><formula xml:id="formula_13">? ce ?z j = p j ? 1 ? 0, q j = q y = 1 p j ? 0, q j = 0.<label>(11)</label></formula><p>In the case of q j = q y = 1 ( ? ce ?zj ? 0), the second term Ap 2 j ? Ap j is an adaptive acceleration term based on p j . Specifically, Ap 2 j ? Ap j is a convex parabolic function in the first quadrant for p j ? [0, 1], and has the maximum value at p j = 0.5. Considering the learning progresses towards p j ? 1, RCE increases DNN prediction on label y with larger acceleration for p j &lt; 0.5 and smaller acceleration for p j &gt; 0.5. In the case of q j = 0 ( ? ce ?zj ? 0), the second term ?Ap j p y is an adaptive acceleration on the minimization of the probability at unlabeled class (p j ), based on the confidence at the labeled class (p y ). Larger p y leads to larger acceleration, that is, if the network is more confident about its prediction at the labeled class, then the residual probabilities at other unlabeled classes should be reduced faster. When p y = 0, there is no acceleration, which means if the network is not confident on the labeled class at all, then the label is probably wrong, no acceleration needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>An easy addition to improve CE would be to upscale its gradients with a larger coefficient (e.g., '2CE', '5CE'). However, this will cause more overfitting (see the '5CE' curve in the following Section 5 <ref type="figure">Figure 9a</ref>). There are also other options to consider, such as MAE. Although motivated from completely different perspectives, that is, CE and RCE are measures of (information theoretic) uncertainty, while MAE is a measure of distance, we can surprisingly show that MAE is a special case of RCE at A = ?2, when there is a single true label for x (e.g. q(y|x) = 1 and q(k = y|x) = 0). For MAE, we have,</p><formula xml:id="formula_14">mae = K k=1 |p(k|x) ? q(k|x)| = (1 ? p(y|x)) + k =y p(k|x) = 2(1 ? p(y|x)),</formula><p>while, for RCE, we have,</p><formula xml:id="formula_15">rce = ? K k=1 p(k|x) log q(k|x) = ?p(y|x) log 1 ? k =y p(k|x)A = ?A k =y p(k|x) = ?A(1 ? p(y|x)).</formula><p>That is, when A = ?2, RCE is reduced to exactly MAE. Meanwhile, different from the GCE loss (i.e., a weighted MAE) <ref type="bibr" target="#b28">[29]</ref>, SL is a combination of two symmetrical learning terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first provide some empirical understanding of our proposed SL approach, then evaluate its robustness against noisy labels on MNIST, CIFAR-10, CIFAR-100, and a large-scale real-world noisy dataset Clothing1M. Noise setting: We test two types of label noise: symmetric (uniform) noise and asymmetric (class-dependent) noise. Symmetric noisy labels are generated by flipping the labels of a given proportion of training samples to one of the other class labels uniformly. Whilst for asymmetric noisy labels, flipping labels only occurs within a specific set of classes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>, for example, for MNIST, flipping 2 ? 7, 3 ? 8, 5 ? 6 and 7 ? 1; for CIFAR-10, flipping TRUCK ? AUTOMOBILE, BIRD ? AIRPLANE, DEER ? HORSE, CAT ? DOG; for CIFAR-100, the 100 classes are grouped into 20 super-classes with each has 5 sub-classes, then flipping between two randomly selected sub-classes within each super-class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Empirical Understanding of SL</head><p>We conduct experiments on CIFAR-10 dataset with symmetric noise towards a deeper understanding of SL. Experimental setup: We use an 8-layer CNN with 6 convolutional layers followed by 2 fully connected layers. All networks are trained using SGD with momentum 0.9, weight decay 10 ?4 and an initial learning rate of 0.01 which is divided by 10 after 40 and 80 epochs (120 epochs in total). The parameter ?, ? and A in SL are set to 0.1, 1 and -6 respectively. Class-wise learning: The class-wise test accuracy of SL on 40% noisy labels has already been presented in <ref type="figure" target="#fig_0">Figure 1d</ref>. Here we provide further results for 60% noisy labels in <ref type="figure">Figure 5</ref>. Under both settings, each class is more sufficiently learned by SL than CE, accompanied by accuracy increases. Particularly for the hard classes (e.g., classes 2/3/4/5), SL significantly improves their learning performance. This is because SL facilitates an adaptive pace to encourage learning from hard classes. During learning, samples from easy classes can be quickly learned to have a high probability p k &gt; 0.5, while samples from hard classes still have a low probability p k &lt; 0.5. SL will balance this discrepancy by increasing the learning speed for samples with p k &lt; 0.5 while decreasing the learning speed for those with p k &gt; 0.5. Prediction confidence and distribution: In comparison to the low confidence of CE on the clean samples in <ref type="figure">Figure  4a</ref>, we train the same network using SL under the same setting. As shown in <ref type="figure">Figure 6a</ref>, on the clean portion of class 3 samples, SL successfully pulls up the confidence to 0.95, while at the same time, pushes down the residual confidence at other classes to almost 0. As further shown in <ref type="figure">Figure 6b</ref>, the prediction distributions demonstrate that each class contains more than 4000 true positive samples, including the hard classes (e.g., class 2/3/4/5). Some classes (e.g., class 1/6/7/8/9) even obtain close to 5000 true positive samples (the ideal case). Compared to the earlier results in Figure  4b, SL achieves considerable improvement on each class. Representations: We further investigate the representations learned by SL compared to that learned by CE. We extract the high-dimensional representation at the second last dense layer, then project to a 2D embedding using t-SNE <ref type="bibr" target="#b13">[14]</ref>. The projected representations are illustrated in <ref type="figure" target="#fig_1">Figures 2 and 7</ref> for 40% and 60% noisy labels respectively. Under both settings, the representations learned by SL are of significantly better quality than that of CE with more separated and clearly bounded clusters. Parameter analysis: We tune the parameters of SL: ?, ? and A. As ? can be reflected by A, here we only show results of ? and A. We tested A in [?8, ?2] with step 2 and ? ? [10 ?2 , 1] on CIFAR-10 under 60% noisy labels. <ref type="figure">Figure 8a</ref> shows that large ? (e.g., 1.0/0.5) tends to cause more overfitting, while small ? (e.g., 0.1/0.01) can help ease the overfitting of CE. Nevertheless, the convergence can become slow when ? is too small (e.g., 0.01), a behaviour like the single RCE. For this reason, a relatively large ? can help convergence on difficult datasets such as CIFAR-100.</p><p>As for parameter A, if the overfitting of CE is well controlled by ? = 0.1, SL is not sensitive to A <ref type="figure">(Figure 8b</ref>). However, if CE overfitting is not properly addressed, SL becomes mildly sensitive to A <ref type="figure">(Figure 8c</ref>).</p><p>Ablation study: For a comprehensive understanding of each term in SL, we further conduct a series of ablation experiments on CIFAR-10 under 60% noisy labels. <ref type="figure">Figure 9a</ref> presents the following experiments: 1) removing the RCE term; 2) removing the CE term; 3) upscaling the CE term; and 4) upscaling the RCE term. We can observe that simply upscaling CE does not help learning, or even leads to more overfitting. The RCE term itself does not exhibit overfitting even when upscaled, but it converges slowly. But when CE and RCE are combined into the SL framework, the performance is drastically improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Robustness to Noisy Labels</head><p>Baselines: We compare SL with 5 recently proposed noisy label learning methods as well as the standard CE loss: (1) Forward <ref type="bibr" target="#b15">[16]</ref>: Training with label correction by multiplying the network prediction with the ground truth noise matrix;</p><p>(2) Bootstrap <ref type="bibr" target="#b17">[18]</ref>: Training with new labels generated by a convex combination of the raw labels and the predicted labels;</p><p>(3) GCE <ref type="bibr" target="#b28">[29]</ref>: Training with a noise robust loss encompassing both MAE and CE; (4) D2L <ref type="bibr" target="#b12">[13]</ref>: Training with subspace dimensionality adapted labels; (5) Label Smoothing Regularization (LSR) <ref type="bibr" target="#b16">[17]</ref>: Training with CE on soft labels, rather than the one-hot labels; and (6) CE: Training with standard cross entropy loss. Experimental setup: Experiments are conducted on MNIST <ref type="bibr" target="#b9">[10]</ref>, CIFAR-10 <ref type="bibr" target="#b8">[9]</ref> and CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>. We use a 4-layer CNN for MNIST, the same network as Section 5.1 for CIFAR-10 and a ResNet-44 <ref type="bibr" target="#b6">[7]</ref> for CIFAR-100. Parameters for the baselines are configured according to their original papers. For our SL, we set A = ?4 for all datasets, and ? = 0.01, ? = 1.0 for MNIST, ? = 0.1, ? = 1.0 for CIFAR-10, ? = 6.0, ? = 0.1 for CIFAR-100 (a dataset known for hard convergence) ? . All networks are trained using SGD with momentum 0.9, weight decay 5 ? 10 ?3 and an initial learning rate of 0.1. The learning rate is divided by 10 after 10 and 30 epochs for MNIST (50 epochs in total), after 40 and 80 epochs for CIFAR-10 (120 epochs in total), and after 80 and 120 epochs for CIFAR-100 (150 epochs in total  <ref type="table" target="#tab_1">Table 1</ref>. As can be seen, SL improves on the baselines via a large margin for almost all noise rates and all datasets. Note that Forward sometimes also delivers a relatively good performance, as we directly provide it with the ground truth noise matrix. We also find that SL can be more effective than GCE, particularly for high noise rates. The complete learning procedures of SL and baselines on CIFAR-10 are illustrated in <ref type="figure">Figure 9b</ref>. SL shows a clear advantage over other methods, especially in the later stages of learning with noisy labels. This is likely because that, in the later stages of DNN learning, other methods all suffer to some extent from under learning on hard classes, while SL ensures sufficient learning on them.</p><p>Enhancing existing methods with SL: We introduce some general principles to incorporate SL into existing methods to further enhance their performance. For methods that use robust loss functions or label corrections, the RCE term of SL can be directly added to the loss function, while for methods that still use the standard CE loss without label corrections, SL can be used with small ? and large ? to replace the existing loss function. This is to avoid overfitting while promote sufficient learning. As a proof-of-concept, we conduct experiments to enhance Forward and LSR with SL. For "Forward+SL", we add the RCE term to the Forward loss with ? = 1.0/0.1 for symmetric/asymmetric noise respectively, while for "LSR+SL", we use the SL loss with the same setting in <ref type="table" target="#tab_1">Table 1</ref>. Results on CIFAR-10 are presented in <ref type="table" target="#tab_2">Table 2</ref>. Both the enhanced methods demonstrate a clear performance improvement over their original versions (Forward or LSR) both on symmetric and asymmetric noise. However, in some scenarios, the enhanced methods  are still not as good as SL. This often occurs when there is a large performance gap between the original methods and SL. We believe that with more adaptive incorporation and careful parameter tuning, SL can be combined with existing approaches to achieve even better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments on Real-world Noisy Dataset</head><p>In the above experiments, we have seen that SL achieves excellent performance on datasets with manually corrupted noisy labels. Next, we assess its applicability for a realworld large-scale noisy dataset: Clothing1M <ref type="bibr" target="#b25">[26]</ref>.</p><p>The Clothing1M dataset contains 1 million images of clothing obtained from online shopping websites with 14 classes: T-shirt, Shirt, Knitwear, Chiffon, Sweater, Hoodie, Windbreaker, Jacket, Down Coat, Suit, Shawl, Dress, Vest, and Underwear. The labels are generated by the surrounding text of images and are thus extremely noisy. The overall accuracy of the labels is ? 61.54%, with some pairs of classes frequently confused with each other (e.g., Knitwear and Sweater), which may contain both symmetric and asymmetric label noise. The dataset also provides 50k, 14k, 10k manually refined clean data for training, validation and testing respectively, but we did not use the 50k clean data. The classification accuracy on the 10k clean testing data is used as the evaluation metric. Experimental setup: We use ResNet-50 with ImageNet pretrained weights similar to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>. For preprocessing, images are resized to 256?256, with mean value subtracted and cropped at the center of 224 ? 224. We train the models with batch size 64 and initial learning rate 10 ?3 , which is reduced by 1/10 after 5 epochs (10 epochs in total). SGD with a momentum 0.9 and weight decay 10 ?3 are adopted as the optimizer. Other settings are the same as Section 5.2.</p><p>Results: As shown in <ref type="table" target="#tab_3">Table 3</ref>, SL obtains the highest performance compared to the baselines. We also find that Forward achieves a relatively good result, though it requires the use of the part of data that both have noisy and clean labels to obtain the noise transition matrix, which is not often available in real-world settings. SL only requires the noisy data and does not require extra auxiliary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we identified a deficiency of cross entropy (CE) used in DNN learning for noisy labels, in relation to under learning of hard classes. To address this issue, we proposed the Symmetric cross entropy Learning (SL), boosting CE symmetrically with the noise robust Reverse Cross Entropy (RCE), to simultaneously addresses its under learning and overfitting problems. We provided both theoretical and empirical understanding on SL, and demonstrated its effectiveness against various types and rates of label noise on both benchmark and real-world datasets. Overall, due to its simplicity and ease of implementation, we believe SL is a promising loss function for training robust DNNs against noisy labels, and an attractive framework to be used along with other techniques for datasets containing noisy labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) CE -clean (b) CE -noisy (c) LSR -noisy (d) SL -noisy The class-wise test accuracy of an 8-layer CNN on CIFAR-10 trained by (a) CE on clean labels with classbiased phenomenon, (b) CE on 40% symmetric/uniform noisy labels with amplified class-biased phenomenon and under learning on hard classes (e.g., class 3), (c) LSR under the same setting to (b) with under learning on hard classes still existing, (d) our proposed SL under the same setting to (b) exhibiting overall improved learning on all classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of learned representations on CIFAR-10 using t-SNE 2D embeddings of deep features at the last second dense layer with (a) CE on clean labels, (b) CE on 40% symmetric noisy labels, (c) the proposed SL on the same setting to (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The class-wise test accuracy at epoch 10, 50 and 100 (120 epochs in total) trained by CE loss on CIFAR-10 with (a) clean labels or (b) 40% symmetric noisy labels.(a) Prediction confidence (b) Prediction distribution Intermediate results of CE loss on CIFAR-10 with 40% symmetric noisy labels. (a) Average confidence of the clean portion of class 3 samples. (b) The true positive samples (correct) out of predictions (predicted) for each class.50-th epoch) and later (the 100-th epoch) stages, with respect to total 120 epochs of training. As illustrated in Figure 3, CE learning starts in a highly class-biased manner (the blue curves) for both clean labels and 40% noisy labels. This is because patterns inside of samples are intrinsically different. For clean labels, the network eventually manages to learn all classes uniformly well, reflected by the relatively flat accuracy curve across classes (the green curve in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Complete derivations can be found in the Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Class-wise test accuracy of CE and SL on CIFAR-10 dataset with 60% symmetric noisy labels. The red solid lines are the overall test accuracies.(a) Prediction confidence (b) Prediction distribution Effect of the proposed SL on prediction confidence/distribution on CIFAR-10 with 40% noisy labels. (a) Average confidence of the clean portion of class 3 samples. (b) The true positive samples (correct) out of predictions (predicted) for each class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Representations learned by CE and SL on CIFAR-10 dataset with 60% symmetric noisy labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>(a) ? (A=-6) (b) A/? (?=0.1) (c) A/? (?=1) Parameter analysis for SL with an 8-layer CNN on CIFAR-10 dataset under 60% symmetric label noise: (a) Tuning ? (fix A = -6); (b) Tuning A/? (fix ? = 0.1); and (c) Tuning A/? (fix ? = 1). (a) Ablation of SL (b) SL vs. baselines Accuracy of different models on CIFAR-10 with 60% symmetric label noise. (a) Ablation study of SL; (b) Comparison between SL and other baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) of different models on benchmark datasets with various rates of symmetric and asymmetric noisy labels. The average accuracy and standard deviation of 5 random runs are reported and the best results are in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Symmetric Noise</cell><cell></cell><cell></cell><cell></cell><cell>Asymmetric Noise</cell></row><row><cell>Datasets</cell><cell>Methods</cell><cell></cell><cell>Noise Rate ?</cell><cell></cell><cell></cell><cell></cell><cell>Noise Rate ?</cell></row><row><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) of SL-boosted Forward, D2L and LSR methods on CIFAR-10 under various label noise.</figDesc><table><row><cell>Method</cell><cell>Symmetric noise 0.4 0.6</cell><cell>Asymmetric noise 0.4</cell></row><row><cell cols="2">Forward+SL 84.54 ? 0.03 79.64 ? 0.04</cell><cell>86.22 ? 0.18</cell></row><row><cell>LSR+SL</cell><cell>85.20 ? 0.01 79.28 ? 0.05</cell><cell>80.99? 0.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) of different models on real-world noisy dataset Clothing1M. The best results are in bold.</figDesc><table><row><cell>Methods</cell><cell>CE</cell><cell cols="2">Bootstrap Forward</cell><cell>D2L</cell><cell>GCE</cell><cell>SL</cell></row><row><cell>Acc</cell><cell>68.80</cell><cell>68.94</cell><cell>69.84</cell><cell cols="3">69.47 69.75 71.02</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? For 40% asymmetric noise, ? is set to 5.0 for CIFAR-10 and ? is set to 2.0 for CIFAR-100. Other parameters are unchanged.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. For symmetric noise:</p><p>where the last equality holds due to K k=1 rce (f (x), k) = ?(K ? 1)A following Eq. (5) and the definition of log 0 = A (a negative constant). Thus,</p><p>. This proves f * is also the global minimizer of risk R ? (f ), that is, rce is noise tolerant.</p><p>For asymmetric or class-dependent noise, 1 ? ? y is the probability of a label being correct (i.e., k = y), and the noise condition ? yk &lt; 1 ? ? y generally states that a sample x still has the highest probability of being in the correct class y, though it has probability of ? yk being in an arbitrary noisy (incorrect) class k = y. Considering the noise transition matrix between classes [? ij ], ?i, j ? {1, 2, ? ? ? , K}, this condition only requires that the matrix is diagonal dominated by ? ii (i.e., the correct class probability 1 ? ? y ). Following the symmetric case, here we have,  </p><p>Next, we prove, f * ? = f * holds following Eq. (13). First, (1 ? ?y ? ? yk ) &gt; 0 as per the assumption that ? yk &lt; 1 ? ?y. Since we are given R(f * ) = 0, we have rce(f * (x), k) = ?A for all k = y. Also, by the definition of ? * rce , we have rce(f * ? (x), k) = ?A(1 ? p k ) ? ?A, ?k = y. Thus, for Eq. (13) to hold (e.g. rec(f * ? (x), k) ? rec(f * (x), k)), it must be the case that p k = 0, ?k = y, that is,</p><p>, k) for all k ? {1, 2, ? ? ? , K}, thus f * ? = f * which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gradient Derivation of SL</head><p>The complete derivartion of the simplified SL (?, ? = 1) with respect to the logits is as follows:</p><p>where</p><p>In the case of k = j:</p><p>In the case of k = j:</p><p>Combining Eq. <ref type="formula">(16)</ref> and <ref type="formula">(17)</ref> into Eq. <ref type="formula">(14)</ref>, we can obtain:</p><p>If q j = q y = 1, then the gradient of SL is:</p><p>Else if q j = 0, then</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxinder</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coteaching: robust training deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>University of Toronto</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cleannet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07131</idno>
		<title level="m">Transfer learning for scalable image classifier training with label noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from noisy largescale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption? In ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
