<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SLURP: A Spoken Language Understanding Resource Package</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bastianelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Interaction Lab, MACS</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vanzo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Interaction Lab, MACS</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
							<email>v.t.rieser@hw.ac.ukp.swietojanski@unsw.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The Interaction Lab, MACS</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SLURP: A Spoken Language Understanding Resource Package</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https: //github.com/pswietojanski/slurp * Authors contributed equally.</p><p>User: "Make a calendar entry for brunch on Saturday morning with Aaronson."</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditionally, Spoken Language Understanding (SLU) uses a pipeline transcribing audio into text using Automatic Speech Recognition (ASR), which is then mapped into a semantic structure via Natural Language Understanding (NLU). However, this modular approach is prone to error propagation from noisy ASR transcriptions, and ASR in turn is not able to disambiguate based on semantic information. End-to-end (E2E) approaches on the other hand, can benefit from joint modelling. One of the main bottlenecks for building E2E-SLU systems, however, is the lack of large and diverse datasets of audio inputs paired with corresponding semantic structures. Publicly available datasets to date are limited in terms of lexical and semantic richness <ref type="bibr" target="#b22">(Lugosch et al., 2019b)</ref>, number of vocalizations <ref type="bibr" target="#b5">(Coucke et al., 2018)</ref>, domain coverage <ref type="bibr" target="#b12">(Hemphill et al., 1990;</ref><ref type="bibr" target="#b7">Dahl et al., 1994)</ref> and semantic contexts <ref type="bibr" target="#b10">(Godfrey et al., 1992;</ref><ref type="bibr" target="#b15">Jurafsky and Shriberg, 1997)</ref>. In this paper, we present the Spoken Language Understanding Resource Package (SLURP), a publicly available multi-domain dataset for E2E-SLU, which is substantially bigger and more diverse than existing SLU datasets. SLURP is a collection of 72k audio recordings of single turn user interactions with a home assistant, annotated with three levels of semantics: Scenario, Action and Entities, as in <ref type="figure" target="#fig_0">Fig. 1</ref>, including over 18 different scenarios, with 46 defined actions and 55 different entity types as listed on https://github.com/pswietojanski/slurp. <ref type="bibr">1</ref> In order to further support SLU development, we propose SLU-F1, a new metric for entity prediction, which is specifically designed to assess error propagation in structured E2E-SLU tasks. This metric has 3 main advantages over the commonly used accuracy/F1 metric, aimed at supporting SLU developers: First, it computes a distribution rather than a single score. This distribution is (1) inspectable and interpretable by system developers, and (2) can be converted into a confidence score which can be used in the system logic (akin to previously available ASR confidence scores). Finally, the distribution reflects errors introduced by ASR and their impact on NLU and thus (3) gives an indication of the scope of improvement that can be gained by E2E approaches. Using this metric, we evaluate 4 baseline systems that represent competitive pipeline approaches, i.e. 2 state-of-the-art NLU systems and 2 ASR engines. We conduct a detailed error analysis of cases where E2E could have made a difference, i.e. error propagation and semantic disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The first corpora containing both audio and semantic annotation reach as far back as the Air Travel Information System (ATIS) corpus <ref type="bibr" target="#b12">(Hemphill et al., 1990)</ref> and the Switchboard-DAMSL Labeling Project <ref type="bibr" target="#b15">(Jurafsky and Shriberg, 1997)</ref>. However, it was not until recently when the first E2E approaches to SLU were introduced <ref type="bibr" target="#b33">(Serdyuk et al., 2018;</ref><ref type="bibr" target="#b11">Haghani et al., 2018)</ref>. Since then, one of the main research questions is how to overcome data sparsity by e.g. using transfer learning <ref type="bibr" target="#b32">(Schuster et al., 2019;</ref><ref type="bibr" target="#b35">Tomashenko et al., 2019)</ref>, or pretraining <ref type="bibr" target="#b22">(Lugosch et al., 2019b)</ref>. Here, we present a new corpus, SLURP, which is considerably bigger than previously available corpora. In particular, we directly compare our dataset to the two biggest E2E-SLU datasets for the English language: The Snips benchmark <ref type="bibr" target="#b5">(Coucke et al., 2018)</ref> and the Fluent Speech Command (FSC) corpus <ref type="bibr" target="#b22">(Lugosch et al., 2019b)</ref>. With respect to these resources, SLURP contains 6 times more sentences than Snips, 2.5 times more audio examples than FSC, while covering 9 times more domains and being on average 10 times lexically richer than both FSC and Snips, see Section 3.3. SLURP represents the first E2E-SLU corpus of this size for the English language. The only existing comparable project is represented by the CASTLU dataset <ref type="bibr" target="#b38">(Zhu et al., 2019)</ref> for Chinese Mandarin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SLURP data 3.1 Data Collection</head><p>SLURP was collected for developing an in-home personal robot assistant <ref type="bibr" target="#b25">(Miksik et al., 2020)</ref>. First, we collected textual data by prompting Mechanical Turk (AMT) workers to formulate commands towards the robot, using 200 pre-defined prompts such as "How would you ask for the time/ set an alarm/ play your favourite music?" etc. We carefully designed the prompts to avoid lexical priming and thus increase linguistic variability of the collected data. This data has been manually annotated at scenario, action and entity level, and released as a text-only NLU benchmark <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>  textual data also serves as gold standard transcriptions for the audio data.</p><p>The audio data was collected in acoustic conditions matched to a typical home or office environment. We asked 100+ participants to read out the collected prompts on a tablet and to provide demographic background information, see <ref type="table">Table 1</ref>. Speech was captured at distance with a microphone array, but some users were also equipped with a close-talking headset microphone (though, distant and close-talk channels are not synchronised at the sample level). Most recording sessions lasted 1 hour and were split into 4 parts. In each part, the technician changed position of the microphone array in the collection place. Users were encouraged to vary their location in the room from utterance to utterance (seating, standing or walking), and for some utterances not to speak directly to the mic array in order to resemble realistic conditions. These parameters are not logged with the dataset, however, they do pose increased challenges for ASR <ref type="bibr" target="#b24">(Marino and Hain, 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Female</head><p>Male Native Non-Native Unk.</p><p>37.3% 32.2% 25.5% 44% 30.5% <ref type="table">Table 1</ref>: Participants' demographic statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Audio Data Processing</head><p>For quality control of the audio data, we automatically verified i) whether the participant uttered the right / complete SLU query as prompted and ii) if the files were appropriately end-pointed. We used the transcriptions of two ASR systems (referred to as Multi-ASR and Google-ASR, see Sec 5.1). These systems were not estimated from SLURP acoustic data, thus remain unbiased and do not reinforce potential errors. First, we removed all data that failed to force-align to transcripts using Multi-ASR. Then for the remainder we derived the SLU related confidences based on the matched Word-Error Rate (WER) between textual prompts and the obtained ASR hypotheses (calculated for both utterance and entity fillers), as well as crossmic validation between close and distant microphones, see <ref type="figure" target="#fig_1">Figure 2</ref> (Right). Note that the higher matched WER does not necessarily imply the file lacks the expected content, as simply the file could be more challenging to automatically recognise. At the same time, from SLU perspective, one does not necessarily need grammatically correct utterances, as long as the they carry the information necessary to understand and execute the query. <ref type="bibr">Figure 2 (Left)</ref> shows that for nearly 60% of the data at least one ASR system achieved a perfect score (WER=0), and this increases to 73% after including utterances with imperfect sentence error rates but correct entity fillers (EntityWER=0). After filtering, SLURP comprises 58 hours of acoustic material. See <ref type="table" target="#tab_2">Table 2</ref> for detailed statistics.</p><p>In addition, we provide SLURP-synth following <ref type="bibr" target="#b21">(Lugosch et al., 2019a)</ref>, where we replace filtered or missing recordings with synthetic vocalisations from Google's Text-to-Speech system 2 using 34 different synthetic English voices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Linguistic Analysis and Comparison</head><p>In this Section, we compare SLURP with the most recent publicly available E2E-SLU datasets: The Fluent Speech Command (FSC) corpus <ref type="bibr" target="#b22">(Lugosch et al., 2019b)</ref> and the Snips benchmark <ref type="bibr" target="#b5">(Coucke et al., 2018)</ref>, which are also set in the smart-home domain. Snips covers 10 domains. However, only 2 domains have been vocalised, resulting in 6K audio files. FSC, on the other hand, is considerably bigger than Snips in terms of audio recordings, including 30k vocalisations. However, the provided semantics only cover a small subset of actions with no more than two fixed entity types as arguments. In the following, we compare these dataset along four dimensions in order to get a first estimate of SLURP's level of complexity. Audio analysis: <ref type="table" target="#tab_2">Table 2</ref> summarises the audio data for each dataset. Audio files are differentiated in close and far range microphone. As shown, SLURP has 1.8? more speakers, more than double the audio files than the biggest dataset FSC, however FSC has an higher audio-per-sentence ratio. Demographic statistics are reported in <ref type="table">Table 1</ref>.  Lexical analysis: <ref type="table" target="#tab_4">Table 3</ref> provides an overview of different measures of lexical richness and diversity, following <ref type="bibr" target="#b26">(Novikova et al., 2017)</ref>, using both lexicalised (LEX) and delexicalised (DELEX) versions of the datasets (delexicalisation is performed by replacing each entity span with the entity label). Note that delexicalisation has a more severe effect on FSC and Snips, which indicates that most of their lexical richness and diversity stems from entity names. On average SLURP has 100? more tokens, lemmas, bigrams and trigrams than FSC, and 10? more than Snips. In addition, we compute the following lexicographic measures using the Lexical Complexity Analyser <ref type="bibr" target="#b20">(Lu, 2012)</ref>. Lexical Sophistication (LS2) <ref type="bibr" target="#b17">(Laufer, 1994)</ref> is defined as T s /T , with T s being the number of sophisticated types of (unique) words 3 and T being the number of types of words in a dataset. The Corrected Verb Sophistication (CSV1) (Wolfe-Quintero et al., 1998) is evaluated as T svb / ? 2N vb , with T svb the number of types of sophisticated verbs and N vb the total number of verbs in a dataset. The Mean Segmental Textto-Token Ratio (MSTTR) <ref type="bibr" target="#b14">(Johnson, 1944)</ref> is the average Text-to-Token Ratio (TTR -T /N ) over all the segments of 10 4 words, with N the number of words in a dataset. The MSTTR is used to capture the variation of classes of words. Again, SLURP shows higher levels of lexical sophistication and richness than the other datasets, especially in the delexicalised case. Note that lexicalised version of Snips contains many names of artists and bands in the music scenario, which contributes to enlarge the set of sophisticated words T s . The only measure where SLURP doesn't outperform the other datasets is average sentence length. SLURP contains, among others, shorter interactions, such short acknowledgements, elliptic questions and atomic commands, whereas Snips is mostly composed of FSC   commands of similar length, often including multiword named entities. Syntactic analysis: Next, we use the D-Level Analyser <ref type="bibr" target="#b19">(Lu, 2009</ref>) to evaluate the syntactic complexity of user utterances according to the revised D-Level scale <ref type="bibr" target="#b6">(Covington et al., 2006)</ref>, where higher levels correspond to more complex, deeper syntactic structures, e.g. 0-1 levels include simple sentences, while higher levels presents embedded structures, subordinating conjunction, etc. <ref type="figure" target="#fig_2">Figure  3 shows</ref>   <ref type="table">Table 4</ref>: Semantic analysis of the number of scenarios, actions and entity types, the total number of annotated entities, and the number of unique entities, i.e. entities whose lexical filler appears only once.</p><p>sentence is annotated with one scenario and one action, see <ref type="figure" target="#fig_0">Fig. 1</ref>, similar to annotations used in <ref type="bibr" target="#b2">(Budzianowski et al., 2018;</ref><ref type="bibr" target="#b32">Schuster et al., 2019)</ref>. FSC and Snips contain actions and entities as well, although they do not explicitly annotate the scenarios, however these can be deducted from the dataset file structure. The results in <ref type="table">Table 4</ref> show that SLURP's semantic coverage is 9 times wider than other datasets in terms of scenarios, and 6.5 times in terms of actions, where a higher number of scenarios results in a higher number of actions. FSC has the highest entity/sentence ratio, though it only has 16 unique entities. Snips appears to be the dataset with highest Unique Entities/Total Entities ratio, 50%, against 33% of SLURP. Again, this is due to the frequent use of proper names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SLURP Metrics</head><p>The standard metric for evaluating E2E-SLU is accuracy, which is defined as "the accuracy of all slots for an utterance taken together -that is, if the predicted intent differs from the true intent in even one slot, the prediction is deemed incorrect" <ref type="bibr" target="#b22">(Lugosch et al., 2019b)</ref>. However, this notion of accuracy is problematic when it comes to evaluating entities, as it does not account for the interplay between semantic mislabelling and textual misalignment. Nor does it differentiate between entity label and lexical  filler, as in <ref type="figure" target="#fig_3">Fig. 4</ref>, where lexical filler is defined as span over tokens in the original sentence. 5 Formally, given a sentence s, let E and? be the set of gold and predicted entities, respectively. Each e i = l i , f i ? E is a tuple where l i ? L is the label drawn from the list of available entity labels L, while f i = [t m , . . . , t n ] is the lexical filler, defined as a span of consecutive tokens of s such that 1 ? m ? n ? |s|. Similarly, predicted entities are of the form? k = l k ,f k ??. In spanbased metrics, two entities e 1 and e 2 are identical (e 1 =:= e 2 ) when both labels and lexical fillers are the same (l 1 = l 2 ?f 1 = f 2 ). A match is thus found only whenever the gold and predicted entities are identical, i.e. e i =:=? k . This evaluation method holds in NLU because entities are tagged over the same textual sequence. When evaluating E2E-SLU, where entities are identified out of a wave form, this strict coupling with the token sequence may no longer apply. Note that pipeline systems for SLU are affected as well since they operate over ASR transcribed sentences, which can consistently differ from the original gold transcription.</p><p>To account for this mismatch, we propose SLU-F1, a new metric which does not overly penalise misalignments caused by ASR errors. In addition, it is able to capture the quality of transcriptions and entity tagging errors at the same time in a single metric. As such, this metric allows to directly compare E2E and pipeline systems. In particular, SLU-F1 combines span-based F1 evaluation with a text-based distance measure dist, e.g. WER. The equality property =:= is relaxed by allowing gold and predicted entities (e i and? k ) to match (e i =:=? k ) when the corresponding labels are identical (l i =l k ), even when the fillers are not identical. In this case we increment the True Positives (TPs) by 1. To account for lexical distance/ mismatch, we compute the dist between gold and predicted fillers (dist(f i ,f k )), and increment the False Positives (FPs) and False Negatives (FNs) <ref type="bibr">5</ref> In traditional NLU systems this is identified with pairs of start-end tokens or chars, or token index spans. of this amount, as in Algorithm 1. In the case of a predicted entity label matching with more than one gold entity, e.g. when two or more entities with the same label are present, we opt for a non-conservative approach, selecting the gold annotation minimising the dist as a candidate. The assumption is that the pair of entities is most likely referring to the same text span. We use two distance functions to capture different aspects of possible transcription mistakes: WER (Word-F1) and the normalised Levenshtein distance on character level (Char-F1). WER is a strict token-level metric, which outputs errors/null matches whenever a mismatching or misalignment of tokens is observed. The character-based Levenshtein distance, on the other hand, offers the opposite perspective. By computing character-based similarities, it is much less susceptible to small variations of input strings, and thus better accounting for local transcription errors which do not affect NLU tagging. For example, Word-F1 will penalise small morphological differences e.g. singular vs. plural as in pizza vs. pizzas, which are often seen in transcriptions. This over-penalises NLU outputs, e.g. the tagging of pizzas may be semantically correct. Char-F1 on the other hand does not over-penalise NLU, but it also may provide a positive score when two fillers have similar characters, but are semantically and phonetically unrelated. In other words, Word-F1 shows the influence of ASR on NLU, whereas Char-F1 gives an indication of NLU performance despite transcription noise. These dist-F1 metrics (dist = Word or Char) metric are similar to the fuzzy matching mechanism proposed in <ref type="bibr" target="#b31">(Rastogi et al., 2020)</ref>. They fundamentally differ for the adopted string matching schema: any dist-F1 considers string ordering to score string similarity, while the fuzzy mechanism is instead order invariant.</p><p>Consider the illustrative entity tagging example in <ref type="figure" target="#fig_3">Figure 4</ref>. Here, Aaronson has been wrongly transcribed into Aron's son, and morning has been wrongly tagged with date. A dist?F1 will score the predicted entities as follows: both [event name: brunch] and [date: Saturday] contribute with a +1 to the TPs, since both label and filler correspond to gold information. The wrong label associated with morning increases the FPs of 1, although it is correctly transcribed. It follows that the entity timeofday is not predicted, increasing the FNs of 1. Finally, [person: Aron's son] is correctly labelled, but its filler is partially wrong. It thus contributes to the TPs by 1, but FPs and FNs are both incremented by dist(Aaronson, Aron's son).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 dist-F1 for a sentence s</head><p>Input E,?, T P, F P, F N ? 0 Ls ? set of gold entity labels in s dist ? a text-based distance metric Output: T P, F P, F N 1: for each? ?? do 2:</p><p>if?.label ? Ls then 3:</p><p>P l ? {(e,?) | ?e ? E. e.label =?.label} 4:</p><p>if P l .size &gt; 0 then 5:</p><p>(e,?) ? arg min (e,?)?P l dist(e,?) 6:</p><p>T P += <ref type="formula">1  7</ref> Finally, we combine Word-F1 and Char-F1 in a single number SLU-F1, which evaluates the final performance over the sum of the confusion matrices obtained with Word-F1 and Char-F1. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We now establish the performance of different baseline systems on the SLURP corpus. As demonstrated in Section 3.1, SLURP is linguistically more diverse than previous datasets, and therefore more challenging for SLU. We first provide an evaluation of two ASR baselines to show the complexity of the acoustic dimension. We then evaluate the semantic dimension, by testing the corpus against state-ofthe-art NLU systems. We finally combine ASR and NLU, implementing several SLU pipelines.</p><p>Note that so far, the direct comparison of E2E-SLU with pipeline approaches are mainly limited to baselines developed on the same dataset, e.g. a multistage neural model in which the two stages that correspond to ASR and NLU are trained independently, but using the same training data <ref type="bibr" target="#b8">(Desot et al., 2019;</ref><ref type="bibr" target="#b11">Haghani et al., 2018)</ref>. We follow a different approach, which, as we argue, is closer to the 6 The official script for analysis and evaluation will be released with SLURP at https://github.com/ pswietojanski/slurp. real-life application scenario: We use competitive ASR systems and state-of-the-art NLU systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Acoustic evaluation</head><p>We run the analysis of the SLURP acoustic complexity by testing 2 different ASR systems: Indomain ASR trained on SLURP data, and Multi-ASR, which leverages a large amount of out-ofdomain data. Both are built with the Kaldi ASR toolkit <ref type="bibr" target="#b29">(Povey et al., 2011)</ref>. Multi-ASR is a largescale system estimated from publicly available acoustic data pooled together -Acoustic data including, among others, LibriSpeech <ref type="bibr" target="#b27">(Panayotov et al., 2015)</ref>, Switchboard <ref type="bibr" target="#b10">(Godfrey et al., 1992)</ref>, Fisher <ref type="bibr" target="#b4">(Cieri et al., 2004)</ref>, CommonVoice <ref type="bibr" target="#b0">(Ardila et al., 2019)</ref>, AMI <ref type="bibr" target="#b3">(Carletta, 2007)</ref> and ICSI <ref type="bibr" target="#b13">(Janin et al., 2003)</ref>, which is further augmented to increase environmental robustness following <ref type="bibr">(Ko et al., 2017) 7</ref> . In total, a time-delay neural network acoustic model <ref type="bibr" target="#b28">(Peddinti et al., 2015)</ref> is trained on 24,000 hours of augmented audio material with lattice-free maximum mutual information objective <ref type="bibr" target="#b30">(Povey et al., 2016)</ref>. For decoding, we use a tri-gram Language Model (LM) that is an interpolation of an in-domain LM estimated from 60k voice-command sentences 8 and a background LM estimated from Fisher transcripts. As shown in the first block of <ref type="table" target="#tab_9">Table 5</ref>, Multi-ASR offers a competitive performance on this data when compared to the off-the-shelf Google-ASR. <ref type="bibr">9</ref> SLURP-ASR shares the overall pipeline with Multi-ASR, except the acoustic model is estimated from the 40 hours of SLURP training data (83 hours when pooled with SLURP-Synth) and bootstrapped from forced-alignments obtained with Gaussian mixture model build for Multi-ASR. Results for this scenario are reported in the second block of <ref type="table" target="#tab_9">Table 5</ref>, where adding synthetic data shows 1.6% improvement. For comparison, estimating acoustic models from synthetic data alone (no augmentations) results in 98% WER on Test partition.</p><p>Finally, we perform supervised acoustic domain adaptation <ref type="bibr">(Bell et al., 2020)</ref> of Multi-ASR with SLURP-Train by a method proposed in <ref type="bibr" target="#b34">(Swietojanski et al., 2016)</ref>, which achieves the best perfor-  mance by around 1% absolute on Test.</p><p>In sum, the large out-of-domain Multi-ASR system performs better than the systems trained on in-domain SLURP data. Best results are achieved by using a pre-training approach, i.e. Multi-ASR adapted to SLURP. This shows that, despite SLURP's absolute size, the acoustic data is still too scarce to fully account for its lexical richness and noise conditions. As such, SLURP is a challenging dataset for ASR as well as for SLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semantic evaluation</head><p>System Descriptions: We evaluate SLURP against two state-of-the-art NLU models: HerMiT <ref type="bibr" target="#b36">(Vanzo et al., 2019)</ref> and <ref type="bibr">SF-ID (E et al., 2019)</ref>. Both systems achieved state-of-the-art results on the NLU Benchmark <ref type="bibr" target="#b18">(Liu et al., 2019)</ref> and on ATIS/Snips respectively. HerMiT's architecture is a hierarchy of self-attention mechanisms and Bidirectional Long Short-Term Memory (BiLSTM) encoders followed by Conditoinal Random Field (CRF) tagging layers. Its multi-layered structure resembles a top-down approach of Scenario, Action and, Entity prediction, where each task benefits from the information encoded by the previous stages, e.g. Entity detection can benefit from sentence-level encodings.</p><p>SF-ID's architecture is also based on attention, using a BiLSTM encoder and CRF tagger. The model defines two subnets that communicate through a reinforce vector. In order to compare with HerMiT's top-down approach, we choose the opposite Entity-first propagation direction for SF-ID, i.e. the entity detection task is executed first and its encodings are used to feed the Intent detection task. Note that while HerMiT uses a multi-layered annotation scheme (Scenario and Action), SF-ID can only handle a single layer of annotation. To this end, we generate another combined semantic layer, Scen Act, to feed SF-ID with a label composed by the concatenation Scenario and Action. Scenario and Action Prediction: We split SLURP in train, development and test as in <ref type="table" target="#tab_11">Table 6</ref>.   We first evaluate accuracy for Scenario, Action and a combination of the two. <ref type="table" target="#tab_12">Table 7</ref> summarises the results, where the top two rows are upper bounds based on gold transcriptions. Note that even for the gold transcriptions, both NLU systems perform substantially below their state-of-the-art results on the NLU benchmark (HerMiT=87.55) and Snips respectively (SF-ID= 97.43). This further demonstrates the complexity of SLURP, which also makes it a challenging test bed for future research not only for SLU, but also NLU. When moving on to ASR transcribed data, the results in the middle of <ref type="table" target="#tab_12">Table  7</ref> show the Multi-ASR system in combination with HerMiT achieves top performance for all 3 tasks. Finally, the 3rd block reports HerMiT with ASR from in-domain SLURP audio data (also see <ref type="table" target="#tab_9">Table  5</ref>). The results show that our best performing system, HerMiT with Multi-ASR + Adapt w/ SLURP, is only 5% below the gold standard despite 16% WER. We hypothesise that this is due to robust Scenario and Action encodings, which we will further examine in our error analysis in Section 6.</p><p>Entity Prediction: We now analyse the results for entity prediction in more detail using our proposed metric SLU-F1. The results in <ref type="table" target="#tab_14">Table 8</ref> confirm that HerMiT is the stronger NLU system on goldtranscribed data and outperforms the other system combinations for SLU in combination with Multi-ASR. Again, these results suggest that the top-down information flow of HerMiT (i.e. first decoding Scenario, then Action and lastly Entity in a sequence) is better suited for this complex dataset, which we will further demonstrate in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-F1</head><p>Char-F1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SLU-F1</head><p>F1  HerMiT SF-ID We further describe the types of errors produced by HerMiT and SF-ID for Entity Prediction on noisy ASR data, as shown in <ref type="figure">Figure 5</ref>. Overall, HerMiT has lower error rates for all but ASR errors. Nevertheless, it is able to recover the correct entities from the transcriptions. These results indicate that HerMiT, using a top-down decoding approachgoing from the more general Scenario to the more specific Action and Entity Prediction, is more robust to noise propagation than the bottom-up SF-ID system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Expressiveness of the SLU-F1 Metric</head><p>The results in <ref type="table" target="#tab_14">Table 8</ref> show that our proposed metrics Word-F1 and Char-F1 both produce the same ordering as F1. However, a Pearson's correlation between Word-F1 and Char-F1 shows that the two metrics are only weakly correlated (? = 0.2, p 0.0001), which confirms that they are in-deed measuring two different aspects despite producing the same final ordering. In addition to an overall performance score, the metrics give us a distribution of value ranges, which can give us insight on system behaviour. <ref type="figure" target="#fig_4">Figure 6</ref> shows distributions of entity-level dist value ranges over the WER of the sentence for our top performing system HerMiT/Multi-ASR. For entity-WER ( <ref type="figure" target="#fig_4">Figure  6a</ref>), the distribution shows high density of entities falling between sentence-WER= [0, 1] and entity-WER= [0, 1]. When analysing sentences with correct transcriptions, i.e. sentence-WER=0, we find only NLU errors, due to span misalignments. When sentence-WER &gt; 0, most of the entities are scored with a values either in (0, 0.5], or in (0.5, 1]. In the first case, we find NLU mistakes caused by shortening entity spans, e.g. "football" instead of "football match". The second range includes span shortening and extensions, e.g. "Saturday morning" instead of "Saturday", as well as many mis-transcribed entities, e.g. due to either morphological errors (singular vs. plural), or transcription errors. The distribution for entity-level normalised Levenshtein is less spiked, as shown in <ref type="figure" target="#fig_4">Fig. 6b</ref>. As for WER, all the entries with sentence-WER=0 and entity-Lev&gt;0 correspond to correctly labelled entities, whose span has been shortened or extended. Entities assigned with character-based Lev values falling between (0, 0.2] mostly contain negligible ASR errors, such as morphological errors, compound merging or explosion, or general transcription mistakes, e.g. Sara vs. Sarah. Entities with Lev= (0.2, 0.5] comprise both ASR errors, as well as including minor NLU errors such as shortened or extended entity spans. When entity-Lev= (0.5, 0.8], we find mostly NLU errors due to wrong span tagging. Finally, two types of NLU errors fall in the range (0.8, 1.0]: Either span errors with a substantial mismatch in length with gold annotations, or more severe ASR errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>SLURP is not only bigger, but also a magnitude more challenging than previous datasets. The purpose of this new data release is not to provide yet another benchmark dataset, but to provide a usecase inspired new challenge, which is currently beyond the capabilities of SOTA E2E approaches (due to scalability, lack of data efficiency, etc.).</p><p>We have tested several SOTA E2E-SLU systems on SLURP, including <ref type="bibr" target="#b22">(Lugosch et al., 2019b)</ref>  produces SOTA results on the FSC corpus. However, re-training these models on this more complex domain did not converge or result in meaningful outputs. Note that these models were developed to solve much easier tasks (e.g. a single domain). Developing an appropriate model architecture is left for future work. For this reason, in this work we focus on benchmarking existing approaches. We show that SOTA modular approaches are able to provide a strong baseline for this challenging data, which has yet to be met by SOTA E2E systems. We also argue that our modular baseline is closer to how real-world applications build SLU systems, nevertheless often overlooked when testing E2E systems. As such, we consider our SOTA modular baseline a major novel contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we present SLURP, a new resource package for SLU. First, we present a novel dataset, which is substantially bigger than other publicly available resources. We show that this dataset is also more challenging by first conducting a linguistic analysis, and then demonstrating the reduced performance of state-of-the-art ASR and NLU systems. Second, we propose the new SLU-F1 metric for evaluating entity prediction in SLU tasks. In a detailed error analysis we demonstrate that the distribution of this metric can be inspected by system developers to identify error types and system weaknesses. Finally, we analyse the performance of two state-of-the-art NLU systems on ASR data. We find that a sequential decoding approach for SLU, which starts from the more abstract notion of scenario and action produces better results for entity tagging, than an approach which works bottom up, i.e. starting from the entities. Our error analysis suggests that this is due to the former approach being able to better account for noise by priming entity tagging, which is a more challenging task than scenario or action recognition.</p><p>In future work, we hope that SLURP will be a valuable resource for developing E2E-SLU systems, as well as more traditional pipeline approaches to SLU. The next step is to extend SLURP with spontaneous speech, which would again increase its complexity, but also move it one step closer to real-life applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example annotation from SLURP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Amounts of data in SLURP matching given WER levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Syntactic complexity on D-Level scale, where higher levels correspond to more complex, deeper syntactic structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Continued example from Figure 1: Errors in SLU entity tagging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Correlation between sentence-level WER (intervals of 0.5) and entity-level (a) WER values (intervals of 0.5), (b) normalised character-based Levenshtein values (intervals of 0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Audio file statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis of Lexical diversity and sophistication.</figDesc><table><row><cell>%</cell><cell cols="3">L0 L1 L2 L3 L4 L5 L6 L7</cell></row><row><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>75</cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>FSC</cell><cell>Snips</cell><cell>SLURP SLURP-synt</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Finally, we compare the datasets according to their semantic content. SLURP is annotated with three layers of semantics, namely scenarios, actions and entities, where each</figDesc><table><row><cell>the percentages on the D-Level scale for each dataset. Overall, all the datasets present a majority of Level 0 and 1 sentences. This can be explained with the nature of the application domain, i.e. a smart-home assistant. FSC contains mostly Level 0 sentences ( 89%), with some ( 9%) Level 4 ones. 89% of Snips sentences fall into Level 0 and 1, against only 74% of SLURP. The remaining 11% of Snips are mostly Level 4 sentences, while SLURP appears more mixed, with even a 5% of Level 7 sentences. Semantic Analysis: FSC Snips SLURP SLURP -synt Scenarios 2 2 18 18 Actions 6 7 46 54 Entities 2 4 56 56 Tot. Entities 334 2,870 16,792 14,623 Entity/Sentence 1.35 0.98 0.97 0.65 Unique Entities 16 1,348 5,613 4619</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Gold: [event name: brunch], [date: Saturday], [timeofday: morning], [person: Aaronson] SLU: [event name: brunch], [date: Saturday], [date: morning], [person: Aron's son]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>SLURP WER for different ASR systems.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Data distribution of train, dev and test sets.</figDesc><table><row><cell></cell><cell cols="3">Scenario Action Scen Act</cell></row><row><cell>Gold/HerMiT</cell><cell>90.15</cell><cell>86.99</cell><cell>84.84</cell></row><row><cell>Gold/SF-ID</cell><cell>86.48</cell><cell>83.69</cell><cell>82.25</cell></row><row><cell>Multi/HerMiT</cell><cell>83.73</cell><cell>79.70</cell><cell>76.68</cell></row><row><cell>Multi/SF-ID</cell><cell>81.90</cell><cell>77.72</cell><cell>75.87</cell></row><row><cell>Google/HerMiT</cell><cell>81.68</cell><cell>76.58</cell><cell>73.41</cell></row><row><cell>Google/SF-ID</cell><cell>78.87</cell><cell>74.31</cell><cell>72.06</cell></row><row><cell>SLURP/HerMiT</cell><cell>82.31</cell><cell>78.07</cell><cell>74.62</cell></row><row><cell>Multi-SLURP/HerMiT</cell><cell>85.69</cell><cell>81.42</cell><cell>78.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>System accuracy of Scenario and Action.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>System performance on entity prediction</figDesc><table><row><cell>6,000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5,196</cell><cell>5,005</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5,000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4,000</cell><cell></cell><cell>3,182</cell><cell>3,010</cell><cell></cell><cell>3,490</cell><cell>3,660</cell></row><row><cell>3,000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2,000</cell><cell></cell><cell></cell><cell></cell><cell>1,205</cell><cell>1,396</cell></row><row><cell>1,000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">No Errors</cell><cell cols="2">ASR Errors</cell><cell cols="2">NLU Errors ASR/NLU Errors</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>No Errors refer to the number of predicted entities that match the gold transcriptions perfectly. ASR Errors count the number of predictions where ASR outputs an unmatched candidate but the NLU system is nevertheless able to recover the correct entities from the transcriptions. NLU Errors count sentences where transcriptions are correct, but entities do not match. ASR/NLU Errors count the sentences where both ASR and NLU errors are present.</figDesc><table><row><cell>Figure 5: Error propagation: 6 Error Analysis</cell></row><row><cell>6.1 Analysis of Error Propagation for</cell></row><row><cell>different NLU Approaches</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that Action &amp; Entities are also referred to as 'Intent'. Entities consist of'Tags' and 'Fillers', aka. 'Slots' and  'Values'.   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://cloud.google.com/ text-to-speech</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Sophisticated words are considered words not in the 2000 more frequent words in English language. 4 Standard size of a segment for written text is 50, but we are here considering short utterances, so we lowered this number to 10.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">System build while third author was with Emotech LTD. 8 This includes SLURP-Train and additional 50k sentences that has been collected, but not annotated for NLU purposes. 9 https://cloud.google.com/ speech-to-text/ tested on 20/05/2020 using the command and search model. Note, that these systems are not directly comparable as Multi-ASR benefits from speaker adaptation, and an in-domain LM data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to Emotech Ltd and H. Zhuang for agreeing to release this data for research purposes. Special thanks to P. Mediano, M. Zhou and X. Chen for help with designing and organising data collection. This research received funding from the EPSRC project MaDrIgAL (EP/N017536/1), as well as Google Research Grant to support NLU and dialog research at Heriot-Watt University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Common voice: A massivelymultilingual speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosana</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Steve Renals, and Pawel Swietojanski. 2020. Adaptation algorithms for speech recognition: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Fainberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiwoz -a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe?</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ultes</forename><surname>Stefan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ramadan Osman, and Milica Ga?i?</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unleashing the killer corpus: experiences in creating the multi-everything ami meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The fisher corpus: a resource for the next generations of speech-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Snips voice platform: an embedded spoken language understanding system for privateby-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma?l</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
		</author>
		<idno>abs/1805.10190</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">How complex is that sentence? a proposed revision of the rosenberg and abbeduto d-level scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzhou</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cati</forename><surname>Brown-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorina</forename><surname>Naci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Expanding the scope of the ATIS task: the ATIS-3 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Er</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ARPA Human Language Technology Workshop &apos;92</title>
		<meeting>ARPA Human Language Technology Workshop &apos;92<address><addrLine>Plainsboro, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slu for voice command in smart home: Comparison of pipeline and end-to-end approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Desot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="822" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel bi-directional interrelated model for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haihong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1544</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5467" to="5471" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Mc-Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 1992 IEEE International Conference on Acoustics, Speech and Signal Processing<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From audio to semantics: Approaches to end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The atis spoken language systems pilot corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<idno type="DOI">10.3115/116580.116613</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Speech and Natural Language, HLT &apos;90</title>
		<meeting>the Workshop on Speech and Natural Language, HLT &apos;90<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The icsi meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Janin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<editor>I-I. IEEE</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;03</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Studies in Language Behavior: I. A Program of Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendell</forename><forename type="middle">Johnson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Foreign Language Center Technical Reports. American Psychological Association</title>
		<imprint>
			<date type="published" when="1944" />
		</imprint>
	</monogr>
	<note>Psychological Monographs: General and Applied</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Switchboard-damsl labeling project coder&quot;s manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A study on data augmentation of reverberant speech for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5220" to="5224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The lexical profile of second language writing: Does it change over time? RELC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Batia</forename><surname>Laufer</surname></persName>
		</author>
		<idno type="DOI">10.1177/003368829402500202</idno>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="33" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Benchmarking natural language understanding services for building conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Spoken Dialogue Systems Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic measurement of syntactic complexity in child language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The relationship of lexical richness to the quality of esl learners&apos; oral narratives. The Modern Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-4781.2011.01232_1.x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="208" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Using speech synthesis to train end-to-end spoken language understanding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<idno>abs/1910.09463</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikrant</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2396</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2019-09-19" />
			<biblScope unit="page" from="814" to="818" />
			<pubPlace>Graz, Austria</pubPlace>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analysis of automatic speech recognition with multiple microphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Munasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Asensio-Cubero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Reddy</forename><surname>Bethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zylfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitrocsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mezza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fountas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Medvesek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Building proactive voice assistants: When and how (not) to interact. CoRR, abs/2005.01322</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The E2E dataset: New challenges for endto-end generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Du?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbr?cken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding</title>
		<imprint>
			<publisher>CONF. IEEE Signal Processing Society</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<title level="m">Purely sequence-trained neural networks for asr based on lattice-free mmi</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Schema-guided dialogue state tracking task at dstc8</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><forename type="middle">Kumar</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Khaitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Dialog System Technology Challenges Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer learning for multilingual task oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3795" to="3805" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1802.08395</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning hidden unit contributions for unsupervised acoustic model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1450" to="1463" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Caubri?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Est?ve</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2158</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="824" to="828" />
			<pubPlace>Graz, Austria. ISCA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical multi-task natural language understanding for crossdomain conversational AI: HERMIT NLU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastianelli</forename><surname>Vanzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lemon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5931</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wolfe-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inagaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Second Language Development in Writing: Measures of Fluency, Accuracy, &amp; Complexity. National Foreign Language Center Technical Reports. Second Language Teaching &amp; Curriculum Center, University of Hawaii at Manoa</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Catslu: The 1st chinese audiotextual spoken language understanding challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340555.3356098</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Multimodal Interaction, ICMI &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="521" to="525" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
