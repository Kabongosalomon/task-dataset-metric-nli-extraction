<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uni. of Catania</orgName>
								<address>
									<settlement>Italy</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Uni. of Toronto</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uni. of Catania</orgName>
								<address>
									<settlement>Italy</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uni. of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Egocentric Vision</term>
					<term>Dataset</term>
					<term>Benchmarks</term>
					<term>First-Person Vi- sion</term>
					<term>Egocentric Object Detection</term>
					<term>Action Recognition and Anticipation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>First-person vision is gaining interest as it offers a unique viewpoint on people's interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict non-scripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, we have seen significant progress in many domains such as image classification <ref type="bibr" target="#b16">[19]</ref>, object detection <ref type="bibr" target="#b34">[37]</ref>, captioning <ref type="bibr" target="#b23">[26]</ref> and visual questionanswering <ref type="bibr" target="#b2">[3]</ref>. This success has in large part been due to advances in deep learning <ref type="bibr" target="#b24">[27]</ref> as well as the availability of large-scale image benchmarks <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b51">55]</ref>. While gaining attention, work in video understanding has been more scarce, mainly due to the lack of annotated datasets. This has been changing recently, with the release of the action classification benchmarks such as <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">54,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b43">46,</ref><ref type="bibr" target="#b12">14]</ref>. With the exception of <ref type="bibr" target="#b43">[46]</ref>, most of these datasets contain videos that are very short in duration, i.e., only a few seconds long, focusing on a single action. Charades <ref type="bibr" target="#b39">[42]</ref> makes a step towards activity recognition by collecting 10K videos of humans performing various tasks in their home. While this dataset is a nice attempt to collect daily actions, the videos have been recorded in a scripted way, by asking AMT workers to act out a script in front of the camera. This makes <ref type="figure" target="#fig_0">Fig. 1</ref>: From top: Frames from the 32 environments; Narrations by participants used to annotate action segments; Active object bounding box annotations the videos look oftentimes less natural, and they also lack the progression and multi-tasking of actions that occur in real life.</p><p>Here we focus on first-person vision, which offers a unique viewpoint on people's daily activities. This data is rich as it reflects our goals and motivation, ability to multi-task, and the many different ways to perform a variety of important, but mundane, everyday tasks (such as cleaning the dishes). Egocentric data has also recently been proven valuable for human-to-robot imitation learning <ref type="bibr" target="#b31">[34,</ref><ref type="bibr" target="#b49">53]</ref>, and has a direct impact on HCI applications. However, datasets to evaluate first-person vision algorithms <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b6">8]</ref> have been significantly smaller in size than their third-person counterparts, often captured in a single environment <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b6">8]</ref>. Daily interactions from wearable cameras are also scarcely available online, making this a largely unavailable source of information.</p><p>In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric dataset. Our data was collected by 32 participants, belonging to 10 nationalities, in their native kitchens <ref type="figure" target="#fig_0">(Fig. 1)</ref>. The participants were asked to capture all their daily kitchen activities, and record sequences regardless of their duration. The recordings, which include both video and sound, not only feature the typical interactions with one's own kitchenware and appliances, but importantly show the natural multi-tasking that one performs, like washing a few dishes amidst cooking. Such parallel-goal interactions have not been captured in existing datasets, making this both a more realistic as well as a more challeng- Altogether, EPIC-KITCHENS has 55hrs of recording, densely annotated with start/end times for each action/interaction, as well as bounding boxes around objects subject to interaction. We describe our object, action and anticipation challenges, and report baselines in two scenarios, i.e., seen and unseen kitchens. The dataset and leaderboards to track the community's progress on all challenges, with held out test ground-truth are at: http://epic-kitchens.github.io.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Datasets</head><p>We compare EPIC-KITCHENS to four commonly-used <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b6">8]</ref> and two recent <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b38">41]</ref> egocentric datasets in <ref type="table" target="#tab_0">Table 1</ref>, as well as six third-person activityrecognition datasets <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b52">56,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b36">39]</ref> that focus on object-interaction activities. We exclude egocentric datasets that focus on inter-person interactions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b37">40]</ref>, as these target a different research question.</p><p>A few datasets aim at capturing activities in native environments, most of which are recorded in third-person <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b25">28]</ref>. <ref type="bibr" target="#b25">[28]</ref> focuses on cooking dishes based on a list of breakfast recipes. In <ref type="bibr" target="#b12">[14]</ref>, short segments linked to interactions with 30 daily objects are collected by querying YouTube, while <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b38">41]</ref> are scripted -subjects are requested to enact a crowd-sourced storyline <ref type="bibr" target="#b39">[42,</ref><ref type="bibr" target="#b38">41]</ref> 1 or a given action <ref type="bibr" target="#b15">[18]</ref>, which oftentimes results in less natural looking actions. All egocentric datasets similarly use scripted activities, i.e. people are told what actions to perform. When following instructions, participants perform steps in a sequential order, as opposed to the more natural real-life scenarios addressed in our work, which involve multi-tasking, searching for an item, thinking what to do next, changing one's mind or even unexpected surprises. EPIC-KITCHENS is most closely related to the ADL dataset <ref type="bibr" target="#b33">[36]</ref> which also provides egocentric recordings in native environments. However, our dataset is substantially larger: In discussion with the primary author and based on our analysis of the released footage, around 70% of videos in Charades-ego are truly egocentric (i.e. recorded using a wearable camera with the action performed by the wearer). We use this percentage in reporting statistics on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: Head-mounted GoPro used in dataset recording</head><p>Use any word you prefer. Feel free to vary your words or stick to a few. Use present tense verbs (e.g. cut/open/close). Use verb-object pairs (e.g. wash carrot). You may (if you prefer) skip articles and pronouns (e.g. "cut kiwi" rather than "I cut the kiwi"). Use propositions when needed (e.g. "pour water into kettle"). Use 'and' when actions are co-occurring (e.g. "hold mug and pour water"). If an action is taking long, you can narrate again (e.g. "still stirring soup"). <ref type="figure">Fig. 3</ref>: Instructions used to collect video narrations from our participants it has 11.5M frames vs 1M in ADL, 90x more annotated action segments, and 4x more object bounding boxes, making it the largest first-person dataset to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The EPIC-KITCHENS Dataset</head><p>In this section, we describe our data collection and annotation pipeline. We also present various statistics, showcasing different aspects of our collected data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>The dataset was recorded by 32 individuals in 4 cities in different countries (in North America and Europe): 15 in Bristol/UK, 8 in Toronto/Canada, 8 in Catania/Italy and 1 in Seattle/USA between May and Nov 2017. Participants were asked to capture all kitchen visits for three consecutive days, with the recording starting immediately before entering the kitchen, and only stopped before leaving the kitchen. They recorded the dataset voluntarily and were not financially rewarded. The participants were asked to be in the kitchen alone for all the recordings, thus capturing only one-person activities. We also asked them to remove all items that would disclose their identity such as portraits or mirrors. Data was captured using a head-mounted GoPro with an adjustable mounting to control the viewpoint for different environments and participants' heights.  Before each recording, the participants checked the battery life and viewpoint, using the GoPro Capture app, so that their stretched hands were approximately located at the middle of the camera frame. The camera was set to linear field of view, 59.94fps and Full HD resolution of 1920x1080, however some subjects made minor changes like wide or ultra-wide FOV or resolution, as they recorded multiple sequences in their homes, and thus were switching the device off and on over several days. Specifically, 1% of the videos were recorded at 1280x720 and 0.5% at 1920x1440. Also, 1% at 30fps, 1% at 48fps and 0.2% at 90fps. The recording lengths varied depending on the participant's kitchen engagement. On average, people recorded for 1.7hrs, with the maximum being 4.6hrs. Cooking a single meal can span multiple sequences, depending on whether one stays in the kitchen, or leaves and returns later. On average, each participant recorded 13.6 sequences. <ref type="figure">Figure 4</ref> presents statistics on time of day using the local time of the recording, high-level goals and sequence durations.</p><formula xml:id="formula_0">???? ? ?? ? ??? ? ? ? ? ? ? ? ? ? ? ? ? ?????? ? ? ? ??? ? ? ? ? ? ? ? ? ? ?? ??? ? ? ? ? ? ? ? ? ? ? ??? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ??v ? ? ?????? ? ???? ? v ?</formula><p>Since crowd-sourcing annotations for such long videos is very challenging, we had our original participants do a coarse first annotation. Each participant was asked to watch their videos, after completing all recordings, and narrate the actions carried out, using a hand-held recording device. We opted for a sound recording rather than written captions as this is arguably much faster for the participants, who were thus more willing to provide these annotations. These are analogous to a live commentary of the video. The general instructions for narrations are listed in <ref type="figure">Fig. 3</ref>. The participant narrated in English if sufficiently fluent or in their native language. In total, 5 languages were used: 17 narrated in English, 7 in Italian, 6 in Spanish, 1 in Greek and 1 in Chinese. <ref type="figure">Figure 4</ref> shows wordles of the most frequent words in each language.</p><p>Our decision to collect narrations from the participants themselves is because they are the most qualified to label the activity compared to an independent observer, as they were the ones performing the actions. We opted for a postrecording narration such that the participant performs her/his daily activities undisturbed, without being concerned about labelling. We tested several automatic audio-to-text APIs [17, <ref type="bibr" target="#b20">23,</ref><ref type="bibr">5]</ref>, which failed to produce accurate transcriptions as these expect a relevant corpus and complete sentences for context. We thus collected manual transcriptions via Amazon Mechanical Turk (AMT), and used the YouTube's automatic closed caption alignment tool to produce accurate timings. For non-English narrations, we also asked AMT workers to translate the sentences. To make the job more suitable for AMT, narration audio files are split by removing silence below a pre-specified decibel threshold (after compression and normalisation). Speech chunks are then combined into HITs with a duration of around 30 seconds each. To ensure consistency, we submit the same HIT three times and select the ones with an edit distance of 0 to at least one other HIT. We manually corrected cases when there was no agreement. Examples of transcribed and timed narrations are provided in <ref type="table" target="#tab_2">Table 2</ref>. The participants were also asked to provide one sentence per sequence describing the overall goal or activity that took place.</p><p>In total, we collected 39, 596 action narrations, corresponding to a narration every 4.9s in the video. The average number of words per phrase is 2.8 words. These narrations give us an initial labelling of all actions with rough temporal alignment, obtained from the timestamp of the audio narration with respect to the video. However, narrations are also not a perfect source of ground-truth:</p><p>-The narrations can be incomplete, i.e., the participants were selective in which actions they chose to narrate. We noticed that they labelled the 'open' actions more than their counter-action 'close', as the narrator's attention has already moved to the next goal. We consider this phenomena in our evaluation, by only evaluating actions that have been narrated. -Temporally, the narrations are belated, after the action takes place. This is adjusted using ground-truth action segments (see Sec. 3.2). -Participants use their own vocabulary and free language. While this is a challenging issue, we believe it is important to push the community to go beyond the pre-selected list of labels (also argued in <ref type="bibr" target="#b51">[55]</ref>). We here resolve this issue by grouping verbs and nouns into minimally overlapping classes (see Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Action Segment Annotations</head><p>For each narrated sentence, we adjust the start and end times of the action using AMT. To ensure the annotators are trained to perform temporal localisation, we The green participant's annotations are selected as the final annotations use a clip from our previous work's understanding <ref type="bibr" target="#b30">[33]</ref> that explains temporal bounds of actions. Each HIT is composed of a maximum of 10 consecutive narrated phrases p i , where annotators label A i = [t si , t ei ] as the start and end times of the i th action. Two constraints were added to decrease the amount of noisy annotations: (1) action has to be at least 0.5 seconds in length; (2) action cannot start before the preceding action's start time. Note that consecutive actions are allowed to overlap. Moreover, the annotators could indicate that the action does not appear in the video. This handles occluded, impossible to distinguish or out-of-bounds cases.</p><p>To ensure consistency, we ask K a = 4 annotators to annotate each HIT. Given one annotation A i (j) (i is the action and j indexes the annotator), we calculate the agreement as follows: ? i (j) = 1 Ka Ka k=1 IoU(A i (j), A i (k)). We first find the annotator with the maximum agreement? = arg max j ? i (j), and find k = arg max k IoU(A i (?), A i (k)). The ground-truth action segment A i is then defined as:</p><formula xml:id="formula_1">Ai = Union(Ai(?), Ai(k)), if IoU(Ai(?), Ai(k)) &gt; 0.5 Ai(?), otherwise<label>(1)</label></formula><p>We thus combine two annotations when they have a strong agreement, since in some cases the single (best) annotation results in a too tight of a segment. <ref type="figure">Figure 5</ref> shows examples of combining annotations.</p><p>In total, we collected such labels for 39, 564 action segments (lengths: ? = 3.7s, ? = 5.6s). These represent 99.9% of narrated segments. The missed annotations were those labelled as "not visible" by the annotators, though mentioned in narrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Active Object Bounding Box Annotations</head><p>The narrated nouns correspond to objects relevant to the action <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b4">6]</ref>. Assume O i is the set of one or more nouns in the phrase p i associated with the action segment A i = [t si , t ei ]. We consider each frame f within [t si ? 2s, t ei + 2s] as a potential frame to annotate the bounding box(es), for each object in O i . We build on the interface from <ref type="bibr" target="#b46">[49]</ref> for annotating bounding boxes on AMT. Each HIT aims to get an annotation for one object, for the maximum duration of 25s, which corresponds to 50 consecutive frames at 2fps. The annotator can also note that the object does not exist in f . We particularly ask the same annotator to annotate consecutive frames to avoid subjective decisions on the extents of objects. We also assess annotators' quality by ensuring that the annotators obtain an IoU ? 0.7 on two golden annotations at the start of every HIT. We request K o = 3 workers per HIT, and select the one with maximum agreement ?:</p><formula xml:id="formula_2">?(q) = f Ko max j =q max k,l IoU(BB(j, f, k), BB(q, f, l))<label>(2)</label></formula><p>where BB(q, f, k) is the k th bounding box annotation by annotator q in frame f . Ties are broken by selecting the worker who provides the tighter bounding boxes. <ref type="figure">Figure 6</ref> shows multiple annotations for four keyframes in a sequence. Overall, 77% of requested annotations resulted in at least one bounding box. In total, we collected 454,255 bounding boxes (? = 1.64 boxes/frame, ? = 0.92). Sample action segments and object bounding boxes are shown in <ref type="figure" target="#fig_3">Fig. 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Verb and Noun Classes</head><p>Since our participants annotated using free text in multiple languages, a variety of verbs and nouns have been collected. We group these into classes with minimal semantic overlap, to accommodate the more typical approaches to multi-class detection and recognition where each example is believed to belong to one class only. We estimate Part-of-Speech (POS), using SpaCy's English core web model. We select the first verb in the sentence, and find all nouns in the sentence excluding any that match the chosen verb. When a noun is absent or replaced by a pronoun (e.g. 'it' ), we use the noun from the directly preceding narration (e.g. p i : 'rinse cup', p i+1 : 'place it to dry').</p><p>We refer to the set of minimally-overlapping verb classes as C V , and similarly C N for nouns. We attempted to automate the clustering of verbs and nouns using combinations of WordNet <ref type="bibr" target="#b29">[32]</ref>, Word2Vec <ref type="bibr" target="#b28">[31]</ref>, and Lesk algorithm <ref type="bibr" target="#b3">[4]</ref>, however, due to limited context there were too many meaningless clusters. We thus elected to manually cluster the verbs and semi-automatically cluster the nouns. We preprocessed the compound nouns e.g. 'pizza cutter' as a subset of the second noun e.g. 'cutter'. We then manually adjusted the clustering, merging the variety of names used for the same object, e.g. 'cup' and 'mug', as well as splitting some base nouns, e.g. 'washing machine' vs 'coffee machine'.</p><p>In total, we have 125 C V classes and 331 C N classes. <ref type="table" target="#tab_3">Table 3</ref> shows a sample of grouped verbs and nouns into classes. These classes are used in all three defined challenges. In <ref type="figure" target="#fig_4">Fig. 8</ref>, we show C V ordered by frequency of occurrence in action segments, as well as C N ordered by number of annotated bounding boxes. These are grouped into 19 super categories, of which 9 are food and drinks, with the rest containing kitchen essentials from appliances to cutlery. Co-occurring classes are presented in <ref type="figure" target="#fig_5">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Annotation Quality Assurance</head><p>To analyse the quality of annotations, we choose 300 random samples, and manually assess correctness. We report:</p><p>-Action Segment Boundaries (A i ): We check that the start/end times fully enclose the action boundaries, with any additional frames not part of other actions -error: 5.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Object Bounding Boxes (O i ):</head><p>We check that the bounding box encapsulates the object or its parts, with minimal overlap with other objects, and that all instances of the class in the frame have been labelled -error: 6.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Verb classes (C V ):</head><p>We check that the verb class is correct -error: 3.3%.</p><p>-Noun classes (C N ): We check that the noun class is correct -error : 6.0%.</p><p>These error rates are comparable to recently published datasets <ref type="bibr" target="#b50">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarks and Baseline Results</head><p>EPIC-KITCHENS offers a variety of potential challenges from routine understanding, to activity recognition and object detection. As a start, we define three challenges for which we provide baseline results, and avail online leaderboards. For the evaluation protocols, we hold out ground truth annotations for 27% of  the data <ref type="table" target="#tab_4">(Table 4</ref>). We particularly aim to assess the generalizability to novel environments, and we thus structured our test set to have a collection of seen and previously unseen kitchens: Seen Kitchens (S1): In this split, each kitchen is seen in both training and testing, where roughly 80% of sequences are in training and 20% in testing. We do not split sequences, thus each sequence is in either training or testing. Unseen Kitchens (S2): This divides the participants/kitchens so all sequences of the same kitchen are either in training or testing. We hold out the complete sequences for 4 participants for this testing protocol. The test set of S2 is only 7% of the dataset in terms of frame count, but the challenges remain considerable. We now evaluate several existing methods on our benchmarks, to gain an understanding of how challenging our dataset is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Object Detection Benchmark</head><p>Challenge: This challenge focuses on object detection for all of our C N classes. Note that our annotations only capture the 'active' objects pre-, during-and post-interaction. We thus restrict the images evaluated per class to those where the object has been annotated. We particularly aim to break the performance down into multi-shot and few-shot class groups, so as to analyse the capabilities of the approaches to quickly learn novel objects (with only a few examples). Our challenge leaderboard reflects the methods' abilities on both sets of classes. Method: We evaluate object detection using Faster R-CNN <ref type="bibr" target="#b34">[37]</ref> due to its stateof-the-art performance. Faster R-CNN uses a region proposal network (RPN) to first generate class agnostic object proposals, and then classifies these and outputs refined bounding box predictions. We use the implementation from <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b19">22]</ref> with a base architecture of ResNet-101 <ref type="bibr" target="#b16">[19]</ref> pre-trained on MS-COCO <ref type="bibr" target="#b27">[30]</ref>. Implementation Details: Learning rate is initialised to 0.0003 decaying by a factor of 10 after 90K and stopped after 120K iterations. We use a mini-batch size of 4 on 8 Nvidia P100 GPUs on a single compute node (Nvidia DGX-1) with distributed training and parameter synchronisation -i.e. overall mini-batch size of 32. As in <ref type="bibr" target="#b34">[37]</ref>, images are rescaled such that their shortest side is 600 pixels and the aspect ratio is maintained. We use a stride of 16 on the last convolution  Evaluation Metrics: For each class, we only report results on I cn?C N , these are all images where class c n has been annotated. We use the mean average precision (mAP) metric from PASCAL VOC <ref type="bibr" target="#b9">[11]</ref>, using IoU thresholds of 0.05, 0.5 and 0.75 similar to <ref type="bibr" target="#b27">[30]</ref>.</p><p>Results: We report results in <ref type="table" target="#tab_5">Table 5</ref> for many-shot classes (those with ? 100 bounding boxes in training) and few shot classes (with ? 10 and &lt; 100 bounding boxes in training), alongside AP for the 15 most frequent classes. There are a total of 202 many-shot classes and 88 few-shot classes. One can see that our objects are generally harder to detect than in most existing datasets, with performance at the standard IoU &gt; 0.5 below 40%. Even at a very small IoU threshold, the performance is relatively low. The more challenging classes are "meat", "knife", and "spoon", despite being some of the most frequent ones. Notice that the performance for the low-shot regime is substantially lower than in the many-shot regime. This points to interesting challenges for the future. However, performances for the Seen and Unseen splits in object detection are comparable, thus showing generalization capability across environments.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Action Recognition Benchmark</head><p>Challenge: Given an action segment A i = [t si , t ei ], we aim to classify the segment into its action class, where classes are defined as C a = {(c v ? C V , c n ? C N )}, and c n is the first noun in the narration when multiple nouns are present. Note that our dataset supports more complex action-level challenges, such as action localisation in the videos of full duration. We decided to focus on the classification challenge first (the segment is provided) since most existing works tackle this challenge. Network Architecture: We train the Temporal Segment Network (TSN) <ref type="bibr" target="#b45">[48]</ref> as a state-of-the-art architecture in action recognition, but adjust the output layer to predict both verb and noun classes jointly, with independent losses, as in <ref type="bibr" target="#b22">[25]</ref>. We use the PyTorch implementation [51] with the Inception architecture <ref type="bibr" target="#b42">[45]</ref>, batch normalization <ref type="bibr" target="#b21">[24]</ref> and pre-trained on ImageNet <ref type="bibr" target="#b7">[9]</ref>. Implementation Details: We train both spatial and temporal streams, the latter on dense optical flow at 30fps extracted using the TV-L 1 algorithm <ref type="bibr" target="#b48">[52]</ref> between RGB frames using the formulation TV-L 1 (I 2t , I 2t+3 ) to eliminate optical flicker, and released the computed flow as part of the dataset. We do not perform stratification or weighted sampling, allowing the dataset class imbalance to propagate into the mini-batch. We train each model on 8 Nvidia P100 GPUs on a single compute node (Nvidia DGX-1) for 80 epochs with a mini-batch size of 512. We set learning rate to 0.01 for spatial and 0.001 for temporal streams decreasing it by a factor of 10 after epochs 20 and 40. After averaging the 25 samples within the action segment each with 10 spatial croppings as in <ref type="bibr" target="#b45">[48]</ref>, we fuse both streams by averaging class predictions with equal weights. All unspecified parameters use the same values as <ref type="bibr" target="#b45">[48]</ref>.  <ref type="figure" target="#fig_0">Fig. 11</ref>: Qualitative results for the action recognition and anticipation challenges Evaluation Metrics: We report two sets of metrics: aggregate and per-class, which are equivalent to the class-agnostic and class-aware metrics in <ref type="bibr" target="#b50">[54]</ref>. For aggregate metrics, we compute top-1 and top-5 accuracy for correct predictions of c v , c n and their combination (c v , c n ) -we refer to these as 'verb', 'noun' and 'action'. Accuracy is reported on the full test set. For per-class metrics, we compute precision and recall, for classes with more than 100 samples in training, then average the metrics across classes -these are 26 verb classes, 71 noun classes, and 819 action classes. Per-class metrics for smaller classes are ? 0 as TSN is better suited for classes with sufficient training data. Results: We report results in <ref type="table" target="#tab_6">Table 6</ref> for aggregate metrics and per-class metrics. We compare TSN (3 segments) to 2SCNN <ref type="bibr" target="#b40">[43]</ref> (1 segment), chance and largest class baselines. Fused results perform best or are comparable to the best stream (spatial/temporal). The challenge of getting both verb and noun labels correct remains significant for both seen (top-1 accuracy 20.5%) and unseen (top-1 accuracy 10.9%) environments. This implies that for many examples, we only get one of the two labels (verb/noun) right. Results also show that generalising to unseen environments is a harder challenge for actions than it is for objects. We give a breakdown per-class metrics for the 15 largest verb classes in <ref type="table" target="#tab_7">Table 7</ref>. <ref type="figure" target="#fig_0">Fig. 11</ref> reports qualitative results, with success highlighted in green, and failures in red. In the first column both the verb and the noun are correctly predicted, in the second column one of them is correctly predicted, while in the third column both are incorrect. Challenging cases like distinguishing 'adjust heat' from turning it on, or pouring soy sauce vs oil are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C T I O N A N T I C I P A T I O N A C T I O N R E C O G N I T I O N</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Action Anticipation Benchmark</head><p>Challenge: Anticipating the next action is a well-mastered skill by humans, and automating it has direct implications in assertive living. Given any of the upcoming wearable system (e.g. Microsoft Hololens or Google Glass), anticipating the wearer's next action, from a first-person view, could trigger smart home appliances, providing a seamless achievement of the wearer's goals. Previous works have investigated different anticipation tasks from an egocentric perspective, e.g. predicting future localisation <ref type="bibr" target="#b32">[35]</ref> or next-active object <ref type="bibr" target="#b13">[15]</ref>. We here consider the task of forecasting an action before it happens. Let ? a be the 'anticipation time', how far in advance to recognise the action, and ? o be the 'observation time', the length of the observed video segment preceding the action. Given an action segment A i = [t si , t ei ], we predict the action class C a by observing the video segment preceding the action start time t si by ? a , that is [t si ? (? a + ? o ), t si ? ? a ]. Network Architecture: As in Sec. 4.2, we train TSN <ref type="bibr" target="#b45">[48]</ref> to provide baseline action anticipation results and compare with 2SCNN <ref type="bibr" target="#b40">[43]</ref>. We feed the model with the video segments preceding annotated actions and train it to predict verb and noun classes jointly as in <ref type="bibr" target="#b22">[25]</ref>. Similarly to <ref type="bibr" target="#b44">[47]</ref>, we set ? a = 1s. We report results with ? o = 1s, and note that performance drops with longer segments. Implementation Details: Models for both spatial and temporal modalities are trained using a single Nvidia Titan X with a batch size of 64, for 80 epochs, setting the initial learning rate to 0.001 and dropping it by a factor of 10 after 30 and 60 epochs. Fusion weights spatial and temporal streams with 0.6 and 0.4 respectively. All other parameters use the values specified in <ref type="bibr" target="#b45">[48]</ref>. Evaluation Metrics: We use the same evaluation metrics as in Sec. 4.2.</p><p>Results: <ref type="table" target="#tab_9">Table 8</ref> reports baseline results for the action anticipation challenge. As expected, this is a harder challenge than action recognition, and thus we note a drop in performance throughout. Unlike the case of action recognition, the flow stream and fusion do not generally improve performances. TSN often offers small, but consistent improvements over 2SCNN. <ref type="figure" target="#fig_0">Fig. 11</ref> reports qualitative results. Success examples are highlighted in green, and failure cases in red. As the qualitative figure shows, the method over-predicts 'put' as the next action. Once an object is picked up, the learned model has a tendency to believe it will be put down next. Methods that focus on long-term understanding of the goal, as well as multi-scale history would be needed to circumvent such a tendency.</p><p>Discussion: The three defined challenges form the base for higher-level understanding of the wearer's goals. We have shown that existing methods are still far from tackling these tasks with high precision, pointing to exciting future directions. Our dataset lends itself naturally to a variety of less explored tasks. We are planning to provide a wider set of challenges, including action localisation <ref type="bibr" target="#b47">[50]</ref>, video parsing <ref type="bibr" target="#b39">[42]</ref>, visual dialogue <ref type="bibr" target="#b5">[7]</ref>, goal completion <ref type="bibr" target="#b17">[20]</ref> and skill determination <ref type="bibr" target="#b8">[10]</ref> (e.g. how good are you at making your eggs for breakfast?). Since real-time performance is crucial in this domain, our leaderboard will reflect this, pressing the community to come up with efficient and effective solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We present the largest and most varied dataset in egocentric vision to date, EPIC-KITCHENS, captured in participants' native environments. We collect 55 hours of video data recorded on a head-mounted GoPro, and annotate it with narrations, action segments and object annotations using a pipeline that starts with live commentary of recorded videos by the participants themselves. Baseline results on object detection, action recognition and anticipation challenges show the great potential of the dataset for pushing approaches that target fine-grained video understanding to new frontiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Release:</head><p>-Dataset sequences, extracted frames and optical flow are available at:</p><p>http://dx.doi.org/10.5523/bris.3h91syskeag572hl6tvuovwv4d -Annotations, challenge leader-board results and updates and news are available at: http://epic-kitchens.github.io</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Abbrir</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>An example of annotated action segments for 2 consecutive actions Object annotation from three AMT workers (orange, blue and green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Sample consecutive action segments with keyframe object annotations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>From Top: Frequency of verb classes in action segments; Frequency of noun clusters in action segments, by category; Frequency of noun clusters in bounding box annotations, by category; Mean and standard deviation of bounding box, by category</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Left: Frequently co-occurring verb/nouns in action segments [e.g. (open/close, cupboard/drawer/fridge), (peel, carrot/onion/potato/peach), (adjust, heat)]; Middle: Next-action excluding repetitive instances of the same action [e.g. peel ? cut, turn-on ? wash, pour ? mix].; Right: Co-occurring bounding boxes in one frame [e.g. (pot, coffee), (knife, chopping board), (tap, sponge)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative results for the object detection challenge layer for feature extraction and for anchors we use 4 scales of 0.25, 0.5, 1.0 and 2.0; and aspect ratios of 1:1, 1:2 and 2:1. To reduce redundancy, NMS is used with an IoU threshold of 0.7. In training and testing we use 300 RPN proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 shows</head><label>10</label><figDesc>qualitative results with detections shown in colour and ground truth shown in black. The examples in the right-hand column are failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparative overview of relevant datasets * action classes with &gt; 50 samples</figDesc><table><row><cell></cell><cell></cell><cell>Non-</cell><cell>Native</cell><cell></cell><cell>Sequ-</cell><cell cols="6">Action Action Object Object Partici-No.</cell></row><row><cell>Dataset</cell><cell cols="4">Ego? Scripted? Env? Year</cell><cell cols="3">Frames ences Segments Classes</cell><cell cols="4">BBs Classes pants Env.s</cell></row><row><cell>EPIC-KITCHENS</cell><cell></cell><cell></cell><cell></cell><cell>2018</cell><cell>11.5M 432</cell><cell cols="4">39,596 149* 454,255 323</cell><cell>32</cell><cell>32</cell></row><row><cell>EGTEA Gaze+ [16]</cell><cell></cell><cell>?</cell><cell>?</cell><cell>2018</cell><cell>2.4M 86</cell><cell cols="2">10,325 106</cell><cell>0</cell><cell>0</cell><cell>32</cell><cell>1</cell></row><row><cell>Charades-ego [41]</cell><cell>70%</cell><cell>?</cell><cell></cell><cell>2018</cell><cell>2.3M 2,751</cell><cell cols="2">30,516 157</cell><cell>0</cell><cell>38</cell><cell>71</cell><cell>N/A</cell></row><row><cell>BEOID [6]</cell><cell></cell><cell>?</cell><cell>?</cell><cell>2014</cell><cell>0.1M 58</cell><cell>742</cell><cell>34</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>1</cell></row><row><cell>GTEA Gaze+ [13]</cell><cell></cell><cell>?</cell><cell>?</cell><cell>2012</cell><cell>0.4M 35</cell><cell>3,371</cell><cell>42</cell><cell>0</cell><cell>0</cell><cell>13</cell><cell>1</cell></row><row><cell>ADL [36]</cell><cell></cell><cell>?</cell><cell></cell><cell>2012</cell><cell>1.0M 20</cell><cell>436</cell><cell>32</cell><cell>137,780</cell><cell>42</cell><cell>20</cell><cell>20</cell></row><row><cell>CMU [8]</cell><cell></cell><cell>?</cell><cell>?</cell><cell>2009</cell><cell>0.2M 16</cell><cell>516</cell><cell>31</cell><cell>0</cell><cell>0</cell><cell>16</cell><cell>1</cell></row><row><cell>YouCook2 [56]</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">2018 @30fps 15.8M 2,000</cell><cell>13,829</cell><cell>89</cell><cell>0</cell><cell>0</cell><cell>2K</cell><cell>N/A</cell></row><row><cell>VLOG [14]</cell><cell>?</cell><cell></cell><cell></cell><cell>2017</cell><cell>37.2M 114K</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">10.7K N/A</cell></row><row><cell>Charades [42]</cell><cell>?</cell><cell>?</cell><cell></cell><cell>2016</cell><cell>7.4M 9,848</cell><cell cols="2">67,000 157</cell><cell>0</cell><cell>0</cell><cell>N/A</cell><cell>267</cell></row><row><cell>Breakfast [28]</cell><cell>?</cell><cell></cell><cell></cell><cell>2014</cell><cell>3.0M 433</cell><cell>3078</cell><cell>50</cell><cell>0</cell><cell>0</cell><cell>52</cell><cell>18</cell></row><row><cell>50 Salads [44]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>2013</cell><cell>0.6M 50</cell><cell>2967</cell><cell>52</cell><cell>0</cell><cell>0</cell><cell>25</cell><cell>1</cell></row><row><cell cols="2">MPII Cooking 2 [39] ?</cell><cell>?</cell><cell>?</cell><cell>2012</cell><cell>2.9M 273</cell><cell>14,105</cell><cell>88</cell><cell>0</cell><cell>0</cell><cell>30</cell><cell>1</cell></row></table><note>ing set of recordings. A video introduction to the recordings is available at: http://youtu.be/Dj6Y3H0ubDw.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Extracts from 6 transcription files in .sbv format</figDesc><table><row><cell>0:14:44.190,0:14:45.310</cell><cell cols="4">0:00:02.780,0:00:04.640 0:04:37.880,0:04:39.620 0:06:40.669,0:06:41.669 0:12:28.000,0:12:28.000</cell><cell>0:00:03.280,0:00:06.000</cell></row><row><cell>pour tofu onto pan</cell><cell>open the bin</cell><cell>Take onion</cell><cell>pick up spatula</cell><cell cols="2">pour pasta into container open fridge</cell></row><row><cell>0:14:45.310,0:14:49.540</cell><cell cols="4">0:00:04.640,0:00:06.100 0:04:39.620,0:04:48.160 0:06:41.669,0:06:45.250 0:12:33.000,0:12:33.000</cell><cell>0:00:06.000,0:00:09.349</cell></row><row><cell>put down tofu container</cell><cell>pick up the bag</cell><cell>Cut onion</cell><cell>stir potatoes</cell><cell>take jar of pesto</cell><cell>take milk</cell></row><row><cell>0:14:49.540,0:15:02.690</cell><cell cols="4">0:00:06.100,0:00:09.530 0:04:48.160,0:04:49.160 0:06:45.250,0:06:46.250 0 :12:39.000,0:12:39.000</cell><cell>0:00:09.349,0:00:10.910</cell></row><row><cell>stir vegetables and tofu</cell><cell>tie the bag</cell><cell>Peel onion</cell><cell>put down spatula</cell><cell>take teaspoon</cell><cell>put milk</cell></row><row><cell>0:15:02.690,0:15:06.260</cell><cell cols="4">0:00:09.530,0:00:10.610 0:04:49.160,0:04:51.290 0:06:46.250,0:06:50.830 0:12:41.000,0:12:41.000</cell><cell>0:00:10.910,0:00:12.690</cell></row><row><cell>put down spatula</cell><cell>tie the bag again</cell><cell>Put peel in bin</cell><cell>turn down hob</cell><cell>pour pesto in container</cell><cell>open cupboard</cell></row><row><cell>0:15:06.260,0:15:07.820</cell><cell cols="4">0:00:10.610,0:00:14.309 0:04:51.290,0:05:06.350 0:06:50.830,0:06:55.819 0:12:55.000,0:12:55.000</cell><cell>0:00:12.690,0:00:15.089</cell></row><row><cell>take tofu container</cell><cell>pick up bag</cell><cell>Peel onion</cell><cell>pick up pan</cell><cell cols="2">place pesto bottle on table take bowl</cell></row><row><cell>0:15:07.820,0:15:10.040</cell><cell cols="4">0:00:14.309,0:00:17.520 0:05:06.350,0:05:15.200 0:06:55.819,0:06:57.170 0:12:58.000,0:12:58.000</cell><cell>0:00:15.089,0:00:18.080</cell></row><row><cell cols="2">throw something into the bin put bag down</cell><cell>Put peel in bin</cell><cell>tip out paneer</cell><cell>take wooden spoon</cell><cell>open drawer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Sample Verb and Noun Classes -on) turn-on, start, begin, ignite, switch-on, activate, restart, light, ...</figDesc><table><row><cell>ClassNo (Key) Clustered Words</cell></row><row><cell>0 (take) 3 (close) 1 (pan) 8 (cupboard) cupboard, cabinet, locker, flap, cabinet door, cupboard door, closet, ... take, grab, pick, get, fetch, pick-up, ... close, close-off, shut pan, frying pan, saucepan, wok, ... 51 (cheese) cheese slice, mozzarella, paneer, parmesan, ... 12 (turnNOUN VERB 78 (top) top, counter, counter top, surface, kitchen counter, kitchen top, tiles, ...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Statistics of test splits: seen (S1) and unseen (S2) kitchens</figDesc><table><row><cell></cell><cell cols="7">#Subjects #Sequences Duration (s) % Narrated Segments Action Segments Bounding Boxes</cell></row><row><cell>Train/Val</cell><cell>28</cell><cell>272</cell><cell>141731</cell><cell></cell><cell>28,587</cell><cell>28,561</cell><cell>326,388</cell></row><row><cell>S1 Test</cell><cell>28</cell><cell>106</cell><cell>39084</cell><cell>20%</cell><cell>8,069</cell><cell>8,064</cell><cell>97,872</cell></row><row><cell>S2 Test</cell><cell>4</cell><cell>54</cell><cell>13231</cell><cell>7%</cell><cell>2,939</cell><cell>2,939</cell><cell>29,995</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Baseline results for the Object Detection challenge 15 Most Frequent Object Classes Totals mAP pan plate bowl onion tap pot knife spoon meat food potato cup pasta cupboard lid few-shot many-shot all S1 IoU &gt; 0.05 78.40 74.34 66.86 65.40 86.40 68.32 49.96 45.79 39.59 48.31 58.59 61.85 77.65 52.17 62.46 31.59 51.60 47.84 IoU &gt; 0.5 70.63 68.21 61.93 41.92 73.04 62.90 33.77 26.96 27.69 38.10 50.07 51.71 69.74 36.00 58.64 20.72 38.81 35.41 IoU &gt; 0.75 22.26 46.34 36.98 3.50 26.59 20.47 4.13 2.48 5.53 9.39 13.21 11..05 80.35 88.38 66.79 47.65 83.40 71.17 63.24 46.36 71.87 29.91 N/A 55.36 78.02 55.17 61.55 23.19 49.30 46.64 IoU &gt; 0.5 67.42 85.62 62.75 26.27 65.90 59.22 44.14 30.30 56.28 24.31 N/A 47.00 73.82 39.49 51.56 16.95 34.95 33.11 IoU &gt; 0.75 18.41 60.43 33.32 2.21 6.41 14.55 4.65 1.77 12.80 7.40 N/A 7.54 36.94</figDesc><table><row><cell>25 22.61</cell><cell>7.37 30.53 2.70</cell><cell>10.07</cell><cell>8.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Baseline results for the action recognition challenge 22.70 10.89 74.29 45.72 25.26 22.54 15.33 05.60 13.06 17.52 05.81</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Top-1 Accuracy</cell><cell cols="2">Top-5 Accuracy</cell><cell cols="2">Avg Class Precision</cell><cell cols="2">Avg Class Recall</cell></row><row><cell></cell><cell></cell><cell cols="8">VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION</cell></row><row><cell></cell><cell>Chance/Random</cell><cell>12.62 1.73</cell><cell>00.22</cell><cell>43.39 08.12</cell><cell>03.68</cell><cell>03.67 01.15</cell><cell>00.08</cell><cell>03.67 01.15</cell><cell>00.05</cell></row><row><cell></cell><cell>Largest Class</cell><cell>22.41 04.50</cell><cell>01.59</cell><cell>70.20 18.89</cell><cell>14.90</cell><cell>00.86 00.06</cell><cell>00.00</cell><cell>03.84 01.40</cell><cell>00.12</cell></row><row><cell>S1</cell><cell cols="6">2SCNN (FUSION) 42.16 29.14 TSN (RGB) 45.68 36.80 19.86 85.56 64.19 41.89 61.64 34.32 13.23 80.58 53.70 30.36 29.39 30.73</cell><cell cols="3">5.35 09.96 23.81 31.62 08.81 14.83 21.10 04.46</cell></row><row><cell></cell><cell>TSN (FLOW)</cell><cell>42.75 17.40</cell><cell>09.02</cell><cell>79.52 39.43</cell><cell>21.92</cell><cell>21.42 13.75</cell><cell>02.33</cell><cell>15.58 09.51</cell><cell>02.06</cell></row><row><cell></cell><cell>TSN (FUSION)</cell><cell cols="3">48.23 36.71 20.54 84.09 62.32</cell><cell>39.79</cell><cell cols="4">47.26 35.42 10.46 22.33 30.53 08.83</cell></row><row><cell></cell><cell>Chance/Random</cell><cell>10.71 01.89</cell><cell>00.22</cell><cell>38.98 09.31</cell><cell>03.81</cell><cell>03.56 01.08</cell><cell>00.08</cell><cell>03.56 01.08</cell><cell>00.05</cell></row><row><cell></cell><cell>Largest Class</cell><cell>22.26 04.80</cell><cell>00.10</cell><cell>63.76 19.44</cell><cell>17.17</cell><cell>00.85 00.06</cell><cell>00.00</cell><cell>03.84 01.40</cell><cell>00.12</cell></row><row><cell>S2</cell><cell cols="2">2SCNN (FUSION) 36.16 18.03 TSN (RGB) 34.89 21.82</cell><cell cols="4">07.31 10.11 74.56 45.34 25.33 19.48 14.67 71.97 38.41 19.49 18.11 15.31</cell><cell>02.86 04.77</cell><cell>10.52 12.55 11.22 17.24</cell><cell>02.69 05.67</cell></row><row><cell></cell><cell>TSN (FLOW)</cell><cell>40.08 14.51</cell><cell>06.73</cell><cell>73.40 33.77</cell><cell>18.64</cell><cell>19.98 09.48</cell><cell cols="2">02.08 13.81 08.58</cell><cell>02.27</cell></row><row><cell></cell><cell>TSN (FUSION)</cell><cell>39.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Sample baseline action recognition per-class metrics (using TSN fusion) 15 Most Frequent (in Train Set) Verb Classes put take wash open close cut mix pour move turn-on remove turn-off throw dry peel S1 RECALL 67.51 48.27 83.19 63.32 25.45 77.64 50.20 26.32 00.00 08.28 05.11 05.45 24.18 36.49 30.43 PRECISION 36.29 43.21 63.01 69.74 75.50 68.71 68.51 60.98 -46.15 53.85 66.67 75.86 81.82 51.85</figDesc><table><row><cell>S2</cell><cell>RECALL PRECISION 29.60 30.68 67.06 56.28 66.67 88.89 70.37 76.47 -74.23 34.05 83.67 43.64 18.40 33.90 35.85 13.13 00.00 00.00 00.00 00.00 00.00 2.70 00.00 -00.00 --100.0 00.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Baseline results for the action anticipation challenge 31.81 16.22 06.00 76.56 42.15 18.21 23.91 19.13 03.13 09.33 11.93 02.39 06.37 01.14 TSN (RGB) 25.30 10.41 02.39 68.32 29.50 09.63 07.63 08.79 00.80 06.06 06.74 01.07 TSN (FLOW) 25.61 08.40</figDesc><table><row><cell></cell><cell cols="2">Top-1 Accuracy</cell><cell cols="2">Top-5 Accuracy</cell><cell cols="2">Avg Class Precision</cell><cell cols="2">Avg Class Recall</cell></row><row><cell></cell><cell cols="8">VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION</cell></row><row><cell></cell><cell>2SCNN (RGB) 29.76 15.15</cell><cell>04.32</cell><cell>76.03 38.56</cell><cell>15.21</cell><cell>13.76 17.19</cell><cell>02.48</cell><cell>07.32 10.72</cell><cell>01.81</cell></row><row><cell></cell><cell>TSN (RGB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S1</cell><cell>TSN (FLOW) 29.64 10.30 TSN (FUSION) 30.66 14.86</cell><cell>02.93 04.62</cell><cell>73.70 30.09 75.32 40.11</cell><cell>10.92 16.01</cell><cell cols="2">18.34 10.70 08.84 21.85 02.25 01.41</cell><cell>06.99 05.48 06.76 09.15</cell><cell>01.00 01.55</cell></row><row><cell>S2</cell><cell>2SCNN (RGB) 25.23 09.97 TSN (FUSION) 25.37 09.76</cell><cell cols="6">02.29 68.66 27.38 05.80 01.78 09.35 16.37 06.98 00.85 67.57 24.62 08.19 10.80 04.99 01.02 06.34 04.72 01.74 68.25 27.24 09.05 13.03 05.13 00.90 05.65 05.58</cell><cell>00.84 00.79</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank all 32 subjects who participated in the dataset collection. The dataset annotation and release has been sponsored by a charitable donation from Nokia Technologies and the University of Bristol's Jean Golding Institute. Research at the University of Bristol is supported by EPSRC DTP, EPSRC GLANCE (EP/N013964/1) and EPSRC LOCATE (EP/N033779/1). Research at the University of Catania is sponsored by Piano della Ricerca 2016-2018 linea di Intervento 2 of DMI. The object detection benchmark baseline results have been helped by code from, and discussions with, Davide Acu?a.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<title level="m">YouTube-8M: A Large-Scale Video Classification Benchmark</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding social relationships in egocentric vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An adapted lesk algorithm for word sense disambiguation using wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>CICLing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Youdo, I-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leelasawassuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bargteil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beltran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Institute</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Who&apos;s better? who&apos;s best? pairwise deep ranking for skill determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Social interactions: A first-person perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02310</idno>
		<title level="m">From lifestyle vlogs to everyday interactions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Next-active-object prediction from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>JVCIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google: Google cloud speech api</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Extended</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gtea</forename><surname>Gaze+</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/speech" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fr?nd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Action completion: A temporal model for moment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heidarivincheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<publisher>Tensorflow Object Detection API</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<ptr target="https://www.ibm.com/watson/services/speech-to-text" />
		<title level="m">IBM: IBM watson speech to text</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Joint learning of object and action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>CACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Trespassing the boundaries: Labeling temporal bounds for object interactions in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Combining self-supervised learning and imitation for vision-based rope manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">ICRA</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Egocentric future localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A Dataset for Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Database for Fine Grained Activity Detection of Cooking Activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">First-person activity recognition: What are they doing to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Charades-ego: A large-scale dataset of paired third and first person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Combining Embedded Accelerometers with Computer Vision for Recognizing Food Preparation Activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mckenna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">UbiComp</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">MovieQA: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Val Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bbox-annotator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<ptr target="https://github.com/kyamagu/bbox-annotator" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuanjun</surname></persName>
		</author>
		<ptr target="https://github.com/yjxiong/tsn-pytorch" />
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2017" />
			<publisher>PyTorch Temporal Segment Network</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep imitation learning for complex manipulation tasks from virtual reality teleoperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09374</idno>
		<title level="m">SLAC: A Sparsely Labeled Dataset for Action Classification and Localization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Scene parsing through ade20k dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09788</idno>
		<title level="m">Towards automatic learning of procedures from web instructional videos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
