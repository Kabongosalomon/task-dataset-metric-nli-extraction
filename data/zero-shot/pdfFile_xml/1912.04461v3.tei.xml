<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Discriminate Information for Online Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Eun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">SK Telecom</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Jung</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hanbat National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Discriminate Information for Online Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From a streaming video, online action detection aims to identify actions in the present. For this task, previous methods use recurrent networks to model the temporal sequence of current action frames. However, these methods overlook the fact that an input image sequence includes background and irrelevant actions as well as the action of interest. For online action detection, in this paper, we propose a novel recurrent unit to explicitly discriminate the information relevant to an ongoing action from others. Our unit, named Information Discrimination Unit (IDU), decides whether to accumulate input information based on its relevance to the current action. This enables our recurrent network with IDU to learn a more discriminative representation for identifying ongoing actions. In experiments on two benchmark datasets, TVSeries and THUMOS-14, the proposed method outperforms state-of-the-art methods by a significant margin. Moreover, we demonstrate the effectiveness of our recurrent unit by conducting comprehensive ablation studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> has been widely studied in an offline setting, which allows making a decision for the detection after fully observing a long untrimmed video. This is called offline action detection. In contrast, online action detection aims to identify ongoing actions from streaming videos, at every moment in time. This task is useful for many real-world applications (e.g., autonomous driving <ref type="bibr" target="#b17">[18]</ref>, robot assistants <ref type="bibr" target="#b18">[19]</ref>, and surveillance systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>).</p><p>Recent methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref> for online action detection mostly employ recurrent neural networks (RNNs) with recurrent units (e.g., long short-term memory (LSTM) <ref type="bibr" target="#b13">[14]</ref> and gated recurrent unit (GRU) <ref type="bibr" target="#b3">[4]</ref>) for modeling the temporal sequence of an ongoing action. They introduce ad- * Work done during a Ph.D. student at KAIST. <ref type="figure">Figure 1</ref>. Comparison between GRU <ref type="bibr" target="#b3">[4]</ref> and Information Discrimination Unit (IDU) which is proposed for our online action detection system. Our IDU extends GRU with two novel components, a mechanism utilizing current information (blue lines) and an early embedding module (red dash boxes). First, reset and update modules in our IDU additionally takes the current information (i.e., x0), which enables to consider whether the past information (i.e, ht?1 and xt) are relevant to an ongoing action such as x0. Second, the early embedding module is introduced to consider the relation between high-level features for both information. ditional modules to learn a discriminative representation. However, these methods overlook the fact that the given input video contains not only the ongoing action but irrelevant actions and background. Specifically, the recurrent unit accumulates the input information without explicitly considering its relevance to the current action, and thus the learned representation would be less discriminative. Note that, in the task of detecting actions online, ignoring such a characteristic of streaming videos makes the problem more challenging <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this paper, we investigate on the question of how RNNs can learn to explicitly discriminate relevant information from irrelevant information for detecting actions in the present. To this end, we propose a novel recurrent unit that extends GRU <ref type="bibr" target="#b3">[4]</ref> with a mechanism utilizing current information and an early embedding module (see <ref type="figure">Fig. 1</ref>). We name our recurrent unit Information Discrimination Unit (IDU). Specifically, our IDU models the relation between an ongoing action and past information (i.e., x t and h t?1 ) by additionally taking current information (i.e., x 0 ) at every time step. We further introduce the early embedding module to more effectively model the relation. By adopting action classes and feature distances as supervisions, our embedding module learns the features for the current and past information describing actions in a high level. Based on IDU, our Information Discrimination Network (IDN) effectively determines whether to use input information in terms of its relevance to the current action. This enables the network to learn a more discriminative representation for detecting ongoing actions. We perform extensive experiments on two benchmark datasets, where our IDN achieves stateof-the-art performances of 86.1% mcAP and 60.3% mAP on TVSeries <ref type="bibr" target="#b7">[8]</ref> and THUMOS-14 <ref type="bibr" target="#b16">[17]</ref>, respectively. These performances significantly outperform TRN <ref type="bibr" target="#b31">[32]</ref>, the previous best performer, by 2.4% mcAP and 13.1% mAP on TVSeries and THUMOS-14, respectively.</p><p>Our contributions are summarized as follows:</p><p>? Different from previous methods, we investigate on how recurrent units can explicitly discriminate relevant information from irrelevant information for online action detection.</p><p>? We introduce a novel recurrent unit, IDU, with a mechanism using current information at every time step and an early embedding module to effectively model the relevance of input information to an ongoing action.</p><p>? We demonstrate that our IDN significantly outperforms state-of-the-arts in extensive experiments on two benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Offline Action Detection. The goal of offline action detection is to detect the start and end times of action instances from fully observed long untrimmed videos. Most methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> consist of two steps including action proposal generation and action classification. SSN <ref type="bibr" target="#b34">[35]</ref> first evaluates actionness scores for temporal locations to generate temporal intervals. Then, these intervals are classified by modeling the temporal structures and completeness of action instances. TAL-Net <ref type="bibr" target="#b2">[3]</ref> including the proposal generation and classification networks is the extended version of Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> for offline action detection. This method changes receptive field alignment, the range of receptive fields, and feature fusion to fit the action detection. Other methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> with LSTM have been also studied for per-frame prediction.</p><p>Early Action Prediction. This task is similar to online action detection but focuses on recognizing actions from the partially observed videos. Hoai and la Torre <ref type="bibr" target="#b12">[13]</ref> introduced a maximum margin framework with the extended structured SVM <ref type="bibr" target="#b28">[29]</ref> to accommodate sequential data. Cai et al. <ref type="bibr" target="#b0">[1]</ref> proposed to transfer the action knowledge learned from full actions for modeling partial actions.</p><p>Online Action Detection. Given a streaming video, online action detection aims to identify actions as soon as each video frame arrives, without observing future video frames. Geest et al. <ref type="bibr" target="#b7">[8]</ref> introduced a new large dataset, TVSeries, for online action detection. They also analyzed and compared several baseline methods on the TVseries dataset. In <ref type="bibr" target="#b8">[9]</ref>, a two-stream feedback network with LSTM is proposed to individually perform the interpretation of the features and the modeling of the temporal dependencies. Gao, Yang, and Nevatia <ref type="bibr" target="#b6">[7]</ref> proposed an encoder-decoder network with a reinforcement module, of which the reward function encourages the network to make correct decisions as early as possible. TRN <ref type="bibr" target="#b31">[32]</ref> predicts future information and utilizes the predicted future as well as the past and current information together for detecting a current action.</p><p>Aforementioned methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> for online action detection adopt RNNs to model a current action sequence. However, the RNN units such as LSTM <ref type="bibr" target="#b13">[14]</ref> and GRU <ref type="bibr" target="#b3">[4]</ref> operate without explicitly considering whether input information is relevant to the ongoing action. Therefore, the current action sequence is modeled based on both relevant and irrelevant information, which results in a less discriminative representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary: Gated Recurrent Units</head><p>We first analyze GRU <ref type="bibr" target="#b3">[4]</ref> to compare differences between the proposed IDU and GRU. GRU is one of the recurrent units, which is much simpler than LSTM. Two main components of GRU are reset and update gates.</p><p>The reset gate r t is computed based on a previous hidden state h t?1 and an input x t as follows:</p><formula xml:id="formula_0">r t = ?(W hr h t?1 + W xr x t ),<label>(1)</label></formula><p>where W hr and W xr are parameters to be trained and ? is the logistic sigmoid function. Then, the reset gate determines whether a previous hidden state h t?1 is ignored as</p><formula xml:id="formula_1">h t?1 = r t h t?1 ,<label>(2)</label></formula><p>whereh t?1 is a new previous hidden state. Similar to r t , the update gate z t is also computed based on h t?1 and x t as</p><formula xml:id="formula_2">z t = ?(W xz x t + W hz h t?1 ),<label>(3)</label></formula><p>(a) Information Discrimination Unit (IDU) (b) Information Discrimination Network (IDN) <ref type="figure">Figure 2</ref>. Illustration of our Information Discrimination Unit (IDU) and Information Discrimination Network (IDN). (a) Our IDU extends GRU with two new components, a mechanism using current information (i.e., x0) (blue lines) and an early embedding module (red boxes). The first encourages reset and update modules to model the relation between past information (i.e., ht?1 and xt) and an ongoing action.</p><p>The second enables to effectively model the relation between high-level features for the input information. (b) Given an input streaming video V = {ct} 0 t=?T consisting of sequential chunks, IDN models a current action sequence and outputs the probability distribution p0 of the current action over K action classes and background.</p><p>where W xz and W hz are learnable parameters. The update gate decides whether a hidden state h t is updated with a new hidden stateh t as follows:</p><formula xml:id="formula_3">h t = (1 ? z t ) h t?1 + z t h t ,<label>(4)</label></formula><p>whereh</p><formula xml:id="formula_4">t = ?(W xh x t + Whhh t?1 ).<label>(5)</label></formula><p>Here W xh and Whh are trainable parameters and ? is the tangent hyperbolic function. Based on reset and update gates, GRU effectively drops and accumulates information to learn a compact representation. However, there are limitations when we applied GRU to online action detection as below:</p><p>First, the past information including x t and h t?1 directly effects the decision of the reset and update gates. For online action detection, the relevant information to be accumulated is the information related to a current action. Thus, it is advantageous to make a decision based on the relation between the past information and the current action instead. To this end, we reformulate the computations of the reset and update gates by additionally taking the current information (i.e., x 0 ) as input.</p><p>This enables the reset and update gates to drop the irrelevant information and accumulate the relevant information regarding the ongoing action. Second, it is implicitly considered that the input features that the reset and update gates use represent valuable information. We augment GRU with an early embedding module with supervisions, action classes and feature distances, so that the input features explicitly describe actions. By optimizing features for the target task and dataset, our early embedding module also lets the reset and update gates focus on accumulating the relevant information along with the recurrent steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>We present the schematic view of our IDU and the framework of IDN in <ref type="figure">Fig. 2</ref>. We first describe our IDU in details and then explain on IDN for online action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Information Discrimination Units</head><p>Our IDU extends GRU with two new components, a mechanism utilizing current information (i.e., x 0 ) and an early embedding module. We explain IDU with early embedding, reset, and update modules, which takes a previous hidden state h t?1 , the features at each time x t , and the features at current time x 0 as input and outputs a hidden state h t (see <ref type="figure">Fig. 2.(a)</ref>).</p><p>Early Embedding Module. Our early embedding module individually processes the features at each time x t and the features at current time x 0 and outputs embedded features x e t and x e 0 as follows:</p><formula xml:id="formula_5">x e t = ?(W xe x t ),<label>(6)</label></formula><formula xml:id="formula_6">x e 0 = ?(W xe x 0 ),<label>(7)</label></formula><p>where W xe is a weight matrix and ? is the ReLU <ref type="bibr" target="#b20">[21]</ref> acti-vation function. Note that we share W xe for x t and x 0 . We omit a bias term for simplicity.</p><p>To encourage x e t and x e 0 to represent specific actions, we introduce two supervisions, action classes and feature distances. First, we process x e t and x e 0 to obtain probability distributions p e t and p e 0 over K action classes and background:</p><formula xml:id="formula_7">p e t = ?(W ep x e t ),<label>(8)</label></formula><formula xml:id="formula_8">p e 0 = ?(W ep x e 0 ),<label>(9)</label></formula><p>where W ep is a shared weight matrix to be learned and ? is the softmax function. We design a classification loss L e by adopting the multi-class cross-entropy loss as</p><formula xml:id="formula_9">L e = ? K k=0 y t,k log(p e t,k ) + y 0,k log(p e 0,k ) ,<label>(10)</label></formula><p>where y t,k and y 0,k are ground truth labels. Second, we use the contrastive loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> proposed to learn an embedding representation by preserving the distance between similar data points close and dissimilar data points far on the embedding space in metric learning <ref type="bibr" target="#b27">[28]</ref>. By using x e t and x e 0 as a pair, we design our contrastive loss L c as</p><formula xml:id="formula_10">L c =1{y t = y 0 }D 2 (x e t , x e 0 ) + 1{y t = y 0 }max(0, m ? D 2 (x e t , x e 0 )),<label>(11)</label></formula><p>where D 2 (a, b) is the squared Euclidean distance and m is a margin parameter. We train our embedding module with L e and L c , which provides more representative features for actions. More details on training will be provided in Section 4.2.</p><p>Reset Module. Our reset module takes the previous hidden state h t?1 and the embedded features x e 0 to compute a reset gate r t as</p><formula xml:id="formula_11">r t = ?(W hr h t?1 + W x0r x e 0 ),<label>(12)</label></formula><p>where W hr and W x0r are weight matrices which are learned. We define ? as the logistic sigmoid function same as GRU. We then obtain a new previous hidden stateh t?1 as follows:h</p><formula xml:id="formula_12">t?1 = r t h t?1 .<label>(13)</label></formula><p>Different from GRU, we compute the reset gate r t based on h t?1 and x e 0 . This enables our reset gate to effectively drop or take the past information according to its relevance to an ongoing action.</p><p>Update Module. Our update module adopts the embedded features x e t and x e 0 to compute an update gate z t as follows:</p><formula xml:id="formula_13">z t = ?(W xtz x e t + W x0z x e 0 ),<label>(14)</label></formula><p>where W xtz and W x0z are trainable parameters. Then, a hidden state h t is computed as follows:</p><formula xml:id="formula_14">h t = (1 ? z t ) h t?1 + z t h t ,<label>(15)</label></formula><p>whereh</p><formula xml:id="formula_15">t = ?(W xth x e t + Whhh t?1 ).<label>(16)</label></formula><p>Hereh t is a new hidden state and ? is the tangent hyperbolic function. W xth and Whh are trainable parameters. There are two differences between the update modules of our IDU and GRU. The first difference is that our update gate is computed based on x e t and x e 0 . This allows the update gate to consider whether x e t is relevant to an ongoing action. Second, our update gate uses the embedded features which are more representative in terms of specific actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Information Discrimination Network</head><p>In this section, we explain our recurrent network, called IDN, for online action detection (see <ref type="figure">Fig. 2.(b)</ref>).</p><p>Problem Setting. To formulate the online action detection problem, we follow the same setting as in previous methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref>. Given a streaming video V = {c t } 0 t=?T including current and T past chunks as input, our IDN outputs a probability distribution p 0 = {p 0,k } K k=0 of a current action over K action classes and background. Here we define a chunk c = {I n } N n=1 as the set of N consecutive frames. I n indicates the nth frame.</p><p>Feature Extractor. We use TSN <ref type="bibr" target="#b29">[30]</ref> as a feature extractor. TSN takes an individual chunk c t as input and outputs an appearance feature vector x a t and a motion feature vector x m t . We concatenate x a t ? R da and x m t ? R dm into a twostream feature vector x t = [x a t , x m t ] ? R dx . Here d x equals to d a + d m . After that, we sequentially feed x t and x 0 into our IDU.</p><p>Training. We feed the hidden state h 0 at current time into a fully connected layer to obtain the final probability distribution p 0 of an ongoing action as follows:</p><formula xml:id="formula_16">p e 0 = ?(W hp h 0 ),<label>(17)</label></formula><p>where W hp is a trainable matrix and ? is the softmax function.</p><p>We define a classification loss L a for a current action by employing the standard cross-entropy loss as</p><formula xml:id="formula_17">L a = ? 0 t=?T K k=0 y t,k log(p t,k ),<label>(18)</label></formula><p>where y t,k are the ground truth labels for the tth time step. We train our IDN by jointly optimizing L a , L e , and L c by designing a multi-task loss L as follows:</p><formula xml:id="formula_18">L = L a + ?(L e + L c ),<label>(19)</label></formula><p>where ? is a balance parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the proposed method on two benchmark datasets, TVSeries <ref type="bibr" target="#b7">[8]</ref> and THUMOS-14 <ref type="bibr" target="#b16">[17]</ref>. We first demonstrate the effectiveness of our IDU by conducting comprehensive ablation studies. We then report comparison results among our IDN and the state-of-the-art methods for online action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>TVSeries <ref type="bibr" target="#b7">[8]</ref>. This dataset includes 27 untrimmed videos on six popular TV series, divided into 13, 7, and 7 videos for training, validation, and test, respectively. Each video contains a single episode, approximately 20 minutes or 40 minutes long. The dataset is temporally annotated with 30 realistic actions (e.g., open door, read, eat, etc). The TVSeries dataset is challenging due to diverse undefined actions, multiple actors, heavy occlusions, and a large proportion of non-action frames.</p><p>THUMOS-14 <ref type="bibr" target="#b16">[17]</ref>. The THUMOS-14 dataset consists of 200 and 213 untrimmed videos for validation and test sets, respectively. This dataset has temporal annotations with 20 sports actions (e.g., diving, shot put, billiards, etc). Each video includes 15.8 action instances and 71% background on average. As done in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref>, we used the validation set for training and the test set for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metrics</head><p>For evaluating performance in online action detection, existing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> measure mean average precision (mAP) and mean calibrated average precision (mcAP) <ref type="bibr" target="#b7">[8]</ref> in a frame-level. Both metrics are computed in two steps: 1) calculating the average precision over all frames for each action class and 2) averaging the average precision values over all action classes.</p><p>mean Average Precision (mAP). On each action class, all frames are first sorted in descending order of their probabilities. The average precision of the kth class over all frames is then calculated based on the precision at cut-off i (i.e., on the i sorted frames). The final mAP is defined as the mean of the AP values over all action classes.</p><p>mean calibrated Average Precision (mcAP). It is difficult to compare two different classes in terms of the AP values when the ratios of positive frames versus negative frames for these classes are different. To address this problem, Geest et al. <ref type="bibr" target="#b7">[8]</ref> propose the calibrated precision as</p><formula xml:id="formula_19">cPrec(i) = wTP(i) wTP(i) + FP(i) ,<label>(20)</label></formula><p>where w is a ratio between negative frames and positive frames. Similar to the AP, the calibrated average precision of the kth class over all frames is computed as Then, the mcAP is obtained by averaging the cAP values over all action classes.</p><formula xml:id="formula_20">cAP k = i cPrec(i)1(i) N P .<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>Problem Setting. We use the same setting as used in state-of-the-art methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref>. On both TVSeries <ref type="bibr" target="#b7">[8]</ref> and THUMOS-14 <ref type="bibr" target="#b16">[17]</ref> datasets, we extract video frames at 24 fps and set the number of frames in each chunk N to 6. We use 16 chunks (i.e., T = 15), which are 4 seconds long, for the input of IDN.</p><p>Feature Extractor. We use a two-stream network as a features extractor. In the two-stream network, one stream encodes appearance information by taking the center frame of a chunk as input, while another stream encodes motion information by processing an optical flow stack computed from an input chunk. Among several two-stream networks, we employ the TSN model <ref type="bibr" target="#b29">[30]</ref> pretrained on the ActivityNet-v1.3 dataset <ref type="bibr" target="#b11">[12]</ref>. Note that this TSN is the same feature extractor as used in state-of-the-art methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref>. The TSN model consists of ResNet-200 <ref type="bibr" target="#b10">[11]</ref> for an appearance network and BN-Inception <ref type="bibr" target="#b14">[15]</ref> for a motion network. We use the outputs of the Flatten 673 layer in ResNet-200 and the global pool layer in BN-Inception as the appearance features x a t and motion features x m t , respectively. The dimensions of x a t and x m t are d a = 2048 and d m = 1024, respectively, and d x equals to 3072.</p><p>IDN Architecture. IDN Training. To train our IDN, we use a stochastic gradient descent optimizer with the learning rate of 0.01 for both THUMOS-14 and TVSeries datasets. We set batch size to 128 and balance the numbers of action and back-ground samples in terms of the class of c 0 . We empirically set the margin parameter m in Eq. (11) to 1.0 and the balance parameter ? in Eq. <ref type="bibr" target="#b18">(19)</ref> to 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>We evaluate RNNs with the simple unit, LSTM <ref type="bibr" target="#b13">[14]</ref>, and GRU <ref type="bibr" target="#b3">[4]</ref>. We name these networks RNN-Simple, RNN-LSTM, and RNN-GRU, respectively. Although many methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> report the performances of these networks as baselines, we evaluate them in our setting to clearly confirm the effectiveness of our IDU.</p><p>In addition, we individually add IDU components to GRU as a baseline for analyzing their effectiveness: Baseline+CI: We add a mechanism using current information to GRU in computing reset and update gates. Specifically, we replace Eq. (1) for r t with</p><formula xml:id="formula_21">r t = ?(W hr h t?1 + W x0r x 0 )<label>(22)</label></formula><p>and Eq. (3) for z t with</p><formula xml:id="formula_22">z t = ?(W xtz x t + W x0z x 0 ),<label>(23)</label></formula><p>where W hr , W x0r , W xtz , and W x0z are trainable parameters. We construct a recurrent network with this modified unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline+CI+EE (IDN):</head><p>We incorporate our main components, a mechanism utilizing current information and an early embedding module, into GRU, which is our IDU. These components enable reset and update gates to effectively model the relation between an ongoing action and input information at every time step. Specifically, Eq. (12) and Eq. (14) are substituted for Eq. (1) and Eq. (3), respectively. We design a recurrent network with our IDU, which is the proposed IDN.</p><p>In <ref type="table">Table 2</ref>, we report the performances of five networks on the TVSeries dataset <ref type="bibr" target="#b7">[8]</ref>. Among RNN-Simple, RNN-LSTM, and RNN-GRU, RNN-GRU results in the highest mcAP of 81.3%. By comparing RNN-GRU (Baseline) with Baseline+CI, we first analyze the effect of using x 0 in calculating reset and update gates. This component enables the gates to decide whether input information at each time is relevant to a current action. As a result, Baseline-CI achieves the performance gain of 2.1% mcAP, which demonstrates the effectiveness of using x 0 . Next, we observe that adding the early embedding module improves the performance by 1.3% mcAP from the comparison between Baseline+CI and Baseline+CI+EE (IDN). Note that our IDN achieves mcAP of 84.7% with a performance gain of 3.4% mcAP compared with Baseline.</p><p>We conduct the same experiment on the THUMOS-14 dataset <ref type="bibr" target="#b16">[17]</ref> to confirm the generality of the proposed components. We obtain performance gains as individually incorporating the proposed components into GRU (see <ref type="table">Table   Method</ref> mcAP <ref type="formula">(</ref>  <ref type="table">Table 3</ref>. Ablation study of the effectiveness of our proposed components on THUMOS-14 <ref type="bibr" target="#b16">[17]</ref>. CI and EE indicate additionally using the current information and early embedding input information, respectively. 3), where our IDN achieves an improvement of 3.3% mAP compared to Baseline. These results successfully demonstrate the effectiveness and generality of our components. <ref type="figure" target="#fig_0">Figure 3</ref> shows qualitative comparisons on predicted and GT probabilities, where our IDN achieves the best results on both action and background frames.</p><p>To confirm the effect of our components, we compare the values of the update gates z t between our IDU and GRU. For a reference, we introduce the relevance score R t of each chunk regarding a current action. Specifically, we set the scores of input chunks representing the current action as 1, otherwise 0 (see <ref type="figure" target="#fig_1">Fig. 4</ref>). Note that the update gate controls how much information from the input will carry over to the hidden state. Therefore, the update gate should drop the irrelevant information and pass over the relevant information related to the current action. In <ref type="figure" target="#fig_3">Fig. 5</ref>, we plot the z t values of IDU and GRU and relevance scores against each time step. On the input sequences containing from one to   five relevant chunks, the z t values of GRU are very high at all time steps. In contrast, our IDU successfully learns the z t values following the relevance scores (see <ref type="figure" target="#fig_3">Fig. 5(a)</ref>). We also plot the average z t values on the input sequences including from 11 to 15 relevant chunks in <ref type="figure" target="#fig_3">Fig. 5(b)</ref>, where our IDU yields the z t values similar to the relevance scores. These results demonstrate that our IDU effectively models the relevance of input information to the ongoing action.</p><p>Compared to GRU, IDU has additional weights W xe ? R dx?512 and W ep ? R 512?(K+1) in the early embedding module. Our early embedding module reduces the dimensions of x t , x 0 ? R dx?512 , which makes the parameters (i.e., W x0r , W xtz ? R 512?512 ) in IDU less than the parameters (i.e., W xr , W xz ? R dx?512 ) in GRU. The other weights have the same number of parameters in IDU and GRU. As a result, the number of parameters for IDU is 75.3% of that for GRU with d x = 3072 and K = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Performance Comparison</head><p>In this section, we compare our IDN with state-of-the-art methods on TVSeries <ref type="bibr" target="#b7">[8]</ref> and THUMOS-14 <ref type="bibr" target="#b16">[17]</ref> datasets. We use three types of input including RGB, Flow, and Two-Stream. As the input of our IDU, we take only appearance features for the RGB input and motion features for the Flow input. IDN, TRN <ref type="bibr" target="#b31">[32]</ref>, RED <ref type="bibr" target="#b6">[7]</ref>, and ED <ref type="bibr" target="#b6">[7]</ref> use same twostream features for the Two-Stream input, which allows a fair comparison. We also employ another feature extractor,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Method mcAP (%) RGB LRCN <ref type="bibr" target="#b5">[6]</ref> 64.1 RED <ref type="bibr" target="#b6">[7]</ref> 71.2 2S-FN <ref type="bibr" target="#b8">[9]</ref> 72.4 TRN <ref type="bibr" target="#b31">[32]</ref> 75  <ref type="table">Table 5</ref>. Performance comparison on THUMOS-14 <ref type="bibr" target="#b16">[17]</ref>. IDN, TRN <ref type="bibr" target="#b31">[32]</ref>, RED <ref type="bibr" target="#b6">[7]</ref>, and ED <ref type="bibr" target="#b6">[7]</ref> use same two-stream features.</p><p>the TSN model <ref type="bibr" target="#b29">[30]</ref> pretrained on the Kinetics dataset <ref type="bibr" target="#b1">[2]</ref>. We name our IDN with this feature extractor IDN-Kinetics. We report the results on TVSeries in <ref type="table">Table 4</ref>. Our IDN significantly outperforms state-of-the-art methods on all types of input, where IDN achieves 76.6% mcAP on the RGB input, 80.3% mcAP on the Flow input, and 84.1% mcAP on the Two-Stream input. Furthermore, IDN-Kinetics achieves the best performance of 86.1% mcAP. Note that IDN effectively reduces wrong detection results occurred from the irrelevant information by discriminating the relevant information. However, 2S-FN, RED, and TRN accumulates the input information without considering its relevance to an ongoing action. In addition, our IDN yields better performance than TRN <ref type="bibr" target="#b31">[32]</ref> although IDN takes shorter temporal information than IDN (i.e., 16 chunks vs. 64 chunks).</p><p>In <ref type="table">Table 5</ref>, we compare performances between our IDN and state-of-the-art approaches for online and offline action detection. The compared offline action detection methods perform frame-level prediction. As a result, both IDN and IDN-Kinetics outperforms all methods by a large margin.</p><p>In online action detection, it is important to identify actions as early as possible. To compare this ability, we mea-  <ref type="table">Table 6</ref>. Performance comparison for different portions of actions on TVSeries <ref type="bibr" target="#b7">[8]</ref> in terms of mcAP (%). The corresponding portions of actions are only used to compute mcAP after detecting current actions on all frames in an online manner. sure the mcAP values for every 10% portion of actions on TVSeries. <ref type="table">Table 6</ref> shows the comparison results among IDN, IDN-Kinetics, and previous methods, where our methods achieve state-of-the-art performance at every time interval. This demonstrates the superiority of our IDU in identifying actions at early stages as well as all stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Portion of action</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative Evaluation</head><p>For qualitative evaluation, we visualize our results on TVSeries <ref type="bibr" target="#b7">[8]</ref> and THUMOS-14 <ref type="bibr" target="#b16">[17]</ref> in <ref type="figure" target="#fig_4">Fig. 6</ref>. The results on the TVSeries dataset show high probabilities on the true action label and reliable start and end time points. Note that identifying actions at the early stage is very challenging in this scene because the only subtle change (i.e., opening a book) happens. On THUMOS-14, our IDN successfully identifies ongoing actions by yielding the contrasting probabilities between true action and background labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed IDU that extends GRU <ref type="bibr" target="#b3">[4]</ref> with two novel components: 1) a mechanism using current information and 2) an early embedding module. These components enable IDU to effectively decide whether input information is relevant to a current action at every time step. Based on IDU, our IDN effectively learns to discriminate relevant information from irrelevant information for identifying ongoing actions. In comprehensive ablation studies, we demonstrated the generality and effectiveness of our proposed components. Moreover, we confirmed that our IDN significantly outperforms state-of-the-art methods on TVSeries <ref type="bibr" target="#b7">[8]</ref> and THUMOS-14 <ref type="bibr" target="#b16">[17]</ref> datasets for online action detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparisons on predicted and GT probabilities for action (top) and background (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Example of relevance scores Rt of input chunks.(a) On the input sequences containing from one to five relevant chunks (i.e., from t = 0 to t = ?4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) On the input sequences containing from 11 to 15 relevant chunks (i.e., from t = ?10 to t = ?14).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparison between the update gate zt values of our IDU and GRU<ref type="bibr" target="#b3">[4]</ref>. Update gate values are measured on the input sequences containing (a) from one to five relevant chunks and (b) from 11 to 15 relevant chunks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative evaluation of IDN on TVSeries [8] (top) and THUMOS-14 [17] (bottom). Each result shows frames, ground truth, and estimated probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Specifications of our IDN. dx is the dimension of the twostream feature vector xt and K + 1 is the number of action and background classes.</figDesc><table><row><cell>Module</cell><cell>Type</cell><cell>Weight</cell><cell>Size</cell></row><row><cell>Early Embedding</cell><cell>FC</cell><cell>W xe</cell><cell>d x ? 512</cell></row><row><cell>Module</cell><cell>FC</cell><cell>W ep</cell><cell>512?(K +1)</cell></row><row><cell>Reset</cell><cell>FC</cell><cell>W hr</cell><cell>512 ? 512</cell></row><row><cell>Module</cell><cell>FC</cell><cell>W xar</cell><cell>512 ? 512</cell></row><row><cell></cell><cell>FC</cell><cell>W xtz</cell><cell>512 ? 512</cell></row><row><cell>Update</cell><cell>FC</cell><cell>W x0z</cell><cell>512 ? 512</cell></row><row><cell>Module</cell><cell>FC</cell><cell>W xth</cell><cell>512 ? 512</cell></row><row><cell></cell><cell>FC</cell><cell>Whh</cell><cell>512 ? 512</cell></row><row><cell>Classification</cell><cell>FC</cell><cell>W hp</cell><cell>512?(K +1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>provides the specifications of IDN considered in our experiments. In the early embedding module, we set the number of the hidden units for W xe to 512. In the reset module, both weights W hr and W xar have 512 hidden units. In the update module, we use 512 hidden units for W xtz , W x0z , W xth , and Whh. According to the number of action classes, we set K + 1 to 31 for TVSeries and 21 for THUMOS-14.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action knowledge transfer for action prediction with partial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Association for the Advancement of Artificial Intelligence (AAAI) Conference on Artificial Intelligence</title>
		<meeting>Association for the Advancement of Artificial Intelligence (AAAI) Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="8118" to="8125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vaids, action recognition? a new model and the kinectics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Red: Reinforced encoderdecoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="92" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="269" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling temporal structure with lstm for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="page" from="1549" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Max-margin early event detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing humans in motion: Trajectory-based aerial video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Padgett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grounding human-to-vehicle advice for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="10591" to="10599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anticipating human activities for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2013-11" />
			<biblScope unit="page" from="2071" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multigranularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted obltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girchick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Conference on Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint inference of groups, events and human roles in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="4756" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Conference on Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class npair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Conference on Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crandall. Temporal recurrent networks for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="5532" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferable self-attentive representations for action recognition in untrimmed videos with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Association for the Advancement of Artificial Intelligence (AAAI) Conference on Artificial Intelligence</title>
		<meeting>Association for the Advancement of Artificial Intelligence (AAAI) Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="9227" to="9242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
