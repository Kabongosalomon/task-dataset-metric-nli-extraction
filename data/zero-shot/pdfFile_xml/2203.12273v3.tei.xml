<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Coquenet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Chatelain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Paquet</surname></persName>
						</author>
						<title level="a" type="main">DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Seq2Seq model</term>
					<term>Segmentation-free</term>
					<term>Handwritten Text Recognition</term>
					<term>Transformer</term>
					<term>Layout Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unconstrained handwritten text recognition is a challenging computer vision task. It is traditionally handled by a two-step approach, combining line segmentation followed by text line recognition. For the first time, we propose an end-to-end segmentation-free architecture for the task of handwritten document recognition: the Document Attention Network. In addition to text recognition, the model is trained to label text parts using begin and end tags in an XML-like fashion. This model is made up of an FCN encoder for feature extraction and a stack of transformer decoder layers for a recurrent token-by-token prediction process. It takes whole text documents as input and sequentially outputs characters, as well as logical layout tokens. Contrary to the existing segmentation-based approaches, the model is trained without using any segmentation label. We achieve competitive results on the READ 2016 dataset at page level, as well as double-page level with a CER of 3.43% and 3.70%, respectively. We also provide results for the RIMES 2009 dataset at page level, reaching 4.54% of CER. We provide all source code and pre-trained model weights at https://github.com/FactoDeepLearning/DAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A handwritten document is a complex structure composed of handwritten text blocks structured according to a specific layout. Handwritten Document Recognition (HDR) can be defined as the joint recognition of both text and layout. To our knowledge, this work is the first attempt to build an end-to-end approach for HDR. Until now, the previous works have only focused on Handwritten Text Recognition (HTR) of isolated text blocks, or Document Layout Analysis (DLA), considering them as two independent tasks.</p><p>Offline HTR consists in recognizing the text from a digitized document. HTR at document level has always been relying on a prior segmentation stage to extract the text blocks from the input image (at character, word, then line or even paragraph level), before recognizing the text itself. Such two-step approach (segmentation + recognition) has several drawbacks. It relies on segmented entities, which do not have a clear definition from the image point of view. For instance, text lines can be defined by X-heights, baselines, bounding boxes, or polygons. This approach also requires segmentation annotations, which are very costly to produce. The resulting predictions accumulate errors from both stages. Another point is about the reading order. Since the segmentation stage is generally a one-shot process, there is no notion of ordered sequence between the different text regions, which prevents learning the reading order.</p><p>In the past, the emergence of end-to-end models enabled to alleviate the need for segmentation labels. The recently proposed approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr">[4]</ref> show that it is possible to recognize text from paragraph images without any explicit text line segmentation step. However, these models are limited to the recognition of single columns of text. It means that they can only be used for simple layouts or used as a second step, after a prior paragraph segmentation step. However, real-world documents have more complex layouts most of the time, including multiple columns of text or annotations in the margin. In this paper, we propose to process whole documents, with such complex layouts, recognizing both text and layout. The proposed approach consists of a unified statistical model which only needs very few annotations for training, without needing any segmentation (or physical) label.</p><p>One example of such a complex (real) document is depicted in <ref type="figure">Figure 1a</ref>. While the textual content can be represented as a sequence of characters, the layout is represented as an oriented graph, in order to model the hierarchy and the reading order of the different layout entities. In <ref type="figure">figure 1a</ref>, this layout graph is projected on the document image: nodes are layout entities and edges model the relations between them. A membership relation is represented by a dashed arrow, while solid arrows represent the reading order. In this example, the document is made up of two pages, each page containing a page number and a sequence of sections, each section being made up of zero, one or many marginal annotations and a single body.</p><p>Text and layout are intrinsically linked: text recognition may help to label a layout entity, and vice versa. Therefore, we have turned toward the joint recognition of text and layout in a unique model. As shown on the right side of <ref type="figure">Figure 1a</ref>, we chose the XML paradigm to generate a serialized representation of the document that is further used as the ground truth of this document. Notice that no physical information is encoded in the ground truth. Contrary to the standard document image analysis tasks, the proposed approach does not rely on segmentation labels of text regions such as bounding boxes, for instance. Instead, the proposed model is able to provide transcriptions enriched with logical layout information, leading to structured transcriptions. It notably enables to reduce the need for costly segmentation annotations.</p><p>The proposed model recurrently predicts the character and layout tokens through a character-level attention mechanism. It enables to deal with slanted lines, as shown in <ref type="figure">Figure 1b</ref>. Processing whole documents instead of isolated paragraphs raises new challenges:</p><p>? Paragraph-level models usually benefit from the fact that all horizontal elements at the same vertical position belong to the same text line. This assumption is not always valid at document-level.</p><p>? The input images are larger and the associated target sequences are longer for documents than for paragraphs, leading to more complex training procedures, especially in the case of attention-based models, which involve a growing need for GPU memory.</p><p>? While paragraphs are read in a monotonous order (characters are read from left to right and lines from top to bottom in case of common occidental languages for example), documents reading order is layout dependent i.e. paragraphs of a single-column document are read from top to bottom only, whereas a multi-column document is commonly read column by column, adding a horizontal constraint on the reading order.</p><p>The transformer architecture <ref type="bibr" target="#b4">[5]</ref> was first introduced in the context of machine translation. Since then, transformerbased models have proven their robustness through a wide variety of computer vision tasks, such as images of mathematical expression recognition <ref type="bibr" target="#b5">[6]</ref> or image classification <ref type="bibr" target="#b6">[7]</ref>. They have also shown promising results for single text line recognition <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and scene text recognition <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this work, we propose a Document Attention Network (DAN) based on an FCN encoder and a transformer decoder for handwritten document recognition.</p><p>In brief, we make the following contributions:</p><p>? We propose the Document Attention Network: the first end-to-end architecture able to recognize text at document level while labeling logical layout information, in a single decoding process.</p><p>? The model is trained in an end-to-end fashion without the need for any segmentation labels. <ref type="bibr">?</ref> We provide the first results on the RIMES 2009 dataset at page and paragraph levels, as well as on the READ 2016 dataset at page and double-page levels. These READ 2016 results are competitive compared to stateof-the-art paragraph-level and line-level approaches, while the proposed approach does not need a prior segmentation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The proposed network achieves state-of-the-art results on the RIMES 2011 dataset at paragraph level and on the READ 2016 dataset at line and paragraph levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose two new metrics to evaluate the quality of the recognized document layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We provide all source code, including metrics and synthetic document generation process, as well as the trained model weights.</p><p>This paper is organized as follows. Section 2 is dedicated to the related works. The architecture and the training strategy are presented in Section 3. We present the proposed metrics in Section 4. Experiments are detailed in Section 5, and we provide a discussion in Section 6. We draw a general conclusion in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Deep learning models are becoming more and more powerful and can now process entire documents. As we aim at recognizing both the text and the logical layout information of a handwritten document image, the task falls into both the handwriting recognition field and the layout analysis field, thus sharing links with document understanding. Therefore, this section is first dedicated to document understanding in a general way and then focuses on layout analysis and handwriting recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document understanding</head><p>Document understanding includes a set of tasks whose purpose is to extract, classify, interpret, contextualize, and search information from documents. It implies, among others, document layout analysis and Optical Character Recognition (OCR). But it also consists in understanding complex structures such as tables, schemes, or images. This is a developing field and here are some related works. The authors of <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> proposed Chargrid, a way to represent textual documents as a 2D representation of one-hot encoded characters, which are produced by an OCR. The idea is to keep the spatial information between the textual entities. Chargrid is then used as the input for a Key Information Extraction (KIE) task, implying three sub-tasks: bounding box regression, semantic segmentation and box masking. In <ref type="bibr" target="#b13">[13]</ref>, the Chargrid paradigm is also used, but the character's encoding is superseded by word embeddings through the use of the BERT model <ref type="bibr" target="#b14">[14]</ref>. In <ref type="bibr" target="#b15">[15]</ref>, the authors tackle the task of Visually-rich Document Understanding (VDU). They used a transformer architecture applied to multiple modalities: OCR text and bounding boxes, and visual embedding. The authors of <ref type="bibr" target="#b16">[16]</ref> proposed a transformer-based model able to tackle multiple tasks such as document classification, KIE and Visual Question-Answering (VQA). All these works imply the use of large datasets, mainly synthetic: they need a lot of information either as input or as ground truth annotation (notably for bounding boxes). One can notice that document understanding is still in its early stages. It mainly focuses on 2D document representation and information extraction. Our work differs from document understanding in that we propose to extract all the text from a document; we do not aim at retrieving only specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document layout analysis</head><p>Document Layout Analysis (DLA) aims at identifying and categorizing the regions of interest in a document image. In <ref type="bibr" target="#b17">[17]</ref>, the authors proposed a method based on connected components to recognize 8 types of objects on heterogeneous documents: text, photographic image, hand-drawn line area, graph, table, edge line, separator and material damage. The authors of <ref type="bibr" target="#b18">[18]</ref> proposed a 2-stage method based on an artificial neural network for the task of semantic segmentation on historical handwritten documents. It detects multiple zone types such as page numbers, marginal notes and main paragraphs.</p><p>Currently, there are mainly two approaches to handle DLA: pixel-by-pixel classification and bounding box prediction.</p><p>Fully Convolutional Networks (FCN) are the most popular approach for pixel-level DLA ( <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>). It consists of an elegant and relatively light end-to-end model that does not require re-scaling the input images. The authors of <ref type="bibr" target="#b21">[21]</ref> applied DLA on printed textual documents: contemporary magazines and academic papers. They trained their model to recognize multiple classes namely figures, tables, section headings, captions, lists and paragraphs. The model presented in <ref type="bibr" target="#b20">[20]</ref> aims at detecting different items from historical documents: text regions, decorations, comments and background. In <ref type="bibr" target="#b19">[19]</ref>, the model is applied to historical newspapers. It recognizes many textual elements such as titles, text blocks and advertisements, as well as images. In <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, the authors focused on text line segmentation, as a first step before text line recognition, also using FCN.</p><p>Object-detection approaches for word bounding box predictions have been studied in <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>. They follow the standard object-detection paradigm based on a region proposal network and a non-maximum suppression algorithm <ref type="bibr" target="#b28">[28]</ref>. In <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, the authors focused on start-of-line prediction to extract normalized lines.</p><p>DLA focuses on identifying physical regions of interest, whether they are textual or not. It is driven by physical ground truth annotations accounting for semantic labels that are associated to document logical elements. In this paper, we focus on the textual components only, for which we target their recognition and semantic labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Handwritten text recognition</head><p>Most of the works on handwriting recognition deal with images of isolated lines or words. It implies the use of a prior line segmentation step, carried out by a DLA stage.</p><p>For the text line images recognition task itself, many architectures have been proposed: Multi-Dimensional Long Short Term Memory (MD-LSTM) <ref type="bibr" target="#b31">[31]</ref>, combination of Convolutional Neural Network (CNN) and LSTM <ref type="bibr" target="#b32">[32]</ref>, CNN <ref type="bibr" target="#b33">[33]</ref> and FCN <ref type="bibr" target="#b34">[34]</ref>. During training, they all rely on the CTC <ref type="bibr" target="#b35">[35]</ref> loss to handle the sequence alignment issue induced by the variability of the input image widths and the target sequence lengths. In <ref type="bibr" target="#b36">[36]</ref>, the CTC loss is superseded by the crossentropy loss, and the sequence alignment issue is handled by an attention-based encoder-decoder architecture. A special end-of-line token is introduced to stop the recurrent process. More recently, transformer-based architectures following the same principle have been proposed in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b37">[37]</ref>.</p><p>All these line-level architectures inherently require linelevel segmentation annotations, which are very costly to produce. To alleviate this issue, some architectures handling single-column pages or paragraphs have been proposed recently. In <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, the authors reformulate the twodimensional problem as a one-dimensional problem in order to use the CTC loss, which was designed for one-dimensional sequence alignment problems only. Indeed, this loss enables one-shot predictions, contrary to the cross entropy loss, which implies a recurrent process. This way, prediction times are shorter. While the model from <ref type="bibr" target="#b1">[2]</ref> learns a representation transformation, the model from <ref type="bibr" target="#b2">[3]</ref> learns to align the predictions on the vertical axis, thanks to a reshaping operation. <ref type="bibr" target="#b0">[1]</ref> and [4] proposed attention-based models. They are based on a recurrent implicit line-segmentation process. Compared to one-shot prediction approaches, the recurrent process implies longer prediction times, but it enables to model long term dependencies. The authors of <ref type="bibr" target="#b0">[1]</ref> used an MD-LSTM-based architecture. The model iterates a fixed number of times and all the probability lattices are concatenated and aligned at paragraph-level with the CTC loss. In [4], the model is an FCN+LSTM network that also learns to detect the end of the paragraph; alignment is performed line by line with the CTC loss.</p><p>In <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref> and <ref type="bibr" target="#b40">[40]</ref>, the authors proposed different models which are based on an attention mechanism to predict the text character by character, trained with the crossentropy loss and using a special end-of-transcription token. While the authors of <ref type="bibr" target="#b38">[38]</ref> proposed an MD-LSTM model, a transformer-based model is used in <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. The models from <ref type="bibr" target="#b39">[39]</ref> and <ref type="bibr" target="#b40">[40]</ref> are trained to recognize, in addition to the text, the presence of non-textual areas (such as tables or drawings) or specific named entities (name, location for example), respectively. These three works rely on curriculum learning using line or word segmentation annotations. Except for <ref type="bibr" target="#b39">[39]</ref>, which uses a page-level private dataset, <ref type="bibr" target="#b38">[38]</ref> and <ref type="bibr" target="#b40">[40]</ref> only evaluate their approach on paragraph images.</p><p>As for the purely paragraph-level approaches, these attention models can be seen as a first step toward the integration of layout recognition through the text recognition process. As a matter of fact, the prediction of line break tokens implies the recognition of text line items. We propose to generalize this layout recognition to whole documents, integrating section and paragraph recognition, for instance.</p><p>In this work, we propose the DAN, an end-to-end transformer-based model for whole handwritten document recognition, including the textual components and the logical layout information. This model is trained without using any segmentation label, and we evaluate it on two public datasets at page and double-page levels. To our knowledge, this is the first attempt that provides experimental results on such a task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ARCHITECTURE AND TRAINING</head><p>The problem can be formalized as follows: the input is a raw document image X and the expected output is a sequence y of tokens, of length L y . Tokens are grouped in the same dictionary D = A ? S ? {&lt; eot &gt;}, where A are tokens of characters from a given alphabet, S are specific layout tokens and &lt;eot&gt; is a special end-of-transcription token. Layout tokens are pairs of tokens (begin and end) that tag sequences of character tokens, as shown in <ref type="figure">Figure 1a</ref>. As for characters, they vary according to the dataset used. In the following, we present the architecture and the training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>We propose the Document Attention Network (DAN), an endto-end encoder-decoder architecture that jointly recognizes both text and layout, from whole documents. We opted for an FCN as encoder since they are known to be efficient for feature extraction from images and can deal with inputs of variable sizes. For the decoder, we chose the transformer <ref type="bibr" target="#b4">[5]</ref> because it is currently the state-of-the-art approach for tasks involving the prediction of sequences of variable lengths.</p><p>The DAN architecture is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. It is made up of an FCN encoder, to extracts 2D feature maps f 2D from an input document image X. 2D positional encoding is added to these features in order to keep the spatial information, before being flattened into a 1D sequence of features f 1D . This representation is computed only once and serves as input to the transformer decoder. The decoder follows a recurrent process at character-level: given the previously predicted tokens (? 0 ,? 1 , . . . ,? t?1 ) and based on the computed features f 1D , it outputs the next token probabilities p t for each token of D. The final predicted token? t is the one with the highest probability. The decoding process starts with an initial &lt;sot&gt; (start-of-transcription) token (? 0 = &lt; sot &gt;), and ends with the prediction of a special &lt;eot&gt; (end-of-transcription) token (? Ly+1 = &lt; eot &gt;). We used the cross-entropy loss (L CE ) for training.</p><p>In the following sections, the encoder and the decoder are described in more detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Encoder</head><p>As in <ref type="bibr" target="#b39">[39]</ref>, we opted for an FCN encoder in order to better model the local dependencies since inputs are images. We used the FCN encoder of the Vertical Attention Network [4] for many reasons. It achieves state-of-the-art results for HTR at paragraph level on many public datasets: RIMES 2011 <ref type="bibr" target="#b41">[41]</ref>, IAM <ref type="bibr" target="#b42">[42]</ref> and READ 2016 <ref type="bibr" target="#b43">[43]</ref>. It can handle inputs of variable sizes. And it implies few parameters (1.7M) compared to other approaches. The encoder takes as input a document image X ? R H?W ?C , with H, W and C being respectively the height, the width and the number of channels (C = 3 for an RGB image). It extracts some features maps for the whole document image: The original transformer architecture <ref type="bibr" target="#b4">[5]</ref> is defined for 1D sequences. Since the inputs are 2D images, we replaced the 1D positional encoding by 2D positional encoding, as proposed in <ref type="bibr" target="#b39">[39]</ref>. 2D positional encoding is defined as a fixed encoding based on sine and cosine functions with different frequencies (as in <ref type="bibr" target="#b4">[5]</ref>); but instead of encoding a 1D position using all the channels, half is dedicated to vertical positional encoding and the other half to the horizontal positional encoding, as depicted in Equations 1 and 2:</p><formula xml:id="formula_0">f 2D ? R H f ?W f ?C f with H f = H 32 , W f = W 8 and C f = 256.</formula><formula xml:id="formula_1">PE 2D (x, y, 2k) = sin(w k ? y), PE 2D (x, y, 2k + 1) = cos(w k ? y), PE 2D (x, y, d model /2 + 2k) = sin(w k ? x), PE 2D (x, y, d model /2 + 2k + 1) = cos(w k ? x), ?k ? [0, d model /4] ,<label>(1)</label></formula><formula xml:id="formula_2">with w k = 1/10000 2k/d model .<label>(2)</label></formula><p>We set d model = C f = 256.</p><p>Features f 2D are summed with 2D positional encoding before being flattened for transformer decoder requirements following Equations 3 and 4:</p><formula xml:id="formula_3">f 1Dj = flatten(f 2Dx,y + PE 2D (x, y)),<label>(3)</label></formula><p>with</p><formula xml:id="formula_4">j = yW f + x. (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Decoder</head><p>The decoder follows a recurrent process. At each iteration, it takes as input the flattened visual features f 1D and the previously predicted tokens (? 0 , ...,? t?1 ), and outputs the probabilities p t for each token in the dictionary D at time step t. We used learned embeddings to convert the tokens to vectors e? i of dimension d model . Embeddings are summed with 1D positional encoding corresponding to the position of the predicted tokens in the predicted sequence, as in <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_5">q t,i = PE 1D (i) + e? i ,<label>(5)</label></formula><p>with:</p><formula xml:id="formula_6">PE 1D (x, 2k) = sin(w k ? x) PE 1D (x, 2k + 1) = cos(w k ? x) ?k ? [0, d model /2] .<label>(6)</label></formula><p>The decoder is made up of a stack of 8 transformer decoder layers (as shown in <ref type="figure" target="#fig_0">Figure 2</ref>) followed by a convolutional layer with kernel 1 ? 1 that computes the next token probabilities p t . The transformer decoder layers are based on multi-head attention <ref type="bibr" target="#b4">[5]</ref> mechanisms we denote as self-attention and mutual attention. Self-attention aims at modeling dependencies among the predicted sequence: it corresponds to multi-head attention where queries Q, keys K and values V are from the same input. Mutual attention is used to extract visual information from the encoder (K and V are from f 1D ), based on Q which comes from the previous predictions. In other words, given the previous predictions, it indicates where the model should look to predict the next token.</p><p>We used 8 decoder layers with dimension d model , feed forward dimension d model , 4 attention heads, ReLU activation and 10% of dropout. Self attention is causal since it is based on the previous predictions. As in <ref type="bibr" target="#b39">[39]</ref>, we used an attention window of length 100 for the self attention in order to reduce the computation time. It means that given an input sequence s of length L s , the t th output frame o t is computed over the range [s a , s t?1 ] with a = max(0, t ? 100). The process starts with a &lt;sos&gt; token and ends when a &lt;eos&gt; token is predicted or after L max iterations. We set L max = 3000 to match the datasets needs.</p><p>The whole model is made up of 7.6 M trainable parameters and is trained in an end-to-end fashion using the crossentropy loss over the sequence of tokens:</p><formula xml:id="formula_7">L = Ly+1 t=1 L CE (y t , p t )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training strategy</head><p>Training a deep attention-based neural network is difficult, especially when dealing with large inputs such as whole documents. The proposed training strategy is designed to improve the convergence with a limited amount of training data, and without using any segmentation label. It is performed in two steps:</p><p>? Pre-training: the aim is to learn the feature extraction part of the DAN. We trained a line-level OCR model on synthetic printed lines and used it for transfer learning purposes for the DAN. We only used synthetic printed lines to avoid using segmentation labels, which are costly annotations to produce. Pre-training is carried out for 2 days with a mini-batch size of 16. Pre-processings, data augmentation and curriculum dropout are used during pre-training, as detailed afterwards.</p><p>? Training: the DAN is trained using teacher forcing (See section 3.2.5) to reduce the training time per epoch. It is trained on both real and synthetic documents. The idea is to learn the attention mechanism, i.e. the reading order, through the synthetic images. Indeed, printed text is easier to recognize than handwritten text and the DAN is pre-trained on printed text lines. Once the reading order is learned, it becomes easier to adapt to real-world images. This is motivated by the nature of the reading order: it is the same between printed and handwritten documents sharing the same layout. Following this idea, the model is first trained with 90% of synthetic documents during a curriculum phase to learn the reading order while using few real training samples. Then, this percentage is slowly decreased to reach 20% through the epochs, in order to fine-tune on the real samples while keeping some synthetic samples acting as unseen training data.</p><p>Training is carried out for 4 days. We did not use minibatch: training is carried out image per image. Preprocessings, data augmentation, curriculum strategies and post-processings are used during training, as described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pre-training</head><p>Training deep attention-based models is difficult. It is beneficial to train part of the model beforehand, whether it is the attention part or the feature extraction part, as shown in <ref type="bibr">[4]</ref>. As in [4], we used a line-level pre-training strategy i.e. we first train a line-level OCR model on isolated line images using the CTC loss. This line-level OCR model consists of an encoder (the same encoder as for the DAN) followed by an adaptive max pooling layer to collapse the vertical dimension, a convolutional layer and a softmax activation to predict the character and CTC blank label probabilities. However, contrary to [4], we do not use real isolated lines (extracted with the bounding boxes annotations) but synthetic printed text lines, generated from the text line transcriptions only. The DAN is then trained using transfer learning with this model to initialize the weights of the encoder and of the decision layer (last convolutional layer of the decoder) with those of this line-level OCR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Curriculum strategies</head><p>We used two curriculum strategies to improve the convergence by progressively increasing the difficulty of the task during training. A curriculum strategy is used for the generation of synthetic data during the training of the DAN. Instead of directly generating whole documents, we progressively increase the number of lines per page contained in the generated documents. We set the minimum number of lines to 1 and the maximum number of lines to l max , to fit the properties of the datasets. In addition, we also crop the synthetic document image below the lowest text line during this curriculum stage. This way, we progressively increase both the length of the target sequence, through the number of text lines, and the input image size, through the iterations of the curriculum learning process.</p><p>We used a second curriculum strategy regarding the dropout, as defined in <ref type="bibr" target="#b44">[44]</ref>. It means that the dropout rate ? evolves during training:</p><formula xml:id="formula_8">? t = (1 ?? ) exp(??t) +? , ? &gt; 0,<label>(8)</label></formula><p>where? is the final dropout rate, t is the number of iterations (weight update) and ? = 1 T , T being the total estimated number of weight updates during training. We set T = 5 ? 10 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Data Augmentation</head><p>We used a data augmentation strategy with a probability of 90%. This data augmentation strategy consists in applying some transformations, in random order, with a probability of 10% for each one. These data augmentation techniques are: resolution modification, perspective transformation, elastic distortion, dilation, erosion, color jittering, Gaussian blur, Gaussian noise and sharpening. Data augmentation is applied to both synthetic and real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Synthetic data</head><p>We generated synthetic printed lines for pre-training and synthetic printed documents for training. We provide the related code 1 . To this end, we arbitrarily chose a set of fonts F to introduce diversity in writing styles. We used these different fonts with various font sizes to bring more variability, making the model more robust. The original dataset D doc is used to extract isolated text line transcriptions y i associated to a layout class c i , leading to a new dataset D line . Synthetic lines are generated on the fly during pretraining, by randomly selecting a text line transcription from D line .</p><p>While generating synthetic documents through learning has been studied in <ref type="bibr" target="#b45">[45]</ref> for instance, here we focused on a rule-based approach for simplicity. Algorithm 1 details this generation process of synthetic documents. It is based on a style sheet s which defines the different layout entities (classes) in the document and a set of constraints on them, such as relative and absolute positioning rules. It also defines properties for each layout entity: maximum height or width in pixels, characters per line, line width or number of lines. Synthetic documents are produced on the fly. A document image X is randomly chosen from the training dataset D doc to get a realistic document shape, which is used as a template for a synthetic document D to be generated. Given the current curriculum number of lines per page l (between 1 and l max ), the actual number of lines for the current synthetic document l doc is randomly chosen between 1 and l. Layout entities are generated one after the other until reaching l doc : the layout class is chosen through "get_next_layout_class" based on the style sheet definition and the current state of D. Given the remaining number of lines (l doc -l current ), a random number of lines for the given entity l entity is chosen in compliance with s. l entity synthetic text line images are generated using a random font from F and a random text line from D line . These images are concatenated on the vertical axis, introducing some random indent spacing. The associated ground truth transcriptions are also concatenated in the same order. The generated layout entity is then placed into D in compliance with s. Ground truth transcriptions of each layout entity are concatenated by adding the corresponding layout tokens, leading to the ground truth of the whole synthetic document y. The document image is cropped below the lowest layout entity, as part of the curriculum strategy.</p><p>To sum up, we introduced variability in many points to generate different synthetic document examples:</p><p>? different fonts and font sizes for the writing style. ? randomness and flexibility in the positioning constraints for the layout.</p><p>? random number of lines and mixed sample text lines for the content.</p><p>? cropping below the lowest text line for the image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Teacher forcing</head><p>We used teacher forcing at training time to parallelize the computations by predicting the whole sequence at once: the ground truth is used in place of the previously predicted tokens. To make the DAN robust to errors occurring at prediction time, we introduced some errors in this sequence of pseudo previously predicted tokens. Some tokens are replaced by a random character or layout token. We found 20% to be a good error rate through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Pre-processings</head><p>To reduce the memory consumption, images are downscaled through a bi-linear interpolation to a 150 dpi resolution. Images are normalized (zero mean, unit variance) based on mean and variance computed on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.7">Post-processings</head><p>We used a rule-based post-processing to correct unpaired predicted layout tokens. This step is essential to compute the metrics. Let us denote &lt;X&gt; a layout start token and &lt;/X&gt; its associated layout end token. The post-processing consists of a forward pass on the whole predicted sequence? during which only tokens of layout are modified in order to have a well-formed global structure. The main rules are:</p><p>? a missing end token is added when there are two successive start tokens.</p><p>? isolated end tokens are removed.</p><p>For instance, omitting text prediction for simplicity, the prediction "&lt;X&gt; &lt;Y&gt; &lt;/Y&gt; &lt;/Z&gt;" becomes "&lt;X&gt; &lt;/X&gt; &lt;Y&gt; &lt;/Y&gt;".</p><p>In addition, the post-processing ensures that the prediction is in accordance with the layout token grammar, i.e. the hierarchical relations between tokens are correct. It means that if a layout entity of class A can only be in an entity of class B, by definition, missing tokens are added. For example, the prediction "&lt;A&gt; &lt;/Y&gt;" becomes "&lt;B&gt; &lt;A&gt; &lt;/A&gt; &lt;/B&gt;"</p><p>We used a second post-processing, for the text prediction: duplicated space characters are removed from the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METRICS</head><p>The proposed approach aims at jointly recognizing both text and layout. While there exist well-established metrics to measure the performance of both tasks independently, we are not aware of an adequate metric to evaluate both tasks when performed altogether. To our knowledge, there is no prior work handling such a task. Therefore, we propose the evaluation of our approach using three different angles: the text recognition only, the layout recognition only and the joint recognition of both text and layout, using two new metrics named LOER and mAP CER .</p><p>In the following, we will take as example the following predicted sequence?, after post-processing: "&lt;X&gt;text1&lt;/X&gt;&lt;B&gt;&lt;A&gt;text2&lt;/A&gt;&lt;A&gt;text3&lt;/A&gt;&lt;/B&gt;"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation of the text recognition</head><p>To evaluate the text recognition, all layout tokens are removed from the ground truth y and from the prediction?, leading to y text and? text , respectively. Here, the above example becomes? text = "text1text2text3".</p><p>Then, we used the standard Character Error Rate (CER) and the Word Error Rate (WER) to evaluate the performance of the text recognition. They are both computed as the sum of the Levenshtein distances (noted d lev ) between the ground truths and the predictions, at document level, normalized by the total length of the ground truths y text leni . For K examples in the dataset:</p><formula xml:id="formula_9">CER = K i=1 d lev (? text i , y text i ) K i=1 y text leni .<label>(9)</label></formula><p>WER is computed in the same way, but at word level. Punctuation characters are considered as words.</p><p>One should note that, contrary to text line or paragraph recognition, the reading order is far more complicated to learn. An inversion in the reading order between two text blocks can severely impact the CER and WER values, even with correctly recognized text blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of the layout recognition</head><p>We cannot use existing DLA metrics, such as Intersection over Union (IoU), mean Average Precision (mAP) <ref type="bibr" target="#b46">[46]</ref> or ZoneMap <ref type="bibr" target="#b47">[47]</ref>, to evaluate the layout recognition, because they are based on physical layout (segmentation) annotations. We decided to model the layout as an oriented graph to take into account both the reading order and the hierarchical relations between layout entities. To evaluate the layout recognition, we introduce a new metric: the Layout Ordering Error Rate (LOER). To this end, we associate to each ground truth and prediction a graph representation: y graph and y graph , as shown in <ref type="figure" target="#fig_5">Figure 3</ref>. We propose to generate this graph representation in two steps. First, we compute y layout and? layout , the ground truth and the prediction from which all but layout tokens are removed. Here,? layout = "&lt;X&gt;&lt;/X&gt;&lt;B&gt;&lt;A&gt;&lt;/A&gt;&lt;A&gt;&lt;/A&gt;&lt;/B&gt;". Second, we map this sequence of layout tokens into a graph following the hierarchical rules of the datasets.</p><p>We designed the LOER following the same paradigm as CER, adapting it to graphs. It is computed as a Graph Edit Distance (GED), normalized by the number of nodes and edges in the ground truth. As shown in <ref type="figure" target="#fig_5">Figure 3</ref>, this graph can be represented by ordering the nodes with respect to a root D which represents the document. This way, the graph can be represented as a multi-level graph where the nodes are the different layout entities, the oriented edges between successive levels (dashed arrows) are their hierarchy and the oriented edges inside the same level (solid arrows) represent their reading order.</p><p>For K samples in the dataset, the LOER is computed as the sum of the graph edit distances, normalized by the sum of the number of edges n e and nodes n n in the ground truths:</p><formula xml:id="formula_10">LOER = K i=1 GED(y graph i ,? graph i ) K i=1 n ei + n ni .<label>(10)</label></formula><p>The graph edit distance is computed using a unit cost of edition whether it is for insertion, removal or substitution and whether it is for edges or nodes. This computation becomes intractable in a reasonable running time for multiple pages. We circumvented this issue by assuming that the prediction of the page tokens was done in the right order. In this way, the GED of a document with several pages corresponds to the sum of the GED computed on the subgraphs representing the isolated pages. Missing ground truth or prediction sub-graphs are compared to the null graph.</p><p>Combining CER and LOER is not sufficient to evaluate the correct recognition of the document. As a matter of fact, one could reach 0% for both metrics by predicting first all the character tokens, and then all the layout tokens, each in the correct order. One misses the evaluation of the association between the layout tokens and their corresponding text parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of joint text and layout recognition</head><p>We propose a second new metric, mAP CER , to evaluate the joint recognition of both text and layout. It is based on the standard mAP score used for object detection approaches <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b48">[48]</ref>; but instead of using the IoU to consider a prediction as true or false, we use the CER. It is computed as follows:</p><p>? The predicted sequence? and the ground truth sequence y are split into sub-sequences. These subsequences are extracted using the start and end tokens of a same class. Then, they are grouped by classes into lists of sub-sequences. Results for our prediction example are given in <ref type="table">Table 1</ref>. For each layout class, sub-sequences are ordered by their confidence score, computed as the mean between prediction probabilities associated with the start and end tokens of this class (probabilities from p t ). A predicted sub-sequence is considered as true positive if the CER between this sub-sequence and a subsequence of the ground truth from the same class is under a given threshold. Otherwise, it is considered as false positive. When associated, the used subsequences are removed until there is no more subsequence in the ground truth or in the prediction. <ref type="table">Table 1</ref>: Sub-sequences are extracted, grouped and ordered by layout classes for mAP CER computation. Left: tokens of the predicted sequence and associated confidence score. Consecutive text tokens are grouped for simplicity, their associated confidence score has been averaged. Right: subsequences are extracted by layout tokens and ordered given confidence score.</p><formula xml:id="formula_11">Token Confidence &lt;X&gt; 90% text1 95% &lt;/X&gt; 70% &lt;B&gt; 95% &lt;A&gt; 82% text2 73% &lt;/A&gt; 86% &lt;A&gt; 80% text3 89% &lt;/A&gt; 80% &lt;/B&gt; 75% Score Text X 80% text1 A 84% text2 80% text3 B 85% text2text3 ?</formula><p>The average precision AP c for a layout class c corresponds to the Area Under the precision-recall Curve (AUC). As in <ref type="bibr" target="#b46">[46]</ref>, it is computed as an approximation by summing the rectangular areas under the curve, formed by each modification of the precision p n and recall r n :</p><formula xml:id="formula_12">AP CERc = (r n+1 ? r n ) ? p interp (r n+1 ),<label>(11)</label></formula><p>with p interp (r n+1 ) = max r&gt;rn+1 p(r).</p><p>? The average precision is itself averaged for different CER thresholds, between ? min = 5% and ? max = 50% with a step ?? = 5%, leading to 10 thresholds: </p><p>? Finally, the global mAP for a set of documents is computed by averaging the mAP of the different documents, weighted by the number of characters in each document.</p><p>This way, the mAP CER gives an idea of how well the text regions have been classified, based on the recognized text. It could not be used with the CER only because it does not evaluate the order of the classified text regions. Combining CER, LOER and mAP CER enables to have a real estimation of the quality of the prediction for joint text and layout recognition.</p><p>However, one could argue that the layout recognition performance is biased by the post-processing we used. To evaluate the impact of the post-processing in the final results, we defined a dedicated metric: the Post Processing Edition Rate (PPER). It is used to understand how much of the layout recognition is due to the raw network prediction and how much is due to the post-processing. It is defined by the number of post-processing edition operations (addition or removal of layout tokens) n ppe , normalized by the number of layout tokens in the ground truth y layout len . For K examples:</p><formula xml:id="formula_15">PPER = K i=1 n ppe i K i=1 y layout leni .<label>(15)</label></formula><p>One should keep in mind that this metric only quantifies how much of the final layout prediction is due to post-processing through edition operations: these modifications can be either beneficial or unfavorable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>This section is dedicated to the evaluation of the Document Attention Network (DAN) for document recognition. We evaluate the DAN on the RIMES 2009 and READ 2016 datasets. We provide a visualization of the attention process and of the predictions. We also provide an ablation study to highlight the key components that made it possible to achieve these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluated the DAN on two public handwritten document datasets: RIMES <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b49">[49]</ref> and READ 2016 <ref type="bibr" target="#b43">[43]</ref>. We also evaluate the DAN on the MAURDOR dataset <ref type="bibr" target="#b50">[50]</ref> for the text recognition task only, as detailed in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">RIMES</head><p>The RIMES dataset corresponds to French gray-scale handwritten page images at a resolution of 300dpi. They have been produced in the context of writing mail scenarios. We used the page-level version of the dataset used during the 2009 evaluation campaign <ref type="bibr" target="#b49">[49]</ref>, referred to as RIMES 2009. It is split into 1,050 pages for training, 100 pages for validation and 100 pages for test. We used two kinds of annotation from this page-level dataset: transcription ground truth and layout analysis annotations. Text regions are classified among 7 classes: sender (S), recipient (R), date &amp; location (W), subject (Y), opening (O), body (B) and PS &amp; attachment (P). We used these classes and associated them with the corresponding text part: we do not use any positional ground truth to train the proposed model. We corrected 3 annotations where at least one text line was missing. Sometimes, the annotators proposed two options for a given word or expression, we systematically selected the first one in every case. Since there is no other work evaluating their model on this dataset, we also evaluate the performance of the DAN on the RIMES 2009 dataset at paragraph-level, as well as on the RIMES 2011 dataset <ref type="bibr" target="#b41">[41]</ref> at paragraph and line levels. The idea is to compare the recognition performance between the different segmentation levels and with the state-of-the-art approaches which rely on pre-segmented input at line or paragraph level. One should notice that the paragraphs from RIMES 2011 are not extracted from RIMES 2009, so we cannot directly compare the results on both datasets, but it is the nearest comparison we can make.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">READ 2016</head><p>The READ 2016 dataset is a subset of the Ratsprotokolle collection (READ project). It was proposed for the ICFHR 2016 competition on handwritten text recognition and corresponds to Early Modern German handwritings. We used the pagelevel version of the dataset. We also generated a double-page version of this same dataset by concatenating the images of successive pages. Only few pages of the original dataset are not paired and have been removed. Based on their positions, we automatically added a class to each text region among the 5 following classes: page (P), page number (N), body (B), annotation (A) and section (S) (group of linked annotation(s) and body). For comparison purposes, we also evaluate the DAN on the READ 2016 dataset at line and paragraph levels.</p><p>For both RIMES 2009 and READ 2016 datasets at page or double-page level, the reading order is deduced automatically from the paragraph positions as follows. For READ 2016, the reading order is from page to page: first, the page number, then section by section. Among a section, all annotations are read before the body. For RIMES 2009, text regions are read from top to bottom. If two text regions share the same vertical space, text regions are read from left to right. An example of each dataset is depicted in <ref type="figure" target="#fig_5">Figure 3</ref>. Text regions are represented by bounding boxes, colored given their class. We also represented the expected reading order as an oriented graph linking the different text regions. Both datasets have their own specificity. READ 2016 has a more regular layout compared to RIMES, but it includes hierarchical layout tokens: bodies are included in sections, themselves included in pages. In addition, it can contain multiple pages. RIMES 2009 provides more variability regarding the layout, but the layout tokens are sequential.</p><p>Datasets are split into training, validation and test sets, as detailed in <ref type="table" target="#tab_0">Table 2</ref>. It corresponds to the official splits, or the most used by the community. We also provide the number of characters for each dataset, as well as the number of layout tokens (two by class: begin and end). We also provide this information at line and paragraph levels for comparison purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training details</head><p>Pre-training and training are carried out with the same following configuration, no matter the dataset:   Adam optimizer with an initial learning rate of 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We use exactly the same hyperparameters for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We do not use any external data, external language model nor lexicon constraints.</p><p>For the generation of synthetic documents, we set the maximum number of lines per page l max to 30 for READ 2016 and to 40 for RIMES 2009 to match the dataset properties. Given a set of arbitrarily-chosen fonts, we only kept those for which all characters were supported, leading to 41 fonts for READ 2016 and 95 for RIMES 2009. The font lists are provided with the code for reproducibility purposes. <ref type="figure" target="#fig_8">Figure  4</ref> illustrates the curriculum process for the generation of synthetic documents for the READ 2016 dataset at doublepage level. As one can note, these synthetic documents are far from being visually realistic compared to the original dataset. This does not matter since the objective here is only to learn the reading order.</p><p>We also evaluate the DAN at paragraph and line levels for comparison purposes. For each training of the same dataset, at paragraph and page levels of RIMES 2009 for instance, the same pre-trained weights are used to initialize the model. Each evaluation corresponds to specific training. Evaluation at paragraph or line levels corresponds to training only on synthetic and real paragraphs or lines, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation</head><p>To our knowledge, there is no work evaluating their system on the RIMES 2009 and READ 2016 datasets at page level. Comparison at paragraph and line levels is carried out with approaches under similar conditions, i.e. without external data nor external language model. In <ref type="table" target="#tab_1">Table 3</ref>, we present the evaluation of the DAN on the RIMES 2009 dataset at paragraph and page levels. One can notice that we reach very satisfying results at page level for both text and layout recognition with a CER of 4.54%, a WER of 11.85%, a LOER of 3.82% and a mAP CER of 93.74%. The closest dataset with which we can compare is the RIMES 2011 dataset at paragraph level <ref type="bibr" target="#b41">[41]</ref>. The DAN achieves new state-of-the-art results at paragraph level and competitive results at line level on this dataset. One should keep in mind that, as said previously, the comparison between RIMES 2009 and RIMES 2011 at paragraph level is not fair because RIMES 2011 only contains body images whose content seems easier to recognize than that of RIMES 2009. Unique character sequences representing dates, postal codes, product and client references, or even proper nouns like names and places,   are mainly in the other text regions. In addition, the body images from the RIMES 2011 dataset are not taken from the page images of RIMES 2009. This explains the CER difference between RIMES 2009 (5.46%) and RIMES 2011 (1.82%) at paragraph level. One can notice a CER improvement from line to paragraph level (for RIMES 2011) and from paragraph to page level (for RIMES 2009). This highlights the negative impact of using a prior segmentation step which is prone to annotation variations or errors.  <ref type="table" target="#tab_2">Table 4</ref> provides an evaluation of the DAN on the READ 2016 dataset, at line, paragraph, single-page and double-page levels. As one can note, we achieve state-of-the-art results at line and paragraph levels. We also reach very interesting results whether it is at single-page or double-page level, with a CER of 3.43% and 3.70%, respectively. It corresponds to slightly higher CER compared to the paragraph level (3.22%). One can note that the LOER and the mAP CER are also satisfying, highlighting the good recognition of the layout. Moreover, these metrics are slightly better for the doublepage level dataset. This could be explained by the higher necessity to understand the layout for complex samples. This is discussed in Section 5.5. One should note that these CER values, for both RIMES 2009 and READ 2016 datasets, should not be compared directly to paragraph-level or line-level HTR approaches. Indeed, it is necessary to understand the impact of the reading order. CER is computed based on the edit distance between two one-dimensional sequences. This way, if the reading order is wrong, it impacts severely the CER. For instance, if the sender coordinates are read before the recipient coordinates while it is the opposite in the ground truth, the edit distance will be very important even if the text is well recognized. It means that part of this CER is due to a wrong reading order and not to wrong text recognition. However, mAP CER would not be impacted: it is invariant to the reading order between the different text regions.</p><p>For both datasets, the PPER metric is very low. It indicates that the good results obtained for the layout recognition are mainly due to the DAN itself and not to the post-processing stage.</p><p>We now focus on the mAP CER metric with the READ 2016 dataset at double-page level. In <ref type="table" target="#tab_3">Table 5</ref>, we detailed this metric for each layout class and for each threshold of CER. As one can notice, pages, page numbers, sections and bodies are very well recognized with at least 91% for a CER as of 10%. The DAN has more difficulty with the annotations, with an average of 80.28%. We assume that this is due to two main points: it is the layout entity with the most variability. There can be zero, one or multiple annotations per body. In addition, they can be placed wherever along the body, from its beginning to its end. The second point is about the length of the annotations: they are very much shorter than bodies. This way, only few errors can lead to an important CER increase, leading to lower average precision.</p><p>The average inference time per image is given in <ref type="table" target="#tab_4">Table 6</ref> for both RIMES 2009 and READ 2016 datasets. As one can note, the inference time grows linearly with the number of characters: 5.8s for an average of 588 characters for RIMES 2009, 4.3s for 468 characters for READ 2016 at single-page level and 9.7s for the double-page level. It has to be noted that the input size does not seem to have a significant impact on the inference time, which is mainly due to the recurrent, attention-based decoding process."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualization</head><p>A visualization of the prediction for a test sample of the RIMES 2009 dataset is depicted in <ref type="figure" target="#fig_3">Figure 5</ref> 2 . On the left, attention weights of the last mutual attention layer are projected on the input image. The colors depend on the last predicted layout token. For visibility, the intensity of the colors is encoded for attention values between 0.02 and 0.25. The text prediction is added in red line by line, under the associated text line. The corresponding layout graph is depicted on the right, with each node corresponding to a text region in the input image. As one can note, even if the DAN is not trained using any segmentation label, it performs a kind of implicit segmentation in its process, which can be globally retrieved through the attention weights. As one can notice, the DAN performs a document recognition: it recognizes both text and layout.</p><p>In addition, the DAN is able to deal with slanted lines through its character-level attention mechanism. A prediction visualization for such example is depicted in <ref type="figure">Figure 1b</ref>. This time, we used one color per line for visibility. As one can note, the attention mechanism correctly follows the slope of the lines, leading to no error in prediction. This is an improvement compared to approaches based on line-level attention, which cannot handle such close and slanted lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation study</head><p>We provide an extensive ablation study in <ref type="table" target="#tab_5">Table 7</ref>. The evaluation is carried out on the test set of both RIMES 2009 and READ 2016 datasets, at single-page and double-page levels, after a 2-day training. We evaluated the proposed training approach through the independent removal of each of the training components we used.</p><p>Experiments (1) to (3) are dedicated to the generation of synthetic documents at training time. In (1), we do not generate synthetic documents: the model is only trained on real documents. In (2), the synthetic documents do not follow a curriculum approach: the maximum number of lines per document is directly set to its upper bound (l = l max ). In (3), the synthetic document images are not cropped below the lowest text line during the curriculum phase: they preserve the original image height. As one can note, the removal of either of these three aspects leads to poorer results for the RIMES 2009 dataset, for each metric. However, it prevents the model from adapting from training to evaluation for the READ 2016 dataset, except for (2) which only leads to degraded performance at single-page level. Indeed, when 2. Full demo at https://www.youtube.com/watch?v=HrrUsQfW66E looking at the predictions, it turned out that the model learned the language from the training set: predictions are coherent (correct succession of words) but do not match the input document image. We assume that the complexity of the task i.e. recognizing text and layout from whole documents directly, for (1) and <ref type="bibr" target="#b1">(2)</ref>, or from images of full size, for (3), combined with few training examples compared to RIMES 2009, lead to the learning of the training language as a shortcut to reduce the loss. The removal of the data augmentation strategy during both pre-training and training, experimented in (4), leads to degraded results, for both datasets.</p><p>In <ref type="formula" target="#formula_5">(5)</ref>, we do not use the curriculum dropout strategy either during pre-training or during training. This time, the results are rather mitigated, with an overall improvement at single-page level and a deterioration of the results at doublepage level. In experiment (6), we do not introduce any error in the teacher forcing strategy: we preserve the ground truth. The introduction of errors improves the results. We assume that training the model with errors helps it to deal with the errors made at prediction time. It also avoids learning the language from the training set in the case of the READ 2016 dataset at double-page level.</p><p>The layout tokens are removed from the ground truth in <ref type="bibr" target="#b6">(7)</ref> leading to text recognition only. As one can notice, it leads to lower CER and WER for the RIMES 2009 dataset but higher ones for the READ 2016 dataset. This can be explained by the fact that the reading order is easier for RIMES 2009 than for READ 2016, especially at double-page level. We assume that recognizing the different layout entities enables to understand the spatial relationship between them and helps to learn the reading order. Annotations are always at the left of a body: after the prediction of an &lt;/annotation&gt; token, the attention must be focused more on the right to predict a &lt;body&gt; token.</p><p>In <ref type="bibr" target="#b7">(8)</ref>, the DAN is trained from scratch, without transfer learning from a prior pre-training step. Results are dramatically worse for the RIMES 2009 dataset. We assume that this is due to its irregular layout. Indeed, compared to READ 2016, RIMES 2009 shows a simpler reading order but with more variability in the layout. This way, we assume that for READ 2016, the reading order can be learned jointly with the feature extraction part, directly during the curriculum step. For RIMES 2009, the layout variability slows down the learning process of the feature extraction part, leading to slower convergence.</p><p>Finally, in <ref type="bibr" target="#b8">(9)</ref> and <ref type="formula" target="#formula_1">(10)</ref>, we evaluate the importance of the 1D and 2D positional encoding components. As one can notice, they both enable to improve the results, especially the 2D positional encoding. This is especially true for the READ 2016 dataset at double-page level. We assume this is due to the double-page nature of this dataset, which leads to a more complex layout, resulting in more important jumps from one character prediction to another. For example, from the last character of the left page to the first one of the right page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>As we have seen, the Document Attention Network reaches great results on both text and layout recognition, whether it is at single-page or double-page level. Indeed, we showed   that the DAN is robust to the dataset varieties: we used the same hyperparameters with two totally different datasets in terms of layout, language, color encoding as well as the number of training samples. We proposed an efficient training strategy, and we highlighted its impact on an extensive ablation study. This training strategy is based on synthetic line and document generation using digital fonts, to overcome the lack of training data. Pre-training is carried out on synthetic text lines only, avoiding using any segmentation annotations: this is a great advantage compared to state-of-the-art approaches. We showed that even with complex handwritings such as those of the READ 2016 dataset, for which the fonts' aspect is very far from the original writings, this approach remains effective.</p><p>However, it should be noted that the datasets are rather specific: letters for RIMES and historical book pages for READ 2016. The obtained results are very dependent on the quality of the synthetic data, i.e. they must be close to the original dataset, notably in terms of layout. We used a reading order automatically generated from pixel positions of the different text regions. This is effective because we used datasets with rather regular layouts. The evaluation on the MAURDOR dataset, detailed in the Appendix, shows interesting results towards the recognition of more heterogeneous documents (mixing languages, printed and handwritten texts, as well as various backgrounds). However, for the time being, there is a lack of large-scale public datasets for handwritten document recognition. We hope that this contribution will lead to the production of datasets at a lower cost: the DAN only needs the ordered transcription annotation and layout tags, without the need for any segmentation annotation.</p><p>The DAN learns the reading order through the transcription annotations. We observed an interesting effect linked to this. In the specific case of READ 2016 at double-page level, the page number is identical for both left and right pages. The DAN focuses on the same area to predict both numbers. Technically, this does not impact the performance, but it shows that the network has not fully learned the concept of reading. We assumed that this phenomenon would disappear by learning on samples with heterogeneous layouts.</p><p>The DAN is based on a recurrent process. This is not a problem at training time since computations are parallelized through teacher forcing. However, this recurrence issue is significant at prediction time: it grows linearly with the number of tokens to be predicted. We aim at reducing this prediction time in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we proposed a new end-to-end segmentationfree architecture for the task of handwritten document recognition. To our knowledge, it is the first end-to-end approach that can deal with whole documents, recognizing both text and layout. We trained this model without using any segmentation label, reducing the annotation cost. We introduced two new metrics to estimate the good recognition of text and layout altogether. We showed its efficiency on two public datasets at single-page and double-page levels: RIMES 2009 and READ 2016. We assume that this approach could be generalized to heterogeneous documents without any problem by labeling a coherent reading order from one example to another. The main drawback of the approach is about the prediction time, which can be an obstacle for an industrial application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -EXTENDED EVALUATION</head><p>In order to have an idea of the robustness of the proposed approach on more complex data, we evaluate the DAN on the MAURDOR dataset <ref type="bibr" target="#b50">[50]</ref>. The MAURDOR dataset consists of 10,000 annotated documents. We used the dataset from the second evaluation campaign, corresponding to 8,129 heterogeneous documents written in three languages (French, English and Arabic), and classified into 5 categories (C1: forms, C2: commercial documents, C3: private manuscripts correspondences, C4: private or professional correspondences, C5: others such as diagrams or drawings).</p><p>Given that the reading order is not annotated homogeneously, we evaluate the DAN on documents from C3 and C4 only. Indeed, for these categories, the reading order can be generated automatically with fairly high confidence, based on the position of the different text regions. Examples from these two categories are shown in <ref type="figure" target="#fig_10">Figure 6</ref>. As one can note, the MAURDOR dataset is very challenging. Even in the same category, the documents look very different in terms of layout, background and non-textual items. In addition, writings can be typed, handwritten or a mix of both.</p><p>We used a sub-part of these two categories. We only used examples written in French and English to preserve the same rules regarding the reading order. Only the images with an A4 size are kept in order to deduce the original resolution and adjust it to 150 DPI (for pre-processing). Images have been rotated when it was necessary to have the text in the right direction (the original images can be rotated by 90, 180 or 270 degrees). This leads to the splits in training, validation and test described in <ref type="table" target="#tab_7">Table 8</ref>. The layout annotation of the MAURDOR dataset is not homogeneous, preventing us to use it for training. Indeed, in the case of a letter for instance, the text representing the information relative to the date and the location can be either considered as two different entities, merged together (associating two classes to the same text part), or the whole text can even be considered as only one of the two entities, ignoring the other one. This way, we only evaluate the text recognition task. We evaluate the DAN on the C3 and C4 categories, separately and jointly, to see the impact of grouping these two categories on the performance. We used exactly the same training strategy (preprocessing, pre-training, curriculum strategy based on synthetic data) as for the READ 2016 and RIMES 2009 datasets. To show the robustness of the proposed approach, we use exactly the same synthetic document generation process for both categories. Results on the test set of the MAURDOR dataset are given in <ref type="table" target="#tab_8">Table 9</ref>. As one can note, the obtained results are comparable for both categories: 8.26% of CER for C3 and 8.02% for C4. To our knowledge, the nearest work we can compare to is the system developed by A2iA <ref type="bibr" target="#b52">[52]</ref>, which is the one giving the best results from the second evaluation campaign of the MAURDOR dataset. However, this work presents many differences compared to ours. The recognition task is evaluated from the isolated paragraphs (the paragraph segmentation step is not taken into account by the metrics). This way, the task is easier in that they do not have to handle the complexity of the reading order at document level; this enables them to use all the categories (C1 to C5) for training and evaluation. It has to be noted that the authors trained one specialized MDLSTM-based network per writing style and per language, leading to 6 networks, whereas we use a unique model. Contrary to our experiment, they also rely on a pre-training strategy using an external dataset, RIMES 2009, and on a 3-gram character language model to improve the results. The A2iA system reached a CER of 8.8% for C3 and 6.2% for C4. Considering all the differences between the two experiments, the DAN results seem interesting.</p><p>Combining both categories leads to an increase of the CER of about 3 points, which is significant. This could be explained by the variety of the input documents, making the reading order more difficult to deduce. As a matter of fact, since we cannot compute the mAP CER , it is difficult to know how much is due to a misrecognition of the characters, and how much is due to a wrong reading order. We also specified the average inference time, input size, and the number of lines and characters per document for the test set. The inference time is given for a single GPU V100 (32Go). As one can note, this is consistent with the inference times of RIMES 2009 and READ 2016. <ref type="table" target="#tab_9">Table 10</ref> details the performance for each writing style (printed, handwritten or a mix of both) and for each language (French, English or a mix of both). The number of corresponding samples is also indicated to give a confidence level on the metrics. As one can note, the results are better for printed than for handwritten documents, which was expected. One should note that the training set is rather well-balanced with respect to the test set in terms of writing style: 2 printed, 522 handwritten and 482 mix for C3, and 345 printed, 7 handwritten and 369 mix for C4.</p><p>The results are better for the French language than for the English language. This can be explained by the training set which includes more French samples: 698 French, 287 English and 21 mix for C3, and 479 French, 232 English and 10 mix for C4.</p><p>These results are of the same order of magnitude as those of the A2iA system which reached 6.8% of CER for French/printed, 8.1% for English/printed, 9.4% for French/handwritten and 17.5 for English/handwritten. It has to be noted that, since the A2iA system takes paragraph images as input, there cannot be a mix of handwritten and printed text among the same example in their case. This way, a document mixing printed and handwritten text in our experiment is considered as handwritten paragraphs and printed paragraphs for them; this should be taken into  account when comparing the metrics. It is the same for the language mixes. When mixing both categories (C3 and C4), one can note that the results are globally worse. Indeed, for a few examples, the model gets confused with the reading order, leading to a high CER. We noticed a single example for which the model does not stop the recurrent process by predicting the end-of-prediction token, but by waiting for the fixed limitation of 3,000 predicted tokens. This dramatically increases the global CER and WER values, leading to a WER greater than 100% for the English handwritten documents.</p><p>These experiments, although limited by the lack of layout annotations, show promising results for the end-to-end recognition of complex, heterogeneous documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the DAN architecture (FCN+transformer decoder).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The encoder is made up of a succession of 18 convolutional layers and 12 depthwise separable convolutional layers, with kernel 3 ? 3 and ReLU activations. Diffused Mix Dropout [4] and Instance Normalization are used to avoid overfitting and improve performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 : 6 c 7 l 14 D</head><label>16714</label><figDesc>1. https://github.com/FactoDeepLearning/DAN Algorithm Synthetic document generation. input : original document image X, number of lines l doc , style sheet s, line-level dataset D line = (Y, C), set of fonts F. output : synthetic document image D, ground truth y. 1 y =" "; 2 l current = 0; 3 H, W = size(X); 4 D = zeros(H, W ); 5 while l current &lt; l doc do = get_next_layout_class(D, s); entity = get_num_lines(c, s, l current , l doc ) ; 8 for k = 1 to l entity do 9 y k = get_random_text(D line , c) ; 10 f k = get_random_font(F) ; 11 i k = generate_text_line_image(y k , f k ) ; 12 i entity = merge_text_line_images(i 1 , ..., i lentity ) ; 13 y entity = merge_text_line_gt(y 1 , ..., y lentity ) ; = add_layout_entity(D, i entity , s, c) ; 15 y = add_text(y, y entity , c) ; 16 l current + = l entity ; 17 D = crop_below_lowest_entity(D) ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>AP 5 ?</head><label>5</label><figDesc>The mean average precision for a document is then computed as a weighted sum over the different layout classes, weighted by the number of characters len c in each class c:mAP CER = c?S AP 5:50:5CERc ? len c c?S len c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>PyTorch framework with Automatic Mixed Precision.? Training with a single GPU Tesla V100 (32Gb).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Images from READ 2016 and RIMES 2009 and associated layout graph annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(a) l = 3 .</head><label>3</label><figDesc>(b) l = 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(c) l = lmax = 30 (end of curriculum stage, no crop).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the curriculum learning strategy through the synthetic document image generation process for the READ 2016 dataset at double-page level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the prediction on a RIMES 2009 test sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Images from the MAURDOR dataset. Top: examples from C3. Bottom: examples from C4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Datasets split in training, validation and test sets and associated number of tokens in their alphabet.</figDesc><table><row><cell>Dataset</cell><cell>Level</cell><cell cols="2">Training Validation</cell><cell>Test</cell><cell># char tokens</cell><cell># layout tokens</cell></row><row><cell>RIMES 2011</cell><cell>Line Paragraph</cell><cell>10,530 1,400</cell><cell>801 100</cell><cell>778 100</cell><cell>97 98</cell><cell></cell></row><row><cell>RIMES 2009</cell><cell>Paragraph Page</cell><cell>5,875 1,050</cell><cell>540 100</cell><cell>559 100</cell><cell>108 108</cell><cell>14</cell></row><row><cell></cell><cell>Line</cell><cell>8,367</cell><cell>1,043</cell><cell>1,140</cell><cell>88</cell><cell></cell></row><row><cell>READ 2016</cell><cell>Paragraph Page</cell><cell>1,602 350</cell><cell>182 50</cell><cell>199 50</cell><cell>89 89</cell><cell>10</cell></row><row><cell></cell><cell>Double page</cell><cell>169</cell><cell>24</cell><cell>24</cell><cell>89</cell><cell>10</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of the DAN on the test set of the RIMES datasets and comparison with the state-of-the-art approaches. WER ? LOER ? mAP CER ? PPER ? This work uses a different split (10,203 for training, 1,130 for validation and 778 for test).</figDesc><table><row><cell cols="3">Dataset Approach Line level [4] FCN [51] CNN+BLSTM a CER ? RIMES 3.04% 2.3% Ours (DAN) c 2.63%</cell><cell>8.32% 9.6% 6.78%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2011</cell><cell>Paragraph level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[41]</cell><cell>[3] FCN</cell><cell>4.17%</cell><cell>15.61%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">[1] CNN+MDLSTM b 2.9%</cell><cell>12.6%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>[4] FCN+LSTM b</cell><cell>1.91%</cell><cell>6.72%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ours (DAN) c</cell><cell>1.82%</cell><cell>5.03%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RIMES 2009 [49]</cell><cell>Paragraph level Ours (DAN) c Page level Ours (DAN) c</cell><cell>5.46% 4.54%</cell><cell>13.04% 11.85%</cell><cell>3.82%</cell><cell>93.74%</cell><cell>1.45%</cell></row></table><note>ab with line-level attention.c with character-level attention.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of the DAN on the test set of the READ 2016 dataset and comparison with the state-of-the-art approaches</figDesc><table><row><cell>Approach</cell><cell>CER ?</cell><cell>WER ?</cell><cell cols="3">LOER ? mAP CER ? PPER ?</cell></row><row><cell>Line level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">[36] CNN+BLSTM a 4.66%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[43] CNN+RNN</cell><cell>5.1%</cell><cell>21.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[4] FCN+LSTM b</cell><cell>4.10%</cell><cell>16.29%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (DAN) a</cell><cell>4.10%</cell><cell>17.64%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Paragraph level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[3] FCN</cell><cell>6.20%</cell><cell>25.69%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[4] FCN+LSTM b</cell><cell>3.59%</cell><cell>13.94%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (DAN) a</cell><cell>3.22%</cell><cell>13.63%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single-page level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (DAN) a</cell><cell>3.43%</cell><cell>13.05%</cell><cell>5.17%</cell><cell>93.32%</cell><cell>0.14%</cell></row><row><cell>Double-page level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (DAN) a</cell><cell>3.70%</cell><cell>14.15%</cell><cell>4.98%</cell><cell>93.09%</cell><cell>0.15%</cell></row><row><cell cols="2">a with character-level attention.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>b with line-level attention.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>mAP CER detailed for each class and each CER threshold, for the READ 2016 double-page dataset.</figDesc><table><row><cell></cell><cell>AP 5:50:5 CER</cell><cell>AP 5 CER</cell><cell>AP 10 CER</cell><cell>AP 15 CER</cell><cell>AP 20 CER</cell><cell>AP 25 CER</cell><cell>AP 30 CER</cell><cell>AP 35 CER</cell><cell>AP 40 CER</cell><cell>AP 45 CER</cell><cell>AP 50 CER</cell></row><row><cell>Page (P)</cell><cell>96.56</cell><cell>71.88</cell><cell>93.75</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>Page number (N)</cell><cell>97.50</cell><cell>95.83</cell><cell>95.83</cell><cell>95.83</cell><cell>95.83</cell><cell>95.83</cell><cell>95.83</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>Section (S)</cell><cell>94.04</cell><cell>73.31</cell><cell>91.00</cell><cell>94.70</cell><cell>96.48</cell><cell>96.48</cell><cell>97.46</cell><cell>97.46</cell><cell>97.46</cell><cell>97.46</cell><cell>98.57</cell></row><row><cell>Annotation (A)</cell><cell>80.28</cell><cell>37.84</cell><cell>60.07</cell><cell>75.49</cell><cell>88.24</cell><cell>88.24</cell><cell>90.20</cell><cell>90.20</cell><cell>90.20</cell><cell>90.20</cell><cell>92.16</cell></row><row><cell>Body (B)</cell><cell>95.18</cell><cell>85.74</cell><cell>93.33</cell><cell>95.93</cell><cell>95.93</cell><cell>95.93</cell><cell>96.98</cell><cell>96.98</cell><cell>96.98</cell><cell>96.98</cell><cell>96.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Prediction using a single GPU V100 (32Go). Times and properties are averaged for a sample of the test set.</figDesc><table><row><cell>Dataset</cell><cell>Time</cell><cell>Input size</cell><cell cols="2"># lines # chars</cell></row><row><cell>RIMES 2009 (page)</cell><cell>5.8s</cell><cell>3, 502 ? 2, 471</cell><cell>18</cell><cell>588</cell></row><row><cell>READ 2016 (single page)</cell><cell>4.3s</cell><cell>3, 510 ? 2, 380</cell><cell>23</cell><cell>468</cell></row><row><cell>READ 2016 (double-page)</cell><cell>9.7s</cell><cell>3, 510 ? 4, 760</cell><cell>46</cell><cell>944</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of the DAN on the RIMES 2009 and READ 2016 datasets. Results are given in percentages, for the test sets, and for a 2-day training. WER ? LOER ? mAP CER ? CER ? WER ? LOER ? mAP CER ? CER ? WER ? LOER ? mAP CER ?</figDesc><table><row><cell></cell><cell></cell><cell cols="2">RIMES 2009 (single-page)</cell><cell></cell><cell></cell><cell cols="2">READ 2016 (single-page)</cell><cell></cell><cell></cell><cell cols="3">READ 2016 (double-page)</cell></row><row><cell cols="2">CER ? Baseline 5.72</cell><cell>13.05</cell><cell>4.18</cell><cell>92.86</cell><cell>3.65</cell><cell>14.64</cell><cell>5.51</cell><cell>92.36</cell><cell>4.50</cell><cell>16.75</cell><cell>4.74</cell><cell>92.37</cell></row><row><cell>(1) No synthetic data</cell><cell>8.26</cell><cell>16.45</cell><cell>8.18</cell><cell>86.34</cell><cell>81.05</cell><cell>94.46</cell><cell>12.04</cell><cell>0.35</cell><cell>80.75</cell><cell>95.65</cell><cell>36.77</cell><cell>0.13</cell></row><row><cell>(2) No curriculum for syn. data</cell><cell>7.59</cell><cell>16.48</cell><cell>6.63</cell><cell>88.92</cell><cell>4.28</cell><cell>15.41</cell><cell>5.62</cell><cell>91.66</cell><cell>78.89</cell><cell>92.05</cell><cell>15.42</cell><cell>0.00</cell></row><row><cell>(3) No crop in curr. for syn. data</cell><cell>5.84</cell><cell>13.73</cell><cell>4.42</cell><cell>91.94</cell><cell>100.00</cell><cell>100.00</cell><cell>&gt; 100</cell><cell>0.00</cell><cell>100.00</cell><cell>100.00</cell><cell>&gt; 100</cell><cell>0.00</cell></row><row><cell>(4) No data augmentation</cell><cell>7.08</cell><cell>15.54</cell><cell>4.78</cell><cell>91.65</cell><cell>4.32</cell><cell>16.67</cell><cell>5.29</cell><cell>91.39</cell><cell>4.92</cell><cell>18.06</cell><cell>5.69</cell><cell>90.92</cell></row><row><cell>(5) No curriculum dropout</cell><cell>5.83</cell><cell>14.41</cell><cell>4.36</cell><cell>92.09</cell><cell>3.92</cell><cell>14.85</cell><cell>5.51</cell><cell>93.13</cell><cell>4.23</cell><cell>16.12</cell><cell>3.68</cell><cell>92.26</cell></row><row><cell>(6) No error in teacher forcing</cell><cell>8.09</cell><cell>15.12</cell><cell>5.91</cell><cell>89.24</cell><cell>7.51</cell><cell>21.87</cell><cell>4.95</cell><cell>83.51</cell><cell>85.78</cell><cell>99.51</cell><cell>42.35</cell><cell>10.73</cell></row><row><cell>(7) No layout recognition</cell><cell>5.30</cell><cell>12.46</cell><cell></cell><cell></cell><cell>4.60</cell><cell>15.59</cell><cell></cell><cell></cell><cell>4.96</cell><cell>16.81</cell><cell></cell><cell></cell></row><row><cell>(8) No pre-training</cell><cell>71.42</cell><cell>87.48</cell><cell>18.46</cell><cell>12.72</cell><cell>4.47</cell><cell>16.32</cell><cell>4.72</cell><cell>90.52</cell><cell>5.84</cell><cell>20.47</cell><cell>5.81</cell><cell>88.24</cell></row><row><cell>(9) No 1D positional encoding</cell><cell>8.04</cell><cell>16.93</cell><cell>5.73</cell><cell>90.65</cell><cell>3.77</cell><cell>14.03</cell><cell>4.95</cell><cell>92.51</cell><cell>4.96</cell><cell>18.28</cell><cell>6.17</cell><cell>88.88</cell></row><row><cell>(10) No 2D positional encoding</cell><cell>12.43</cell><cell>20.83</cell><cell>8.42</cell><cell>89.81</cell><cell>5.63</cell><cell>16.25</cell><cell>4.27</cell><cell>92.79</cell><cell>65.54</cell><cell>88.43</cell><cell>34.40</cell><cell>25.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>DenisCoquenet received the Engineering Degree in Information Systems Architecture at the National Institute of Applied Sciences in Rouen, France. He is a third-year Ph.D. student at the University of Rouen, France. His research interests include handwriting text recognition, document analysis and more globally computer vision and deep learning approaches. Ph.D students on these topics and published more than 100 papers in international conferences and scientific journals. From 2008 to 2016, he was a member of the governing board of the French association for pattern recognition AFRIF. He was the president of the French association Research Group on Document Analysis and Written Communication (GRCE) from 2002 to 2010.</figDesc><table><row><cell>Cl?ment Chatelain is an Associate Professor</cell></row><row><cell>in the Department of Information Systems Engi-</cell></row><row><cell>neering at INSA Rouen Normandy, France. His re-</cell></row><row><cell>search interests include machine learning applied</cell></row><row><cell>to handwriting recognition, document image anal-</cell></row><row><cell>ysis and medical image analysis. His teaching</cell></row><row><cell>interests include signal processing, deep learning</cell></row><row><cell>and pattern recognition. In 2019, Dr. Chatelain</cell></row><row><cell>received the French ability to conduct researches</cell></row><row><cell>from the University of Rouen.</cell></row><row><cell>Thierry Paquet was appointed Professor at the</cell></row><row><cell>University of Rouen Normandie in 2002. He was</cell></row><row><cell>the head of LITIS the research laboratory in Com-</cell></row><row><cell>puter Science associated Rouen and Le Havre</cell></row><row><cell>Universities, and Rouen INSA school of engineer-</cell></row><row><cell>ing. His research interests are machine learning,</cell></row><row><cell>statistical pattern recognition, deep learning, for</cell></row><row><cell>sequence modelling, with application to docu-</cell></row><row><cell>ment image analysis and handwriting recognition.</cell></row><row><cell>He has supervised 18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>MAURDOR split in training, validation and test sets and associated number of tokens in their alphabet.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Training Validation Test</cell><cell># char tokens</cell><cell># layout tokens</cell></row><row><cell>C3</cell><cell>1,006</cell><cell>148</cell><cell>166</cell><cell>134</cell></row><row><cell>C4</cell><cell>721</cell><cell>111</cell><cell>114</cell><cell>127</cell></row><row><cell>C3 &amp; C4</cell><cell>1,727</cell><cell>259</cell><cell>280</cell><cell>141</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Evaluation of the DAN on the test set of the MAURDOR dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="3">CER ? WER ? Time</cell><cell>Input size</cell><cell cols="2"># lines # num chars</cell></row><row><cell>C3</cell><cell>8.26</cell><cell>18.94</cell><cell>5.76s</cell><cell>3, 316 ? 2, 672</cell><cell>16</cell><cell>481</cell></row><row><cell>C4</cell><cell>8.02</cell><cell>14.57</cell><cell>7.66s</cell><cell>3, 508 ? 2, 480</cell><cell>22</cell><cell>706</cell></row><row><cell>C3 &amp; C4</cell><cell>11.59</cell><cell>27.68</cell><cell>6.61s</cell><cell>3, 394 ? 2, 594</cell><cell>18</cell><cell>575</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Evaluation of the DAN on the test set of the MAURDOR dataset per language and writing type. .95 58.71 14.44 17.84 124.51 56.87 18.42 22.26 24.09 19.23 17.10 68.27 34.52 27.68</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell></cell><cell cols="2">Printed</cell><cell></cell><cell></cell><cell cols="2">Hanwdritten</cell><cell></cell><cell></cell><cell></cell><cell>Mix</cell><cell></cell><cell></cell><cell>All</cell><cell></cell></row><row><cell></cell><cell></cell><cell>FR</cell><cell>EN</cell><cell>Mix</cell><cell>All</cell><cell>FR</cell><cell>EN</cell><cell>Mix</cell><cell>All</cell><cell>FR</cell><cell>EN</cell><cell>Mix</cell><cell>All</cell><cell>FR</cell><cell>EN</cell><cell>Mix</cell><cell>All</cell></row><row><cell></cell><cell># samples</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>42</cell><cell>55</cell><cell>0</cell><cell>97</cell><cell>63</cell><cell>4</cell><cell>2</cell><cell>69</cell><cell>105</cell><cell>59</cell><cell>2</cell><cell>166</cell></row><row><cell>C3</cell><cell>CER (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.13</cell><cell>13.39</cell><cell></cell><cell>8.57</cell><cell>7.86</cell><cell>8.46</cell><cell>10.46</cell><cell>7.98</cell><cell>7.17</cell><cell cols="2">12.99 10.46</cell><cell>8.26</cell></row><row><cell></cell><cell>WER (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14.83</cell><cell>30.69</cell><cell></cell><cell>20.50</cell><cell cols="4">17.10 20.23 25.96 17.50</cell><cell cols="4">16.22 29.89 25.96 18.94</cell></row><row><cell></cell><cell># samples</cell><cell>47</cell><cell>9</cell><cell>2</cell><cell>58</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>35</cell><cell>18</cell><cell>2</cell><cell>55</cell><cell>82</cell><cell>28</cell><cell>4</cell><cell>114</cell></row><row><cell>C4</cell><cell>CER (%)</cell><cell>5.39</cell><cell cols="2">0.86 10.93</cell><cell>5.05</cell><cell></cell><cell>12.94</cell><cell></cell><cell>12.94</cell><cell cols="4">10.67 12.89 12.79 11.26</cell><cell>7.42</cell><cell>9.17</cell><cell>12.01</cell><cell>8.02</cell></row><row><cell></cell><cell>WER (%)</cell><cell>9.94</cell><cell cols="2">2.12 12.64</cell><cell>9.05</cell><cell></cell><cell>35.04</cell><cell></cell><cell>35.04</cell><cell cols="4">18.36 24.61 23.61 20.45</cell><cell cols="4">13.42 17.60 18.98 14.57</cell></row><row><cell></cell><cell># samples</cell><cell>47</cell><cell>9</cell><cell>2</cell><cell>58</cell><cell>42</cell><cell>56</cell><cell>0</cell><cell>98</cell><cell>98</cell><cell>22</cell><cell>4</cell><cell>124</cell><cell>187</cell><cell>87</cell><cell>6</cell><cell>280</cell></row><row><cell>C3 &amp; C4</cell><cell>CER (%)</cell><cell>8.49</cell><cell cols="2">0.26 59.83</cell><cell>9.55</cell><cell>6.87</cell><cell>36.01</cell><cell></cell><cell>16.96</cell><cell>9.20</cell><cell cols="2">12.59 13.11</cell><cell>9.90</cell><cell>8.51</cell><cell cols="3">21.14 27.05 11.59</cell></row><row><cell></cell><cell>WER (%)</cell><cell cols="2">13.96 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The present work was performed using computing resources of CRIANN (Regional HPC Center, Normandy, France) and HPC resources from GENCI-IDRIS (Grant 2020-AD011012155). This work was financially supported by the French Defense Innovation Agency and by the Normandy region.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint line segmentation and transcription for end-toend handwritten paragraph recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Origaminet: Weakly-supervised, segmentation-free, one-step, full page text recognition by learning to unfold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SPAN: A simple predict &amp; align network for handwritten paragraph recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coquenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end handwritten paragraph text recognition using a vertical attention network</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Handwritten mathematical expression recognition with bidirectionally trained transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12822</biblScope>
			<biblScope unit="page" from="570" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer for handwritten text recognition using bidirectional post-decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Z?llner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gr?ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pay attention to what you read: Non-recurrent handwritten text-line recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.13044" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision transformer for fast and efficient scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Atienza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="319" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chargrid: Towards understanding 2d documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4459" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Chargrid-ocr: End-to-end trainable optical character recognition through semantic segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spinaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?hne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="http://arxiv.org/abs/1909.044693" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bertgrid: Contextualized embedding for 2d document representation and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.049483" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Layoutlmv2: Multi-modal pre-training for visuallyrich document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2579" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going full-tilt boogie on document understanding with text-image-layout transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12822</biblScope>
			<biblScope unit="page" from="732" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A typed and handwritten text block segmentation system for heterogeneous and complex documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Document Analysis Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-task handwritten document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quir?s</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.088523" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale gated fully convolutional densenets for semantic labeling of historical newspaper images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="435" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">dhsegment: A generic deep-learning approach for document segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional network with dilated convolutions for handwritten text line segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A two-stage method for text line detection in historical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="302" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple document datasets pre-training improves text line detection with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boillet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2134" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust text line detection in historical documents: learning and evaluation methods</title>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural model for text localization, transcription and named entity recognition in full pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A computationally efficient pipeline approach to full page offline handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delteil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Learning, WML@ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training full-page handwritten text recognition models without annotated line breaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Full-page text recognition: Learning where to start and when to stop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moysset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="871" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Handwriting recognition with large multidimensional long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="228" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data augmentation for recognition of handwritten words and lines using a CNN-LSTM network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Have convolutions already made recurrence obsolete for unconstrained handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coquenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Soullard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in International Workshop on Machine Learning, WML@ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrence-free unconstrained handwritten text recognition using gated fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coquenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating sequence-to-sequence models for handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Trocr: Transformer-based optical character recognition with pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.102823" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scan, attend and read: End-to-end handwritten paragraph recognition with MDLSTM attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Messina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Full page handwriting recognition via image to sequence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transformer-based approach for joint handwriting and named entity recognition in historical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ICDAR 2011 -french handwriting recognition competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Abed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The iam-database: an english sentence database for offline handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ICFHR2016 competition on handwritten text recognition on the READ dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Curriculum dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3564" to="3572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Docsynth: A layout guided approach for controllable document image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12823</biblScope>
			<biblScope unit="page" from="555" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The zonemap metric for page segmentation and area classification in scanned documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Galibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oparin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2594" to="2598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Results of the RIMES evaluation campaign for handwritten mail processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The maurdor project: Improving automatic processing of digital documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Document Analysis Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Are multidimensional recurrent layers really necessary for handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The a2ia multi-lingual text recognition system at the second maurdor evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
