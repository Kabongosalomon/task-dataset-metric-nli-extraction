<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autoregressive Unsupervised Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
							<email>yassine.ouali@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay, CentraleSup?lec</orgName>
								<orgName type="institution" key="instit2">MICS</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
							<email>celine.hudelot@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay, CentraleSup?lec</orgName>
								<orgName type="institution" key="instit2">MICS</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
							<email>myriam.tami@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit? Paris-Saclay, CentraleSup?lec</orgName>
								<orgName type="institution" key="instit2">MICS</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Autoregressive Unsupervised Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image segmentation</term>
					<term>Autoregressive models</term>
					<term>Unsupervised learning</term>
					<term>Clustering</term>
					<term>Representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a new unsupervised image segmentation approach based on mutual information maximization between different constructed views of the inputs. Taking inspiration from autoregressive generative models that predict the current pixel from past pixels in a raster-scan ordering created with masked convolutions, we propose to use different orderings over the inputs using various forms of masked convolutions to construct different views of the data. For a given input, the model produces a pair of predictions with two valid orderings, and is then trained to maximize the mutual information between the two outputs. These outputs can either be low-dimensional features for representation learning or output clusters corresponding to semantic labels for clustering. While masked convolutions are used during training, in inference, no masking is applied and we fall back to the standard convolution where the model has access to the full input. The proposed method outperforms current state-of-the-art on unsupervised image segmentation. It is simple and easy to implement, and can be extended to other visual tasks and integrated seamlessly into existing unsupervised learning methods requiring different views of the data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised deep learning has enabled great progress and achieved impressive results across a wide number of visual tasks, but it requires large annotated datasets for effective training. Designing such fully-annotated datasets involves a significant effort in terms of data cleansing and manual labeling. It is especially true for fine-grained annotations such as pixel-level annotations needed for segmentation tasks, where the annotation cost per image is considerably high <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>. This hurdle can be overcome with unsupervised learning, where unknown but useful patterns can be extracted from the easily accessible unlabeled data. Recent advances in unsupervised learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39]</ref>, that closed the performance gap with its supervised counterparts, make it a strong possible alternative.  <ref type="figure">Fig. 1</ref>. Overview. Given an encoder-decoder type network F and two valid orderings (o1, o2) as illustrated in (c). The goal is to maximize the Mutual Information (MI) between the two outputs over the different views, i.e. different orderings. (a) For Autoregressive Clusterings (AC), we output the cluster assignments in the form of a probability distribution over pixels, and the goal is to have similar assignments regardless of the applied ordering. (b) For Autoregressive Representation Learning (ARL), the objective is to have similar representations at each corresponding spatial location and its neighbors over a window of small displacements ?.</p><p>Recent works are mainly interested in two objectives, unsupervised representation learning and clustering. Representation learning aims to learn semantic features that are useful for down-stream tasks, be it classification, regression or visualization. In clustering, the unlabeled data points are directly grouped into semantic classes. In both cases, recent works showed the effectiveness of maximizing Mutual Information (MI) between different views of the inputs to learn useful and transferable features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13]</ref> or discover clusters that accurately match semantic classes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Another line of study in unsupervised learning is generative modeling. In particular, for image modeling, generative autoregressive models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9]</ref>, such as PixelCNN, are powerful generative models with tractable likelihood computation. In this case, the high-dimensional data, e.g., an image x, is factorized as a product of conditionals over its pixels. The generative model is then trained to predict the current pixel x i based on the past values x ?i?1 in a raster scan fashion using masked convolutions <ref type="bibr" target="#b36">[37]</ref>  <ref type="figure" target="#fig_1">(Fig. 3 (a)</ref>).</p><p>In this work, instead of using a single left to right, top to bottom ordering, we propose to use several orderings obtained with different forms of masked convolutions and attention mechanism. The various orderings over the input pixels, or the intermediate representations, are then considered as different views of the input image * , and the model is then trained to maximize the MI between the outputs over these different views.</p><p>Our approach is generic, and can be applied for both clustering and representation learning (see <ref type="figure">Fig. 1</ref>). For a clustering task ( <ref type="figure">Fig. 1 (a)</ref>), we apply a pair of distinct orderings over a given input image, producing two pixel-level predictions in the form of probability distribution over the semantic classes. We then maximize the MI between the two outputs at each corresponding spatial location and its intermediate neighbors. Maximizing the MI helps avoiding degeneracy (e.g., uniform output distributions) and trivial solutions (e.g., assigning all of the pixels to the same cluster). For representation learning ( <ref type="figure">Fig. 1 (b)</ref>), we maximize a lower bound of MI between the two output feature maps over the different views.</p><p>We evaluate the proposed method using standard image segmentation datasets: Potsdam <ref type="bibr" target="#b13">[14]</ref> and COCO-stuff <ref type="bibr" target="#b4">[5]</ref>, and show competitive results. We present an extensive ablation study to highlight the contribution of each component within the proposed framework, and emphasizing the flexibility of the method.</p><p>To summarize, we propose following contributions: (i) a novel unsupervised method for image segmentation based on autoregressive models and MI maximization; (ii) various forms of masked convolutions to generate different orderings; (iii) an attention augmented version of masked convolutions for a larger receptive field, and a larger set of possible orderings; (iv) an improved performance above previous state-of-the-art on unsupervised image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Autoregressive models. Many autoregressive models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> for natural image modeling have been proposed. They model the joint probability distribution of high-dimensional images as a product of conditionals over the pixels. PixelCNN <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> specifies the conditional distribution of a sub-pixel (i.e., a color channel of a pixel) as a full 256-way softmax, while PixelCNN++ <ref type="bibr" target="#b42">[43]</ref> uses a mixture of logistics. In both cases, masked convolutions are used to process the initial image x in an autoregressive manner. In Image <ref type="bibr" target="#b39">[40]</ref> and Sparse <ref type="bibr" target="#b9">[10]</ref> transformers, self-attention <ref type="bibr" target="#b45">[46]</ref> is used over the input pixels, while PixelSNAIL <ref type="bibr" target="#b8">[9]</ref> combines both attention and masked convolutions. Clustering and unsupervised representation learning. Recent works in clustering aim at combining traditional clustering algorithms <ref type="bibr" target="#b19">[20]</ref> with deep learning, such as using K-means style objectives when training deep nets training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref>. However, such objective can lead to trivial and degenerate solutions <ref type="bibr" target="#b5">[6]</ref>. IIC <ref type="bibr" target="#b27">[28]</ref> proposed to use a MI based objective which is intrinsically more robust to such trivial solutions. Unsupervised learning of representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b15">16]</ref> rather aims to train a model, mapping the unlabeled inputs into some lower-dimensional space, while preserving semantic information and discarding instance-specific details. The pre-trained model can then be fine-tuned on a down-stream task with fewer labels. Unsupervised learning and MI maximization. Maximizing MI for unsupervised learning is not a new idea <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, and recent works demonstrated its effectiveness for unsupervised learning. For representation learning, the training objective is to maximize a lower bound of MI over continuous random variables between distinct views of the inputs. These views can be the input image and its representation <ref type="bibr" target="#b23">[24]</ref>, the global and local features <ref type="bibr" target="#b22">[23]</ref>, the features at different scales <ref type="bibr" target="#b0">[1]</ref>, a sequence of extracted patches from an image in some fixed order <ref type="bibr" target="#b38">[39]</ref> or different modalities of the image <ref type="bibr" target="#b43">[44]</ref>. For a clustering objective, with discrete random variables as outputs, the exact MI can be maximized over the different views, e.g., IIC <ref type="bibr" target="#b27">[28]</ref> maximizes the MI between the image and its augmented version. Unsupervised Image Segmentation. Methods that learn the segmentation masks entirely from data with no supervision can be categorized as follows: (1) GAN based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref> that extract and redraw the main object in the image for object segmentation. Such methods are limited to only instances with two classes, a foreground and a background. The proposed method is more generalizable and is independent of the number of ground-truth classes; (2) Iterative methods <ref type="bibr" target="#b24">[25]</ref> consisting of a two-step process. The features produced by a CNN are first grouped into clusters using spherical K-means. The CNN is then trained for better feature extraction to discriminate between the clusters. We propose an end-to-end method simplifying both training and inference; (3) MI maximization based methods <ref type="bibr" target="#b27">[28]</ref> where the MI between two views of the same instance at the corresponding spatial locations is maximized. We propose an efficient and effective way to create different views of the input using masked convolutions. Another line of work consists of leveraging the learned representations of a deep network for unsupervised segmentation, e.g., CRFs <ref type="bibr" target="#b29">[30]</ref> and deep priors <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to learn a representation that maximizes the MI, denoted as I, between different views of the input. These views are generated using various orderings, capturing different aspects of the inputs. Formally, let x ? X be an unlabeled data point, and F : X ? Y be a deep representation to be learned as a mapping between the inputs and the outputs. For clustering, Y is the set of possible clusters corresponding to semantic classes, and for representation learning, Y corresponds to a lower-dimensional space of the output features. Let (o i , o j ) ? O be two orderings o i and o j obtained from the set of possible and valid orderings O <ref type="figure">(Fig. 2</ref>). For two outputs y ? F(x; o i ) and y ? F(x; o j ), the objective is to maximize the predictability of y from y and vice-versa, where F(x; o i ) corresponds to applying the learning function F with a given ordering o i to process the image x. This objective is equivalent to maximizing the MI between the two encoded variables:</p><formula xml:id="formula_0">max F I(y; y )<label>(1)</label></formula><p>We start by presenting different forms of masked convolutions to generate various raster-scan orderings, and propose an attention augmented variant for a larger receptive field and use it to extend the set of possible orderings (Section 3.1). We then formulate the training objective for maximizing Eq. (8) for both clustering and unsupervised representation learning (Section 3.2). We finally conclude with a flexible design architecture for the function F (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Orderings</head><p>Masked Convolutions In neural autoregressive modeling <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9]</ref>, for an input image x ? R H?W ?3 with 3 color channels, a raster-scan ordering is first imposed on the image (see <ref type="figure">Fig. 2</ref>, ordering o 1 ). Such an ordering, where the pixel x i only depends on the pixels that come before it, is maintained using masked convolutions ? <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>  <ref type="figure" target="#fig_1">(Fig. 3 (a)</ref>).</p><p>Our proposition is to use all 8 possible raster-scan type orderings as the set of valid orderings O as illustrated in <ref type="figure">Fig. 2</ref>. A simple way to obtain them is to use a single ordering o 1 with the standard masked convolution ( <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>), along with geometric transformations g (i.e., image rotations by multiples of 90 degrees and horizontal flips), resulting in 8 versions of the input image. We can then maximize the MI between the two outputs, i.e., I(y; g ?1 (y )) with y ? F(g(x); o j ). In this case, since the masked weights are never trained, we cannot fall-back to the normal convolution where the function F has access to the full input during inference, greatly limiting the performance of such approach.</p><p>This point motivates our approach. Our objective is to learn all the weights of the masked convolution during training, and use an unmasked version during inference. This can be achieved by using a normal convolution, and for a given ordering o i , we mask the corresponding weights during the forward pass to construct the desired view of the inputs. Then in the backward pass, we only update the unmasked weights and the masked weights remain unchanged. In this case, all of the weights will be learned and we will converge to a normal convolution given enough training iterations. During inference, no masking is applied, giving the function F full access to the inputs.</p><p>A straight forward way to implement this is to use 8 versions of the standard masked convolution to create the set O ( <ref type="figure" target="#fig_1">Fig. 3 (d)</ref>). However, for each forward</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Padding Unmaskded Weights</head><p>Shifts as padding Masked Weights Input</p><formula xml:id="formula_1">Shift 1 Conv A Conv B Conv C Shift 4 Conv D Shift 2 Shift 1 Conv A Conv B Conv C Conv D Shift 2 Shift 3 Shift 3 Shift 4</formula><p>Standard Masked Convolution pass, the majority of the weights are masked, resulting in a reduced receptive field and a fewer number of weights will be learned at each iteration, leading to some disparity between them. Given that we are interested in a discriminative task, rather than generative image modeling where the access to the current pixel is not allowed. We start by relaxing the conditional dependency, and allow the model to have access to the current pixel, reducing the number of masked locations by one ( <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>). To further reduce the number of masked weights, for an F ? F convolution, instead of masking the lower rows, we can simply shift the input by the same amount and only mask the weights of the last row. We thus reduce the number of masked weight from F 2 /2 ( <ref type="figure" target="#fig_1">Fig. 3</ref> (b)) to F/2 ( <ref type="figure" target="#fig_1">Fig. 3 (c)</ref>). With four possible masked convolutions: {Conv A , Conv B , Conv C , Conv D } and four possible shifts: ? {Shift 1 , Shift 3 , Shift 2 , Shift 4 }, we can create all of 8 raster-scan orderings as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref> (e). The proposed masked convolutions do not introduce any additional computational overhead, neither in training, nor inference, making them easy to implement and integrate into existing architectures with minor changes. ? e.g., for Shift1 and a 3 ? 3 convolution, an image of spatial dimensions H ? W is first padded on the top resulting in (H + 1) ? W , the last row is then cropped, going back to H ? W . Attention Augmented Masked Convolutions As pointed out by <ref type="bibr" target="#b36">[37]</ref>, the proposed masked convolutions are limited in terms of expressiveness since they create blind spots in the receptive field ( <ref type="figure">Fig. 6</ref>). In our case, by applying different orderings, we will have access to all of the input x over the course of training, and this bug can be seen as a feature where the blind spots can be considered as an additional restriction. This restricted receptive filed, however, can be overcome using the self-attention mechanism <ref type="bibr" target="#b45">[46]</ref>. Similar to previous works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b2">3]</ref>, we propose to add attention blocks to model long range dependencies that are hard to access through standalone convolutions. Given an input tensor of shape (H, W, C in ), after reshaping it into a matrix X ? R HW ?Cin , we can apply a masked version of attention <ref type="bibr" target="#b45">[46]</ref> in a straight forward manner. The output of the attention operation is:</p><formula xml:id="formula_2">A = Softmax((QK ) M oi )V (2) with Q = XW q , K = XW k and V = XW v , where W q , W k ? R Cin?d and W v ? R Cin?d</formula><p>are learned linear transformations that map the input X to queries Q, keys K and values V , and M oi ? R HW ?HW corresponds to a masking operation to maintain the correct ordering o i . The output is then projected into the output space using a learned linear transformation W O ? R d?Cin obtaining X att = AW O . The output of the attention operation X att is concatenated channel wise with the input X, and then merged using a 1 ? 1 convolution resulting in the output of the attention block.</p><p>Zigzag Orderings. Using attention gives us another benefit, we can extend the set of possible orderings to include zigzag type orderings introduced in [9] ( <ref type="figure">Fig. 4</ref>). With zigzag orderings, the outputs at each spatial location will be mostly influenced by the values of the corresponding neighboring input pixels, which can give rise to more semantically meaningful representations compared to that of raster-scan orderings. This is done by simply using a mask M oi corresponding to the desired zigzag ordering o i . Resulting in a set O of 16 possible and valid orderings o i with i ? {1, . . . , 16} in total. See <ref type="figure">Fig. 5</ref> for an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Objective</head><p>In information theory, the MI I(X; Y ) between two random variables X and Y measures the amount of information learned from the knowledge of Y about X and vice-versa. The MI can be expressed as the difference of two entropy terms:</p><formula xml:id="formula_3">I(X; Y ) = H(X) ? H(X|Y ) = H(Y ) ? H(Y |X)<label>(3)</label></formula><p>Raster-Scan Mask Masked Unmasked Zigzag Mask <ref type="figure">Fig. 5</ref>. Attention Masks. Examples of the different attention masks Mo i of shape HW ? HW applied for a given ordering oi. With HW = 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blind Spots</head><p>Receptive Field Ordering Ordering <ref type="figure">Fig. 6</ref>. Blind Spots. Blind spots in the receptive field of pixel as a result of using a masked convolution for a given ordering oi.</p><p>Intuitively, I(X; Y ) can be seen as the reduction of uncertainty in one of the variables, when the other one is observed. If X and Y are independent, knowing one variable exposes nothing about the other, in this case, I(X; Y ) = 0. Inversely, if the state of one variable is deterministic when the state of the other is revealed, the MI is maximized. Such an interpretation explains the goal behind maximizing Eq. <ref type="bibr" target="#b7">(8)</ref>. The neural network F must be able to preserve information and extract semantically similar representations regardless of the applied ordering o i , and learn representations that encode the underlying shared information between the different views. The objective can also be interpreted as having a regularization effect, forcing the function F to focus on the different views and subparts of the input x to produce similar outputs, reducing the reliance on specific objects or parts of the image.</p><p>Let p(y, y ) be the joint distribution produced by sampling examples x ? X and then sampling two outputs y ? F(x; o i ) and y ? F(x; o j ) with two possible orderings o i and o j . In this case, the MI in Eq. (8) can be defined as the Kullback-Leibler (KL) divergence between the joint and the product of the marginals:</p><formula xml:id="formula_4">I(y, y ) = D KL (p(y, y ) p(y)p(y ))<label>(4)</label></formula><p>To maximize Eq. (13), we can either maximize the exact MI for a clustering task over discrete predictions, or a lower bound for an unsupervised learning of representations over the continuous outputs. We will now formulate the loss functions L AC and L ARL of both objectives for a segmentation task.</p><p>Autoregressive clustering (AC). In a clustering task, the goal is to train a neural network F to predict a cluster assignment corresponding to a given semantic class k ? {1, . . . , K} with K possible clusters at each spatial location. In this case, the encoder-decoder type network F is terminated with K-way softmax, outputting y ? [0, 1] H?W ?K of the same spatial dimensions as the input. Concretely, for a given input image x and two valid orderings (o i , o j ) ? O, we forward pass the input through the network producing two output probability distributions F(x; o i ) = p(y|x, o i ) and F(x; o j ) = p(y |x, o j ) over the K clusters and at each spatial location. After reshaping the outputs into two matrices of shape HW ? K, with each element corresponding to the probability of assigning pixel x l with l ? {1, . . . , HW } to cluster k, we can compute the joint distribution p(y, y ) of shape K ? K as follows:</p><formula xml:id="formula_5">p(y, y ) = F(x; o i ) F(x; o j )<label>(5)</label></formula><p>The marginals p(y) and p(y ) can then be obtained by summing over the rows and columns of p(y, y ). Similar to IIC <ref type="bibr" target="#b27">[28]</ref>, we symmetrize p(y, y ) using [p(y, y )+ p(y, y ) ]/2 to maximize the MI in both directions. The clustering loss L AC in this case can be written as follows:</p><formula xml:id="formula_6">L AC = E x?X E p(y,y ) log p(y, y ) p(y)p(y )<label>(6)</label></formula><p>In practice, instead of only maximizing the MI between two corresponding spatial locations, we maximize it between each spatial location and its intermediate neighbors over small displacements u ? ? (see <ref type="figure">Fig. 1</ref>). This can be efficiently implemented using a convolution operation as demonstrated in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Autoregressive representation learning (ARL). Although the clustering objective in Eq. (6) can also be used as a pre-training objective for F, Tschannen et al . <ref type="bibr" target="#b44">[45]</ref> recently showed that maximizing the MI does not often results in transferable and semantically meaningful features, especially when the downstream task is a priori unknown. To this end, we follow recent representation learning works based on MI maximization <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b43">44]</ref>, where a lower bound estimate of MI (e.g., InfoNCE <ref type="bibr" target="#b38">[39]</ref>, NWJ <ref type="bibr" target="#b35">[36]</ref>) is maximized between different views of the inputs. These estimates are based on the simple intuitive idea, that if a critic f is able to differentiate between samples drawn from the joint distribution p(y, y ) and samples drawn from the marginals p(y)p(y ), then the true MI is maximized. We refer the reader to <ref type="bibr" target="#b44">[45]</ref> for a detailed discussion.</p><p>In our case, with image segmentation as the target down-stream task, we maximize the InfoNCE estimator <ref type="bibr" target="#b38">[39]</ref> over the continuous outputs. Specifically, with two outputs (y, y ) ? R H?W ?C as C-dimensional feature maps. The training objective is to maximize the infoNCE based loss L ARL :</p><formula xml:id="formula_7">L ARL = E x?X log e f (y l ,y l ) 1 N N m=1 e f (y l ,y m )<label>(7)</label></formula><p>For an input image x and two outputs y and y . Let y l and y m correspond to C-dimensional feature vectors at spatial positions l and m in the first and second outputs respectively. We start by creating N pairs of feature vectors (y l , y m ), with one positive pair drawn from the joint distribution and N ?1 negative pairs drawn from the marginals. A positive pair is a pair of feature vectors corresponding to the same spatial locations in the two outputs, i.e., a pair (y l , y m ) with m = l. The negatives are pairs (y l , y m ) corresponding to two distinct spatial positions m = l. In practice, we also consider small displacements ? <ref type="figure">(Fig. 1)</ref> when constructing positives. Additionally, the negatives are generated from two distinct images, since two feature vectors might share similar characteristics even with different spatial positions. By maximizing Eq. <ref type="formula" target="#formula_0">(14)</ref>, we push the model F to produce similar representations for the same spatial location regardless of the applied ordering, so that the critic function f is able to give high matching scores to the positive pairs and low matching to the negatives. We follow <ref type="bibr" target="#b22">[23]</ref> and use separable critics f (y, y ) = ? 1 (y) ? 2 (y ), where the functions ? 1 /? 2 nonlinearly transform the outputs to a higher vector space, and f (y l , y m ) produces a scalar corresponding to a matching score between the two representations at two spatial positions l and m of the two outputs.</p><p>Note that both losses L AC and L ARL can be applied interchangeably for both objectives, a case we investigate in our experiments (Section 4.1). For L AC , we can consider the clustering objective as an intermediate task for learning useful representations. For L ARL , during inference, K-means <ref type="bibr" target="#b28">[29]</ref> algorithm can be applied over the outputs to obtain the cluster assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model</head><p>The representation F can be implemented in a general manner using three subparts, i.e., F = h ? g ar ? d, with a feature extractor h, an autoregressive encoder g ar and a decoder d. With such a formulation, the function F is flexible and can take different forms. With h as an identity mapping, F becomes a fully autoregressive network, where we apply different orderings directly over the inputs. Inversely, if g ar is an identity mapping, F becomes a generic encoder-decoder network, where h plays the role of an encoder. Additionally, h can be a simple convolutional stem that plays an important role in learning local features such as edges, or even multiple residual blocks <ref type="bibr" target="#b20">[21]</ref> to extract higher representations. In this case, the orderings are applied over the hidden features using g ar . g ar is similar to h, containing a series of residual blocks, with two main differences, the proposed masked convolutions are used, and the batch normalization <ref type="bibr" target="#b25">[26]</ref> layers are omitted to maintain the autoregressive dependency, with an optional attention block. The decoder d can be a simple conv1 ? 1 to adapt the channels to the number of cluster K, followed by bilinear upsampling and a softmax operation for a clustering objective. For representation learning, d consists of two separable critics ? 1 /? 2 , which are implemented as a series of conv3 ? 3 ? BN ? ReLU and conv1 ? 1 for projecting to a higher dimensional space. See sup. mat. for the architectural details. In the experimental section, we will investigate different versions of F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>After stating the experimental setting, we start by presenting an extensive ablation study of the proposed method and its various parts. We then compare the method to state-of-the-art approaches on unsupervised image segmentation.</p><p>Datasets. The experiments are conducted on the newly established and challenging baselines by <ref type="bibr" target="#b27">[28]</ref>. Potsdam <ref type="bibr" target="#b13">[14]</ref> with 8550 RGBIR satellite images of size 200 ? 200, of which 3150 are unlabeled. We experiment on both the 6-labels variant (roads and cars, vegetation and trees, buildings and clutter) and Potsdam-3, a 3-label variant formed by merging each of the pairs. We also use COCO-Stuff <ref type="bibr" target="#b4">[5]</ref>, a dataset containing stuff classes. Similarly, we use a reduced version of COCO-Stuff with 164k images and 15 coarse labels, reduced to 52k by taking only images with at least 75% stuff pixel. In addition to COCO-Stuff-3 with only 3 labels, sky, ground and plants.</p><p>Evaluation Metrics. We report the pixel classification Accuracy (Acc). For a clustering task, with a mismatch between the learned and ground truth clusters. We follow the standard procedure and find the best one-to-one permutation to match the output clusters to ground truth classes using the Hungarian algorithm <ref type="bibr" target="#b32">[33]</ref>. The Acc is then computed over the labeled examples.</p><p>Implementation details. The different variations of F are trained using ADAM with a learning rate of 10 ?5 to optimize both objectives in Eqs. <ref type="formula" target="#formula_6">(6)</ref> and <ref type="formula" target="#formula_0">(14)</ref>. We train on 200 ? 200 crops for Potsdam and 128 ? 128 for COCO. The training is conducted on NVidia V100 GPUs, and implemented using the PyTorch framework <ref type="bibr" target="#b40">[41]</ref>. For more experimental details, see sup. mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Studies</head><p>We start by performing comprehensive ablation studies on the different components and variations of the proposed method. <ref type="table">Table 1</ref> and <ref type="figure">Fig. 7</ref> show the ablation results for AC, and <ref type="table" target="#tab_3">Table 2</ref> shows a comparison between AC and ARL, analyzed as follows: Variations of F. <ref type="table">Table 1a</ref> compares different variations of the network F. With a fixed decoder d (i.e., a 1 ? 1 Conv followed by bilinear upsampling and softmax function), we adjust h and g ar going from a fully autoregressive model (F 1 ) to a normal decoder-encoder network (F 4 and F 5 ). When using masked versions, we see an improvement over the normal case, with up to 8 points for Potsdam, and to a lesser extent for Potsdam-3 where the task is relatively easier with only three ground truth classes. When using a fully autoregressive model (F 1 ), and applying the orderings directly over the inputs, maximizing the MI becomes much harder, and the model fails to learn meaningful representations. Inversely, when no masking is applied (F 4 and F 5 ), the task becomes comparatively simpler, and we see a drop in performance. The best results are obtained when applying the orderings over low-level features (F 2 and F 3 ). Interestingly, the unmasked versions yield results better than random, and perform competitively with 3 output classes for Potsdam-3, validating the effectiveness of maximizing the MI over small displacements u ? ?. For the rest of the experiments we use F 2 as our model. Attention and different orderings. <ref type="table">Table 1c</ref> shows the effectiveness of adding attention blocks to our model. With a single attention block added at a shallow level, we observe an improvement over the baseline, for both raster-scan and zigzag orderings, and their combination, with up to 4 points for Potsdam. In this case, given the quadratic complexity of attention, we used an output stride of 4. Data augmentations. For a given training iteration, we pass the same image two times through the network, applying two different orderings at each forward pass. We can, however, pass a transformed version of the image as the second input. We investigate using photometric (i.e., color jittering) and geometric (i.e., rotations and H-flips) transformations. For geometric transformations, we bring the outputs back to the input coordinate space before computing the loss. Results are shown in <ref type="table">Table 1e</ref>. As expected, we obtain relative improvements with data augmentations, highlighting the flexibility of the approach. Dropout. To add some degree of stochasticity to the network, and as an additional regularization, we apply dropout to the intermediate activations within  <ref type="figure">Fig. 7</ref>. Overclustering. The Acc obtained when using a number of output clusters greater than the number of ground truth classes K &gt; Kgt. With variable number of images used to find the best many-to-one matching between the outputs and targets.</p><p>residual blocks of the network. <ref type="table">Table 1f</ref> shows a small increase in Acc for Potsdam.</p><p>Orderings. Until now, at each forward pass, we sample a pair of possible orderings with replacement from the set O. With such a sampling procedure, we might end-up with the same pair of orderings for a given training iteration.</p><p>As an alternative, we investigate two other sampling procedures. First, with no repetition (No Rep.), where we choose two distinct orderings for each training iteration. Second, using hard sampling, choosing two orderings with opposite receptive fields (e.g., o 1 and o 6 ). <ref type="table">Table 1d</ref> shows the obtained results. We see 2 points improvement when using hard sampling for Potsdam. For simplicity, we use random sampling for the rest of the experiments.</p><p>Additionally, to investigate the effect of the number of orderings (i.e., the cardinality of O), we compute the Acc over different choices and sizes of O. <ref type="table">Table 1b</ref> shows best results are obtained when using all 8 raster-scan orderings. Interestingly, for some choices, we observe better results, which may be due to selecting orderings that do not share any receptive fields, as the ones used in hard sampling. Overclustering. To compute the Acc for a clustering task using linear assignment, the output clusters are chosen to match the ground truth classes K = K gt . Nonetheless, we can choose a higher number of clusters K &gt; K gt , and then find the best many-to-one matching between the output clusters and ground truths based a given number of labeled examples. In this case, however, we are not in a fully unsupervised case, given that we extract some information, although limited, from the labels. <ref type="figure">Fig. 7</ref> shows that, even with a very limited number of labeled examples used for mapping, we can obtain better results than the fully unsupervised case. AC and ARL To compare AC and ARL, we apply them interchangeably on both clustering and representation learning objectives. In clustering, for ARL, after PCA Whitening, we apply K-means over the output features to get the cluster assignments. In representation learning, we evaluate the quality of the learned representations using both linear and non-linear separability as a proxy  for disentanglement, and as a measure of MI between representations and class labels. <ref type="table" target="#tab_3">Table 2</ref> shows the obtained results.</p><p>Clustering. As expected, AC outperforms ARL on a clustering task, given that the clusters are directly optimized by computing the exact MI during training. Quality of the learned representations. Surprisingly, AC outperforms ARL on both linear and non-linear classifications. We hypothesize that unsupervised representation learning objectives that work well on image classification, fail in image segmentation due to the dense nature of the task. The model in this case needs to output distinct representations over pixels, rather than the whole image, which is a harder task to optimize. This might also be due to using only a small number of features (i.e., N pairs) for each training iteration. <ref type="table" target="#tab_4">Table 3</ref> shows the results of the comparison. AC outperforms previous work, and by a good margin for harder segmentation tasks with a large number of output classes (i.e., Potsdam and COCO-Stuff), highlighting the effectiveness of maximizing the MI between the different orderings as a training objective. We note that no regularization or data augmentation were used, and we expect that better results can be obtained by combining AC with other procedures as demonstrated in the ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a novel method to create different views of the inputs using different orderings, and showed the effectiveness of maximizing the MI over these views for unsupervised image segmentation. We showed that for image segmentation, optimizing over the discrete outputs by computing the exact MI works better for both clustering and unsupervised representation learning, due to the dense nature of the task. Given the simplicity and ease of adoption of the method, we hope that the proposed approach can be adapted for other visual tasks and used in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we provide architectural details, hyperparameters settings, further discussions about the loss functions and the masked convolutions. We also provide some qualitative results and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Architectural details</head><p>Tables 4 to 7 present the building blocks of the representation function F. Specifically, we describe the architecture of the convolutional stem, the residual blocks, the decoder for AC and the separable critics used for ARL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Stem Layer</head><p>Output size </p><formula xml:id="formula_8">Input 3 ? H ? W Conv 3 ? 3 64 ? H ? W Batch Norm -ReLU 64 ? H ? W Max Pool 3 ? 3, s = 2 64 ? H/2 ? W/2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Layer</head><p>Output size <ref type="table">Table 5</ref>. Decoder used for a clustering objective. In this case, we have an output stride of 2 and K clusters.</p><formula xml:id="formula_9">Input C ? H/2 ? W/2 Conv 1 ? 1 K ? H ? W Bilinear Interpolation K ? H ? W Softmax K ? H ? W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separable Critics Layer</head><p>Output size <ref type="table">Table 6</ref>. Separable critics used for representation learning to non-linearly project the outputs to a higher vector space.</p><formula xml:id="formula_10">Input C ? H ? W Conv 1 ? 1 -ReLU 2C ? H ? W Conv 1 ? 1 -ReLU 2C ? H ? W Conv 1 ? 1 applied to the input 2C ? H ? W Residual Connection + Batch Norm 2C ? H ? W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Block Layer</head><p>Output size </p><formula xml:id="formula_11">Input C ? H ? W Conv 3 ? 3 -ReLU 2C ? H ? W Conv 1 ? 1 -ReLU 2C ? H ? W Zero padding of the input to 2C 2C ? H ? W Residual Connection 2C ? H ? W Conv 1 ? 1 -ReLU 2C ? H ? W Conv 1 ? 1 -ReLU 2C ? H ? W Residual Connection 2C ? H ? W Conv 1 ? 1 -ReLU 2C ? H ? W Conv 1 ? 1 -ReLU 2C ? H ? W Residual Connection 2C ? H ? W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>We discuss the hyperparameters used in our experiments. We note that we have noticed that the network is very sensitive to the initialization. In our case, we initialize the parameters using Xavier initialization <ref type="bibr" target="#b17">[18]</ref>, and noticed a somehow more stable results with such an instantiation scheme. The optimizer of choice is Adam <ref type="bibr" target="#b31">[32]</ref> with the default parameters (? 1 = 0.9 and ? 2 = 0.999). The rest of the hyperparamters used are detailed in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Loss functions</head><p>In this section, we will go into more details about the loss functions introduced in Paper Section 3.2. For a given unlabeled input x ? X , and two outputs y ? F(x; o i ) and y ? F(x; o j ) with two valid orderings (o i , o j ) ? O, the training objective is to maximize the MI between the two encoded variables:</p><formula xml:id="formula_12">max F I(y; y )<label>(8)</label></formula><p>C.1 Autoregressive Clustering L AC To see the benefits of maximizing Eq. (8) for a clustering objective, we expand the objective as the difference between two entropy terms:</p><formula xml:id="formula_13">I(y; y ) = H(y) ? H(y|y )<label>(9)</label></formula><p>By such a formulation, we can see that maximizing the MI involves maximizing the entropy and minimizing the conditional entropy. The compromise between these two terms help us avoid both degenerate and trivial solutions. For degenerate solution, where the model F outputs uniform distributions over all of the pixels, not assigning any cluster to any pixel, the entropy H(y) in this case is maximized, however the second term H(y|y ) is also maximized, since the outputs are not deterministic and there is no predictability of the second output from the first. Inversely, with trivial solutions, where all of the pixel are assigned to the same cluster. the second output y is totally deterministic from the first, and the conditional entropy H(y|y ) is minimized, yet, the entropy H(y) is also minimized and we fail to maximize the MI. By balancing the maximization of the first term and the minimization of the second, we are more likely to end-up with the correct assignments, than if we only maximized the entropy. Given that the two outputs are generated using the same input and two different orderings, there is a strong statistical dependency between them. In this case, y ? F(x; o i ) and y ? F(x; o j ) are dependent and we compute the joint probability p(y, y ) as a matrix of size K ? K:</p><formula xml:id="formula_14">p(y, y ) = F(x; o i ) T F(x; o j )<label>(10)</label></formula><p>In practice we also marginalize over the batch, with an input x of shape B ? 3 ? H ? W as a batch of B input images. Let x i correspond to the i-th image in the batch x of B images. In this case the joint probability is computed as follows:</p><formula xml:id="formula_15">p(y, y ) = 1 B B i=1 F(x i ; o i ) T F(x i ; o j )<label>(11)</label></formula><p>Additionally, following <ref type="bibr" target="#b27">[28]</ref>, we also compute the joint probability over small possible displacements u ? ?. Let the input x (u) correspond to shifting the input x by u pixels (i.e., zero padding and cropping). In such a case, we also need to marginalize over all possible displacements u as follows:</p><formula xml:id="formula_16">p(y, y ) = 1 B 1 |?| B i=1 u?? F(x i ; o i ) T F(x (u) i ; o j )<label>(12)</label></formula><p>Finally, by summing over the rows and columns of p(y, y ), we can compute the marginals, and then the MI:</p><formula xml:id="formula_17">I(y, y ) = D KL (p(y, y ) p(y)p(y ))<label>(13)</label></formula><p>Positives Negatives </p><p>The goal of Eq. <ref type="formula" target="#formula_0">(14)</ref> is to push the network F to produce similar features between the two outputs y and y at the same spatial locations, so that the critic is able to give high scores between two feature vectors (y l , y m ) at the same spatial position m = l, and low scores for feature vectors from distinct spatial position m = l or from two distinct images. To compute the loss in Eq. <ref type="formula" target="#formula_0">(14)</ref>, we need to create a set of positive and negative pairs. With a batch of images x of shape B ? 3 ? H ? W , we generate two outputs y and y of shape B ? C ? H ? W , with C-dimensional output feature maps. In this case the output of the critic f (y, y ) = ? 1 (y) ? 2 (y ) is a matrix of shape BHW ?BHW . To construct the positive and negative pairs, we reshape the scoring matrix as B 2 matrices of shape HW , in this case the positives are the diagonals of each matrix from the same images with a given shift u ? ?. The negatives are all of the possible combination across the matrices from distinct images. See <ref type="figure" target="#fig_3">Fig. 8</ref> for an illustration for B = 2 and HW = 4. Note that we avoid using the same image to construct negative pairs, and only construct them across images, given that even with distinct spatial positions, it is very likely that two feature vectors share similar characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Receptive fields</head><p>To further illustrate how a given ordering o i is constructed, we present a toy example where we plot the receptive field of a given pixel at the center of an image of size 16 ? 16. After each application of a masked convolution with the corresponding shift, we compute the gradient of the target pixel and plot the non-zero values in blue, which correspond to the receptive field of the target pixel. The results are illustrated in <ref type="figure">Fig. 9</ref>.</p><p>Step 5</p><p>Step 6</p><p>Step 7</p><p>Step 8</p><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Step 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ordering</head><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Step 4</p><p>Step 5</p><p>Step 6</p><p>Step 7</p><p>Step 8</p><p>Ordering <ref type="figure">Fig. 9</ref>. Examples of the growing receptive field of pixel for two orderings; o1 and o2, over 8 consecutive applications of masked convolutions to get the correct orderings. As expected, after enough convolutions, and with the correct shift, we can construct the desired ordering. Note that in both cases we have a significant number of pixels in the blind spots, which can be accessed using an attention block. In this case, we use ConvA with Shift1 and Shift3.</p><p>Orderings. For a given pair of distinct orderings, the resulting dependencies and receptive fields of the two outputs will be different even if the applied orderings are quite similar. It is however likely that the two outputs share some overlap in their receptive fields, but such an overlap is small and helps reduce the difficulty of the task. An illustration of the resulting receptive fields for a given pixel using raster-scan orderings is shown in <ref type="figure">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ordering Ordering Ordering</head><p>Ordering Ordering Ordering Ordering</p><p>Ordering <ref type="figure">Fig. 10</ref>. The resulting receptive fields with the various raster-scan type orderings E Qualitative Results <ref type="figure">Fig. 11</ref> shows qualitative results of Autoregressive Clustering (AC) on COCOstuff 3 test set, in addition to linear and non-linear evaluations, where the model trained for AC is frozen and then the corresponding layers are added on top of the decoder, that are then trained on the train set. Surprisingly, even if the accuracy with linear and non-linear evaluations is higher, we see that qualitatively, the fully unsupervised method gives slightly better results. This might be due to the dense nature of image segmentation, where the prediction at a given pixel is very dependent of its neighbors, and we lose this locality with linear evaluation, given that we consider each pixel as a standalone data point. This is similar to what we observed with ARL where we optimize the representations at each spatial location separately. Note that we have noticed some minor annotation errors in the ground truths that might be due to the conversion done by <ref type="bibr" target="#b27">[28]</ref>, these are very minor and can be overlooked.</p><p>We also present some examples where AC fails in <ref type="figure">Fig. 12</ref>. We observe that the model is very dependent on the appearance and colors for making the predictions. However, in special cases, like tennis courts with grass or asphalt floors, the model predicts green or sky classes, and the correct prediction is ground. This can be overcome with additional data augmentations like color jittering, or in case where a limited amount of labeled examples are available, the model can be fine-tunned to correct such mistakes. We already see some slight improvements with linear and non-linear evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Ground Truth Linear Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-linear Evaluation</head><p>Ignored Pixels (Non-stuff) Ground Plants Sky Autoregressive Clustering <ref type="figure">Fig. 11</ref>. Qualitative Results from COCO-Stuff 3 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Ground Truth Linear Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-linear Evaluation</head><p>Ignored Pixels (Non-stuff) Ground Plants Sky Autoregressive Clustering <ref type="figure">Fig. 12</ref>. Failure Cases for Autoregressive Clustering from COCO-Stuff 3 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Masked Convolutions. (a) Standard masked convolution used in autoregressive generative modeling, yielding an ordering o1. (b) A relaxed version of standard masked convolution where we have access to the current pixel at each step. (c) A simplified version of masked convolution with a reduced number of masked weights. (d) The 8 versions of the standard masked convolution to construct all of the possible raster-scan type orderings. (e) The proposed types of masked convolutions with the corresponding shifts to obtain all of the 8 desired raster-scan types orderings. F = 3 in this case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Left: Examples of positive and negative pairs for B = 2 and HW = 4. yi refers to the i-th element of the output y corresponding to the i-th image in the input batch. Right: Examples of positive pairs with possible displacements ? = {?1, 0, 1}. C.2 Autoregressive Representation Learning L AC For unsupervised representation learning objective, we maximize the infoNCE [39] as a lower bound of MI over the continuous outputs: L ARL = log e f (y l ,y l ) 1 N N m=1 e f (y l ,y m )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Variation of F : we compare different variants of the network F using different feature extractors f and autoregressive encoders gar. The decoder d is fixed.</figDesc><table><row><cell cols="2">Network F = h ? gar ? d</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">h F1 Id F2 Stem F3 Res. block Random gar 5 Res. blocks 39.3 POS POS3 28.5 38.2 56.3 5 Res. blocks 46.4 66.4 4 Res. blocks 47.9 64.5 F4 5 Res. blocks Id 35.1 63.4 F5 ResNet-18 Id 40.7 51.9 (a) |O| 2 4 8 (b) Number of orderings: we com-POS POS3 43.2?2.19 59.5?5.12 45.6?3.22 63.55?3.52 46.4 66.4 pare different sizes of the set O. For |O|= 2 and |O|= 4 , we report the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">mean and std over 4 runs using differ-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ent possible pairs and quadruples re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>spectively.</cell><cell></cell><cell></cell></row><row><cell>Orderings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Raster-Scan Zigzag Attention POS POS3</cell><cell cols="4">Sampling oi POS POS3</cell></row><row><cell>?</cell><cell>?</cell><cell>45.2</cell><cell>61.0</cell><cell cols="2">Random</cell><cell>46.4</cell><cell>66.4</cell></row><row><cell>?</cell><cell></cell><cell>47.9</cell><cell>66.3</cell><cell cols="2">No Rep.</cell><cell>48.6</cell><cell>64.8</cell></row><row><cell>?</cell><cell></cell><cell>47.8</cell><cell>66.5</cell><cell></cell><cell>Hard</cell><cell>48.9</cell><cell>65.2</cell></row><row><cell></cell><cell></cell><cell>49.3</cell><cell>65.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(d) Sampling of oi: we com-</cell></row><row><cell cols="4">(c) Attention: we add a single attention block at</cell><cell cols="4">pare different possible sampling</cell></row><row><cell cols="4">a shallow level, and change the applied masks to</cell><cell cols="4">procedures of the orderings oi</cell></row><row><cell cols="4">get the desired orderings. Output stride = 4 in this</cell><cell cols="2">during training.</cell><cell></cell></row><row><cell>instance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell>Transf.</cell><cell cols="2">POS POS3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>None</cell><cell>-</cell><cell>46.4</cell><cell>66.4</cell><cell>p</cell><cell cols="2">POS POS3</cell></row><row><cell cols="3">Photometric Col. Jittering 47.9</cell><cell>65.5</cell><cell>0</cell><cell>46.4</cell><cell>66.4</cell></row><row><cell>Geometric</cell><cell>Flip</cell><cell>46.7</cell><cell>68.0</cell><cell cols="2">0.1 47.9</cell><cell>64.7</cell></row><row><cell>Geometric</cell><cell>Rot.</cell><cell>48.5</cell><cell>68.3</cell><cell cols="2">0.2 46.9</cell><cell>65.1</cell></row><row><cell>Geo. &amp; Pho.</cell><cell>All</cell><cell cols="2">48.5 68.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(f) Dropout: we inspect</cell></row><row><cell cols="4">(e) Transformations: we apply a given trans-</cell><cell cols="4">the addition of dropout</cell></row><row><cell cols="4">formation to the inputs of the second forward</cell><cell cols="4">to the inner activations</cell></row><row><cell cols="3">pass during a single training iteration.</cell><cell></cell><cell cols="3">of a residual block.</cell></row></table><note>Table 1. AC Ablations. Ablations studies conducted on Potsdam (POS) and Potsdam-3 (POS3) for Autoregressive Clusterings. We show the pixel classification accuracy (%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparing ARL and AC. We compare ARL and AC on a clustering task (left). And investigate the quality of the learned representations by freezing the trained model, and reporting the test Acc obtained when training a linear (center) and non-linear (right) functions trained on the labeled training examples.</figDesc><table><row><cell cols="2">Clustering</cell><cell></cell><cell cols="3">Linear Evaluation</cell><cell cols="3">Non-Linear Evaluation</cell></row><row><cell>Method</cell><cell cols="2">POS POS3</cell><cell cols="3">Method POS POS3</cell><cell cols="3">Method POS POS3</cell></row><row><cell cols="3">Random CNN 28.5 AC 46.4 66.4 38.2</cell><cell>AC</cell><cell cols="2">23.7 41.4</cell><cell>AC</cell><cell cols="2">68.0 81.8</cell></row><row><cell>ARL</cell><cell>45.1</cell><cell>57.1</cell><cell>ARL</cell><cell>23.7</cell><cell>38.5</cell><cell>ARL</cell><cell>47.6</cell><cell>63.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">COCO-Stuff-3 COCO-Stuff</cell><cell cols="2">Potsdam-3 Potsdam</cell><cell></cell></row><row><cell cols="2">Random CNN</cell><cell></cell><cell>37.3</cell><cell></cell><cell>19.4</cell><cell>38.2</cell><cell>28.3</cell><cell></cell></row><row><cell cols="2">K-means [42]</cell><cell></cell><cell>52.2</cell><cell></cell><cell>14.1</cell><cell>45.7</cell><cell>35.3</cell><cell></cell></row><row><cell cols="2">SIFT [35]</cell><cell></cell><cell>38.1</cell><cell></cell><cell>20.2</cell><cell>38.2</cell><cell>28.5</cell><cell></cell></row><row><cell cols="3">Doersch 2015 [11]</cell><cell>47.5</cell><cell></cell><cell>23.1</cell><cell>49.6</cell><cell>37.2</cell><cell></cell></row><row><cell cols="2">Isola 2016 [27]</cell><cell></cell><cell>54.0</cell><cell></cell><cell>24.3</cell><cell>63.9</cell><cell>44.9</cell><cell></cell></row><row><cell cols="3">DeepCluster 2018 [6]</cell><cell>41.6</cell><cell></cell><cell>19.9</cell><cell>41.7</cell><cell>29.2</cell><cell></cell></row><row><cell cols="2">IIC 2019 [28]</cell><cell></cell><cell>72.3</cell><cell></cell><cell>27.7</cell><cell>65.1</cell><cell>45.4</cell><cell></cell></row><row><cell>AC</cell><cell></cell><cell></cell><cell>72.9</cell><cell></cell><cell>30.8</cell><cell>66.5</cell><cell>49.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Unsupervised image segmentation. Comparison of AC with state-of- the-art methods on unsupervised segmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Convolutional Stem for an output stride of 2. For an output stride of 4, we use a Conv 3 ? 3 with stride of 2, yielding an output of size 64 ? H/4 ? W/4. For the fully autoregressive case, the Batch Norm and Max pool are omitted, and the Max pool is replaced with a strided masked convolution.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Architecture of residual blocks, for residual blocks used in the autoregressive encoder gar, normal convolutions are replaced with masked ones.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>For the transformations applied in Paper Section 4.1, we used color jittering where we randomly change the brightness, hue, contrast and saturation of an image up to 10% for photometric transformations. For geometric transformation, we apply random horizontal flips and random rotations by multiples of 90 degrees.</figDesc><table><row><cell>Parameter</cell><cell cols="4">COCO-stuff 3 COCO-stuff Potsdam-3 Potsdam</cell></row><row><cell>LR</cell><cell>4.10 ?5</cell><cell>6.10 ?6</cell><cell>10 ?6</cell><cell>4.10 ?5</cell></row><row><cell>Batch size</cell><cell>60</cell><cell>60</cell><cell>30</cell><cell>30</cell></row><row><cell>Crop size</cell><cell>128?128</cell><cell>128?128</cell><cell>200?200</cell><cell>200?200</cell></row><row><cell>Rescale factor</cell><cell>0.33</cell><cell>0.33</cell><cell cols="2">No rescal. No rescal.</cell></row><row><cell>Output stride</cell><cell>4</cell><cell>4</cell><cell>2</cell><cell>2</cell></row><row><cell>Num. of displacements for LAC</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>Attention</cell><cell>False</cell><cell>False</cell><cell>True</cell><cell>True</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Hyperparameters used for training per dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Y. Ouali is supported by Randstad corporate research chair in collaboration with CentraleSup?lec. We would also like to thank Saclay-IA plateform of Universit? Paris-Saclay and M?socentre computing center of CentraleSup?lec and?cole Normale Sup?rieure Paris-Saclay for providing the computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-organizing neural network that discovers surfaces in random-dot stereograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6356</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emergence of object segmentation in perturbed generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised object segmentation by redrawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arti?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pixelsnail: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10069</idno>
		<title level="m">Deep k-means: Jointly clustering with kmeans and learning representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning robust representations via multi-view information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07017</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Use of the stair vision library within the isprs 2d semantic labeling benchmark (vaihingen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Direct clustering of a data matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">337</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">k-anmi: A mutual information based clustering algorithm for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="233" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<title level="m">Learning visual groups from co-occurrences in space and time</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised image segmentation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised image segmentation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Dy, J., Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<title level="m">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
