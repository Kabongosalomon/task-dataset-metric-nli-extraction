<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep One-Class Classification via Interpolated Gaussian Descriptor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep One-Class Classification via Interpolated Gaussian Descriptor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-class classification (OCC) aims to learn an effective data description to enclose all normal training samples and detect anomalies based on the deviation from the data description. Current state-of-the-art OCC models learn a compact normality description by hyper-sphere minimisation, but they often suffer from overfitting the training data, especially when the training set is small or contaminated with anomalous samples. To address this issue, we introduce the interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples. The Gaussian anomaly classifier differentiates the training samples based on their distance to the Gaussian centre and the standard deviation of these distances, offering the model a discriminability w.r.t. the given samples during training. The adversarial interpolation is enforced to consistently learn a smooth Gaussian descriptor, even when the training data is small or contaminated with anomalous samples. This enables our model to learn the data description based on the representative normal samples rather than fringe or anomalous samples, resulting in significantly improved normality description. In extensive experiments on diverse popular benchmarks, including MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves better detection accuracy than current state-of-the-art models. IGD also shows better robustness in problems with small or contaminated training sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Anomaly detection and segmentation are critical tasks in many real-world applications, such as the identification of defects on industry objects <ref type="bibr" target="#b4">(Bergmann et al. 2019)</ref> or abnormalities from medical images <ref type="bibr" target="#b47">(Schlegl et al. 2017</ref><ref type="bibr" target="#b46">(Schlegl et al. , 2019</ref>. Given that most of the training sets available for this task contain only normal images, existing methods are typically formulated as one-class classifiers (OCC) <ref type="bibr" target="#b59">(Venkataramanan et al. 2019;</ref><ref type="bibr" target="#b41">Ruff et al. 2018)</ref>. OCCs aim to first learn a data description of normal samples in the training set and then use a criterion (e.g., distance to the one-class centre ) to detect and localise anomalies in test samples.  , and our proposed IGD trained with the CIFAR10 training set contaminated with 1%, 5% and 10% of anomalous samples (left), and small training sets, consisting of 20%, 60%, and 100% of the CIFAR10 training set (right).</p><p>State-of-the-art (SOTA) OCC models are trained by minimising the radius of a hyper-sphere to enclose all training samples in the representation space <ref type="bibr" target="#b36">Perera and Patel 2019;</ref><ref type="bibr" target="#b42">Ruff et al. 2020)</ref>. To avoid catastrophic collapse, where all training samples are projected to a single point in the representation space, these OCC models fix the hyper-sphere centre and remove the bias terms from the model. Even though these SOTA OCC models show accurate anomaly detection results in several benchmarks, they can overfit the training data, particularly when the training set is small or contaminated with anomalous samples, as shown by the results of DSVDD  in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In this paper, we introduce the interpolated Gaussian descriptor (IGD) method to address the overfitting issue presented in SOTA OCC models. IGD is based on a oneclass Gaussian anomaly classifier modelled with adversarially interpolated training samples. The classifier is trained to build a normality description to discriminate training samples based on their distance to the Gaussian centre and the standard deviation of these distances. The smoothness of the normality description is enforced by the adversarial interpolation of the training samples that constrains the training of IGD to be based on representative normal samples rather than fringe or anomalous samples. This allows the normality description of IGD to be more robust than the SOTA OCC models, particularly when the training set is small or contaminated with anomalous samples, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> and t-SNE results in appendix.</p><p>In summary, our paper makes the following contributions:</p><p>? One novel OCC model that targets the learning of an effective normality description based on representative normal samples rather than fringe or anomalous samples, resulting in an improved anomaly classifier, compared with the SOTA; ? One new OCC optimisation approach based on a theoretically sound derivation of the expectation-maximisation (EM) algorithm that optimises a Gaussian anomaly classifier constrained by adversarially interpolated training samples and multi-scale structural and non-structural image reconstruction to enforce a smooth normality description; and ? One new OCC benchmark to assess the robustness of anomaly detectors to training sets that are small or contaminated with anomalous samples. Extensive empirical results on six popular anomaly detection benchmarks for semantic anomaly detection, industrial defect detection, and malignant lesion detection show that our model IGD can generalise well across these diverse application domains and perform consistently better than current SOTA detectors. We also show that IGD is more robust than current OCC approaches when dealing with small and contaminated training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Unsupervised anomaly detection (UAD) is generally solved with OCCs <ref type="bibr" target="#b25">(Li et al. 2021;</ref><ref type="bibr" target="#b58">Tian et al. 2021d</ref><ref type="bibr" target="#b56">Tian et al. , 2020</ref><ref type="bibr" target="#b57">Tian et al. , 2021c</ref><ref type="bibr" target="#b41">Ruff et al. 2018;</ref><ref type="bibr" target="#b6">Bergmann et al. 2020;</ref><ref type="bibr" target="#b35">Perera, Nallapati, and Xiang 2019;</ref><ref type="bibr" target="#b45">Salehi et al. 2021;</ref><ref type="bibr" target="#b61">Wang et al. 2021;</ref><ref type="bibr" target="#b54">Tian et al. 2021a;</ref><ref type="bibr" target="#b4">Bergman and Hoshen 2020;</ref><ref type="bibr" target="#b16">Golan and El-Yaniv 2018;</ref><ref type="bibr" target="#b12">Defard et al. 2021;</ref><ref type="bibr" target="#b67">Zavrtanik et al. 2021;</ref><ref type="bibr" target="#b60">Wang et al. 2016)</ref>. A representative OCC model is DSVDD , which forces normal image features to be inside a hyper-sphere with a pre-defined centre and a radius that is minimised to include all training images. Then, test images that fall inside the hyper-sphere are classified as normal, and the ones outside are anomalous. Although powerful, the hard boundary of SVDD can cause the model to overfit the training data -this problem was tackled with a soft-boundary SVDD , but it can still overfit given that it lacks enough generalisation constraints. OCC methods can also rely on generative models, such as generative adversarial network (GAN) or Auto-encoder (AE). In <ref type="bibr" target="#b35">(Perera, Nallapati, and Xiang 2019)</ref>, a GAN is trained to produce normal samples, and its discriminator is used to detect anomalies, but the complex training process of GANs represents a disadvantage of this approach. An AE <ref type="bibr" target="#b21">(Ionescu et al. 2019;</ref><ref type="bibr" target="#b17">Gong et al. 2019;</ref><ref type="bibr" target="#b32">Nguyen and Meunier 2019;</ref><ref type="bibr" target="#b43">Sabokrou et al. 2017</ref><ref type="bibr" target="#b44">Sabokrou et al. , 2018</ref><ref type="bibr" target="#b59">Venkataramanan et al. 2019</ref>) is trained to reconstruct normal data, and the anomaly score is defined as the reconstruction error between the input and reconstructed images. AE approaches depend on the MSE reconstruction loss, which does not work well for structural anomalies. Alternatively, single-scale SSIM loss <ref type="bibr" target="#b5">(Bergmann et al. 2018</ref>) tends to work well for structural anomalies of a specific size, but it may work poorly for non-structural anomalies and structural anomalies outside that specific size.</p><p>A more detailed review of these methods can be found in <ref type="bibr" target="#b33">(Pang et al. 2021</ref>).</p><p>An important aspect of current UAD approaches is their dependence on pre-trained models to produce SOTA results. UAD models can be pre-trained on ImageNet <ref type="bibr" target="#b59">(Venkataramanan et al. 2019;</ref><ref type="bibr" target="#b6">Bergmann et al. 2020)</ref> or self-supervised tasks <ref type="bibr" target="#b16">(Golan and El-Yaniv 2018;</ref><ref type="bibr" target="#b4">Bergman and Hoshen 2020)</ref>. To allow a fair comparison with current UAD methods, we pre-train IGD with self-supervision and ImageNet.</p><p>Unsupervised anomaly localisation targets the segmentation of anomalous image pixels or patches, containing, for example, lesions in medical images <ref type="bibr" target="#b26">(Li et al. 2019a</ref>), defects in industry images <ref type="bibr" target="#b4">(Bergmann et al. 2019</ref>, or road anomalies in traffic images <ref type="bibr" target="#b34">(Pathak, Sharang, and Mukerjee 2015;</ref><ref type="bibr" target="#b55">Tian et al. 2021b</ref>). The main idea explored is based on extending the image based OCC to a pixel-based OCC, where testing produces a pixel-wise anomaly score map <ref type="bibr" target="#b3">(Baur et al. 2018;</ref><ref type="bibr" target="#b5">Bergmann et al. 2018)</ref>. In general, methods that can localise anomalies <ref type="bibr" target="#b59">(Venkataramanan et al. 2019;</ref><ref type="bibr" target="#b6">Bergmann et al. 2020</ref>) are tuned to particular anomaly sizes and structure, which can cause then to miss anomalies outside that range of sizes and structure. To avoid this issue, we design IGD to detect multi-scale structural and nonstructural anomalies to improve the anomaly localisation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We denote the training set containing only normal samples by </p><formula xml:id="formula_0">D = {x i } |D| i=1 , where x ? X ? R W</formula><formula xml:id="formula_1">= {(x i , y i , b (yi) i } |T | i=1 , where y i ? Y = {0,</formula><p>1} (0 denotes a normal and 1 denotes an anomalous image), the segmentation map with the anomaly is denoted by b (yi) i ? {0, 1} W ?H (i.e., a pixel-wise anomaly map for image x i ) if y i = 1, and b</p><formula xml:id="formula_2">(yi) i = 0 W ?H if y i = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolated Gaussian Descriptor (IGD)</head><p>As depicted in <ref type="figure">Fig. 2</ref>, the IGD model is represented by the general classifier p ? (y = 0|x, P X ) that consists of an encoder z = f ? (x) that transforms a training sample from the image space X to a representation space Z ? R Z , a Gaussian anomaly classifier p ? (y = 0|?, x) ? [0, 1] that takes the normal image distribution parameter ? and image x to estimate the probability that it is normal, a decoderx = g ? (z) that reconstructs an image from the representation space, and a critic module ? = d ? (g ? (?z 1 + (1 ? ?)z 2 )) that predicts the interpolation constraint parameter ? ? [0, 1], with z 1 , z 2 obtained from the encoder f ? (.). The IGD parameter ? ? ? represents all module parameters {?, ?, ?} and is estimated with maximum likelihood estimation (MLE):</p><formula xml:id="formula_3">? * = arg max ? 1 |D| xi?D log p ? (y i = 0|x i , P X ). (1)</formula><p>We train the one-class classifier in (1) using an EM optimisation <ref type="bibr" target="#b14">(Dempster and Others 1977)</ref>, where the mean and  <ref type="figure">Figure 2</ref>: Our IGD consists of an encoder that transforms image x into representation z, a decoder to reconstruct the image (trained with MS-SSIM and MAE losses), a Gaussian anomaly classifier trained to push the normal image representation close to the centre of the estimated normal image distribution (denoted by a Gaussian with mean ? and standard deviation ?), and a critic module that constrains the likelihood maximisation by predicting the interpolation coefficient ? that produces a convex combination of training sample representations. Note that critic is a module similar to a GAN discriminator. standard deviation of the normal image distribution are estimated during the E-step, instead of being explicitly optimised , reducing the risk of overfitting. To encourage the M-step to learn an effective normality description (such that the optimisation is robust to small and contaminated training sets), we add an adversarial interpolation constraint to enforce linear combinations of normal image representations to belong to the normal distribution. We further increase the robustness of IGD to overfitting by constraining the optimisation of the M-step to enforce accurate image reconstruction from its representation. Below, we provide more details about the training process.</p><p>To formulate the EM optimisation, we re-write the loglikelihood in (1) as</p><formula xml:id="formula_4">log p ? (y i = 0|x i , P X ) = ELBO (q, ?) + KL[q(?)||p ? (?|P X )].<label>(2)</label></formula><p>with ? ? W ? R Z ? R denoting the latent variables (mean and standard deviation) that describe the distribution of normal image representations (defined in more detail below). In (2), we remove the conditional dependence of p ? (?|P X ) on y i = 0 and x i because ? is a variable for the whole training distribution defined as</p><formula xml:id="formula_5">p ? (?|P X ) = ? a ( ?(1) ? ? x 2 )? a (?(2) ? ? x ),<label>(3)</label></formula><p>where ? a (b) = 1 |a| ? ? exp ?(b/a) 2 (a ? 0 approximates a Dirac delta function, and a ? ? approximates a uniform function),</p><formula xml:id="formula_6">? x = E x?P X [f ? (x)] and ? 2 x = E x?P X [ f ? (x)? ? x 2 2</formula><p>], with f ? (.) representing the encoder; and in (2), we also have</p><formula xml:id="formula_7">ELBO (q, ?) = E q(?) [log p ? (y i = 0, ?|x i , P X )] ? E q(?) [log q(?)],<label>(4)</label></formula><p>where KL[?] denotes the Kullback-Leibler divergence, and q(?) represents the variational distribution that approximates p ? (?|P X ), defined in <ref type="formula" target="#formula_5">(3)</ref>.</p><p>The E-step of the EM optimisation zeroes the KL divergence in (2) by setting q(?) = p ? old (?|P X ), where ? old represents the previous EM iteration parameter value. In practice, the E-step sets ?(1) to ? x and ?(2) to ? x , defined in (3). Next, the M-step maximises ELBO in (4), with:</p><formula xml:id="formula_8">? = arg max ? 1 |D| xi?D E q(?) [ log p ? (y i = 0|?, x i ) + log p ? (?|P X ) ,<label>(5)</label></formula><p>where E q(?) [log(q(?))] is removed from ELBO because it depends only on the previous iteration parameter ? old , q(?) is defined in the E-step above, and the conditional dependence of p ? (y = 0|?, x i ) on P X is removed because the information from that distribution is summarised in ?. Therefore, (5) has two components: 1) the classification term represented by the Gaussian anomaly classifier p ? (y = 0|?,</p><formula xml:id="formula_9">x i ) = exp ? f ? (x)??(1) 2 2 ?(2) 2</formula><p>, with mean ?(1) and standard deviation ?(2); and 2) p ? (?|P X ) defined in (3), which approximates a uniform distribution to prevent the confirmation bias of the estimated ? x and ? x from (3). To promote an effective normality description of IGD, we constrain the M-step (5) as follows:</p><formula xml:id="formula_10">max ? 1 |D| xi?D E q(?) [log(p ? (y = 0|?, x i ))] s.t. d (x i , ?) = 0, ?x i ? D, f,g (x i , ?) = 0, ?x i ? D,<label>(6)</label></formula><p>where d (.) is a constraint, defined in (10), to enforce the adversarial linear interpolation of normal image representations to belong to the normal representation distribution, and f,g (.) is a constraint, defined in (11), to enforce accurate structural and non-structural multi-scale image reconstruction. Note that the maximisation in (6) constrains the optimisation in (5), which means that we are maximising a lower bound to the original M-step. Using Lagrange multipliers, the optimisation in (6) is reformulated to minimise the following loss function:</p><formula xml:id="formula_11">(?, ?, D) = 1 |D| |D| i=1 h (xi, ?, ?) + ?1 d (xi, ?) + ?2 f,g (xi, ?),<label>(7)</label></formula><p>where <ref type="formula" target="#formula_8">(5)</ref>, and ? 1 , ? 2 denoting the Lagrange multipliers. The interpolation constrain d (.) in <ref type="formula" target="#formula_10">(6)</ref> and <ref type="formula" target="#formula_11">(7)</ref> regularises the training by linearly interpolating the representations from training images, and estimating the interpolation coefficient with the critic network <ref type="bibr" target="#b7">(Berthelot et al. 2018)</ref>. This interpolation constrains the normal image distribution denser in the representation space, reducing the likelihood that anomalous representations may land in the same region of the representation space occupied by normal samples. Unlike Mix-up <ref type="bibr" target="#b68">(Zhang et al. 2017)</ref>, our interpolation constraint is a self-supervised method that does not rely on data augmentation on the input space and does not interpolate training labels, making it more adequate for our problem because it enforces a compact and dense distribution of normal samples to be estimated for the Gaussian anomaly classifier. The critic network is represented b?</p><formula xml:id="formula_12">h (x, ?, ?) = 1 ? p ? (y = 0|?, x) = p ? (y = 1|?, x), (8) with p ? (y = 0|?, x) defined in</formula><formula xml:id="formula_13">? = d ? (x ? ) ,<label>(9)</label></formula><formula xml:id="formula_14">wherex ? = g ? (?z 1 + (1 ? ?)z 2 ) represents the recon- struction of the interpolation of z 1 = f ? (x 1 ) and z 2 = f ? (x 2 ) (with ? ? U(0, 0.5), x 1 , x 2 ? D, x 1 = x 2 , and</formula><p>U denoting a uniform distribution ) <ref type="bibr" target="#b7">(Berthelot et al. 2018)</ref>, and g ? (.) denotes the decoder. The goal of the critic network d ? (.) is to predict the interpolation coefficient ?. The critic network in <ref type="formula" target="#formula_13">(9)</ref> is similar to the discriminator in GAN (Goodfellow et al. 2014), and relies on the following adversarial loss to be optimised <ref type="bibr" target="#b7">(Berthelot et al. 2018</ref>)</p><formula xml:id="formula_15">d (x, ?) = d ? (x ? ) ? ? 2 2 + d ? (x ? ) 2 2 ,<label>(10)</label></formula><p>wherex ? is defined in (9), andx ? = ?x + (1 ? ?)x, with ? ? U(0, 1) andx denoting a reconstruction of x by the auto-encoder. The first term of (10) minimises the critic's prediction error for ? and the second term regularises the training to ensure that the critic predicts? = 0 when the original image is interpolated with its own reconstruction in the image space X .</p><p>The image reconstruction constrain f,g (.) in <ref type="formula" target="#formula_10">(6)</ref> and <ref type="formula" target="#formula_11">(7)</ref> is defined as</p><formula xml:id="formula_16">f,g (x, ?) = r (x,x, ?) + ? 3 d ? (x ? ) 2 2 ,<label>(11)</label></formula><p>wherex is a reconstruction of x by the auto-encoder, with the image reconstruction loss r (.) to be defined below in (12), and ? 3 is a hyperparameter to weight the regularisation term. This regularisation fools the critic to output? = 0 for interpolated embeddings, independently of ?, following standard adversarial training <ref type="bibr" target="#b18">(Goodfellow et al. 2014</ref>). In (11), we also have</p><formula xml:id="formula_17">r (x,x, ?) = ??? ?|x(?) ?x(?)| + (1 ? ?) 1 ? m x(?),x(?) ,<label>(12)</label></formula><p>with ? denoting the image lattice, ? ? [0, 1], |x(?) ?x(?)| representing the MAE loss, and m(x(?),x(?)) ? [0, 1] being the MS-SSIM score <ref type="bibr" target="#b63">(Wang, Simoncelli, and Bovik 2003)</ref>, with larger values indicating higher similarity between patches ? ? ? of the original and reconstructed images. Please see details on how to compute the MS-SSIM score in the Supp. Material. The loss in <ref type="formula" target="#formula_11">(7)</ref> is used to train two models (see 'Global and Local IGD Models' section in the Supp. Material). A global model that works on the whole image x, and a local model that works on image patches</p><formula xml:id="formula_18">x (L) (?) ? X (L) ? R W (L) ?H (L) ?3 , with W (L) &lt; W and H (L) &lt; H, centred at pixel ? ? ? (? is the image lattice)</formula><p>. During inference, the results from the global and local models are combined to produce multi-scale anomaly detection and localisation. Please see the Supp. Material for a visual example of the results produced by the global and local models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Guarantees</head><p>IGD maximises a constrained ELBO (q, ?) in (6) rather than maximising p ? (y = 0|x, P X ) in (1). Using Theorem 1 in <ref type="bibr" target="#b14">(Dempster and Others 1977)</ref>, Lemma 3 demonstrates the correctness of IGD, where an increase to the constrained ELBO (q, ?) implies an increase to p ? (y = 0|x, P X ). Using Theorem 2 in <ref type="bibr" target="#b14">(Dempster and Others 1977)</ref>, Lemma 4 proves the convergence conditions of IGD.</p><p>Lemma 1. Assuming that the maximisation of the constrained ELBO in (6) produces ? that makes</p><formula xml:id="formula_19">E q(?) [log p ? (y = 0, ?|x, P X )] ? E q(?) [log p ? old (y = 0, ?|x, P X )], we have that (log p ? (y = 0|x, P X ) ? log p ? old (y = 0|x, P X )) is lower bounded by E q(?) [log p ? (y = 0, ?|x, P X )] ? E q(?) [log p ? old (y = 0, ?|x, P X )] ? 0, with q(?) = p ? old (?|P X ).</formula><p>Proof. Please see proof in Supp. Material.</p><p>Lemma 2. Assume that {? (e) } +? e=1 denotes the sequence of trained model parameters from the constrained optimisation of ELBO in (6) such that: 1) the sequence {log p ? (e) (y = 0|x, P X )} +? e=1 is bounded above, and 2)</p><formula xml:id="formula_20">E q(?) [log p ? (e+1) (y = 0, ?|x, P X )] ? E q(?) [log p ? (e) (y = 0, ?|x, P X )] ? ? ? (e+1) ? ? (e) ? (e+1) ? ? (e)</formula><p>, for ? &gt; 0 and all e ? 1, and</p><formula xml:id="formula_21">q(?) = p ? (e) (?|P X ). Then {? (e) } +? e=1 converges to some ? ? ?.</formula><p>Proof. Please see proof in Supp. Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference</head><p>The global and local IGD models are trained separately (see <ref type="figure" target="#fig_0">Fig. S1</ref>), following the EM optimisation, where the E-step estimates the the latent variable ? in (3), and the M-step minimises the loss in (7) to obtain ? .</p><p>During inference, anomaly detection is performed by combining the global and local IGD anomaly scores for a testing image x as in:</p><formula xml:id="formula_22">s(x) = s (G) (x) + s (L) (x).<label>(13)</label></formula><p>Figure 3: Example of the multi-scale structural and non-structural anomaly localisation result for an MVTec AD <ref type="bibr" target="#b4">(Bergmann et al. 2019</ref>) image, using both the local and global IGD models. The global model tends to produce smooth results but with some mistakes, while the local model produces jagged results, but without the global mistakes, so by combining the two results, we obtain a smooth and correct anomaly heatmap.</p><p>The global score in <ref type="formula" target="#formula_5">(13)</ref> is defined as</p><formula xml:id="formula_23">s (G) (x) = (G) r (x,x, ? * ) + (G) h (x, ? * ),<label>(14)</label></formula><p>where (G) r (.) denotes the reconstruction loss from <ref type="formula" target="#formula_4">(12)</ref> and</p><formula xml:id="formula_24">(G)</formula><p>h (.) denotes the Gaussian anomaly classification loss from (8) (both computed with the global IGD model using the whole images), andx is the reconstruction of x produced by the auto-encoder. The local score in <ref type="formula" target="#formula_5">(13)</ref> is defined as</p><formula xml:id="formula_25">s (L) (x) = max ??? (L) r x (L) (?),x (L) (?), ? * + (L) h x (L) (?), ? * ,<label>(15)</label></formula><p>where (L) r (.) and (L) h (.) are the reconstruction and Gaussian anomaly classification losses computed from the local model, with x (L) (?) denoting an image patch of size W (L) ? H (L) ? 3 at pixel ? ? ?. The use of max pooling of the local scores in (15) facilitates detection of images that contain anomalies covering a small region of the image. Anomaly localisation is computed for each pixel ? ? ? to produce a local score</p><formula xml:id="formula_26">l(x(?)) = (G) r x(?),x(?), ? * + (L) r x (L) (?),x (L) (?), ? * ,<label>(16)</label></formula><formula xml:id="formula_27">with (G) r x(?),x(?), ? * = ? x(?) ?x(?) + (1 ? ?) 1 ? m (G) x(?),x(?) ,<label>(17)</label></formula><p>where ? and m (G) (.) are defined in <ref type="formula" target="#formula_4">(12)</ref> andx is a reconstruction of x produced by the global IGD model. The <ref type="formula" target="#formula_10">(16)</ref> is similarly defined using the local IGD model. Thus, the anomaly localisation final map is a heatmap with high values representing regions that are likely to contain anomalies, as displayed in 'Global and Local IGD Models' section in the Supp. Material.</p><formula xml:id="formula_28">(L) r x (L) (?),x (L) (?), ? * in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Evaluation Metric</head><p>Datasets: We use four computer vision and two medical image datasets to evaluate our methods. The computer vision datasets are MNIST (LeCun, Cortes, and Burges 2010), Fashion MNIST <ref type="bibr" target="#b65">(Xiao, Rasul, and Vollgraf 2017)</ref>, CIFAR10 <ref type="bibr" target="#b23">(Krizhevsky, Nair, and Hinton 2014)</ref> and MVTec AD <ref type="bibr" target="#b4">(Bergmann et al. 2019)</ref>; and the medical image datasets are Hyper-Kvasir <ref type="bibr" target="#b9">(Borgli and et al. 2020</ref>) and LAG <ref type="bibr" target="#b27">(Li et al. 2019b)</ref>. MNIST, Fashion MNIST and CIFAR10 have been widely used as benchmarks for image anomaly detection, and we follow the same experimental protocol as described in . CIFAR10 contains 60,000 images with 10 classes. MNIST and Fashion MNIST contain 70,000 images with 10 classes of handwritten digits and fashion products, respectively. MVTec AD <ref type="bibr" target="#b4">(Bergmann et al. 2019</ref>) contains 5,354 high-resolution real-world images of 15 different industry object and textures. The normal class of MVTec AD is formed by 3,629 training and 467 testing images without defects. The anomalous class has more than 70 categories of defects (such as dents, structural fails, contamination, etc.) and contains 1,258 testing images. MVTec AD provides pixel-wise ground truth annotations for all anomalies in the testing images, allowing the evaluation of anomaly detection and localisation. We also tested our method on two publicly available medical datasets: Hyper-Kvasir <ref type="bibr" target="#b9">(Borgli and et al. 2020</ref>) and LAG <ref type="bibr" target="#b27">(Li et al. 2019b</ref>) for polyp and glaucoma detection, respectively. For Hyper-Kvasir, we has 1,600 normal images without polyps in the training set and 500 in the testing set; and 1,000 abnormal images containing polyps in the testing set. For LAG, we have 2,343 normal images without glaucoma in the training set; and 800 normal images and 1,711 abnormal images with glaucoma for testing.</p><p>Evaluation: For anomaly detection, we assess performance with the area under the receiver operating characteristic curve (AUC) and classification accuracy. On MNIST, Fashion MNIST and CIFAR10, we use the same protocol as other methods in Tab. 1, where training uses a single class as the normal data, with the nine remaining classes denoting as semantically anomalous samples, and inference relies on a non-augmented test image. We report the mean AUC over the 10 classes for the above three data sets. On MVTec AD <ref type="bibr" target="#b59">Venkataramanan et al. 2019)</ref>, we evaluate anomaly detection with mean AUC and accuracy. Follow previous works <ref type="bibr">(Tian et al. 2021d,a)</ref>, we evaluate the methods using AUC for the Hyper-Kvasir and LAG. For anomaly localisation, we follow <ref type="bibr" target="#b59">(Venkataramanan et al. 2019</ref>) and compute the mean pixel-level AUC between the generated heatmap and the ground truth segmentation map for each anomalous image in the testing set of MVTec AD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implement our framework using Pytorch. The model was trained with Adam optimiser using a learning rate of 0.0001, weight decay of 10 ?6 , batch size of 64 images, 256 epochs for all dataset. We defined the representation space produced by the encoder to have Z = 128 dimensions. Following <ref type="bibr" target="#b15">(Godard, Mac Aodha, and Brostow 2017)</ref>,  we set ? = 0.15 to balance the contribution of MAE and MS-SSIM losses in <ref type="formula" target="#formula_4">(12)</ref> and <ref type="formula" target="#formula_11">(17)</ref>. We set ? 1 = ? 2 = 1 in <ref type="formula" target="#formula_11">(7)</ref> and ? 3 = 0.1 in (11), based on cross validation experiments. We use Resnet18 and its reverse architecture as the encoder and decoder for both the global and local IGD models. When computing the accuracy of anomaly detection in MVTec AD, the threshold of the anomaly detection score s(x) in (13) (to classify an image as anomalous) is set to 0.5 <ref type="bibr" target="#b59">(Venkataramanan et al. 2019</ref>). To enable a fair comparison between our method and previous approaches in the field <ref type="bibr" target="#b59">Venkataramanan et al. 2019;</ref><ref type="bibr" target="#b4">Bergman and Hoshen 2020;</ref><ref type="bibr" target="#b16">Golan and El-Yaniv 2018)</ref>, we pre-train the encoders for the global and local IGD models either with self-supervised learning (SSL) <ref type="bibr" target="#b10">(Chen et al. 2020)</ref> or ImageNet knowledge distillation (KD) <ref type="bibr">Gou et al. 2020)</ref>. For this SSL pre-training, we use the SGD optimiser with a learning rate of 0.01, weight decay 10 ?1 , batch size of 32, and 2,000 epochs. Once we obtain the pre-trained encoder with SSL, we remove the MLP layer and attach a linear layer to the backbone with fixed parameters. Note that this SSL is trained from scratch. In contrast to the vanilla self-supervised learning <ref type="bibr" target="#b10">(Chen et al. 2020)</ref> suggesting large batch size, we notice that a medium batch size yields significantly better performance for unsupervised anomaly detection. For the ImageNet KD pre-training, we minimise the 2 norm between the 512-dimensional feature vector output from encoder and an intermediate layer of the ImageNet pretrained ResNet18 with the same 512-dimensional features. For this ImageNet KD pre-training, we use the Adam optimiser with a learning rate of 0.0001, weight decay 10 ?5 , batch size of 64, and 50,000 iterations. Once we obtain the pre-trained encoder of KD, we fix the network parameters and attach a linear layer to reduce the dimensionality of the feature space to 128. <ref type="table" target="#tab_3">Table 1</ref> compares the unsupervised anomaly detection mean AUC testing results between our method and the current SOTA on MNIST, Fashion MNIST and CIFAR10. The rows labelled as 'Scratch' show results of models that were not pre-trained, and the ones with 'SSL' display results from models using self-supervised learning method (Golan and El-Yaniv 2018; Bergman and Hoshen 2020). The ones with 'ImageNet' show results from models that use ImageNet KD pre-training <ref type="bibr" target="#b59">(Venkataramanan et al. 2019;</ref><ref type="bibr" target="#b6">Bergmann et al. 2020)</ref>. Our proposed IGD outperforms current SOTA methods for the majority of pre-training methods on all three datasets. Please see additional results in the Supp. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on MNIST, Fashion MNIST and CIFAR10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on MVTec AD</head><p>We report the results, based on SSL and ImageNet KD pretrained models, for both anomaly detection (Tab. 2) and localisation (Tab. 3) on MVTec AD, which contains real-world images of industry objects and textures containing different types of anomalies. Following <ref type="bibr" target="#b59">(Venkataramanan et al. 2019)</ref> the score threshold is set to 0.5 for calculating the mean accuracy of anomaly detection. For anomaly detection, our method produces the best accuracy (at least 2% better than previous SOTA) and AUC (at least 5% better than previous SOTA) results independently of the pre-training technique. For anomaly localisation, we compare our method and the SOTA using the mean pixel-level AUC of all anomalous images in the testing set of MVTec AD. Notice that our method with ImageNet and SSL pre-training are better than the previous SOTA CAVGA-R u <ref type="bibr" target="#b59">(Venkataramanan et al. 2019</ref>) by 2% and 4%, respectively. <ref type="figure" target="#fig_1">Fig. 4</ref> shows anomaly localisation results on MVTec AD images, where red regions in the heatmap indicate higher anomaly probability. From this results, we can see that our approach can localise anomalous regions of different sizes and structures from different object  <ref type="bibr" target="#b5">(Bergmann et al. 2018)</ref> 0.630 DAE <ref type="bibr" target="#b19">(Hadsell et al. 2006)</ref> 0.710 AnoGAN <ref type="bibr" target="#b47">(Schlegl et al. 2017)</ref> 0.550 ?-VAE u <ref type="bibr" target="#b13">(Dehaene et al. 2020)</ref> 0.770 LSA <ref type="bibr" target="#b0">(Abati et al. 2019)</ref> 0.730 CAVGA-D u <ref type="bibr" target="#b59">(Venkataramanan et al. 2019)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Medical Datasets</head><p>To show that our method can generalise to other domains, we evaluate our approach on two public medical datasets -Hyper-Kvasir for polyp detection and LAG for glaucoma detection. As shown in Tab. 4, our SSL and Ima-geNet based results achieve the best AUC results on both datasets. Our methods surpass the recent proposed CAVGA-R u <ref type="bibr" target="#b59">(Venkataramanan et al. 2019</ref>) on both datasets by a minimum 0.9% and maximum 3.8%. Also, our model performs better compared to the anomaly detector specifically designed for medical data, such as f-anogan <ref type="bibr" target="#b46">(Schlegl et al. 2019</ref>) and ADGAN . We show qualitative polyp segmentation results in the Supp. Material. The abnormalities in medical data (i.e., colon polyps, glaucoma)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Hyper-Kvasir LAG DAE <ref type="bibr" target="#b31">(Masci and et al. 2011)</ref> 0.705 0.651 CAM <ref type="bibr" target="#b69">(Zhou et al. 2016)</ref> -0.663 GBP <ref type="bibr" target="#b51">(Springenberg et al. 2014)</ref> -0.787 SmoothGrad <ref type="bibr" target="#b49">(Smilkov et al. 2017)</ref> -0.795 OCGAN  0.813 0.737 F-anoGAN <ref type="bibr" target="#b46">(Schlegl et al. 2019)</ref> 0.907 0.778 ADGAN  0.913 0.752 CAVGA-Ru <ref type="bibr" target="#b59">(Venkataramanan et al. 2019)</ref> 0.928 0.819 Ours -ImageNet 0.931 0.838 Ours -SSL 0.937 0.857    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Small/Contaminated Training Sets</head><p>To show the improved robustness of our approach to small training sets on CIFAR10 and MVTec, we compare the performance of DSVDD, DSVDD+REC (i.e., DSVDD combined with our reconstruction loss), and our proposed IGD, using less normal data in the training sets in Tab. 6. In particular, we randomly sub-sample 20%, 60%, and 100% of the original training sets of CIFAR10 and MVTec AD, to form a smaller training set. The results indicate that IGD achieves comparable performance under significantly less training data, while the performance of DSVDD and DSVDD+REC deteriorate dramatically when the number of training samples decreases. This result shows that IGD has better robustness than DSVDD and DSVDD+REC to small training sets. To show the improved robustness of our approach contaminated training sets, in Tab. 7, we compare the performance of DSVDD, DSVDD+REC, and our IGD, using training sets corrupted with anomalous samples (this contamination facilitates overfitting). In particular, we reorganise the original training and test data of CIFAR10 and MVTec AD by randomly sampling 1%, 5% and 10% of anomalies from the test data to inject into the training data. With different rates of anomaly contamination, the maximum fluctuation of our IGD is 1.3% on CIFAR10 and 0.44% on MVTec AD. While the competing method DSVDD shows a much larger maximum fluctuation of 7.8% and 3.5% mean AUC, on CIFAR10 and MVTec AD, respectively. The results show the substantially better robustness of IGD over DSVDD and DSVDD+REC for the anomalycontaminated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We do not compare some of the SOTA works <ref type="bibr" target="#b50">Sohn et al. 2020;</ref><ref type="bibr" target="#b52">Tack et al. 2020)</ref> in <ref type="table" target="#tab_3">Table 1</ref>, 2, and 3 due to unfair comparison. In particular, the comparison with PANDA  is not fair because it uses a WideResNet50 ? 2 for MVTec and ResNet152 for CI-FAR, both being much larger backbones than our ResNet18. Regarding CSI <ref type="bibr" target="#b52">(Tack et al. 2020)</ref>, it has much slower inference (because of the 40? data augmentation of test images) and more complex training that needs a coreset and large batch size of 512 for pre-training, which challenges its use for problems with small training sets or high-resolution images. For both CSI and DROC <ref type="bibr" target="#b50">(Sohn et al. 2020)</ref>, their gains are mostly from the SSL pre-training. To show that point for CSI, we use our training approach to fine-tune a pre-trained CSI model and obtain 94.6% AUC on CIFAR10, which is higher than CSI (94.3% AUC). Also, for the vanilla SSL pretraining reported in DROC paper, their performance reduces from 92.5% to 89.0% AUC on CIFAR10, and from 86.5% to 80.2% AUC on MVTec. Note that all above results are collected from their published papers unless stated otherwise.</p><p>Furthermore, on MVTec, our approach obtains (93.4% AUC), which is much better than CSI (63.6% AUC from Tab.2 of (Reiss and Hoshen 2021)) and PANDA (86.5%). For anomaly localisation on MVTec, our 93% AUC is better than DROC (90%) and worse than PANDA (96%). On highresolution image datasets (e.g., Hyper-Kvasir), our approach (93.7% AUC) is better than CSI (trained by us) that reaches 91.6% AUC. Other important results shown by our paper, but missed by CSI, PANDA and DROC, are the ones with small training sets and contaminated training sets, which are new and important benchmarks for real-world industrial applications and early detection of medical diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we presented a new OCC model, called interpolated Gaussian descriptor (IGD), to perform unsupervised anomaly detection and segmentation. IGD learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples to enable an effective normality description based on representative normal samples rather than fringe or anomalous samples. The optimisation of IGD is formulated as an EM algorithm, which we show to be theoretically correct and to converge to a stationary solution under certain conditions. To our knowledge, IGD is the first method that is able to achieve the best performance across diverse application datasets, including MNIST, CIFAR10, Fashion MNIST, MVTec AD, and two large scale medical datasets, in terms of anomaly detection and localisation. We also show that IGD is more robust than DSVDD and an image-reconstruction contrained DSVDD in problems with small or contaminated training sets. We plan to study the use of Gaussian anomaly classifier in the pixel-wise localisation of anomalies and to investigate new self-supervised learning approaches specifically designed for anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>CIFAR10 contains 60,000 images with 10 classes. MNIST and Fashion MNIST contain 70,000 images with 10 classes of handwritten digits and fashion products, respectively. MVTec AD <ref type="bibr" target="#b4">(Bergmann et al. 2019)</ref> contains 5,354 high-resolution realworld images of 15 different industry object and textures. The normal class of MVTec AD is formed by 3,629 training and 467 testing images without defects. The anomalous class has more than 70 categories of defects (such as dents, structural fails, contamination, etc.) and contains 1,258 testing images. MVTec AD provides pixel-wise ground truth annotations for all anomalies in the testing images, allowing the evaluation of anomaly detection and localisation. Hyper-Kvasir has 1,600 normal images without polyps in the training set and 500 in the testing set; and 1,000 abnormal images containing polyps in the testing set. For LAG, we have 2,343 normal images without glaucoma in the training set; and 800 normal images and 1,711 abnormal images with glaucoma for testing. <ref type="figure" target="#fig_0">Figure S1</ref> shows an example of a multi-scale structural and nonstructural anomaly localisation result for an MVTec AD image, using both the local and global IGD models. <ref type="figure" target="#fig_0">Figure S1</ref>: Example of the multi-scale structural and non-structural anomaly localisation result for an MVTec AD <ref type="bibr" target="#b4">(Bergmann et al. 2019)</ref> image, using both the local and global IGD models. The global model tends to produce smooth results but with some mistakes, while the local model produces jagged results, but without the global mistakes, so by combining the two results, we obtain a smooth and correct anomaly heatmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global and Local IGD Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale Structure Similarity Index (MS-SSIM) Score</head><p>The MS-SSIM loss uses the MS-SSIM global score, defined as</p><formula xml:id="formula_29">m (G) (x(?),x(?)) = [lM (x(?),x(?))] ? M ? m (G) m=1 [cm(x(?),x(?))] ?m [sm(x(?),x(?))] ?m ,<label>(S1)</label></formula><p>where x(?) denotes an image patch centred at ? ? ? of size 11 ? 11 ? 3,</p><formula xml:id="formula_30">lM (x(?),x(?)) = 2? x(?) ?x (?) + C1 ? 2 x(?) + ? 2 x(?) + C1 ,<label>(S2)</label></formula><formula xml:id="formula_31">cm(x(?),x(?)) = 2? x(?) ?x (?) + C2 ? 2 x(?) + ? 2 x(?) + C2 ,<label>(S3)</label></formula><formula xml:id="formula_32">sm(x(?),x(?)) = ? x(?)x(?) + C3 ? x(?) ?x (?) + C3 ,<label>(S4)</label></formula><p>with C1, C2, C3 representing pre-defined constants, ? x(?) denoting the mean intensities of x(?), ? 2 x(?) the variance of x(?), and ? x(?)x(?) the covariance of x(?) andx(?). In (S1), m (G) = 5 denotes the number of scales, ?1 = ?1 = 0.0448, ?2 = ?2 = 0.2856, ?3 = ?3 = 0.3001, ?4 = ?4 = 0.2363, ?5 = ?5 = ?5 = 0.1333 <ref type="bibr" target="#b63">(Wang, Simoncelli, and Bovik 2003)</ref>. We follow Ci = (KiL) 2 (for i ? {1, 2, 3}) according to <ref type="bibr" target="#b62">(Wang et al. 2004)</ref> and define L = 4.7579 as the pixel range with K1 = 0.01, K2 = 0.03 and C3 = C2/2.</p><p>The local score m (L) (x (L) (?),x (L) (?)) is defined in the same way as in (S1), where x (L) (?) is an image patch centred at ? ? ? of size 3?3?3, m (L) = 4 scales with weights ?1 = ?1 = 0.0516, ?2 = ?2 = 0.3295, ?3 = ?3 = 0.3463, ?4 = ?4 = ?4 = 0.2726 modified based on the original proportion for m (G) = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For this SSL pre-training, we use the SGD optimiser with a learning rate of 0.01, weight decay 10 ?1 , batch size of 32, and 2,000 epochs. Once we obtain the pre-trained encoder with SSL, we remove the MLP layer and attach a linear layer to the backbone with fixed parameters. Note that this SSL is trained from scratch. In contrast to the vanilla self-supervised learning <ref type="bibr" target="#b10">(Chen et al. 2020)</ref> suggesting large batch size, we notice that a medium batch size yields significantly better performance for unsupervised anomaly detection.</p><p>For the ImageNet KD pre-training, we minimise the 2 norm between the 512-dimensional feature vector output from encoder and an intermediate layer of the ImageNet pre-trained ResNet18 <ref type="bibr" target="#b20">(He et al. 2016)</ref> with the same 512-dimensional features. For this Ima-geNet KD pre-training, we use the Adam optimiser with a learning rate of 0.0001, weight decay 10 ?5 , batch size of 64, and 50,000 iterations. Once we obtain the pre-trained encoder of KD, we fix the network parameters and attach a linear layer to reduce the dimensionality of the feature space to 128. <ref type="figure">Figure S2</ref> shows the distribution of testing samples in the representation space, using the t-SNE visualisation, for DSVDD , Gaussian anomaly classifier (GAC), and our IGD. Notice that the normal samples seem to be more compactly represented with fewer anomalous samples appearing inside the normal cluster. This suggests that IGD has a superior normality description, compared with DSVDD and GAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualisation of the Distribution of Testing Samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DSVDD GAC IGD</head><p>Abnormal Normal Center <ref type="figure">Figure S2</ref>: t-sne visualisation from MVTec (class bottle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correctness Proof</head><p>Lemma 3. Assuming that the maximisation of the constrained ELBO produces ? that makes</p><formula xml:id="formula_33">E q(?) [log p?(y = 0, ?|x, PX )] ? E q(?) [log p ? old (y = 0, ?|x, PX )],</formula><p>we have that (log p?(y = 0|x, PX ) ? log p ? old (y = 0|x, PX )) is lower  <ref type="table" target="#tab_3">Table S1</ref>: Anomaly detection: mean testing accuracy and AUC on MVTec AD produced by the SOTA and our method.</p><p>bounded by</p><formula xml:id="formula_34">E q(?) [log p?(y = 0, ?|x, PX )] ? E q(?) [log p ? old (y = 0, ?|x, PX )] ? 0, with q(?) = p ? old (?|PX ).</formula><p>Proof. We follow the proof for Theorem 1 in <ref type="bibr" target="#b14">(Dempster and Others 1977</ref>   <ref type="table" target="#tab_7">Table S3</ref>: Anomaly detection: class-level testing AUC on FMNIST produced by our methods.</p><p>their approach by 7% and 3%, respectively. With ImageNet KD pre-training, our model achieves the best accuracy results in ten categories of the MVTec AD. The shallow generative baselines, such as DAE, AE-SSIM and AnoGAN yield sub-optimal results on MVTec AD. When compared with methods recently considered to be the MVTec AD SOTA, such as LSA <ref type="bibr" target="#b0">(Abati et al. 2019)</ref> and ?-VAEu <ref type="bibr" target="#b13">(Dehaene et al. 2020</ref>), our approach shows more than 7% improvement. We also show the AUC anomaly detection results in Tab. S1, where our method, with SSL and ImageNet KD pre-training, surpasses all previous methods by at least 5.3%, and produces the best results in eleven categories. The results of IGD for MNIST in Tab.S2 show that our approach pre-trained with Im-ageNet KD is competitive with the Student-Teacher , and both are better than any of the previously proposed methods in the field. In <ref type="table" target="#tab_7">Table S3</ref>, we only show the results of our approach because we could not find the class-level results for other approaches. On the class-level results for CIFAR10, on Tab. S4, we notice that our approach pre-trained with ImageNet and SSL shows the best AUC result in the field by a large margin (around 10%) compared with the Student-Teacher  approach. Finally, the class-level anomaly localisation AUC results for MVTec on Tab. S5 only shows the results of our approach because we could not find results from other approaches. <ref type="figure" target="#fig_3">Figure S3</ref> shows the polyp segmentation results on Hyper-Kvasir testing set images, and <ref type="figure" target="#fig_1">Figure S4</ref> displays the defect results on MVTec AD testing set images.   <ref type="table" target="#tab_9">Table S5</ref>: Anomaly localisation: class-level testing pixel-wise localisation AUC results on the anomalous images of MVTec AD produced by our methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Localisation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Mean testing AUC of DSVDD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of our anomaly localisation results on the MVTec AD (red = high probability of anomaly). Top, middle and bottom rows show the testing images, ground-truth masks and predicted heatmaps, respectively. Please see additional results in the Supp. Material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>et al. 2018) 0.671 0.547 0.529 0.545 0.651 0.603 0.585 0.625 0.758 0.665 0.6180 LSA (Abati et al. 2019) 0.735 0.580 0.690 0.542 0.761 0.546 0.751 0.535 0.717 0.548 0.6410 GradCon (Kwon et al. 2020) 0.760 0.598 0.648 0.586 0.733 0.603 0.684 0.567 0.784 0.678 0.6640 ?-VAE u (Dehaene et al. 2020) 0.702 0.663 0.68 0.713 0.77 0.689 0.805 0.588 0.813 0.744 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure S3 :</head><label>S3</label><figDesc>Qualitative visual results from Hyper-Kvasir testing set (red = anomaly).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S4 :</head><label>S4</label><figDesc>Qualitative results of our anomaly localisation results on the MVTec AD testing set (red = high probability of anomaly).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>?H?3 represents an RGB image of width W and height H and sampled from the distribution of normal images as in x ? P X . The testing set contains normal and anomalous images, where anomalous images can have segmentation map annotations. This testing set is defined by T</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Anomaly detection: mean AUC testing results on MNIST, CIFAR10 and Fashion MNIST. The results are split into 'Scratch' (without any pre-training), pretrained with 'ImageNet', and self-supervised learning ('SSL'). Bold numbers represent the best result (within 0.5%) for each data set, discriminated by Scratch, SSL or ImageNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Anomaly detection: mean testing accuracy and AUC on MVTec AD produced by the SOTA and our IGD. categories. Please see additional results in the Supp. material.</figDesc><table><row><cell>Method</cell><cell>MVTec AD</cell></row><row><cell>DAE (Hadsell et al. 2006)</cell><cell>0.82</cell></row><row><cell>AESSIM (Bergmann et al. 2018)</cell><cell>0.87</cell></row><row><cell>AVID (Sabokrou et al. 2018)</cell><cell>0.78</cell></row><row><cell>SCADN (Yan et al. 2021)</cell><cell>0.75</cell></row><row><cell>LSA (Abati et al. 2019)</cell><cell>0.79</cell></row><row><cell>?-VAEu (Dehaene et al. 2020)</cell><cell>0.86</cell></row><row><cell>AnoGAN (Schlegl et al. 2017)</cell><cell>0.74</cell></row><row><cell>ADVAE (Liu et al. 2020)</cell><cell>0.86</cell></row><row><cell>CAVGA-Du (Venkataramanan et al. 2019)</cell><cell>0.85</cell></row><row><cell>CAVGA-Ru (Venkataramanan et al. 2019)</cell><cell>0.89</cell></row><row><cell>Ours -ImageNet</cell><cell>0.91</cell></row><row><cell>Ours -SSL</cell><cell>0.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Anomaly localisation: mean pixel-level AUC testing results on the anomalous images of MVTec AD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Anomaly detection: AUC testing results on two medical datasets: Hyper-Kvasir and LAG.</figDesc><table><row><cell cols="3">MSE REC GAC INTER AUC -Full AUC -ST AUC -AC</cell></row><row><cell>0.615</cell><cell>0.552</cell><cell>0.565</cell></row><row><cell>0.731</cell><cell>0.655</cell><cell>0.677</cell></row><row><cell>0.819</cell><cell>0.785</cell><cell>0.781</cell></row><row><cell>0.836</cell><cell>0.822</cell><cell>0.812</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>Please see more results in the Supp. material.</figDesc><table><row><cell>: Ablation study of our method on CIFAR10 using</cell></row><row><cell>anomaly detection mean testing AUC w.r.t standard OCC</cell></row><row><cell>setup (AUC -Full), small training set containing 20% of</cell></row><row><cell>training data (AUC -ST), and anomaly contaminated train-</cell></row><row><cell>ing set with 10% contamination (i.e., 10% of the anoma-</cell></row><row><cell>lous samples are removed from the testing set and inserted</cell></row><row><cell>into the training set) (AUC -AC). MSE denotes the baseline</cell></row><row><cell>deep autoencoder with MSE loss, REC denotes the base-</cell></row><row><cell>line deep autoencoder with MS-SSIM + MAE losses, GAC</cell></row><row><cell>denotes our proposed Gaussian anomaly classifier, INTER</cell></row><row><cell>represents our interpolation regularisation. The encoder of</cell></row><row><cell>all above methods are initialised based on the knowledge</cell></row><row><cell>distillation from ImageNet.</cell></row><row><cell>are significantly different than the popular image bench-</cell></row><row><cell>marks and MVTec AD in terms of appearance and structural</cell></row><row><cell>anomalies, suggesting that our model works in disparate do-</cell></row><row><cell>mains. Ablation Study</cell></row><row><cell>To investigate the effectiveness of each component of our</cell></row><row><cell>method, we show the mean AUC results of our method with</cell></row><row><cell>different proposed variants in Tab. 5. Note that all results</cell></row><row><cell>are based on the initialisation of knowledge distillation from</cell></row><row><cell>ImageNet. For standard anomaly detection settings (AUC -</cell></row><row><cell>Full), each proposed component of our IGD improves per-</cell></row><row><cell>formance by a minimum 1.7% and maximum 11.6% mean</cell></row><row><cell>AUC. Tab. 5 also shows the effectiveness of each compo-</cell></row><row><cell>nent when trained with small (20% of full training data) or</cell></row><row><cell>anomaly contaminated (10% of contamination rate) training</cell></row><row><cell>sets, where our proposed Gaussian anomaly classifier (GAC)</cell></row><row><cell>significantly improves over the REC (i.e., MS-SSIM+MAE</cell></row><row><cell>losses) baseline by 13% and 10.4% mean AUC. The pro-</cell></row><row><cell>posed adversarial interpolation regularisation (INTER) fur-</cell></row><row><cell>ther improves the AUC by 3.7% and 3.1%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Mean testing AUCs on CIFAR10 and MVTec with small training sets, where REC=MS-SSIM+MAE losses.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Noise Ratio DSVDD DSVDD+REC IGD (Ours)</cell></row><row><cell></cell><cell>1%</cell><cell>0.7502</cell><cell>0.7694</cell><cell>0.8252</cell></row><row><cell>CIFAR10</cell><cell>5%</cell><cell>0.7124</cell><cell>0.7448</cell><cell>0.8193</cell></row><row><cell></cell><cell>10%</cell><cell>0.6717</cell><cell>0.7073</cell><cell>0.8122</cell></row><row><cell></cell><cell>1%</cell><cell>0.8523</cell><cell>0.7873</cell><cell>0.9363</cell></row><row><cell>MVTec</cell><cell>5%</cell><cell>0.8391</cell><cell>0.7733</cell><cell>0.9319</cell></row><row><cell></cell><cell>10%</cell><cell>0.8175</cell><cell>0.7687</cell><cell>0.9363</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Mean testing AUCs on CIFAR10 and MVTec with different contamination noise rates. REC defined in Tab. 6.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>). From the main paper, we have Mean DAE (Hadsell et al. 2006) 0.894 0.999 0.792 0.851 0.888 0.819 0.944 0.922 0.740 0.917 0.8766 VAE (Kingma and Welling 2013) 0.997 0.999 0.936 0.959 0.973 0.964 0.993 0.976 0.923 0.976 0.9696 KDE (Bishop 2006) 0.885 0.996 0.710 0.693 0.844 0.776 0.861 0.884 0.669 0.825 0.8140 OCSVM (Sch?lkopf et al. 2001) 0.988 0.999 0.902 0.950 0.955 0.968 0.978 0.965 0.853 0.955 0.9510</figDesc><table><row><cell>AnoGAN (Schlegl et al. 2017)</cell><cell>0.966 0.992 0.850 0.887 0.894 0.883 0.947 0.935 0.849 0.924 0.9127</cell></row><row><cell>DSVDD (Ruff et al. 2018)</cell><cell>0.980 0.997 0.917 0.919 0.949 0.885 0.983 0.946 0.939 0.965 0.9480</cell></row><row><cell cols="2">OCGAN (Perera, Nallapati, and Xiang 2019) 0.998 0.999 0.942 0.963 0.975 0.980 0.991 0.981 0.939 0.981 0.9750</cell></row><row><cell>PixelCNN (Van den Oord et al. 2016)</cell><cell>0.531 0.995 0.476 0.517 0.739 0.542 0.592 0.789 0.340 0.662 0.6180</cell></row><row><cell>CapsNet PP (Li et al. 2020)</cell><cell>0.998 0.990 0.984 0.976 0.935 0.970 0.942 0.987 0.993 0.990 0.9770</cell></row><row><cell>CapsNet RE (Li et al. 2020)</cell><cell>0.947 0.907 0.970 0.949 0.872 0.966 0.909 0.934 0.929 0.871 0.9250</cell></row><row><cell>ADGAN (Deecke et al. 2018)</cell><cell>0.999 0.992 0.968 0.953 0.960 0.955 0.980 0.950 0.959 0.965 0.9680</cell></row><row><cell>LSA (Abati et al. 2019)</cell><cell>0.993 0.999 0.959 0.966 0.956 0.964 0.994 0.980 0.953 0.981 0.9750</cell></row><row><cell>GradCon (Kwon et al. 2020)</cell><cell>0.995 0.999 0.952 0.973 0.969 0.977 0.994 0.979 0.919 0.973 0.9730</cell></row><row><cell>?-VAE u (Dehaene et al. 2020)</cell><cell>0.991 0.996 0.983 0.978 0.976 0.972 0.993 0.981 0.98 0.967 0.9820</cell></row><row><cell>ULSLM (Wolf et al. 2020)</cell><cell>0.991 0.972 0.919 0.943 0.942 0.872 0.988 0.939 0.96 0.967 0.9490</cell></row><row><cell>CAVGA-D u (Venkataramanan et al. 2019)</cell><cell>0.994 0.997 0.989 0.983 0.977 0.968 0.988 0.986 0.988 0.991 0.9860</cell></row><row><cell>Student-Teacher (Bergmann et al. 2020)</cell><cell>0.999 0.999 0.990 0.993 0.992 0.993 0.997 0.995 0.986 0.991 0.9935</cell></row><row><cell>Ours -ImageNet</cell><cell>0.998 0.999 0.992 0.991 0.993 0.991 0.997 0.990 0.984 0.991 0.9927</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S2 :</head><label>S2</label><figDesc>Anomaly detection: class-level testing AUC on MNIST produced by the SOTA and our methods.</figDesc><table><row><cell>Method</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Mean</cell></row><row><cell cols="6">Ours -ImageNet 0.908 0.992 0.902 0.946 0.93</cell><cell cols="6">0.95 0.818 0.993 0.938 0.981 0.935</cell></row><row><cell>Ours -SSL</cell><cell cols="11">0.926 0.992 0.922 0.946 0.931 0.971 0.832 0.992 0.946 0.982 0.944</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table S4 :</head><label>S4</label><figDesc>Anomaly detection: class-level testing AUC on CIFAR10 produced by the SOTA and our methods.</figDesc><table><row><cell>Method</cell><cell cols="5">Bottle Hazelnut Capsule Metal Nut Leather</cell><cell>Pill</cell><cell>Wood Carpet</cell><cell>Tile</cell><cell cols="3">Grid Cable Transistor Toothbrush Screw Zipper Mean</cell></row><row><cell cols="2">Ours -ImageNet 0.928</cell><cell>0.981</cell><cell>0.967</cell><cell>0.902</cell><cell>0.983</cell><cell cols="4">0.962 0.827 0.901 0.727 0.916 0.835</cell><cell>0.843</cell><cell>0.974</cell><cell>0.960</cell><cell>0.932 0.909</cell></row><row><cell>Ours -SSL</cell><cell>0.922</cell><cell>0.980</cell><cell>0.977</cell><cell>0.926</cell><cell>0.995</cell><cell cols="4">0.973 0.891 0.947 0.780 0.977 0.847</cell><cell>0.844</cell><cell>0.977</cell><cell>0.970</cell><cell>0.967 0.931</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ak?ay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MVTec AD-A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02359</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Classification-based anomaly detection for general data</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07543</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HyperKvasir, a comprehensive multiclass image and video dataset for gastrointestinal endoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borgli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint european conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Padim: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Iterative energy-based projection on a normal data manifold for anomaly localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Combrexelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eline</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03734</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05525</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc. Gou</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Knowledge Distillation: A Survey</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prabhushankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Temel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09507</idno>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html,55" />
	</analytic>
	<monogr>
		<title level="j">Backpropagated Gradient Representations for Anomaly Detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">MNIST handwritten digit database</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention Based Glaucoma Detection: A Large-Scale Database and CNN Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention based glaucoma detection: A largescale database and cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10571" to="10580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring deep anomaly detection methods based on capsule net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="375" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards visually explaining variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8642" to="8651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10345</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Photoshopping Colonoscopy Video Frames. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
	<note>Nguyen, L. 2020. Tutorial on EM algorithm</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning for anomaly detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Anomaly localization in topic-based analysis of surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="389" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2806" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03844</idno>
		<title level="m">Mean-Shifted Contrastive Loss for Anomaly Detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepcascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1992" to="2004" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiresolution Knowledge Distillation for Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14902" to="14912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Estimating the support of a highdimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02578</idno>
		<title level="m">Learning and evaluating representations for deep one-class classification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08176</idno>
		<title level="m">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anomaly Detection Neural Network with Dual Auto-Encoders GAN and Its Industrial Inspection Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3336</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Self-supervised Multi-class Pretraining for Unsupervised Anomaly Detection and Segmentation in Medical Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01303</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12264</idno>
		<title level="m">Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Few-Shot Anomaly Detection for Polyp Frames from Colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z C T</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference</title>
		<meeting><address><addrLine>Lima, Peru</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-10-04" />
			<biblScope unit="page" from="274" to="284" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 23</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03423</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Conditional image generation with pixelcnn decoders</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Attention Guided Anomaly Detection and Localization in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08616</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">What&apos;s wrong with that object? Identifying images of unusual objects by modelling the detection score distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1573" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Glancing at the Patch: Anomaly Localization With Global and Local Feature Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Unsupervised learning of the set of local maxima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05026</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<title level="m">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>arXiv</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning Semantic Context from Normal Samples for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3110" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">DRAEM -A Discriminatively Trained Reconstruction Embedding for Surface Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
