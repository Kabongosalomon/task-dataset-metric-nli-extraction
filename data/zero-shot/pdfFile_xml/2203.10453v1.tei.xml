<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Tuning Graph Neural Networks via Graph Topology induced Optimal Transport</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiying</forename><surname>Zhang</surname></persName>
							<email>zhangjiy20@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
							<email>xiaox@sz.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Bian</surname></persName>
							<email>yatao.bian@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Tuning Graph Neural Networks via Graph Topology induced Optimal Transport</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the pretrain-finetuning paradigm has attracted tons of attention in graph learning community due to its power of alleviating the lack of labels problem in many real-world applications. Current studies use existing techniques, such as weight constraint, representation constraint, which are derived from images or text data, to transfer the invariant knowledge from the pre-train stage to fine-tuning stage. However, these methods failed to preserve invariances from graph structure and Graph Neural Network (GNN) style models. In this paper, we present a novel optimal transport-based fine-tuning framework called GTOT-Tuning, namely, Graph Topology induced Optimal Transport fine-Tuning, for GNN style backbones. GTOT-Tuning is required to utilize the property of graph data to enhance the preservation of representation produced by fine-tuned networks. Toward this goal, we formulate graph local knowledge transfer as an Optimal Transport (OT) problem with a structural prior and construct the GTOT regularizer to constrain the fine-tuned model behaviors. By using the adjacency relationship amongst nodes, the GTOT regularizer achieves node-level optimal transport procedures and reduces redundant transport procedures, resulting in efficient knowledge transfer from the pre-trained models. We evaluate GTOT-Tuning on eight downstream tasks with various GNN backbones and demonstrate that it achieves state-of-theart fine-tuning performance for GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning from limited number of training instances is a fundamental problem in many real-word applications. A popular approach to address this issue is fine-tuning a model that pretrains on a large dataset. In contrast to training from scratch, fine-tuning usually requires fewer labeled data, allows for faster training, and generally achieves better performance <ref type="bibr">He et al., 2019]</ref>. * Correspondence to: Yatao <ref type="bibr">Bian (yatao.bian@gmail.com)</ref> Conventional fine-tuning approaches can be roughly divided into two categories: (i) weight constraint , i.e. directly constraining the distance of the weights between pretrained and finetuned models. Obviously, they would fail to utilize the topological information of graph data. (ii) representation constraint . This type of approach constrains the distance of representations produced from pretrained and finetuned models, preserving the outputs or intermediate activations of finetuned networks. Therefore, both types of approaches fail to take good account of the topological information implied by the middle layer embeddings. However, it has been proven that GNNs explicitly look into the topological structure of the data by exploring the local and global semantics of the graph <ref type="bibr" target="#b11">Xu et al., 2021]</ref>, which means that the implicit structure between the node embeddings is very significant. As a result, these fine-tuning methods, which cover only weights or layer activations and ignore the topological context of the input data, are no longer capable of obtaining comprehensive knowledge transfer.</p><p>To preserve the local information of finetuned network from pretrained models, in this paper, we explore a principled representation regularization approach. i) Masked Optimal Transport (MOT) is formalized and used as a knowledge transfer procedure between pretrained and finetuned models. Compared with the Typical OT distance <ref type="bibr" target="#b10">[Peyr? and Cuturi, 2020]</ref>, which considers all pair-wise distance between two domains <ref type="bibr" target="#b2">[Courty et al., 2016]</ref>, MOT allows for choosing the specific node pairs to sum in the final OT distance due to the introduced mask matrix. ii) The topological information of graphs is incorporated into the Masked Wasserstein distance (MWD) via setting the mask matrix as an adjacency matrix, leading to a GTOT distance within the node embedding space. The embedding distance between finetuned and pretrained models is minimized by penalizing the GTOT distance, preserving the local information of finetuned models from pretrained models. Finally, we propose a new finetuning framework: GTOT-Tuning as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Using the adjacency relationship between nodes, the proposed GTOT regularizer achieves precise node-level optimal transport procedures and omits unnecessary transport procedures, resulting in efficient knowledge transfer from the pretrained models <ref type="figure">(Fig. 2)</ref>. Moreover, thanks to the OT optimization that dynamically updates the transport map (weights arXiv:2203.10453v1 <ref type="bibr">[cs.</ref>LG] 20 Mar 2022  <ref type="bibr">GTOT-Tuning, where</ref> LGT OT denotes GTOT regularizer loss and LCE represents Cross Entropy loss. The gray lattice of P indicates that Pij = 0 when the vertex pair (vi, vj) is not adjacent. Assume that the input graph has self-loops.</p><p>to summing cosine dissimilarity) during training, the proposed regularizer is able to adaptively and implicitly adjust the distance between fine-tuned weights and pre-trained weights according to the downstream task. The experiments conducted on eight different datasets with various GNN backbones show that GTOT-Tuning achieves the best performance among all baselines, validating the effectiveness and generalization ability of our method.</p><p>Our main contributions can be summarized as follows. 1) We propose a masked OT problem as an extension of the classical Optimal Transport problem by introducing a mask matrix to constrain the transport procedures. Specially, we define Masked Wasserstein distance (MWD) for providing a flexible metric to compare two distributions.</p><p>2) We propose a fine-tuning framework called GTOT-Tuning tailored for GNNs, based on the proposed MWD. The core component of this framework, GTOT Regularizer, has the ability to utilize graph structure to preserve the local feature invariances between finetuned and pretrained models. To the best of our knowledge, it is the first fine-tuning method tailored for GNNs.</p><p>3) Empirically, we conduct extensive experiments on various benchmark datasets, and the results demonstrate a competitive performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pretraining GNNs. Pre-training techniques have been shown to be effective for improving the generalization ability of GNN models. The existing methods for pre-training GNNs are mainly based on self-supervised paradigms. Some self-supervised tasks, e.g. context prediction <ref type="bibr" target="#b10">Rong et al., 2020]</ref>, edge/attribute generation  and contrastive learning <ref type="bibr" target="#b6">([You et al., 2020;</ref><ref type="bibr" target="#b11">Xu et al., 2021]</ref>), have been designed to obtain knowledge from unlabeled graphs. However, most of these methods only use the vanilla fine-tuning methods, i.e. the pretrained weights act as the initial weights for downstream tasks. It remains open to exploiting the optimal performance of the pre-trained GNN models. Our work is expected to utilize the graph structure to achieve better performance on the downstream tasks.</p><p>Fine-tuning in Transfer learning. Fine-tuning a pretrained model to downstream tasks is a popular paradigm in transfer learning (TL). <ref type="bibr">[Donahue and Jia, 2014;</ref><ref type="bibr" target="#b10">Oquab et al., 2014]</ref> indicate that transferring features extracted by pre-trained AlexNet model to downstream tasks yields better performance than hand-crafted features. Further studies by <ref type="bibr">[Yosinski et al., 2014;</ref><ref type="bibr" target="#b0">Agrawal et al., 2014]</ref> show that fine-tuning the pre-trained networks is more effective than fixed pre-trained representations. Recentl research primarily focuses on how to better tap into the prior knowledge of pre-trained models from various perspectives. i) Weights: <ref type="bibr">L2 SP [Xuhong et al., 2018]</ref> propose a L 2 distance regularization that penalizes L 2 distance between the fine-tuned weights and pre-trained weights. ii) Features: DELTA <ref type="bibr" target="#b13">[Li et al., 2018b]</ref> constrains feature maps with the pre-trained activations selected by channel-wise attention. iii) Architecture: BSS  penalizes smaller singular values to suppress untransferable spectral components to prevent negative transfer. StochNorm  uses Stochastic Normalization to replace the classical batch normalization of pre-trained models. Despite the encouraging progress, it still lacks a fine-tuning method specifically for GNNs.</p><p>Optimal Transport. Optimal Transport is frequently used in many applications of deep learning, including domain adaption <ref type="bibr" target="#b2">[Courty et al., 2016;</ref>, knowledge distillation , sequence-to-sequence learning <ref type="bibr" target="#b1">[Chen and Zhang, 2019]</ref>, graph matching <ref type="bibr" target="#b11">[Xu et al., 2019]</ref>, cross-domain alignment , rigid protein docking <ref type="bibr" target="#b2">[Ganea et al., 2022]</ref>, and GNN architecture <ref type="bibr">design [B?cigneul et al., 2020]</ref>. Some classical solutions to OT problem like Sinkhorn Algorithm can be found in <ref type="bibr" target="#b10">[Peyr? and Cuturi, 2020]</ref>. The work closely related to us may be , which raises a fine-tuning method based on typical OT. The significant differences are that our approach is i) based on the proposed MOT, ii) tailored for GNNs, and iii) able to exploit the structural information of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Notations. We define the inner product ?, ? for matrices U, V ? R m?n by U, V = tr(U V) = i,j U ij V ij . I ? R n?n denotes the identity matrix, and 1 n ? R n represents the vector with ones in each component of size n. We use boldface letter x ? R n to indicate an n-dimensional vector, where x i is the i th entry of x. Let G(V, E) be a graph with vertices V and edges E. We denote by A ? R |V|?|V| the adjacency matrix of G and the Hadamard product.</p><p>For convenience, we follow the terms of transfer learning and call the signal graph output from fine-tuned models (target networks) as target graph (with node embeddings {x T 1 , ..., x T |V| }), and correspondingly, the output from pre-trained models (source networks) is called source graph (with node embeddings {x S 1 , ..., x S |V| }). Note that these two graphs have the same adjacency matrix in fine-tuning setting.</p><p>Wasserstein Distance. Wasserstein distance (WD) <ref type="bibr" target="#b10">[Peyr? and Cuturi, 2020]</ref> is commonly used for matching two empirical distributions (e.g., two sets of node embeddings in a graph) <ref type="bibr" target="#b2">[Courty et al., 2016]</ref>. The WD can be defined below.</p><p>Definition 1. Let ? = n i a i ? xi and ? = m i b i ? y be two discrete distributions with ? xi as the Dirac function concentrated at location x. ?(?, ?) denotes all the joint distributions ?(x, y), with marginals ?(x) and ?(y). a ? R n + and b ? R m + are weight vectors satisfying</p><formula xml:id="formula_0">n i=1 a i = m i=1 b i = 1.</formula><p>The definition of Wasserstein distance between the two discrete distributions ?, ? is:</p><formula xml:id="formula_1">D w (?, ?) = inf ???(?,?) E (x,y)?? c(x, y) (1) or L w (a, b) = min P?U(a,b) P, C = min P?U(a,b) ij P ij C ij (2) where U(a, b) = {P ? R n?m | P1 m = a, P 1 n = b} and C ij = c(x i , y j )</formula><p>is a cost scalar representing the distance between x i and y j . The P ? R n?m is called as transport plan or transport map, and P ij represents the amount of mass to be moved from a i to b j . a, b are also known as marginal distributions of P.</p><p>The discrepancy between each pair of samples across the two domains can be measured by the optimal transport distance D w (?, ?). This seems to imply that L w (a, b) is a natural choice as a representation distance between source graph and target graph for GNN fine-tuning. However, the local dependence exists between source and target graph, especially when the graph is large (see section 5). Therefore, it is not appropriate for WD to sum the distances of all node pairs. Inspired by this observation, we propose a masked optimal transport problem, as an extension of typical OT (section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Masked Optimal Transport</head><p>Recall that in typical OT (definition 1), a i can be transported to any b j ? {b k } m k=1 with amount P ij . Here, we assume that the a i can only be transported to b j ? U where U ? {b k } m k=1 is a subset. This constraint can be implemented by limiting the transport plan: P ij = 0 if b j / ? U. Furthermore, the problem is formalized as follows by introducing the mask matrix.</p><p>Definition 2 (Masked Wasserstein distance). Following the same notation of definition 1 and given a mask matrix M 1 1 In this paper, Mij = 0 represents the ij-th element of the matrix being masked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Computing Masked Wasserstein Distance</head><formula xml:id="formula_2">Input: Cost Matrix C, Mask matrix M ? {0, 1} n?m , Marginals a ? R n + , b ? R m + , Threshold ? . Initialize: u = v = 0. for i = 1, 2, 3, ... do u1 = u u = (log(a)? log(M exp(?C + u1 m + 1nv )1m)) + u v = (log(b)? log((M exp(?C + u1 m + 1nv )) 1n)) + v if u1 ? u 1 &lt; ? then Break end if end for P = M exp(?M C + u1 m + 1nv ) Dmw = P, C Output: P, Dmw ? {0</formula><p>, 1} n?m where every row or column is not all zeros, the masked Wasserstein distance (MWD) is defined as</p><formula xml:id="formula_3">L mw (M, a, b) = min P?U(M,a,b) M P, C ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">U(M, a, b) := {P ? R n?m + | (M P)1 m = a, (M P) 1 n = b, P (1 n?m ? M) = 0 n?m } and C ? R n?m is a cost matrix.</formula><p>From Eq. (3), the mask matrix M indicates that the elements of P need to be optimized, in other words, the costs need to be involved in the summation when calculating the inner product. Notably, different mask matrices M lead to different transport maps and obtain the OT distance associated with M. One can design M carefully to get a specific WD. Moreover, the defined MWD can recover WD by setting M = 1 n?m and it is obvious that L mw (M, a, b) ? L w (a, b). This problem can obtain approximate solutions by adding an entropic regularization penalty, which is essential for deriving algorithms suitable for parallel iterations <ref type="bibr" target="#b10">[Peyr? and Cuturi, 2020]</ref>.</p><p>Proposition 1. The solution to definition 2 with entropic regularization H(M P) 2 is unique and has the form</p><formula xml:id="formula_5">P ij = u i M ij K ij v j (4) where K ij = exp(?C ij / ) and (u, v) ? R n + ? R m + are two unknown scaling variables.</formula><p>It is clear from the result that MWD is not equivalent to weighting the distance matrix C directly , so the masked OT problem is nontrivial. We provide the proof in Appendix and the key to it is the observation that exp(M X) = M exp(X) + 1 n?m ? M, where X ? R n?m is an arbitrary given matrix.</p><p>A conceptually simple method to compute the solution would be through the Sinkhorn Knopp iterations(Appendix 2 Namely, min</p><formula xml:id="formula_6">P?U(M,a,b) M P, C ? H(M P), where H(?) is</formula><p>Entropy function. Assume that 0 log 0 = 0 to ensure that H(M P) is well defined.  <ref type="figure">Figure 2</ref>: An example of calculating GTOT distance. When preserving the i-th node representation in target graph, the corresponding vertices in the source graph within 1-hop vertices distance (i.e.{2 , 3 , 4 , 5 }) would be considered. This implies that GTOT regularizer is a local knowledge transfer regularizer.</p><p>A.2). However, as we know, the Sinkhorn algorithm suffers from numerical overflow when the regularization parameter is too small compared to the entries of the cost matrix C [Peyr? and <ref type="bibr" target="#b10">Cuturi, 2020]</ref>. This problem may be more severe when the sparsity of the mask matrix is large. Fortunately, this concern can be somewhat alleviated by performing computations in the log domain. Therefore, for numerical stability and speed, we propose the Log-domain masked Sinkhorn algorithm (the derivation can be seen in Appendix A.3). Algorithm 1 provides the whole process.</p><p>A further extension of the masking idea involves adding a mask matrix to the Gromov-Wasserstein distance <ref type="bibr" target="#b10">[Peyr? et al., 2016]</ref>(MGWD), which can be used to compute distances between pairs of nodes in each domain, as well as to determine how these distances compare with those in the counterpart domain. The definition and the algorithm can be found in Appendix A.4, C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Fine-Tuning GNNs via Masked OT Distance</head><p>In our proposed framework, we use Masked Optimal Transport (MOT) for GNNs fine-tuning, where a transport plan P ? R n?m is learned to optimize the knowledge transfer between pretrained and fine-tuned models. There are several distinctive characteristics of MOT that make it an ideal tool for fine-tuning GNNs. (i) Self normalization: All the elements of P sum to 1. (ii) Sparsity: The introduced mask matrix can effectively limit the sparsity of the transport map, leading to a more interpretable and robust representation regularizer for fine-tuning ( <ref type="figure" target="#fig_0">Fig. 12</ref> in Appendix.). (iii) Efficiency: Our solution can be easily obtained by Algorithm 1 that only requires matrix-vector products, therefore is readily applicable to GNNs. (iv) Flexibility: The Masked matrix can assign exclusive transport plans for specific transport tasks and reduce the unnecessary optimal transport procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GTOT Regularizer</head><p>Given the node embeddings</p><formula xml:id="formula_7">{x S i } |V| i=1 and {x T i } |V| i=1</formula><p>extracted from pre-trained GNN and fine-tuned GNN message passing period, respectively, we calculate the cosine dissimilar-</p><formula xml:id="formula_8">ity C ij = 1 2 (1 ? cos(x S i , x T j ))</formula><p>as the cost matrix of MWD.</p><p>Indeed, cosine dissimilarity is a popular choice that used in many OT application . Intuitively, in most cases, the more (geodesic) distant two vertices in a graph are, the less similar their features are. This implies that when the MWD is used for fine-tuning, the adjacency of the graph should be taken into account, rather than summing all pairwise distances of the cost matrix. Therefore, we set the mask matrix as the adjacency matrix A (with selfloop) here, based on the assumption of 1-hop dependence, i.e., the vertices in the target graph are assumed to be related only to the vertices within 1-hop of the corresponding vertices in the source graph <ref type="figure">(Fig. 2)</ref>. This assumption is reasonable because the neighboring node embeddings extracted through the (pretrained) GNN are somewhat similar <ref type="bibr" target="#b7">[Li et al., 2018a]</ref>, which also reveals that considering only the node-to-node distance but without the neighbors is sub-optimal. We call this MOT with graph topology as Graph Topology induced Optimal Transport(GTOT). With marginal distributions being uniform, the GTOT regularizer is formally defined as</p><formula xml:id="formula_9">L mw (A, q, q) = min P?U(A,q,q) A P, C (GTOT regularizer)<label>(5)</label></formula><p>where q is defined as uniform distribution 1 |V| /|V|. Noted that the inner product in Eq. (5) can be rewritten as</p><formula xml:id="formula_10">i j?N (i) {i} P ij C ij ,</formula><p>where N (i) denotes the set of neighbors of node i. We have tried to incorporate the graph structure into the OT using different methods, such as adding a large positive number to the cost at non-adjacent positions directly, but it is not as concise as our method and is tricky to avoid the summation of non-adjacent vertices due to the challenges posed by large numerical computations. Moreover, our method has the potential to use sparse matrix acceleration when the adjacency matrix is sparse. Our method is easy to extend to i) the weighted graph by element-wise multiplying the edge-weighted matrix W with cost matrix, i.e. A P, W C or ii) the k-hop dependence assumption by replacing A with A k or g(A), where g is a polynomial function.</p><p>Similarly, using MOT we can define the MGWD-based Regularizer for regularizing the representations on edgelevel. We defer the details to Appendix C since we'd like to focus on node-level representation regularization with MWD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GTOT-Tuning Framework</head><p>Unlike the OT representation regularizer proposed by  which calculates the Wasserstein distance between mini-batch samples of training data, GTOT regularizer in our framework focus on the single sample, that is, the GTOT distance between the node embeddings of one source graph and the corresponding target graph. Considering the GTOT distance between nodes allows for knowledge transfer at the node level. This makes the fine-tuned model able to output representations that are more appropriate for the downstream task, i.e., node representations that are as identical as possible to these output from the pre-trained model, but with specific differences in the graph-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall</head><p>Objective. Given N training samples {(G 1 , y 1 ), ? ? ? , (G N , y N )}, the overall objective of GTOT-  Tuning is to minimize the following loss:</p><formula xml:id="formula_11">L = 1 N N i=1 l(f, G i , y i )<label>(6)</label></formula><p>where</p><formula xml:id="formula_12">l(f, G i , y i ) := ?(f (G i ), y i ) + ?L mw (A (i) , q (i) , q (i) ),</formula><p>f denotes a given GNN backbone, ? is a hyper-parameter for balancing the regularization with the main loss function, and ?(?) is Cross Entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Theoretical Analysis</head><p>We provide some theoretical analysis for GTOT-Tuning. Related to Graph Laplacian. Given a graph signal s ? R n?1 , if one defines</p><formula xml:id="formula_13">C ij := (s i ? s j ) 2 , then L mw = min P?U(A,a,b) ij P ij A ij (s i ? s j ) 2 . As we know, 2s T L a s = ij A ij (s i ? s j ) 2 , where L a = D ?</formula><p>A is the Laplacian matrix and D is the degree diagonal matrix. Therefore, our distance can be viewed as giving a smooth value of the graph signal with topology optimization. Algorithm Stability and Generalization Bound. We analyze the generalization bound of GTOT-Tuning and expect to find the key factors that affect its generalization ability. We first give the uniform stability below.</p><formula xml:id="formula_14">Lemma 1 (Uniform stability for GTOT-Tun- ing). Let S := {z 1 = (G 1 , y 1 ), z 2 = (G 2 , y 2 ), ? ? ? , z i?1 = (G i?1 , y i?1 ), z i = (G i , y i ), z i+1 = (G i+1 , y i+1 ), ? ? ? , z N = (G N , y N )} be a training set with N graphs, S i := {G 1 , G 2 , ..., G i?1 , G i , G i+1 , ..., G N }</formula><p>be the training set where graph i has been replaced. Assume that the number of vertices |V Gj | ? B for all j and</p><formula xml:id="formula_15">0 ? ?(f S , z) ? M , then |l(f S , z) ? l(f S i , z)| ? 2M + ? ? B (7)</formula><p>where ? is the hyper-parameter used in Eq. (6).</p><p>Based on Lemma 1 and following the conclusion of <ref type="bibr">[Bousquet and Elisseeff, 2002]</ref>, the generalization error bound of GTOT-Tuning is obtained as follows.</p><p>Proposition 2. Assume that a GNN with GTOT regularization satisfies 0 ? l(f S , z) ? Q. For any ? ? (0, 1), the following bound holds with probability at least 1 ? ? over the random draw of the sample S,</p><formula xml:id="formula_16">R(f S ) ?R m (f S ) + 4M + 2? ? B + (8N M + 4N ? ? B + Q) ln 1/? 2N (8)</formula><p>where R(f S ) denotes the generalization error and R m (f S ) represents empirical error. Proof is deferred to Appendix. This result shows that the generalization bound of GNN with GTOT regularizer is affected by the maximum number of vertices (B) in the training dataset.</p><p>regularization (L2 SP) can't obtain improvement on pure self-supervised tasks. This implies L2 SP may require the pretrained task to be similar to the downstream task. Fortunately, our method can consistently boost the performance of both supervised and self-supervised pretrained models. Observation <ref type="formula" target="#formula_3">(3)</ref>: The performance of the Euclidean distance regularization (Features(DELTA w/o ATT)) is worse than vanilla fine-tuning, indicating that directly using the node representation regularization may cause negative transfer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ablation Studies</head><p>Effects of the Mask Matrix. We conduct the experiments on GNNs fine-tuning with GTOT regularizer to verify the efficiency of the introduced adjacency matrix. The results shown in <ref type="table" target="#tab_2">Table 3</ref> suggest that when using adjacency matrix as mask matrix, the performance on most downstream tasks will be better than using classic WD directly. Besides, the competitive performance when the mask matrix is identity also implies that we can select possible pre-designed mask matrices, such as polynomials of A, for specific downstream tasks. This also indicates that our MOT can be flexibly used for fine-tuning. Effects of Different Proportion of Labeled Data. We also investigate the performance of the proposed method with different proportions of labeled data on MUV and SIDER datasets. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, MWD outperforms the baseline (vanilla Fine-Tuning) given different amounts of labeled data consistently, indicating the generalization of our method.  <ref type="formula">(h)</ref>) is adopted to measure the discrepancy between the pretrained and finetuned domains, i.e. the domain gap between pre-trained data D S and fine-tuned data D T . ?(h) is the error of a linear SVM classifier h discriminating the two domains . We calculate the d A with representations extracted by GIN (contextpred) as input and show the results in <ref type="table" target="#tab_2">Table 3</ref>. From <ref type="figure">Fig. 5</ref>, it can be observed that GTOT-Tuning constrains the weights distance between fine-tuned and pre-trained model when domain gap is relatively small (MUV). Conversely, when the domain gap is large (BACE), our method does not necessarily increase the distance between weights, but rather increases the distance between weights. This reveals that GOT-Tuning can adaptively and implicitly adjust the distance between fine-tuned weights and pre-trained weights according to the downstream task, yielding powerful fine-tuned models. b) Mitigating Negative Transfer under Multi-task. <ref type="figure" target="#fig_3">Fig.  4</ref> shows that GTOT-Tuning boosts the performance of most tasks on multi-task datasets, demonstrating the ability of our method in mitigating negative transfer.  <ref type="figure">Figure 5</ref>: Weights distance between pre-trained initialization weights and fine-tuned weights. Benefit from the soft alignment of MOT, the GTOT regularizer is able to implicitly adjust the distance of the weights according to the downstream task.</p><p>Due to space limitations, we defer additional experimental results on GCN (contextpred) backbone, sensitivity analysis, runtimes to Appendix D.5, D.6, D.7, respectively. Code would be available at https://github.com/youjibiying/ GTOT-Tuning.</p><p>Despite the good results, there are some aspects which worth further explorations in the future: i) The MOT-based methods require relatively high computational cost, which calls for designing a more efficient algorithm to solve it. ii) MWD has the potential to perform even better when used in conjunction with MGWD, which may require a more suitable combination by design. iii) The optimal transport plan obtained by MOT can be used to design new message passing schemes in graph. iv) The proposed method can be potentially extended for more challenging settings that needs advanced knowledge transfer techniques, such as the graph out of distribution learning <ref type="bibr" target="#b4">[Ji et al., 2022]</ref>. v) Other applications in graph knowledge distillation or MOT barycenters can be envisioned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contents</head><formula xml:id="formula_17">P ij = u i M ij K ij v j<label>(4)</label></formula><p>where K ij = exp(?C ij / ) and (u, v) ? R n + ? R m + are two unknown scaling variables. Proof. Introducing two dual variables f ? R n , g ? R m for the two marginal constraints, respectively, the Lagrangian of Eq.</p><p>(3) with entropic regularization is</p><formula xml:id="formula_18">L(P, f , g) = M P, C ? H(M P) ? f , M P1 m ? a ? g, (M P) 1 n ? b .<label>(9)</label></formula><p>Using first order conditions, we get</p><formula xml:id="formula_19">?L(P, f , g) ?P ij = M ij C ij + M ij log(M ij P ij ) ? f i M ij ? g j M ij = 0.</formula><p>That is From the constrain P (1 n?m ?M) = 0 n?m in U(M, a, b), we have P ij = 0 if M ij = 0. To sum up, the P can be uniformly written as</p><formula xml:id="formula_20">P = (exp(f / )1 n ) K M (1 m exp(g/ ) )<label>(10)</label></formula><p>Replacing exp(f / ) and exp(g/ ) with nonnegative vectors u, v, respectively , we get</p><formula xml:id="formula_21">P ij = u i M ij K ij v j .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Masked Sinkhorn Iterations</head><p>Proposition 3 (Masked Sinkhorn Iterations). The masked optimal transport problem can be solved iteratively. These two updates define masked Sinkhorn's algorithm,</p><formula xml:id="formula_22">u (k+1) := a (M K)v (k) and v (k+1) := b (M K) u (k+1)<label>(12)</label></formula><p>initialized with an arbitrary positive vector v (0) = 1 m .</p><p>Proof: Substituting Eq. (4) into constraints (M P)1 m = a, (M P) 1 n = b, we have</p><formula xml:id="formula_23">j P ij M ij = j u i M ij K ij v j = a i u i = a i j M ij K ij v j Similarly, v j = b j i u i M ij K ij</formula><p>Remark 1 (Complexity of Masked Sinkhorn Algorithm). Assume that n = m in Definition 2 for the sake of simplicity. Let P denote a approximate solution satisfying P , C ? L mw (M, a, b) + ? . When = ? 4log(n) , from [Altschuler et al., 2017], we know the time complexity of the classical Sinkhorn iterations to getP is O(n 2 C 2 ? ? ?3 (? log(s) + C ? log(n))), where C ? := max ij C ij and s := ij (K) ij . In MOT, the K in OT can be replaced with M K, so the s can be replaced with</p><formula xml:id="formula_24">s = ij M ij K ij ? s. Hence our Masked optimal transport complexity is O(n 2 C 2 ? ? ?3 (? log(s ) + C ? log(n))) ? O(n 2 C 2 ? ? ?3 (? log(s) + C ? log(n))</formula><p>). This means that the complexity of our method is smaller than the typical OT Sinkhorn algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Log-domain Masked Sinkhorn</head><p>Proposition 4. The masked optimal transport problem (3) is equivalent to</p><formula xml:id="formula_25">L MC = max f ?R n ,g?R m f , a + g, b ? e f / , (M K)e g/<label>(13)</label></formula><p>where (f , g) are linked to (u, v) that appears in (4) via (u, v) = (e f / , e g/ ).</p><p>Proof. From Eq. (10), we know the optimal primal solution P and dual multipliers f and g satisfy the following equation:</p><formula xml:id="formula_26">P = (exp(f / )1 n ) K M (1 m exp(g/ ) ) = M e (f1 n / ) e ?C/ e (1mg / )</formula><p>Substituting the optimal P into the problem <ref type="formula" target="#formula_3">(3)</ref>, we obtain the equivalent Lagrangian dual function as</p><formula xml:id="formula_27">e f / , (K M C)e g/ ) ? H(M e (f1 n / ) e ?C/ e (1mg / ) ) ? (f , g) (14) Note that ? H(M P) = M P, log(M P) ? 1 n?m = M P, log(M e (f1 n / ) e ?C/ e (1mg / ) ) ? 1 n?m = M P, f1 n ? C + 1 m g ? 1 n?m = ? M P, C + M P, f1 n + M P, 1 m g ? M P, 1 n?m = ? M P, C + a, f + b, g ? M P, 1 n?m = ? M P, C + a, f + b, g ? M e (f1 n / ) e ?C/ e (1mg / ) , 1 n?m = ? M P, C + a, f + b, g ? e f / , (K M)e g/ ) .</formula><p>The last equation comes from that</p><formula xml:id="formula_28">M e (f1 n / ) e ?C/ e (1mg / ) , 1 n?m = ij e fi M ij K ij e gj = i e fi j M ij K ij e gj = e f / , (K M)e g/ )</formula><p>.</p><p>Putting the result back to Eq. (14), we have Proposition 5 (Log-domain Masked Sinkhorn). The Log-domain Masked Sinkhorn is defined as a two-step update</p><formula xml:id="formula_29">f (k+1) = log(a) ? log([M exp(?C + f (k) 1 m + 1 n (g (k) ) )]1 m ) + f (k) (15) g (k+1) = log(b) ? log([M exp(?C + f (k+1) 1 m + 1 n (g (k) ) ] 1 n ) + g (k)<label>(16)</label></formula><p>with initialized f (0) = 0, g (0) = 0.</p><p>Proof. Writing F(f , g) := f , a + g, b ? e f / , (M K)e g/ for the objective of (13). The unconstrained maximization problem (13) can be solved by an exact block coordinate ascent strategy</p><formula xml:id="formula_30">?| f F = a ? e f / (M K)e g/ = 0 (17) ?| g F = b ? e g/ (M K)e f / = 0 (18)</formula><p>Applying the following updates, with any initialized vector g (0) , we have</p><formula xml:id="formula_31">f (k+1) = log(a) ? log((M K)e g (k) / ) (19) g (k+1) = log(b) ? log((M K)e f (k+1) / )<label>(20)</label></formula><p>Indeed, the updates above are equivalent to the masked Sinkhorn updates in (12) by applying the primal-dual relationships (f (k) , g (k) ) = (log(u (k) ), log(v (k) ))</p><p>Following the derivation of log-domain Sinkhorn iterations <ref type="bibr" target="#b10">([Peyr? and Cuturi, 2020]</ref>,Remark 4.22), we use soft-min rewriting to write min z for the soft-minimum of the its coordinates</p><formula xml:id="formula_32">min = ? log i e ?zi/ .<label>(21)</label></formula><p>Obviously, lim ?0 min z = min z. Based on this notation, we further define the operator that takes an arbitrary given matrix A ? R n?m and outputs a column vector of soft-minimum vales of its columns or rows Min row (A) i := min (A i, * ), ?i ? {1, 2, ..., n}</p><p>Min col (A) j := min (A * ,j ), ?j ? {1, 2, ..., m} Using this notation, Eq. <ref type="formula" target="#formula_18">(19) (20)</ref> can be rewritten as</p><formula xml:id="formula_33">f (k+1) = log(a) + Min row (log(M) + C ? 1 n (g (k) ) ) (22) g (k+1) = log(b) + Min row (log(M) + C ? f (k) 1 m )<label>(23)</label></formula><p>where we define log <ref type="formula">(0)</ref>  </p><p>and replacing the z with the practical scaling computed previously. Finally, the stabilized iterations of log-domain masked Sinkhorn can be obtained as</p><formula xml:id="formula_35">f (k+1) = Min row (S(f (k) , g (k) ) + f (k) + log(a) (25) g (k+1) = Min col (S(f (k+1) , g (k) )) + g (k) + log(b) (26) where we defined S(f (k) , g (k) ) := log(M) + C ? f (k) 1 m ? 1 n (g (k) ) (27)</formula><p>The only difference from the classical log-domain sinkhorn algorithm is that here we have an additional term log(M). Note that when M = 1 n?m , log(M) = 0. Therefore, our masked iterations recover the non-masked iterations and can be seen as an extension of classical results <ref type="bibr" target="#b10">[Peyr? and Cuturi, 2020]</ref>.</p><p>Rewriting the Eq. (25), Eq. <ref type="formula" target="#formula_11">(26)</ref> to explicit forms</p><formula xml:id="formula_36">f (k+1) = log(a) ? log([M exp(?C + f (k) 1 m + 1 n (g (k) ) )]1 m ) + f (k) (28) g (k+1) = log(b) ? log([M exp(?C + f (k+1) 1 m + 1 n (g (k) ) ] 1 n ) + g (k)<label>(29)</label></formula><p>A.4 Masked Gromov-Wasserstein Distance Definition 3 (Masked Gromov-Wasserstein distance). Following the same notation as Definition 2, the masked Gromov-Wasserstein distance (MGWD) L mgw (M, a, b) is defined as</p><formula xml:id="formula_37">min P?U(M,a,b) ijkl L(C ik ,C jl )(M P) ij (M P) kl (30)</formula><p>where C ? R n?n andC ? R m?m are distance matrices that represent the pair-wise point distance within the same space and L is the cost function evaluating intra-domain structural diversity between two pairs of points x i , x j and y k , y l . Lemma 2. <ref type="bibr" target="#b10">([Peyr? et al., 2016]</ref> proposition 1) For any tensor L = (L i,j,k,l ) i,j,k,l and matrix (P i,j ) i,j , the tensor matrix multiplication is defined as L ? P := kl (L i,j,k,l P kl ) ij If the loss can be calculated as</p><formula xml:id="formula_38">L(a, b) = f 1 (a) + f 2 (b) ? h 1 (a)h 2 (b) for functions (f 1 , f 2 , h 1 , h 2 )</formula><p>, then, for any P <ref type="figure" target="#fig_0">? U(1 n?m , a, b)</ref>,</p><formula xml:id="formula_39">L(C,C) ? P = c C,C ? h 1 (C)Ph 2 (C)<label>(31)</label></formula><p>where c C,C := f 1 (C)a1 m + 1 n b f 2 (C) is independent of P. Proposition 6. Using the same notation as Lemma 2, If the loss can be calculated as</p><formula xml:id="formula_40">L(a, b) = f 1 (a) + f 2 (b) ? h 1 (a)h 2 (b) for functions (f 1 , f 2 , h 1 , h 2 ), then, for any P ? U(M, a, b), L(C,C) ? (M P) = c C,C ? h 1 (C)(M P)h 2 (C)<label>(32)</label></formula><p>where c C,C := f 1 (C)a1 m + 1 n b f 2 (C) is independent of P. Let? denote L(C,C) ? (M P) as a pseudo-cost matrix, then the solution to Definition 3 with entropic regularization H(M P) is</p><formula xml:id="formula_41">P ij = u i M ijKij v j<label>(33)</label></formula><p>whereK ij = exp(?? ij / ) and (u, v) ? R n + ? R m + are two unknown scaling variables .</p><p>Proof. Eq. (30) can be rewritten as ijkl L(C ik ,C jl )(M P) ij (M P) kl = L(C,C) ? (M P), M P From Eq. (31), for (M P) ? U(1 n?m , a, b), we have</p><formula xml:id="formula_42">L(C,C) ? (M P) = c C,C ? h 1 (C)(M P)h 2 (C) .</formula><p>Note that (M P) <ref type="figure" target="#fig_0">? U(1 n?m , a, b</ref>) is equivalent to P ? U <ref type="figure">(M, a, b)</ref>. Therefore, Eq. (32) holds. Since Eq. (30) can be rewritten as</p><formula xml:id="formula_43">L mgw (M, a, b) = min P?U(M,a,b) M P,? ,</formula><p>this problem is equivalent to masked Wasserstein distance. Therefore, by proposition 1, the solution is</p><formula xml:id="formula_44">P ij = u i M ijKij v j .</formula><p>Following the iterations methods proposed by <ref type="bibr" target="#b10">[Peyr? et al., 2016]</ref>, with replacing the Sinkhorn iterations by log-domain Sinkhorn iterations, we summarize the computation of masked Gromov-Wasserstein distance in the Algorithm 2.</p><p>Algorithm 2 Computing Masked Gromov-Wasserstein Distance</p><formula xml:id="formula_45">Input: {x s i } n i=1 , {x t i } n i=1 , Masked Matrix M ? {0, 1} n?m , Marginals (a, b), Distance matrices (C,C) Compute c C,C := f 1 (C)a1 m + 1 n b f 2 (C) for i = 1, 2, 3, ... do // compute the pseudo-cost matrix C = c C,C ? h 1 (C)(M P)h 2 (C)</formula><p>Apply Algorithm 1 to solve transport plan P. end for D mgw = M P,? Output: D mgw When we set L(a, b) to be (a ? b) 2 , f 1 (a) = a 2 , f 2 (b) = b 2 , h 1 (a) = 2a, h 2 (b) = b. In fine-tuning, we can use L(a, b) = (a ? b) 2 to calculate the Masked GWD.</p><p>B Missing Proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 The proof of Lemma 1</head><p>Lemma 1 (Uniform stability for GTOT-Tuning). Let S := {z 1 = (G 1 , y 1 ), z 2 = (G 2 , y 2 ), ? ? ? , z i?1 = (G i?1 , y i?1 ), z i = (G i , y i ), z i+1 = (G i+1 , y i+1 ), ? ? ? , z N = (G N , y N )} be a training set with N graphs, S i := {G 1 , G 2 , ..., G i?1 , G i , G i+1 , ..., G N } be the training set where graph i has been replaced. Assume that the number of vertices |V Gj | ? B for all j and 0 ? ?(f S , z) ? M , then</p><formula xml:id="formula_46">|l(f S , z) ? l(f S i , z)| ? 2M + ? ? B (7)</formula><p>where ? is the hyper-parameter used in Eq. (6). Proof: For any (G, y) ? Z(Z := X ? Y), Let h(G) denote the message passing phase of GNN f (G) with graph G as input (h(G i ) j denotes the j-th node representation of G i after message passing.)</p><formula xml:id="formula_47">|l(f S , z) ? l(f S i )| = | (f(G), y) + ? jk P jk (1 ? cos(h S (G) j , h (s) (G) k )/2 ? (f S i (G), y) ? ? jk P jk (1 ? cos(h S i (G) j , h (s) (G) k )/2| ? 2M + ? tr( 1 2 (1 ? RNorm(h S (G))(RNorm(h (s) (G))) ) ? 1 2 (1 ? RNorm(h S i (G))(RNorm(h (s) (G))) ) P ,</formula><p>where RNorm(?) denotes the row normalization for calculating cosine. Let</p><formula xml:id="formula_48">C G := 1 2 (1 ? RNorm(h S (G))(RNorm(h (s) (G))) ) ? 1 2 (1 ? RNorm(h S i (G))(RNorm(h (s) (G))) ), then 0 ? |C G | ij ? 1.</formula><p>Thus</p><formula xml:id="formula_49">|l(f S , z) ? l(f S i )| ? 2M + ?tr(|C G | P) ? 2M + ? tr(|C G | |C G |)tr(P P) = 2M + ? ij |C G | 2 ij ij P 2 ij ? 2M + ? ij 1( 1 |V G | 2 ij (|V G |P ij ) 2 ) ? 2M + ? ij (|V G |P ij ) ? 2M + ? |V G | ? 2M + ? ? B B.2</formula><p>The proof of Proposition 2 <ref type="bibr">Lemma 3. ([Bousquet and Elisseeff, 2002]</ref>,Theorem 12) Let f be an algorithm with uniform stability ? with respect to a loss function l such that 0 ? l(f S , z) ? Q, for all z ? Z and all sets S. Then, for any m ? 1, and any ? ? (0, 1), the bounds hold (separately) with probability at least 1 ? ? over any random draw of the sample S,</p><formula xml:id="formula_50">R(f S ) ? R m (f S ) + 2? + (4m? + Q) ln 1/? 2m<label>(34)</label></formula><p>Proof: Substituting ? = 2M + ? ? B into Lemma 3, we can get the proposition 2. Empirical Evidence. Empirically, <ref type="table" target="#tab_8">Table 7</ref> show the B of MUV is the smallest among all the datasets. And our method gains a great improvement compared with fine-Tuning (Table 1,2. ) This corroborates the correctness of our theorical results to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C GTOT-MGWD Regularizer</head><p>Along with the design of the GTOT-Regularizer, we propose the GTOT-MGWD Regularizer with adjacency matrix as mask matrix in Eq. (30). Unlike the GTOT-Regularzer concentrating on preserving the local node invariances, GTOT-MGWD Regularizer expects to preserve the local edge invariances <ref type="figure" target="#fig_9">(Fig. 7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Objective Function</head><p>Following the usage of GTOT Regularizer, the GTOT-MGWD Regularizer can be used as follows</p><formula xml:id="formula_51">m i=1 ( (f (G i ), y i ) + ?L mgw (A i , q, q))<label>(35)</label></formula><p>Intuitively, one can jointly use MWD and MGWD to achieve better fine-tuning performance via the simultaneous alignment of node and edge information. Here, we provide a naive method to combine the two regularizers ( <ref type="figure">Figure 6</ref>  <ref type="figure">Figure 6</ref>: The finetuning framework after combining two regularizers. The gray lattice of P indicates that Pij = 0 when the vertex pair (vi, vj) is not adjacent. Assume that the input graph has self-loops.</p><formula xml:id="formula_52">) named GTOT- (MWD+MGWD) m i=1 ( (f (G i ), y i ) + ?L mw (A i , q, q) + ?L mgw (A i , q, q))<label>(36)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Experiments</head><p>Comparison of Different Fine-Tuning Strategies <ref type="table" target="#tab_4">Table 4</ref> shows the results of GTOT-MGWD and GTOT-(MWD+MGWD). As we can see in the table, GTOT-MGWD slightly outperforms GTOT-MWD on average, indicating that GTOT-MGWD is also an efficient regularizer for finetuning GNNs. However, GTOT (MWD+MGWD) cannot obtain better performance than either of the two regularizers (GTOT-MWD, GTOT-(MWD+MGWD)). More effective joint methods are worth exploring in the future. Ablation studies. We do the ablation study for MGWD to investigate the impact of the adjacency matrix and the results show in table 5. It is clear that the introduced Mask matrix A (adjcancy matrix) of GTOT-MGWD is effective.  For the cost matrix C ij = 1 2 (1 ? cosine(x S i , x T j )) in GTOT regularizer, we use the maximum normalization 2C/C max instead to scale the matrix elements to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Noted that this operation has essentially no effect on the results.</p><p>In our experiments, we use the output from the last linear layer ('gnn.gnns.4.mlp.2') of GNN message passing to calculate the GTOT distance as well as the baselines Feature(Delta w/o ATT) and DELTA. Indeed, the GTOT regularizer can be used in any middle layer of the backbone.</p><p>We follow 's data split (Train:validation:test = 8:1:1) to fine-tuning pretrained models for downstream tasks. Each method we run 10 random seeds (0 ? 9) and we report the mean ? standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Hyper-parameter Strategy</head><p>We use cross-entropy loss and Adam optimizer with early stopping with patience of 20 epochs to train GTOT-Tuning models. For other hyper-parameters, We use grid search strategies and the range of hyper-parameters listed in <ref type="table" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Datasets</head><p>The downstream dataset statistics are summarized in <ref type="table" target="#tab_8">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Backbones</head><p>Graph Neural Networks Generalizing the convolution operator to irregular domains is typically expressed as a neighborhood aggregation or message passing scheme <ref type="bibr">[Yadati, 2020]</ref>. With x (k?1) i ? R F denoting node features of node i in layer (k ? 1) and e j,i ? R D denoting (optional) edge features from node j to node i, message passing graph neural networks (MPNNs) can be described as</p><formula xml:id="formula_53">x (k) i = ? (k) x (k?1) i , j?N (i) ? (k) x (k?1) i , x (k?1) j , e j,i<label>(37)</label></formula><p>where denotes a differentiable, permutation invariant function, e.g., sum, mean or max, and ? and ? denote differentiable functions such as MLPs (Multi Layer Perceptrons).  Hyper-parameter Range ? (GTOT regularizer) {1e-7,1e-6,5e-6,1e-5,5e-5,1e-4,5e-4,1e-3,5e-3,1e-2,5e-2,1e-1,5e-1,1,5,10} ? (MGWD regularizer Sec. C) <ref type="bibr">{1e-7,1e-6,5e-6,1e-5,5e-5,1e-4,5e-4,1e-3,5e-3,1e-2,5e-2,1e-1,5e-1,1,5</ref> Our GTOT-Regularization acts on the output node embeddings of message passing {x</p><formula xml:id="formula_54">(k) 1 , ..., x<label>(k)</label></formula><p>|V| }. GCN <ref type="bibr" target="#b5">[Kipf and Welling, 2017]</ref> and GIN  can be written as MPNNs, one can refer to PyG 5 . GIN (contextpred) and GIN (supervised contextpred). The GIN (contextpred) and GIN (supervised contextpred) we directly adopt the released models from . For node-level self-supervised pre-training, the pre-trained data are 2 million unlabeled molecules sampled from the ZINC15 database <ref type="bibr">[Sterling and Irwin, 2015]</ref>. For graph-level multi-task supervised pre-training, the pre-trained dataset is a pre-processed ChEMBL dataset <ref type="bibr" target="#b10">[Mayr et al., 2018]</ref>, containing 456K molecules with 1310 kinds of diverse and extensive biochemical assays.</p><p>The GCN (contextpred) used in additional experiments is also from . The details of these architectures can be found in Appendix A of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Baselines</head><p>Fine-Tuning (Baseline) We reuse the AUC reported by .</p><p>We refer to the transfer learning package 6 for the implementations of the following baselines.</p><p>L2 SP [Xuhong et al., 2018] The L2 SP is a weights regularization. The key concept of L2 SP penalty is "starting point as reference"</p><p>:</p><formula xml:id="formula_55">?(W ) = ? 2 W S ? W T 2 2 + ? 2 WT 2 2<label>(38)</label></formula><p>where W S is the pre-trained weight parameters of the shared architecture, W T is weight parameters of the model, WT is weight parameters of the task-specific classifier, ? is a trade-off hyper-parameter to control the strength of the penalty. L2 SP penalty tries to drive weight parameters to pre-trained values.</p><p>DELTA  Based on the key insight of "unactivated channel re-usage", Delta is proposed as a regularized transfer learning framework. Specifically, DELTA selects the discriminative features from higher layer outputs with a supervised attention mechanism. However, it is designed for Convolutional Neural Networks(CNN). Therefore, we have to make some adjustments to accommodate GNNs. The channels of CNN correspond the column of the learnable parameter matrix ? ? R d l ? d l+1 in GIN or GCN (i.e. the ? in Eq. (37)), where ? totally has d l+1 filters and would generates d l+1 feature maps. Each feature map is a n-dim vector, so here we call it Feature Vector (FV). Therefore, the Delta is calculated as follows for adapting </p><formula xml:id="formula_56">?(W ) = ?? (W T , W S , x i , y i , z) + ?? (WT )</formula><p>where</p><formula xml:id="formula_57">? (W T , W S , x i , y i , z) := d l+1 j D j (z, W S , x i , y i ) FV j (z, W T , x i ) ? FV j (z, W S , x i ) 2 2 and D j (z, W S , x i , y i ) := softmax(L(z(x i , W S/j ), y i ) ? L(z(x i , W S ), y i ))<label>(39)</label></formula><p>where L is the cross-entropy loss in our experiments. z is the model, ? is behavioral regularizer, ? constrains the L 2 -norm of the private parameters in W T ; D j (z, W S , x i , y i ) refers to the behavioral difference between the two feature vectors and the weight assigned to the j-th filter and the i-th graph (for 1 ? j ? d l+1 ); ? and ? are trade-off hyper-parameters to control the strength of the two regularization terms. Feature(DELTA w/o ATT) When the attention coefficients D j (z, W S , x i , y i ) (Eq. (39) ) in DELTA is 1/2, DELTA will become the Feature(DELTA w/o ATT) regularization. Actually, it is equivalent to a node-to-node representation regularization</p><formula xml:id="formula_58">? (W T , W S , x i , y i , z) := 1 2 n j NFV j (z, W T , x i ) ? NFV j (z, W S , x i ) 2 2<label>(40)</label></formula><p>where NFV j (z, W T , x i ) ? R 1?d l+1 denote the j-th Node Feature Vector (NFV) produced by message passing neural networks z W T (x i ). Remark 2. Following the notations from DELTA and Feature(DELTA w/o ATT), our GTOT Regularizer can be rewritten as</p><formula xml:id="formula_59">? (W T , W S , x i , y i , z) := min P?U(A,q,q) 1 2 n i n j A ij P ij (1 ? cosine(NFV i (z, W T , x i ), NFV j (z, W S , x i )))<label>(41)</label></formula><p>This suggests that our method focuses on the distance of node-level embeddings, rather than the distance of channel-level features.</p><p>BSS. Inheriting from , we use the SVD to compute the feature matrix F = [g 1 , g 2 , ..., g b ] from the output of the readout layer in GNNs, where g i denotes the i-th graph representation in a batch. Finally, the BSS regularization is :</p><formula xml:id="formula_60">L bss = ? k i=1 ? 2 ?i</formula><p>where ? is a trade-off hyper-parameter to control the strength of spectral shrinkage, k is the number of singular values to be penalized, and ? ?i refers to the i-th smallest singular value of F. Stochnorm. The Stochnorm is used directly from  without any modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Additional Experiments</head><p>Catastrophic forgetting. We conduct experiments to verify that our GTOT-Tuning is able to implicitly constrain the distance between fine-tuned and pre-trained weights, alleviating the Catastrophic forgetting issue in fine-tuning. The results are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. Domain Adaptation. As alluded to in the main text, GTOT is capable of adaptively constraining the distance of weights according to the domain gap <ref type="table" target="#tab_0">(Table 11)</ref>, which also make it an ideal regularizer to handle the out of distribution problems (when the domain gap is large). Negative Transfer. We provide additional results on multi-task datasets <ref type="figure" target="#fig_10">(Fig. 8</ref>) to support our conclusion in section 7.3 of main text.</p><p>GCN(contextpred). <ref type="table" target="#tab_9">Table 8</ref> shows the results of GCN (contextpred). It is clear that our methods have performance gains compared to the baseline in most cases. D.6 Hyper-parameter Sensitivity <ref type="figure" target="#fig_11">Figure 9</ref> shows test AUC w.r.t. different ?'s on SIDER, BBBP, BACE and MUV, where the backbone is GIN (contextpred). The performance benefits from a proper selection of ? (from 0.001 to 10 in our experiments). When ? is too small, the GTOT regularization term does not work; if it is too large, the Cross entropy term would be neglected(appears on BBBP and MUV), leading to performance degradation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 Runtimes</head><p>We report the average training epoch time of different fine-tuning methods in <ref type="table" target="#tab_10">Table 9</ref>. We note that our method is between 1 to 3 times slower than the vanilla Fine-Tuning baselineThis is caused by the frequent calls to the MOT solver. On the other hand, our method has a competitive speed-up compared with other methods.     <ref type="figure" target="#fig_0">Figure 12</ref>: Comparison of GTOT (MWD(A)) and OT 's (MWD(132?32)) transport maps.Although when solved exactly, typical OT yields a sparse solution P * containing (2 max(n, m) ? 1) non-zero elements at most <ref type="bibr">[De Goes et al., 2011]</ref>, the actual solution solved by iterative algorithm may tend to be dense. When the mask matrix is the adjacency matrix of a given graph, the transport plan is restricted to neighboring nodes and a more sparse transport map is obtained. Another important observation is that the values of the main diagonal of both are large, and GTOT retains most of the important values, removing unnecessary transfer processes.</p><p>Masked matrix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>Masked matrix 32?32 (b) <ref type="figure" target="#fig_0">Figure 13</ref>: Comparison of MGWD (M = A) and GWD's(M = 132?32) transport maps. When masked matrix is the adjacency matrix of a given graph, the transport plan is restricted to neighboring nodes and a more sparse transport map is obtained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall framework of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Test AUC with different proportion of labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Scatter plot comparisons of ROC-AUC scores for a pair of fine-tuning strategies on the multi-task datasets (MUV). Each point represents a particular downstream task. There are many downstream tasks where vanilla fine-tuning performs worse than nonpre-trained model, indicating negative transfer. Meanwhile, negative transfer is alleviated when the GTOT regularizer is used.7.3 Why GTOT-Tuning Worksa) Adaptive Adjustment of Model Weights to Downstream Tasks. The A-distance [Ben-David et al., 2007], d A (D S , D T ) = 2(1 ? 2?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>A</head><label></label><figDesc>Derivations of Algorithm 1 9 A.1 Masked Optimal Transport Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 A.2 Masked Sinkhorn Iterations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 A.3 Log-domain Masked Sinkhorn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 A.4 Masked Gromov-Wasserstein Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 B Missing Proofs. 13 B.1 The proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 B.2 The proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 C GTOT-MGWD Regularizer 15 C.1 Objective Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 C.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 D Details of Experiments. 16 D.1 Hyper-parameter Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 D.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 D.3 Backbones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 D.4 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 D.5 Additional Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 D.6 Hyper-parameter Sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 D.7 Runtimes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A Derivations of Algorithm 1 A.1 Masked Optimal Transport Problem. Definition 2 (Masked Wasserstein distance). Following the same notation of definition 1 and given a mask matrix M 3 ? {0, 1} n?m where every row or column is not all zeros, the masked Wasserstein distance (MWD) is defined as L mw (M, a, b) a, b) := {P ? R n?m + | (M P)1 m = a, (M P) 1 n = b, P (1 n?m ? M) = 0 n?m } and C ? R n?m is a cost matrix. Proposition 1. The solution to definition 2 with entropic regularization H(M P) 4 is unique and has the form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>exp(M log(M P)) = exp(diag(f )M/ ) exp(?C M/ ) exp(Mdiag(g)/ )where exp(X) := exp(X ij ), exp(x) := exp(x i ) for any matrix X or vector x, respectively.LetM ij := 1 ? M ij . Note that ? M ? {0, 1} n?m , ? X ? R n?m , we have exp(M X) = M exp(X) +M. Therefore, exp(diag(f )M/ ) = M (exp(f / )1 n ) +M.Then we have M exp(log(M P)) +M = exp(diag(f )M/ ) exp(?C M/ ) exp(Mdiag(g)/ ) Further simplification yields M P = exp(diag(f )M/ ) exp(?C M/ ) exp(Mdiag(g)/ ) ?M = (M (exp(f / )1 n ) +M) exp(?C M/ ) (M (1 m exp(g/ ) ) +M) ?M = M (exp(f / )1 n ) exp(?C M/ ) (M (1 m exp(g/ ) ) +M exp(?C M/ ) M ?M = (exp(f / )1 n ) K M (1 m exp(g/ ) ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>M</head><label></label><figDesc>P, C ? H(M P) = M P, C ? M P, C + a, f + b, g ? e f / , (K M)e g/ ) = f , a + g, b ? e f / , (K M)e g/ ) ? (f , g)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>:= ?? and e log(0) := 0 to make the log(M) well-defined. This log-sum-exp trick can avoid underflow for small values of , when min ? min as ? 0. Writing z = min z, [Peyr? and Cuturi, 2020] suggest evaluating min z as min z = z ? log i e ?(zi?z)/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>An example of using masked WD and GWD on a graph. When one uses an adjacency matrix as a Masked matrix, the MWD can be rewritten as i j?N (i) {i} PijCij ,where i = 3, N (i) = {2 , 3 , 4 , 5 }, and MGWD can be rewritten as i j?N (i) {i} k l?N (k) {k} L(C ik ,C jl )PijP kl , where i = 2, N (i) {i} = {1 , 2 , 3 } and k = 3, N (k) {k} = {2 , 3 , 4 , 5 }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Scatter plot comparisons of ROC-AUC scores for a pair of fine-tuning strategies on the multi-task datasets (Tox21,SIDER). Each point represents a particular downstream task. There are many downstream tasks where vanilla fine-tuning performs worse than non-pretrained model, indicating negative transfer. Meanwhile, negative transfer is alleviated when the GTOT regularization is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Effect of hyper-parameter ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Weights Distance curves between pre-trained initialization weights and fine-tuned weights. Comparing the vanilla finetuning, our GTOT regularizer can implicitly constrain the weights to deviate from the pre-trained parameters, alleviating the catastrophic forgetting to some extent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>(left (a)(c)): Weights distance between pre-trained initialization weights and fine-tuned weights. (right (b)(d)): Representations distance between vertex features extracted from pre-trained model and fine-tuned model. Benefitting from the soft alignment of MOT, the GTOT regularizer constrains the behaviors but allows the weights to be self-adapted to the downstream task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test ROC-AUC (%) of GIN(contexpred) on downstream molecular property prediction benchmarks.('?' denotes performance improvement compared to the Fine-Tuning baseline. ) 0?2.3? 2.0 75.6?0.7? 0.1 64.0?0.3? 0.1 63.5?0.6? 2.6 72.0?5.4? 6.1 80.0?1.8? 4.2 78.2?0.7? 0.9 83.4?1.9? 3.8 73.34? 2.49</figDesc><table><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>Toxcast</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell><cell>Average</cell></row><row><cell>Fine-Tuning (baseline)</cell><cell>68.0?2.0</cell><cell>75.7?0.7</cell><cell>63.9?0.6</cell><cell>60.9?0.6</cell><cell>65.9?3.8</cell><cell>75.8?1.7</cell><cell>77.3?1.0</cell><cell>79.6?1.2</cell><cell>70.85</cell></row><row><cell cols="2">L2 SP [Xuhong et al., 2018] 68.2?0.7</cell><cell>73.6?0.8</cell><cell>62.4?0.3</cell><cell>61.1?0.7</cell><cell>68.1?3.7</cell><cell>76.7?0.9</cell><cell>75.7?1.5</cell><cell>82.2?2.4</cell><cell>70.25</cell></row><row><cell>DELTA[Li et al., 2018b]</cell><cell>67.8?0.8</cell><cell>75.2?0.5</cell><cell>63.3?0.5</cell><cell>62.2?0.4</cell><cell>73.4?3.0</cell><cell>80.2?1.1</cell><cell>77.5?0.9</cell><cell>81.8?1.1</cell><cell>72.68</cell></row><row><cell>Feature(DELTA w/o ATT)</cell><cell>61.4?0.8</cell><cell>71.1?0.1</cell><cell>61.5?0.2</cell><cell>62.4?0.3</cell><cell>64.0?3.4</cell><cell>78.4?1.1</cell><cell>74.0?0.5</cell><cell>76.3?1.1</cell><cell>68.64</cell></row><row><cell>BSS[Chen et al., 2019]</cell><cell>68.1?1.4</cell><cell>75.9?0.8</cell><cell>63.9?0.4</cell><cell>60.9?0.8</cell><cell>70.9?5.1</cell><cell>78.0?2.0</cell><cell>77.6?0.8</cell><cell>82.4?1.8</cell><cell>72.21</cell></row><row><cell cols="2">StochNorm [Kou et al., 2020] 69.3?1.6</cell><cell>74.9?0.6</cell><cell>63.4?0.5</cell><cell>61.0?1.1</cell><cell>65.5?4.2</cell><cell>76.0?1.6</cell><cell>77.6?0.8</cell><cell>80.5?2.7</cell><cell>71.03</cell></row><row><cell>GTOT-Tuning (Ours)</cell><cell>70.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test ROC-AUC (%) of GIN(supervised contexpred) on downstream molecular property prediction benchmarks. 8? 2.8 78.6?0.3? 0.5 66.6?0.4? 0.9 63.3?0.6? 0.6 77.9?3.2? 5.3 85.0?0.9? 3.7 81.1?0.5? 1.2 85.3?1.5? 0.8 76.16? 1.97</figDesc><table><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>Toxcast</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell><cell>Average</cell></row><row><cell>Fine-Tuning (baseline)</cell><cell>68.7?1.3</cell><cell>78.1?0.6</cell><cell>65.7?0.6</cell><cell>62.7?0.8</cell><cell>72.6?1.5</cell><cell>81.3?2.1</cell><cell>79.9?0.7</cell><cell>84.5?0.7</cell><cell>74.19</cell></row><row><cell cols="2">L2 SP [Xuhong et al., 2018] 68.5?1.0</cell><cell>78.7?0.3</cell><cell>65.7?0.4</cell><cell>63.8?0.3</cell><cell>71.8?1.6</cell><cell>85.0?1.1</cell><cell>77.5?0.9</cell><cell>84.5?0.9</cell><cell>74.44</cell></row><row><cell>DELTA[Li et al., 2018b]</cell><cell>68.4?1.2</cell><cell>77.9?0.2</cell><cell>65.6?0.2</cell><cell>62.9?0.8</cell><cell>72.7?1.9</cell><cell>85.9?1.3</cell><cell>75.6?0.4</cell><cell>79.0?1.1</cell><cell>73.50</cell></row><row><cell>Feature(DELTA w/o ATT)</cell><cell>68.6?0.9</cell><cell>77.9?0.2</cell><cell>65.7?0.2</cell><cell>63.0?0.6</cell><cell>72.7?1.5</cell><cell>85.6?1.0</cell><cell>75.7?0.3</cell><cell>78.4?0.7</cell><cell>73.45</cell></row><row><cell>BSS[Chen et al., 2019]</cell><cell>70.0?1.0</cell><cell>78.3?0.4</cell><cell>65.8?0.3</cell><cell>62.8?0.6</cell><cell>73.7?1.3</cell><cell>78.6?2.1</cell><cell>79.9?1.4</cell><cell>84.2?1.0</cell><cell>74.16</cell></row><row><cell cols="2">StochNorm [Kou et al., 2020] 69.8?0.9</cell><cell>78.4?0.3</cell><cell>66.1?0.4</cell><cell>62.2?0.7</cell><cell>73.2?2.1</cell><cell>82.5?2.6</cell><cell>80.2?0.7</cell><cell>84.2?2.3</cell><cell>74.58</cell></row><row><cell>GTOT-Tuning (Ours)</cell><cell>71.5?0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Test ROC-AUC (%) of GIN(contextpred) on downstream</cell></row><row><cell cols="5">tasks. (The parentheses indicate the masked matrix. MWD(A) is</cell></row><row><cell>GTOT distance.)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>Toxcast</cell><cell>SIDER</cell></row><row><cell># BPTs</cell><cell>1</cell><cell>12</cell><cell>617</cell><cell>27</cell></row><row><cell># A distance</cell><cell>1.57</cell><cell>1.56</cell><cell>1.56</cell><cell>1.41</cell></row><row><cell>w/o MWD</cell><cell cols="4">68.7?3.4 75.9?0.5 63.1?0.6 60.2?0.9</cell></row><row><cell cols="5">w/ MWD(1 n?n ) 66.2?3.3 75.3?0.9 63.6?0.6 62.7?0.7</cell></row><row><cell>w/ MWD(A)</cell><cell cols="4">69.6?2.6 75.7?0.5 63.8?0.4 63.5?0.6</cell></row><row><cell>w/ MWD(I)</cell><cell cols="4">68.6?3.5 75.4?0.7 64.1?0.3 63.7?0.5</cell></row><row><cell>Methods</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell></row><row><cell># BPTs</cell><cell>2</cell><cell>17</cell><cell>1</cell><cell>1</cell></row><row><cell># A distance</cell><cell>1.41</cell><cell>1.19</cell><cell>1.65</cell><cell>1.65</cell></row><row><cell>w/o MWD</cell><cell cols="4">69.5?5.0 69.5?1.3 78.2?1.2 82.5?1.7</cell></row><row><cell cols="5">w/ MWD(1 n?n ) 69.5?5.0 74.3?1.3 78.2?0.8 83.7?1.9</cell></row><row><cell>w/ MWD(A)</cell><cell cols="4">70.9?5.8 80.7?0.6 78.5?1.5 83.1?1.9</cell></row><row><cell>w/ MWD(I)</cell><cell cols="4">69.7?4.0 80.2?0.9 78.3?1.3 82.6?2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test ROC-AUC (%) of GIN (contexpred) on downstream molecular property prediction benchmarks.</figDesc><table><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>Toxcast</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell><cell>Average</cell></row><row><cell>Fine-Tuning (baseline)</cell><cell>68.0?2.0</cell><cell>75.7?0.7</cell><cell>63.9?0.6</cell><cell>60.9?0.6</cell><cell>65.9?3.8</cell><cell>75.8?1.7</cell><cell>77.3?1.0</cell><cell>79.6?1.2</cell><cell>70.85</cell></row><row><cell cols="2">D Details of Experiments.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>n?n ) 68.6?1.9 75.3?0.5 63.6?0.5 61.4?1.0 GTOT-MGWD(A) 69.1?2.5 75.2?0.8 64.2?0.4 62.0?1.2</figDesc><table><row><cell cols="5">: Test ROC-AUC (%) of GIN (contextpred) on downstream molecular property prediction benchmarks.(BPTs means Binary prediction</cell></row><row><cell>tasks.)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>Toxcast</cell><cell>SIDER</cell></row><row><cell># BPTs</cell><cell>1</cell><cell>12</cell><cell>617</cell><cell>27</cell></row><row><cell>GTOT-MGWD(1 Methods</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell></row><row><cell># BPTs</cell><cell>2</cell><cell>17</cell><cell>1</cell><cell>1</cell></row><row><cell cols="5">GTOT-MGWD(1 n?n ) 62.8?9.5 75.8?2.0 77.8?0.9 83.2?1.7</cell></row><row><cell>GTOT-MGWD(A)</cell><cell cols="4">68.5?7.2 77.2?1.8 77.7?0.9 83.4?2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameter search range for GTOT-Tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Statistical information on downstream task datasets. We provide (median, max, min, mean, std) number of graph vertices in different datasets.</figDesc><table><row><cell>dataset</cell><cell cols="6"># Molecules # training set median max min mean</cell><cell>std</cell></row><row><cell>BBBP</cell><cell>2039</cell><cell>1631</cell><cell>22</cell><cell>63</cell><cell>2</cell><cell>22.5</cell><cell>8.1</cell></row><row><cell>Tox21</cell><cell>7831</cell><cell>6264</cell><cell cols="2">14 114</cell><cell>1</cell><cell>16.5</cell><cell>9.5</cell></row><row><cell>Toxcast</cell><cell>8575</cell><cell>6860</cell><cell cols="2">14 103</cell><cell>2</cell><cell>16.7</cell><cell>9.7</cell></row><row><cell>SIDER</cell><cell>1427</cell><cell>1141</cell><cell cols="2">23 483</cell><cell>1</cell><cell cols="2">30.0 39.7</cell></row><row><cell>ClinTox</cell><cell>1478</cell><cell>1181</cell><cell cols="2">23 121</cell><cell>1</cell><cell cols="2">25.5 15.3</cell></row><row><cell>MUV</cell><cell>93087</cell><cell>74469</cell><cell>24</cell><cell>44</cell><cell>6</cell><cell>24.0</cell><cell>5.0</cell></row><row><cell>HIV</cell><cell>41127</cell><cell>32901</cell><cell cols="2">23 222</cell><cell>2</cell><cell cols="2">25.3 12.0</cell></row><row><cell>BACE</cell><cell>1513</cell><cell>1210</cell><cell>32</cell><cell>66</cell><cell>10</cell><cell>33.6</cell><cell>7.8</cell></row><row><cell>to GNNs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Test ROC-AUC (%) of GCN (contexpred) on downstream molecular property prediction benchmarks. Tuning (baseline) 68.5?1.3 73.8?0.7 63.7?0.9 60.3?1.1 68.1?6.9 75.3?1.3 77.4?1.1 79.5?4.3 70.83 GTOT-Tuning (Ours) 70.3?1.7 75.5?0.5 64.2?0.5 60.2?1.0 78.6?2.8 79.3?1.6 77.8?0.9 81.5?1.7 73.44 GTOT-MGWD (Ours) 69.9?1.9 74.0?0.7 64.0?0.5 60.2?0.9 76.2?5.2 80.8?1.1 79.1?0.9 81.8?1.3 73.25</figDesc><table><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>Toxcast</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell><cell>Average</cell></row><row><cell>Fine-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>The average fine-tuning training time per epoch with different methods is shown below and timings are measured in seconds. The float in parentheses is the standard deviation. ) 22?0.47 13.95?0.65 11.34?1.19 3.26?0.20 2.65?0.19 43.39?0.31 30.49?8.52 6.80?0.32</figDesc><table><row><cell>Methods</cell><cell>BBBP</cell><cell>Tox21</cell><cell>Toxcast</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>MUV</cell><cell>HIV</cell><cell>BACE</cell></row><row><cell>Fine-Tuning (baseline)</cell><cell>5.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">ExperimentsWe conduct experiments on graph classification tasks to evaluate our methods.7.1 Comparison of Various Fine-tuning Strategies.Settings. We reuse two pretrained models released by(https://github.com/snap-stanford/pretrain-gnns) as backbones: GIN (contextpred), which is only pretrained via self-supervised task Context Prediction, and GIN (supervised contextpred), an architecture that is pretrained by Context Prediction + Graph Level multitask supervised strategy. Both networks are pre-trained on the Chemistry dataset (with 2 million molecules). In addition, eight binary classification datasets in MoleculeNet serve as downstream tasks for evaluating the finetuning strategies, where the scaffold split scheme is used for dataset split. More details can be found in Appendix. Baselines. Since we have not found related works about finetuning GNNs, we extend several typical baselines tailored for Convolutional networks to GNNs, includingL2 SP [Xuhong  et al., 2018], DELTA, BSS, SotchNorm. Results. The results with different fine-tuning strategies are shown inTable 1,2. Observation (1): GTOT-Tuning gains a competitive performance on different datasets and outperforms other methods on average. Observation (2): Weights</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In this paper, Mij = 0 represents the ij-th element of the matrix being masked. 4 Namely, min P?U(M,a,b) M P, C ? H(M P), where H(?) is Entropy function. Assume that 0 log 0 = 0 to ensure that H(M P)is well defined.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://pytorch-geometric.readthedocs.io/en/latest/notes/create gnn.html 6 https://github.com/thuml/Transfer-Learning-Library</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Ximei Wang ,Guoji Fu and Guanzi Chen for their sincere and selfless help.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Near-linear time approximation algorithms for optimal transport via sinkhorn iteration</title>
		<idno type="arXiv">arXiv:2006.04804</idno>
	</analytic>
	<monogr>
		<title level="m">Optimal transport graph neural networks</title>
		<editor>B?cigneul et al., 2020] Gary B?cigneul, Octavian-Eugen Ganea, Benson Chen, Regina Barzilay, and Tommi Jaakkola</editor>
		<imprint>
			<date type="published" when="1965" />
			<biblScope unit="page" from="329" to="344" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving sequence-to-sequence learning via optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Olivier Bousquet and Andr? Elisseeff. Stability and generalization. JMLR</title>
		<imprint>
			<publisher>Chen and Zhang</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="499" to="526" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jeff Donahue and Yangqing et all. Jia. Decaf: A deep convolutional activation feature for generic visual recognition</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>Donahue and Jia</publisher>
			<date type="published" when="1908" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
	</analytic>
	<monogr>
		<title level="m">ICLR (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery -A Focus on Affinity Prediction Problems with Noise Annotations</title>
		<idno type="arXiv">arXiv:2201.09637</idno>
		<imprint>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic normalization</title>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delta: Deep learning transfer using feature map with attention for convolutional networks</title>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno type="arXiv">arXiv:2007.06737</idno>
		<title level="m">R?mi Flamary, Nicolas Courty, and Dejing Dou. Representation transfer by optimal transport</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning and transferring midlevel image representations using convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>NeurIPS, 2020. [Sterling and Irwin, 2015] Teague Sterling and John J Irwin</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="513" to="530" />
		</imprint>
	</monogr>
	<note>Chemical science. Moleculenet: a benchmark for molecular machine learning. Chemical science</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Naganand Yadati. Neural message passing for multi-relational ordered and recursive hypergraphs</title>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? NeurIPS</title>
		<editor>Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen</editor>
		<meeting><address><addrLine>Jason Yosinski, Jeff Clune</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
	<note>ICLR. You et al., 2020] Yuning You. Graph contrastive learning with augmentations. NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sp [xuhong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delta[li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bss[chen</surname></persName>
		</author>
		<idno>2019] 68.1?1.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Stochnorm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gtot-Mgwd</surname></persName>
		</author>
		<imprint>
			<publisher>Ours</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">GTOT(MWD+MGWD)(Ours)</title>
		<imprint>
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L2</forename><surname>Sp [xuhong</surname></persName>
		</author>
		<idno>2018b] 5.74?0.32 11.42?1.25 11.06?0.25 5.77?0.63 3.18?0.20 113.62?11.92 123.79?7.89 5.03?1.19</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Feature(DELTA w/o ATT)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bss [chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Stochnorm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kou</surname></persName>
		</author>
		<idno>2020] 4.54?0.45 17.84?0.28 31.11?2.38 3.29?0.51 3.29?0.37 251.43?86.79 79.59?11.15 3.82?0.25</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gtot-Tuning</surname></persName>
		</author>
		<imprint>
			<publisher>Ours</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
