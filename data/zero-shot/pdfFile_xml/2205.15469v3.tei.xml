<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Co-saliency</term>
					<term>CoSOD</term>
					<term>Group Collaborative Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel end-to-end group collaborative learning network, termed GCoNet+, which can effectively and efficiently (250 fps) identify co-salient objects in natural scenes. The proposed GCoNet+ achieves the new state-of-the-art performance for co-salient object detection (CoSOD) through mining consensus representations based on the following two essential criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module (GAM); 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module (GCM) conditioning on the inconsistent consensus. To further improve the accuracy, we design a series of simple yet effective components as follows: i) a recurrent auxiliary classification module (RACM) promoting the model learning at the semantic level; ii) a confidence enhancement module (CEM) helping the model to improve the quality of the final predictions; and iii) a group-based symmetric triplet (GST) loss guiding the model to learn more discriminative features. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that our GCoNet+ outperforms the existing 12 cutting-edge models. Code has been released at https://github.com/ZhengPeng7/GCoNet plus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C O-SALIENT object detection (CoSOD) aims at detecting the most common salient objects among a group of given relevant images. Compared with the standard salient object detection (SOD) task, CoSOD is more challenging and needs to distinguish co-occurring objects across different images where others act as distractors. To this end, intra-class compactness and inter-class separability are two important cues and should be learned simultaneously. With the success achieved by the latest CoSOD methods, CoSOD is often used as a pre-processing for other vision tasks, including co-segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, semantic segmentation <ref type="bibr" target="#b3">[4]</ref>, video analysis <ref type="bibr" target="#b4">[5]</ref>, image quality assessment <ref type="bibr" target="#b5">[6]</ref>, etc.</p><p>Existing works attempt to facilitate the consistency among given images to solve the CoSOD task within an image group by leveraging semantic connections <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr">[</ref> Switzerland, and also with KU Leuven, Leuven, Belgium. ? A preliminary version of this work has appeared in CVPR 2021 <ref type="bibr" target="#b0">[1]</ref> ? ? The major part of this work was done while Peng Zheng was an intern at IIAI mentored by Deng-Ping Fan.  <ref type="bibr" target="#b6">[7]</ref>. We conduct the comparison of existing representative deep-learning based CoSOD approaches in terms of both speed (the horizontal axis) and accuracy (the vertical axis). Smaller bubbles means lighter models. Our GCoNet+ outperforms these models in terms of both efficiency and effectiveness. The "Train-1, 2, and 3" represents the DUTS class, COCO-9k, and COCO-SEG datasets, respectively (see Tab. 3 for more related details).</p><p>or varied shared cues <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Some of these works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref> try to jointly optimize a unified network for saliency maps and co-saliency information. Despite the improvement brought by these methods, most existing models only depend on the consistent feature representations in an individual group <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b18">[19]</ref>, which may introduce the arXiv:2205.15469v3 [cs.CV] 30 Jul 2022 following limitations. First, images from the same group can only provide positive relations instead of both positive and negative relations between different objects. Training models with only positive samples from a single group may lead to overfitting and ambiguous results for outlier images. Besides, there are typically a limited number of images in one group (20 to 40 images for most groups on existing CoSOD datasets). Therefore, information learned from a single group tends to be insufficient for a discriminative representation. Finally, individual image groups may not be easy to mine semantic cues, which are vital in distinguishing noisy objects during testing in complex real-world scenes. Due to the complexity of image context in real scenarios, a module designed for common information mining is in high demand. Apart from these, when supervised with the Binary Cross Entropy (BCE) loss, pixel values of generated saliency maps tend to get closer to 0.5 instead of 0 or 1. Suffering from the uncertainty, these maps are difficult to be directly applied in realistic applications.</p><p>To break the above restrictions, we design a new group collaborative learning network (GCoNet) to establish the semantic relationship between different image groups. Our GCoNet includes three basic modules: GAM (Group Affinity Module), GCM (Group Collaborating Module), and ACM (Auxiliary Classification Module), which simultaneously learn the inter-group separability and intra-group compactness. The GAM enables the model to learn the consensus feature in the same image group, while the GCM discriminates target attributes between different groups, thus making the network trainable on existing rich SOD datasets 1 . To learn a better embedding space, we utilize the ACM on each image to improve the feature representation at a global semantic level. Compared with our CVPR 2021 version of GCoNet <ref type="bibr" target="#b0">[1]</ref>, three novel designs have been introduced to GCoNet+ in this paper, and more details about our enhanced approach are summarized as follow:</p><p>? We upgrade the ACM to a recurrent auxiliary classification module (RACM) to further guide the model learning at a global semantic level. ? To achieve better final predictions, we propose the confidence enhancement module (CEM) in our GCoNet+. ? Based on the metric learning strategy, we introduce a group-based symmetric triplet (GST) loss for GCoNet+ to learn more discriminative features. ? We provide more qualitative and quantitative results to better understand how GCoNet+ achieves state-of-theart performance as well as high test speed (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Salient Object Detection</head><p>Among traditional salient object detection (SOD) methods, hand-crafted features play the most important role in detection <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>. In the early years of deep learning, features are extracted from image patches, object proposals <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>, or super-pixels <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Although these methods have <ref type="bibr" target="#b0">1</ref>. There are about 60k SOD images publicly available, which is about 10 times larger than existing CoSOD datasets. This means the insufficient training data issue in CoSOD may be partially alleviated in the proposed framework. made some progress, they are time-consuming for extracting the target regions and their features. With the success of fully convolutional networks <ref type="bibr" target="#b30">[31]</ref> in segmentation tasks, recent SOD researches mainly focus on the models which make pixel-wise predictions. More details and a summary can be found in recent review works <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, the network architectures in SOD methods are divided into five categories, i.e., single-steam, multi-stream, side-fusion, Ushape, and multi-branch. Among these architectures, the U-shape is the most widely used one, especially the base structure of FPN <ref type="bibr" target="#b35">[36]</ref> and U-Net <ref type="bibr" target="#b36">[37]</ref>. Multi-stage supervision is employed at the early stages by aggregating features from different stages of these U-shape networks to make the output features more robust and stable <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[38]</ref>. In <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b41">[42]</ref>, the attention mechanism and the related modules are designed for improvement. Besides, external information is introduced as extra guidance on its training processes, such as edge <ref type="bibr" target="#b37">[38]</ref> and boundary <ref type="bibr" target="#b42">[43]</ref>.</p><p>In the binary segmentation tasks (e.g., Salient Object Detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b42">[43]</ref>, Optical Character Recognition <ref type="bibr" target="#b43">[44]</ref>- <ref type="bibr" target="#b45">[46]</ref>), ground truths are the binary maps of target objects. However, the predicted maps are not fully binary ones due to pixel-level loss (i.e., mean-square error loss, binary cross-entropy loss). In many practical applications, maps with much uncertainty are unsuitable for programs to make decisions <ref type="bibr" target="#b46">[47]</ref>. In that case, some up-to-date methods are proposed for improving the quality of binary maps. In <ref type="bibr" target="#b47">[48]</ref>, specific components are designed to enhance the integrity of objects. In <ref type="bibr" target="#b42">[43]</ref>, hybrid losses are also employed to make models focus on more attributes beyond pixel-level errors. High-quality maps are also important in applications <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Co-Salient Object Detection</head><p>The SOD task <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b50">[51]</ref> aims at segmenting salient objects separately in a single image, while CoSOD targets finding common salient objects across a group of semanticrelated images. Previous CoSOD methods mainly aimed at mining intra-group cues to segment the co-salient objects. For example, early CoSOD approaches used to explore the correspondence among a group of relevant images based on handcrafted cues. With computational fractions (e.g., superpixels <ref type="bibr" target="#b51">[52]</ref>) segmented from each image, these methods establish the correspondence model and discover the common regions by employing a ranking scheme, clustering guidance, or translation alignment <ref type="bibr" target="#b52">[53]</ref>. Metric learning <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b53">[54]</ref>, statistics of histograms and contrasts <ref type="bibr" target="#b19">[20]</ref>, and pairwise similarity ranking are also applied to formulate better semantic attributes for further computation.</p><p>In the deep learning era, many end-to-end deep CoSOD models have been proposed. Some methods try to discover the common objects by learning the consensus in a single group <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>. With the development of the upstream deep learning methods, some algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> build their models with more powerful CNN models (e.g., ResNet <ref type="bibr" target="#b56">[57]</ref>, VGGNet <ref type="bibr" target="#b57">[58]</ref>, and Inception <ref type="bibr" target="#b58">[59]</ref>) or even Transformer models (e.g., ViT <ref type="bibr" target="#b59">[60]</ref> and PVT <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>), which help achieve SOTA performances. Although most existing works design their models by full supervision, others try to achieve acceptable results via weakly-supervised strategies (e.g., GWSCoSal <ref type="bibr" target="#b62">[63]</ref>, FASS <ref type="bibr" target="#b63">[64]</ref>, SP-MIL <ref type="bibr" target="#b64">[65]</ref>, CODW <ref type="bibr" target="#b65">[66]</ref>, and GONet <ref type="bibr" target="#b66">[67]</ref>). shows, our GCoNet+has a more global response and does not make specific prediction in very early stages, where the quality of feature maps are hard to produce precise results. (g) Prediction of co-saliency maps. Compared with GCoNet, GCoNet+ obtained a more global response on the objects and its surrounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCoNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCoNet+</head><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Intra-and Inter-image Consistency Learning</head><p>With the rapid development of deep learning, deep models also achieved great performance in exploring intraand inter-image consistency, such as graph convolutional networks (GCN) <ref type="bibr" target="#b67">[68]</ref>- <ref type="bibr" target="#b69">[70]</ref>, co-attention <ref type="bibr" target="#b6">[7]</ref>, co-clustering <ref type="bibr" target="#b70">[71]</ref>, recurrent units <ref type="bibr" target="#b71">[72]</ref>, correlation techniques <ref type="bibr" target="#b17">[18]</ref>, selflearning methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b72">[73]</ref>, and quality measuring <ref type="bibr" target="#b73">[74]</ref>. Some approaches simultaneously optimize co-salient object detection tasks and other sub-tasks in a multi-task learning way, including co-segmentation <ref type="bibr" target="#b74">[75]</ref> and co-peak search <ref type="bibr" target="#b2">[3]</ref>. Some works extract hierarchical features in a multiscale <ref type="bibr" target="#b75">[76]</ref>, multi-stage <ref type="bibr" target="#b76">[77]</ref>, and multi-layer <ref type="bibr" target="#b77">[78]</ref> style. Significant improvement has also been achieved in exploring group-wise semantic representation, which is then used to detect regions of objects with common classes. On the other side, more advanced methods on learning discriminative representation are also given, such as gradient feedback <ref type="bibr" target="#b13">[14]</ref>, group attentional semantic aggregation <ref type="bibr" target="#b15">[16]</ref>, integrated multi-layer graph <ref type="bibr" target="#b78">[79]</ref>, and united fully convolutional network <ref type="bibr" target="#b14">[15]</ref>. Semi-supervised <ref type="bibr" target="#b79">[80]</ref> and unsupervised <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b80">[81]</ref>- <ref type="bibr" target="#b82">[83]</ref> methods are also proposed to solve the CoSOD task. Studies <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref> are available on detecting co-salient objects from a single image. Among the existing works, most of the attention has been paid to intra-group cues to learn the consensus feature of a specific category. In contrast, the inter-group information is paid less attention to, which also contributes a lot to guiding the model to learn more discriminative and general features for each class. In <ref type="bibr" target="#b13">[14]</ref>, a jigsaw training strategy is used to implicitly introduce images from other groups to facilitate the group training. But there is no more advanced and explicit design for learning inter-group information, and their models still target at the intra-group information. Our approach varies a lot from existing models in exploring inter-group relations. We try to learn discriminative features semantically, explicitly, and precisely at a group level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We introduce our GCoNet+ for the CoSOD task. The overview of the architecture is presented in Sec. 3.1. Then, we sequentially introduce the proposed basic modules: group affinity module (GAM), group collaborating module (GCM), confidence enhancement module (CEM), groupbased symmetric triplet (GST) loss, and the recurrent auxiliary classification module (RACM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The basic framework of the proposed GCoNet+ is based on our GCoNet <ref type="bibr" target="#b0">[1]</ref>, which is one of the latest state-of-the-art methods. Unlike existing CoSOD models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> that only exploit the common information inside a single class group, GCoNet+ exploits both the internal and external relationship between different groups in a siamese style.</p><p>The flowchart of GCoNet+ is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. First, our model simultaneously takes two groups of raw images G 1 , G 2 as input. With the concatenated image groups (?), our encoder extracts the feature maps F, which is then fed to the auxiliary classification module (ACM) for classification and to our group collaborative module (GCoM) for further processing. In GCoM, F is split into two parts by their classes, i.e., F 1 = {F 1,n } N n=1 , F 2 = {F 2,n } N n=1 ? R N ?C?H?W , where C denotes the channel number, H ? W is the spatial size, and N denotes the group size. These two features are separately given to the group affinity module (GAM), where all the single-image features are combined to distill the consensus features E ? 1 ? R 1?C?1?1 . Meanwhile, a group collaborating module (GCM) is applied to obtain a more discriminative representation of target attributes between different image groups. The output features F out 1 , F out 2 of GCoM are concatenated to be fed to our decoder. Simultaneously, the decoder is connected with the encoder by 1x1 convolution layers. Then, the confidence enhancement module (CEM) takes the prediction of decoder F d as input to refine and provide the final co-saliency maps M 1 , M 2 . At last, the network's output is multiplied with original images G to eliminate the irrelevant regions. Our group-based symmetric triplet (GST) loss is applied on the masked images G M to supervise GCoNet+ in a metric learning way. Besides, the masked images are then fed to the encoder again to obtain the masked encoded feature F r . Different from F, F r contains only the features of predicted regions and has a more precise semantic representation to be applied in the recurrent auxiliary classification module (RACM) to obtain the classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Group Affinity Module (GAM)</head><p>In most cases of real life, objects of the same class share similarity in their appearance and features, which have been widely used in many computer vision tasks. For example, self-supervised video tracking methods <ref type="bibr" target="#b85">[86]</ref>- <ref type="bibr" target="#b88">[89]</ref> often propagate the segmentation maps of target objects based on the pixel-wise correspondences between two adjacent frames. Therefore, we introduce this motivation to the CoSOD task by computing the global affinity among all images in the same group.</p><p>For features {F 1,n , F 1,m } ? F 1 of any two images 2 , we compute their pixel-wise correlations in the format of an inner product:</p><formula xml:id="formula_1">S (n,m) = ?(F n ) T ?(F m ),<label>(1)</label></formula><p>where ?, ? denote linear embedding functions (3 ? 3 ? 512 convolutional layer). The affinity map S (n,m) ? R HW ?HW efficiently captures the common features of co-salient objects in the given image pair (n, m). Then we can generate F n 's affinity map A n?m ? R HW ?1 by finding the maxima for each of F n 's pixels conditioned on F m , which alleviates the influence of noisy correlation values in the map. Similarly, we can extend the use of the local affinity of an image pair to the global affinity of an entire image group. Specifically, we compute the affinity map S F ? R N HW ?N HW of all image features F using Eq. 1. Then, we find the maxima for each image A F ? R N HW ?N from S F , and average all the maxima of N images to generate the global affinity attention map A F ? R N HW ?1 . In this way, the affinity attention map is globally optimized on all images, and the influence of occasional co-occurring bias is thus alleviated. Then, we use a softmax operation to normalize A F and reshape it to produce the attention map A S ? R N ?(1?H?W ) . With the attention map A S , we multiply it with the original feature F to generate the attention feature maps F a ? R N ?C?H?W . Finally, the attention feature maps F a for the whole group are used to generate the attention consensus E a by the average pooling along both the batch dimension and spatial dimension, as illustrated in <ref type="figure">Fig. 4</ref>.</p><p>The GAM focuses on capturing the commonality of co-occurring salient objects within the same group, thus improving the intra-group compactness of the consensus representation. Such intra-group compactness alleviates the distraction made by co-occurring noise and encourages the model to concentrate on the co-salient regions. This allows the shared attributes of co-salient objects to be better captured, resulting in better consensus representation. The obtained attention consensus E a is combined with the original feature maps F through depth-wise correlation <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref> to achieve efficient information association. The generated feature maps F out of different groups are then concatenated together and fed to the decoder. After the confidence enhancement module (CEM), final co-saliency maps M are produced for all images.</p><p>2. All analyses in Sec. 3.2 on F 1 can be applied to F 2 . We omit the group subscript for notation simplicity, i.e., we use Fn to represent F 1,n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Group Collaborating Module (GCM)</head><p>Currently, most existing CoSOD approaches tend to focus on the intra-group compactness of the consensus. Still, the intergroup separability is crucial for distinguishing distracting objects, especially when processing complex images with more than one salient object. To this end, we propose a simple but effective module, i.e., the GCM, by learning to encode the inter-group separability.</p><p>With the GAM, we can obtain the attention consensus {E a 1 , E a 2 } of images in two groups. Then, we apply an intraand inter-group cross-multiplication (?) between the corresponding features {F 1 , F 2 } and the attention consensus to get the the intra-group collaboration:</p><formula xml:id="formula_2">F 1 1 = F 1 ? E a 1 and F 2 2 = F 2 ? E a 2 .</formula><p>In contrast, the inter-group multiplication deals with the features and consensus of different groups, i.e., F 2 1 = F 1 ? E a 2 and F 1 2 = F 2 ? E a 1 , to represent the inter-group interaction. The intra-group representation</p><formula xml:id="formula_3">F + = {F 1 1 , F 2 2</formula><p>} is computed to predict co-saliency maps, and the inter-group representation F ? = {F 2 1 , F 1 2 } is employed for a consensus with group separability. Specifically, we feed the inter-group and intra-group features {F + , F ? } to a small convolutional network with an upsampling layer and obtain the saliency maps {M + , M ? } 3 with different supervision signals. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, we use the ground truth maps to make supervision on F + , while all-zero maps on F ? . The loss function is:</p><formula xml:id="formula_4">L GCM = 1 N N n L FL (&lt; M + n , M ? n &gt;, &lt; G n , G 0 n &gt;),<label>(2)</label></formula><p>where L FL denotes the focal loss <ref type="bibr" target="#b35">[36]</ref>, G n denotes the ground truth, G 0 n denotes the all-zero map, and &lt; ? &gt; denotes the concatenation operation.</p><p>Consequently, GCM lets the consensus have a high intergroup separability between different groups and makes it easier to identify distractors in a complex environment. Specifically, this module doesn't introduce additional computation during inference and can be fully discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Confidence Enhancement Module (CEM)</head><p>In SOD tasks, the pixel values of predicted saliency maps range between 0 and 1 since the network is usually appended with a Sigmoid function. However, although the ground truth maps are all 0 or 1, the predicted saliency maps may get close to 0.5 in some conditions, which means there is more uncertainty and less confidence in the prediction. Results with more unconfidence may increase the similarity between the prediction and ground truth maps, while far from practical application, which is the final goal.</p><p>To deal with the uncertain values of the prediction, we conduct research from both the perspective of the loss function and network architecture. From the view of the loss function, we set up comparative experiments to verify that different loss functions can introduce different optimization directions to the same network. To be more specific, IoU loss guides the outputs to be almost 0 or 1, but the low accuracy of existing metrics, i.e., S-measure <ref type="bibr" target="#b91">[92]</ref>, E-measure <ref type="bibr" target="#b92">[93]</ref>. In contrast, BCE loss directs the network to predict more uncertain values but achieve better performance in Input images are obtained from two groups and fed into an encoder. Then we employ the GCoM (Group Collaborative Module), where intra-group collaborative learning is conducted for each group by the group affinity module (GAM), and the inter-group collaborative learning is conducted via the group collaborating module (GCM). The original images and RoIs masked by the output are given to the encoder to do an auxiliary classification to make the features of different classes more discriminative to each other. The decoder output is put through the confidence enhancement module (CEM) to make the final result more binarized and easy to use. Furthermore, the RoIs of two groups obtained by the multiplication of original images and predicted saliency maps are measured with a triplet loss to enlarge the distance between inter-group features and reduce the distance between intra-group features. <ref type="figure">Fig. 4</ref>. Group Affinity Module. We first utilize the affinity attention to obtain the attention maps of the input features by collaborating all images in group. Subsequently, the maps are multiplied with the input features to generate the consensus for the group. Then the obtained consensus is used to coordinate the original feature maps and is also fed to the GCM for inter-group collaborative learning. is supervised with the available ground truth labels. Otherwise, it is supervised by the all-zero maps.</p><formula xml:id="formula_5">3. M + = {M + 1 , M + 2 } and M ? = {M ? 1 , M ? 2 }.</formula><formula xml:id="formula_6">N x (1 x H x W) ? ? a NHW x NHW NHW x N x HW NHW x N NHW x 1 ? ? ? ? R S R M A A A R Reshape M Maximize A Average S R Softmax &amp; Reshape A A Batch &amp; Spatial Average Multiplication N x (C x H x W)</formula><p>the above metrics. As the expected maps shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, although IoU loss brings high confidence to predicted maps, the optimization is too rough. It acts up in terms of the integrity of the saliency maps. Therefore, BCE loss is still a necessity for training. To improve the quality of saliency maps in terms of binarization for practical application, we try to balance the BCE and IoU loss as a mixed pixel loss for supervision.</p><p>From the view of network architecture, we employ the confidence enhancement module (CEM) at the end of <ref type="figure" target="#fig_2">Fig. 3</ref>. In previous SOD approaches, the Sigmoid function is usually applied to squeeze the output values from 0 to 1. However, as is described in <ref type="bibr" target="#b43">[44]</ref>, the Sigmoid activation function is not steep enough, and the values produced by it are not binarized enough. To address this issue, as shown in <ref type="figure">Fig. 7</ref>, the output feature F d of the decoder is fed into the CEM. Firstly, the feature F d goes through two parallel branches with two 3x3 convolution layers, which are both followed by batch normalization, a ReLU activation function, and an 1x1 convolution layer followed by a Sigmoid activation function. After that, the probability map P and the threshold map T are generated and put into  <ref type="figure">Fig. 7</ref>. Confidence Enhancement Module. After the decoder, we adapt the CEM to bring higher quality and binarization to the predicted saliency maps. CBR means a convolution layer followed by a batch normalization layer and a ReLU activation function.</p><formula xml:id="formula_7">L BCE + L IoU F d</formula><p>the differentiable binarization function to obtain the final prediction. According to <ref type="bibr" target="#b43">[44]</ref>, the final co-saliency maps M can be represented as:</p><formula xml:id="formula_8">M i,j = 1 1 + e ?k(Pi,j ?Ti,j ) ,<label>(3)</label></formula><p>where k is the factor that controls the steepness of the step function. In our implementation, k is set to 300 as the default value. When loss meets Nan during training, it will be replaced with 50 for current propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Group-based Symmetric Triplet (GST) Loss</head><p>In the past few years, some approaches are designed to solve the CoSOD task from the perspective of metric learning <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b53">[54]</ref>. However, most of existing CoSOD approaches based on metric learning use super-pixel <ref type="bibr" target="#b93">[94]</ref> to extract fractions as the unit of measurement. Most of these methods are usually not end-to-end and in low efficiency. Besides, existing works typically introduce the class labels to help the model learn more representative features with high semantics. Specifically, in <ref type="bibr" target="#b13">[14]</ref>, Zhang et al. divide the DUTS dataset <ref type="bibr" target="#b94">[95]</ref> into different groups by the class of main salient objects to build the training set. However, the absolute class labels may not be given in realistic scenarios. In contrast, there is only the relative label of two groups (whether they belong to the same group). In 2015, Schroff et al. proposed the triplet loss <ref type="bibr" target="#b95">[96]</ref> to help the face recognition, which is a good way to learn the discriminative feature of different identities by Push Force Pull Force</p><formula xml:id="formula_9">G 1 M 1 M 2 G 2 F 1A r F 1B r F 2A r F 2B r F 1 r F 2 r</formula><p>Euclidean Distance <ref type="figure">Fig. 8</ref>. Group-based Symmetric Triplet Loss. In GST loss, each group is divided averagely into two sub-groups. The sub-groups from the same group pull to each other and push to features of other groups. ? ? represents the backbone (see <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>pulling the positive samples and pushing negative samples.</p><p>Because of the success of triplet loss in face recognition <ref type="bibr" target="#b95">[96]</ref>, visual tracking <ref type="bibr" target="#b96">[97]</ref>, person re-id <ref type="bibr" target="#b97">[98]</ref>, etc., we modify the original triplet loss to GST loss to learn more discriminative features from different groups, which could improve the uniqueness and discrimination of the consensus features of objects with different class labels. Note that our GST loss is only activated in the training process. Specifically, it is applied on F r , which is the output feature extracted by the encoder from the multiplication result G M between predicted saliency maps M and the original images G (see <ref type="figure" target="#fig_2">Fig. 3</ref>). In this way, only the pixels of targeted objects are used for the measurement. Taking G 1 in <ref type="figure">Fig. 8</ref> as an example, the backbone ? ? extracts the semantic representation F 1 r from the raw images masked with M 1 . Then, the F 1 r is split into two parts by class, i.e., F 1A r , F 2A r . Features from the same group are seen as positive samples of each other, while those from the other group are negative. As shown in <ref type="figure">Fig. 8</ref>, our GST loss is calculated in a symmetric structure. Finally, the triplet loss is computed on both the </p><formula xml:id="formula_10">(F 1A r , F 1B r , F 2A r ) and (F 1B r , F 2B r , F 2A r ),</formula><formula xml:id="formula_11">||F 1A r ? F 1B r || 2 ? ||F 1B r ? F 2A r || 2 + ?,<label>(4)</label></formula><p>where ? denotes the margin that is a hyper-parameter enforced between positive and negative pairs <ref type="bibr" target="#b95">[96]</ref>. || ? || 2 denotes the two-norm of input. Because of the symmetry of GST loss, L Tri (F 1B r , F 2B r , F 2A r ) is also measured with the Euclidean distance in the same way.</p><p>The final GST loss is a combination of double L Tri when G 1 and G 2 act as the positive samples alternately from the images masked with predicted maps:</p><formula xml:id="formula_12">L GST = L Tri (F 1A r , F 1B r , F 2A r ) + L Tri (F 1B r , F 2B r , F 2A r ),<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Recurrent Auxiliary Classification Module (RACM)</head><p>Existing works typically train the model with images within the same group to extract the common information. Specifically, images in a certain batch only have the ground truth maps on the objects belonging to the same class, where only the common intra-group features can be learned. However, since there are no constraints on the features learned, common features of different classes may get close to each other and be hard to distinguish. In <ref type="bibr" target="#b0">[1]</ref>, the auxiliary classification module (ACM) facilitates the high-level semantic representation to obtain more discriminative features for consensus learning. Specifically, a class predictor consisting of a global average pooling layer and one fully connected layer is applied after the backbone. The features of objects with the same class are clustered together through class-level supervision. Although ACM works well in GCoNet <ref type="bibr" target="#b0">[1]</ref>, it has some defects: the features from the backbone are unstable and can be something else other than the right objects. As a consequence, the ACM may give a wrong optimizing direction. Meanwhile, it runs implicitly and is difficult to monitor.</p><p>We propose to use the Recurrent ACM (RACM) to overcome the problems mentioned above. The pipeline of RACM is kept almost the same as that of the original ACM. In contrast, RACM takes the model's output as the mask to obtain the pixels of target objects only rather than the whole image used in ACM. Then the masked images will be sent again to the encoder and class predictor. After eliminating other distracting regions, our RACM focuses only on the interesting areas. When the prediction of our GCoNet+ is far from the ground truth map, RACM can give an enhanced penalty to help accelerate the convergence of training. Combining the raw images and ground truth maps to formulate the loss, RACM enables the model to learn more discriminative features for the inter-group separability and intra-group compactness, respectively. The loss functions about classification are as follows:</p><formula xml:id="formula_13">Y ACM = ?(? ? (G)),<label>(6)</label></formula><formula xml:id="formula_14">Y RACM = ?(? ? (G ? M)),<label>(7)</label></formula><formula xml:id="formula_15">L CLS = L CE (? RACM , Y CLS ) + L CE (? ACM , Y CLS ),<label>(8)</label></formula><p>where ? and ? ? denote the class predictor (GAP and one linear layer) and encoder, respectively. L CE is the crossentropy loss, Y CLS are the ground truth class labels, and Y ACM and? RACM are the class labels predicted by ACM and RACM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Objective Function</head><p>The objective function is a weighted combination of saliency map loss (combination of BCE loss and IoU loss), GCM loss, our GST loss, and classification loss. The BCE loss and IoU loss are illustrated as:</p><formula xml:id="formula_16">L BCE = ? [Y log(? ), (1 ? Y )log(1 ?? )],<label>(9)</label></formula><formula xml:id="formula_17">L IoU = 1 ? 1 N Y ?? Y ?? ,<label>(10)</label></formula><p>where Y is the ground truth and? is the prediction. With the GCM loss (Eq. 2), GST loss (Eq. 5), and classification loss (Eq. 8), our final objective function is:</p><formula xml:id="formula_18">L = ? 1 L BCE + ? 2 L IoU + ? 3 L GCM + ? 4 L GST + ? 5 L CLS ,<label>(11)</label></formula><p>where ? 1 , ? 2 , ? 3 , ? 4 , and ? 5 are respectively set to 30, 0.5, 250, 3, and 3 to keep all the losses on the same quantitative level at the beginning of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>This section provides the guidelines and details in our base and extensive experiments, i.e., datasets, settings, evaluation protocol, and analysis in training and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Training Set. We follow the GICD <ref type="bibr" target="#b13">[14]</ref> to use DUTS class as our training set to design the experiments. After removing the noisy samples by Zhang et al. <ref type="bibr" target="#b13">[14]</ref>, the whole DUTS class is divided into 291 groups, which contains 8,250 images in total. The DUTS class dataset is the only training set used for evaluation in our ablation study. Nowadays, there is still a lack of a fully recognized training dataset. To keep a fair comparison with some other up-to-date works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>, we also employed the widelyadopted COCO-9k <ref type="bibr" target="#b14">[15]</ref>, a subset of the COCO [102] with 9,213 images of 65 groups, and the COCO-SEG <ref type="bibr" target="#b99">[100]</ref> which is also a subset of the COCO <ref type="bibr" target="#b101">[102]</ref>, while it has 200k images, to train our GCoNet+ as supplementary experiments. Test Sets. To obtain a comprehensive evaluation of our GCoNet+, we test it on three widely used CoSOD datasets, i.e., CoCA <ref type="bibr" target="#b13">[14]</ref>, CoSOD3k <ref type="bibr" target="#b6">[7]</ref>, and CoSal2015 <ref type="bibr" target="#b98">[99]</ref>. Among these three datasets, CoCA is the most challenging dataset. It is of much higher diversity and complexity in terms of background, occlusion, illumination, surrounding objects, etc. Following the latest benchmark <ref type="bibr" target="#b6">[7]</ref>, we do not evaluate on iCoseg <ref type="bibr" target="#b102">[103]</ref> and MSRC <ref type="bibr" target="#b103">[104]</ref>, since only one salient object is given in most images there. It is more convincing to evaluate CoSOD methods on images with more salient objects, which is closer to real-life applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Protocol</head><p>Following the GCoNet <ref type="bibr" target="#b0">[1]</ref>, we employ the S-measure <ref type="bibr" target="#b91">[92]</ref>, maximum F-measure <ref type="bibr" target="#b104">[105]</ref>, maximum E-measure <ref type="bibr" target="#b92">[93]</ref>, and mean absolute error (MAE) to evaluate the performance in our experiments. Evaluation toolbox can be referred to https://github.com/zzhanghub/eval-co-sod. S-measure <ref type="bibr" target="#b91">[92]</ref> is a structural similarity measurement between a saliency map and its corresponding ground truth map. The evaluation with S ? can be obtained at high speed without binarization. S-measure is computed as:</p><formula xml:id="formula_19">S ? = ? ? S o + (1 ? ?) ? S r ,<label>(12)</label></formula><p>where S o and S r denote object-aware and region-aware structural similarity, and ? is set to 0.5 by default, as suggested by Fan et al. in <ref type="bibr" target="#b0">[1]</ref>. F-measure <ref type="bibr" target="#b104">[105]</ref> is designed to evaluate the weighted harmonic mean value of precision and recall. The output of the saliency map is binarized with different thresholds to obtain a set of binary saliency predictions. The predicted saliency maps and ground truth maps are compared for precision and recall values. The best F-measure score obtained with the best threshold for the whole dataset is defined as F max ? . F-measure can be computed as: 1 Quantitative ablation studies of the overall modification on the framework of our GCoNet+. We conduct the ablation studies of our GCoNet+ on the effectiveness of overall modification on the framework, including the network simplification (Net-Sim), batch normalization (BN), and hybrid loss (HL).</p><formula xml:id="formula_20">F ? = (1 + ? 2 )P recision ? Recall ? 2 (P recision + Recall) ,<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modules</head><p>CoCA <ref type="bibr" target="#b13">[14]</ref> CoSOD3k <ref type="bibr" target="#b6">[7]</ref> CoSal2015 <ref type="bibr" target="#b98">[99]</ref>  where ? 2 is set to 0.3 to emphasize the precision over recall, following <ref type="bibr" target="#b31">[32]</ref>. E-measure <ref type="bibr" target="#b92">[93]</ref> is designed as a perceptual metric to evaluate the similarity between the predicted maps and ground truth maps from both local and global views. Emeasure is defined as:</p><formula xml:id="formula_21">ID Net-Sim BN HL E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? 1 0.</formula><formula xml:id="formula_22">E ? = 1 W ? H W x=1 H y=1 ? ? (x, y),<label>(14)</label></formula><p>where ? ? indicates the enhanced alignment matrix. Similar to F-measure, we also adopt max E-measure (E max ? ) as our evaluation metrics.</p><p>MAE is a simple pixel-level evaluation metric that measures the absolute difference between the predicted maps and the ground truth maps without binarization. It is defined as:</p><formula xml:id="formula_23">= 1 W ? H W x=1 H y=1 |? (x, y) ? GT(x, y)|.<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Based on GCoNet <ref type="bibr" target="#b0">[1]</ref>, we employ VGG-16 with batch normalization <ref type="bibr" target="#b105">[106]</ref> as backbone. We randomly pick N samples from two different groups in each training batch. N = min(#groupA, #groupB, 32), <ref type="bibr" target="#b15">(16)</ref> where N denotes the batch size for training, and # means the number of images in the corresponding group.</p><p>To clarify our proposed network, we provide the hyperparameters in the newly proposed modules. The steep step function produces some Nan values after the backpropagation in the confidence enhancement module (CEM). So, we set the k in differentiable binarization (DB) to a radical value of 300 and a conservative value of 50. When Nan is produced in a certain step, 50 will be used for replacement, which never produces Nan in our experiments. In groupbased symmetric triplet (GST) loss, the margin value is set to 1.0.</p><p>The images are resized to 256x256 for training and testing. The output maps are resized to the original size for evaluation. Three data augmentation strategies are applied in our training process, i.e., horizontal flip, color enhancement, and rotation. Our GCoNet+ is trained over 320 epochs with the Adam optimizer. The initial learning rate is set to 3e-4, ? 1 = 0.9, and ? 2 = 0.99. The whole training process takes around 20 hours. All the experiments are implemented based on PyTorch <ref type="bibr" target="#b106">[107]</ref> with a single Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We study the effectiveness of each extension component (i.e., RACM, CEM, and GST) employed in our GCoNet+ and investigate why they can help learn both good consensus features and discriminative features in our framework. The qualitative results regarding to each module are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. For more ablation studies and experimental settings, can be referred to our conference version <ref type="bibr" target="#b0">[1]</ref>.</p><p>Baseline. We follow GCoNet <ref type="bibr" target="#b0">[1]</ref> to design our GCoNet+ in a siamese way. Note that GCoNet follows the architecture of GICD <ref type="bibr" target="#b13">[14]</ref> without extensive experiments on the validity of each component in GICD, including the multi-head supervision, loss function, feature normalization, etc. Although these components bring additional parameters and complexity to the network itself, experimental proof still does not support the effectiveness. Instead of taking these components as granted, we conduct extensive experiments on each component. Firstly, we try to substitute the blocks of multiple convolutions in lateral connections with only one 1x1 convolution layer as the original FPN <ref type="bibr" target="#b35">[36]</ref> does. Secondly, we try to remove the multi-stage supervision of the saliency maps on the decoder. Thirdly, we try to add batch normalization behind every convolution layer except 1x1 convolution layers. Finally, as our experiments show, BCE loss brings higher accuracy to our experiments, while IoU loss brings more binarized final saliency maps and faster convergence. To better combine the two losses, we control the initial BCE and IoU loss on the same quantity level with different weights and sum them up.</p><p>These modifications can be summarized into three parts, i.e., network architecture simplification, normalization layers, and the hybrid loss. Following Occam's Razor 4 , we try to remove all uncertain modules used in many existing works without enough experimental proof. These modifications improve our GCoNet+ in terms of both simplicity and accuracy with a large margin compared with the baseline model GCoNet (ID:1 in Tab. 1). As shown in Tab. 1, combining all of them yields 2.6% and 2.8% relative improvement on CoSOD3k and CoSal2015 in terms of E-measure, respectively. It also achieves 2.5% E-measure relative improvement on CoCA, the most challenging CoSOD test set.</p><p>Effectiveness of RACM. The ACM guides the model to learn more discriminative features to distinguish objects of different classes. Compared with the original ACM, it works more accurately and accelerates (see <ref type="figure" target="#fig_0">Fig. 10</ref>) the convergence of our GCoNet+. As seen in Tab. 2, the RACM 4. https://en.wikipedia.org/wiki/Occams razor Quantitative ablation studies of the proposed components in our GCoNet+. We conduct the ablation studies of our GCoNet+ on the effectiveness of the proposed components, including RACM (Recurrent Auxiliary Classification Module), CEM (Confidence Enhancement Module), GST (Group-based Symmetric Triplet Loss), and their combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modules</head><p>CoCA <ref type="bibr" target="#b13">[14]</ref> CoSOD3k <ref type="bibr" target="#b6">[7]</ref> CoSal2015 <ref type="bibr" target="#b98">[99]</ref>   As column (a) shows, our GCoNet+ has superiority in precisely putting its attention on the target object next to other objects around. In (b), our GCoNet+ shows better performance in focusing on objects of the correct class though it has some disturbing surroundings. In the last column (c), some complex samples make both models mistaken. Although some wrong attention is put on objects of the wrong classes, our GCoNet+ can still put most of the attention on the right objects and see them as the main parts of the images. The classification activation maps provided here are produced by our GCoNet+ trained on DUTS class only.</p><formula xml:id="formula_24">ID RACM CEM GST E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? 1 0.</formula><p>slightly improves the baseline performance on CoCA and CoSOD3k in terms of most metrics. The activation maps in <ref type="figure" target="#fig_5">Fig. 9</ref> show that our GCoNet+ gives a higher accuracy in various cases and guides the model to focus on the targets more precisely. The feature maps on each stage of the decoder in GCoNet+ are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. As the results show, GCoNet+ has a better performance than GCoNet <ref type="bibr" target="#b0">[1]</ref> in discriminating objects of different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of CEM.</head><p>Among the previous CoSOD methods, IoU and BCE loss tend to be employed as the training loss. However, in most of these methods, only one single loss is used for the supervision during training. BCE guides the supervision from the pixel perspective, and IoU guides the supervision from the view of regions. Despite the outstanding performance achieved by many existing methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b108">[109]</ref>, <ref type="bibr" target="#b110">[111]</ref>, using the BCE and IoU separately suffers from some issues. Specifically, with IoU loss supervising the model on the region level, the predicted saliency maps are usually rough and cannot handle the small details very well. BCE can guide the model to focus on the details. At the same time, saliency maps supervised with it tend to contain much uncertainty, which makes the predictions challenging to use in the application directly. In this case, we apply the CEM to simultaneously predict more accurate and binarized maps closer to the demand of real-world applications. As shown in <ref type="figure" target="#fig_0">Fig. 11</ref> and Tab. 2, the TABLE 3 Quantitative comparisons between our GCoNet+ and other methods. "?" ("?") means that the higher (lower) is better. Methods with open-source codes or paper sources are attached with links. Since there are several datasets used in CoSOD task for training, we list all the training sets used in corresponding methods, i.e., Train-1, 2, and 3 represents the DUTS class <ref type="bibr" target="#b13">[14]</ref>, COCO-9k <ref type="bibr" target="#b14">[15]</ref>, and COCO-SEG <ref type="bibr" target="#b99">[100]</ref>, respectively.</p><p>CoCA <ref type="bibr" target="#b13">[14]</ref> CoSOD3k <ref type="bibr" target="#b6">[7]</ref> CoSal2015 <ref type="bibr" target="#b11">[12]</ref> TIP 2013 -0.641 0.523 0.313 0.180 0.637 0.528 0.466 0.228 0.656 0.544 0.532 0.233 GWD <ref type="bibr" target="#b14">[15]</ref> IJCAI 2017 Train-2 0.701 0.602 0.408 0.166 0.777 0.716 0.649 0.147 0.802 0.744 0.706 0.148 RCAN <ref type="bibr" target="#b71">[72]</ref> IJCAI 2019 Train-2 0.702 0.616 0.422 0.160 0.808 0.744 0.688 0.130 0.842 0.779 0.764 0.126 GCAGC <ref type="bibr" target="#b69">[70]</ref> CVPR 2020 Train-3 0.754 0.669 0.523 0.111 0.816 0.785 0.740 0.100 0.866 0.817 0.813 0.085 GICD <ref type="bibr" target="#b13">[14]</ref> ECCV CEM can make the predicted maps better in terms of both accuracy and visualization. Effectiveness of GST Loss. Consensus features play an important role in the CoSOD task for detecting common objects. However, consensus features of some categories are close to each other. To this end, we need to keep consensus features more distinguishing and the distance far away from other features. We introduce the GST loss to make features of different classes learned more discriminative to each other. As experiments show in Tab. 2 and <ref type="figure" target="#fig_0">Fig. 11</ref>, GST loss successfully differentiates the features on a global and RoI level and further improves model's competitiveness.</p><formula xml:id="formula_25">[99] Method Pub. &amp; Year Training Set E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? CBCS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Competing Methods</head><p>Since not all CoSOD models are publicly available, we only compare our GCoNet and GCoNet+ with one representative traditional algorithm CBCS <ref type="bibr" target="#b11">[12]</ref> and 11 deep-learning based CoSOD models, including all update-to-date models, i.e., GWD <ref type="bibr" target="#b14">[15]</ref>, RCAN <ref type="bibr" target="#b71">[72]</ref>, CSMG <ref type="bibr" target="#b111">[112]</ref>, GCAGC <ref type="bibr" target="#b69">[70]</ref>, GICD <ref type="bibr" target="#b13">[14]</ref>, ICNet <ref type="bibr" target="#b17">[18]</ref>, CoADNet <ref type="bibr" target="#b15">[16]</ref>, CoEGNet <ref type="bibr" target="#b6">[7]</ref>, DeepACG <ref type="bibr" target="#b108">[109]</ref>, CADC <ref type="bibr" target="#b100">[101]</ref>, UFO <ref type="bibr" target="#b109">[110]</ref>, and DCFM <ref type="bibr" target="#b110">[111]</ref>. Because of the much more excellent performance of the latest CoSOD methods compared with single-SOD methods, we do not list the single-SOD ones. A complete leader-board of previous methods can be found in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Quantitative Results. Tab. 3 shows the quantitative results of our GCoNet+ and previous state-of-the-art methods. Our GCoNet+ outperforms all of them in all metrics, especially on the CoCA and CoSOD3k datasets. CoCA is the most difficult dataset to specify the common objects compared with two other datasets because of the larger number of objects in a single image and more diverse backgrounds. Our GCoNet+ shows stronger ability in segmentation, which benefits from the improved features in terms of saliency detection and consensus learning, respectively. CoSOD3k has similar attributes, and our GCoNet+ keeps its best performance over all other methods on this dataset. CoSal2015 is the easiest dataset since most of its images contain only one salient object, making it easy to handle with single-SOD methods. Despite the low difficulty and absence of co-saliency, our GCoNet+ still outperforms other methods with a relatively smaller margin. Besides, our GCoNet+ has fewer parameters and makes the faster inference compared with most of the existing methods, as shown in Tab. 4. Qualitative Results. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the saliency maps generated by different methods for qualitative comparison. Images of beer bottle group contain multiple salient objects of numerous classes, where our GCoNet+ can precisely detect the co-salient objects while others cannot. In the crutch group, targets are slim sketches, but our GCoNet+ can still segment the sketches with high accuracy while others even fail to make the correct segmentation. We put the tennis group here to compare the ability of models to detect small objects, where our GCoNet+ performs better than others in terms of both classification and precision. In contrast, others may miss the small objects or focus on objects with other classes. Many tomatoes appear as co-salient objects in the tomato group and need detecting simultaneously. Our method can find all the tomatoes with a good saliency map, but others may omit some of these salient tomatoes or segment other very close objects. Among the examples  above, our GCoNet+ better finds the intra-group common information and discriminates the inter-group information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion of Existing CoSOD Training Sets</head><p>Even though many great works have been proposed in CoSOD, there remains a lack of a standard training set. DUTS class, COCO-9k, and COCO-SEG are three commonly-used training sets but have their limitations, e.g., wrong groud-truth and few target objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUTS class.</head><p>Due to DUTS class aiming only at detecting salient objects, there are salient objects of different classes in a single image. As <ref type="figure" target="#fig_0">Fig. 14</ref>  there are only a very few target objects in a single image, which makes the training lack the segmentation ability of common objects. COCO-9k/COCO-SEG. As mentioned in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b99">[100]</ref>, COCO-9k and COCO-SEG are both collected from COCO dataset <ref type="bibr" target="#b101">[102]</ref>. However, neither of them takes the salient objects into account. Therefore, objects with ground truth may not be salient. Thus, models training on only COCO-9k or only COCO-SEG may perform well on segmenting common objects while badly on segmenting salient objects.</p><p>Experiments. Among the three public test sets and realistic scenarios, cases can be difficult or easy, with various objects and complex contexts, or simply a dominant object on a white paper. To deal with all these cases with a satisfying result, the models need to behave well on both the common object segmentation and salient object detection, which are the main optimization goals that can be learned from COCO-9k <ref type="bibr" target="#b14">[15]</ref>/COCO-SEG <ref type="bibr" target="#b99">[100]</ref> and DUTS class <ref type="bibr" target="#b13">[14]</ref>, respectively. As mentioned in 4.1, CoCA <ref type="bibr" target="#b13">[14]</ref> focuses more on segmenting the common objects in complex context, while CoSal2015 <ref type="bibr" target="#b98">[99]</ref> plays a more important role in testing the ability of models to detect salient objects. We take these two datasets to check the different aspects of the model's performance.</p><p>We train GCoNet+ on the DUTS class and COCO-9k/COCO-SEG both separately and jointly. Taking the results of CoSal2015 <ref type="bibr" target="#b98">[99]</ref> shown in <ref type="figure" target="#fig_0">Fig. 13</ref>, models trained on DUTS class <ref type="bibr" target="#b13">[14]</ref> show a better performance on SOD tasks but weakness on detecting objects of common class. However, model trained on COCO-9k or COCO-SEG enables model to learn a good ability to segment objects with common class while a relatively worse performance on detecting simple salient objects. Compared with models training on DUTS class, models trained only on COCO-9k often fail to detect the salient objects.</p><p>To deal with the two sub-tasks in CoSOD, i.e., segmenting common objects and detecting salient objects, we need to optimize our GCoNet+ in two directions. Therefore, we set a joint training of our GCoNet+ on DUTS class <ref type="bibr" target="#b13">[14]</ref> and COCO-9k/COCO-SEG <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b99">[100]</ref>. Under the setting of combined training, the same model shows more robust <ref type="figure" target="#fig_0">Fig. 15</ref>. Application #1. Content aware object co-segmentation visual results ("Helicopter") obtained by our GCoNet+. performance in both the two directions mentioned above. As the performance shown in Tab. 3, the jointly trained (i.e., Train-1, 3) model achieves much better results on all these three test sets. Specifically, compared with the model trained only on DUTS class, our GCoNet+ shows comparable performance on CoSal2015 while much better performance on CoCA. Meanwhile, compared with the model trained only COCO-9k or only COCO-SEG, the jointly trained model shows similar performance on CoCA while much better results on CoSal2015. The same phenomenon also occurs on the predicted maps, as shown in <ref type="figure" target="#fig_0">Fig. 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">POTENTIAL APPLICATIONS</head><p>We show the potential to utilize the extracted co-saliency maps to produce segmentation masks of high quality for related downstream image processing tasks.</p><p>Application #1: Content-Aware Co-Segmentation. Cosaliency maps have been widely used in image preprocessing tasks. Taking the unsupervised object segmentation in our implementation as an example, we firstly find a group of images by keywords on the Internet. Then, our GCoNet+ is applied to generate co-saliency maps. Finally, the salient objects of the specific group can be extracted with the co-saliency maps. Following <ref type="bibr" target="#b19">[20]</ref>, we can use GrabCut <ref type="bibr" target="#b112">[113]</ref> to obtain the final segmentation results. Adaptive threshold <ref type="bibr" target="#b113">[114]</ref> is chosen here to initialize GrabCut for the binary version of the saliency maps. As shown in <ref type="figure" target="#fig_0">Fig. 15</ref>, our method works well in the content-aware object co-segmentation task, which should benefit existing e-commerce applications in the background replacement process.</p><p>Application #2: Automatic Thumbnails. The idea of paired-image thumbnails is derived from <ref type="bibr" target="#b52">[53]</ref>. With the same goal 5 , we introduce a CNN-based application of photographic triage, which is valuable for sharing images on the website. As <ref type="figure" target="#fig_0">Fig. 16</ref> shows, the orange box is generated by the saliency maps obtained from GCoNet+. We can also scale them up with the orange box and get the larger red one. Finally, the collection-aware crops technique <ref type="bibr" target="#b52">[53]</ref> can be adopted to produce the results shown in the second row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This work proposes a novel group collaborative model (GCoNet+) to deal with the CoSOD task. With the experiments conducted, we find that group-level consensus can in-5. Jacobs et al. 's work <ref type="bibr" target="#b52">[53]</ref> is limited to the case of image pairs <ref type="figure" target="#fig_0">Fig. 16</ref>. Application #2. Co-localization based automatic thumbnails ("Butterfly") generated by our GCoNet+.</p><p>troduce effective semantic information, auxiliary classification, and metric learning to improve the feature representation in terms of intra-group consensus and inter-group separability. Qualitative and quantitative experiments demonstrate the superiority and state-of-the-art performance of our GCoNet+. We show that the techniques of GCoNet+ can also be transferred and easily used in many relevant applications, such as co-detection and co-segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?Fig. 1 .</head><label>1</label><figDesc>Peng Zheng and Huazhu Fu share equal contribution. Corresponding author: Deng-Ping Fan (Email: dengpfan@gmail.com). Comparisons of seven representative CoSOD approaches on the CoSOD3k dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Visualizations of feature maps. (a) Source image and ground truth. (b-f) Feature maps on different levels of the decoder from our GCoNet [1] and GCoNet+ (Train-1 in Tab. 3), captured from high to low levels. The feature maps shown in (b) has the lowest resolution. As (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>LFig. 3 .</head><label>3</label><figDesc>BCE + L IoU (Eq. 9), (Eq. 10) L GCM (Eq. 2) L GST (Eq. 5) L CLS (Eq. 8) Pipeline of the proposed Group Collaborative Learning Network plus (GCoNet+).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Group Collaborating Module. Both groups' original feature maps and consensus are fed to the GCM. The predicted output conditioned on the consistent feature and consensus (from the same group)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Prediction results produced by our GCoNet+ trained with different losses. (a) Source image. (b) Ground truth. (c) Results of GCoNet+ trained with only BCE loss. (d) Results of GCoNet+ trained with only IoU loss. (e) Results of GCoNet+ trained with balanced BCE and IoU loss. All the results here are generated from models trained with DUTS class dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>The class activation maps<ref type="bibr" target="#b107">[108]</ref> are obtained on the classification branch. For each comparison cell, the left half results are the activation maps obtained from the original GCoNet<ref type="bibr" target="#b0">[1]</ref> using ACM; the right half results are the activation maps generated by our GCoNet+ with extra RACM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Learning curve comparison. We record the overall losses obtained during the training of our baseline (see Sec. 4.4) with additional RACM and with only original ACM, where DUTS class is used as the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>2020 Train-1 0.715 0.658 0.513 0.126 0.848 0.797 0.770 0.079 0.887 0.844 0.844 0.071 ICNet [18] NeurIPS 2020 Train-2 0.698 0.651 0.506 0.148 0.832 0.780 0.743 0.097 0.900 0.856 0.855 0760 0.673 0.544 0.105 0.860 0.802 0.777 0.071 0.887 0.845 0.847 0.068 GCoNet+ (Ours) Submission Train-1 0.786 0.691 0.574 0.113 0.881 0.828 0.807 0.068 0.917 0.875 0.876 0.054 GCoNet+ (Ours) Submission Train-2 0.798 0.717 0.605 0.098 0.877 0.819 0.796 0.075 0.902 0.853 0.857 0.073 GCoNet+ (Ours) Submission Train-3 0.787 0.712 0.602 0.100 0.875 0.820 0.793 0.075 0.899 0.853 0.852 0.071 GCoNet+ (Ours) Submission Train-1, 2 0.808 0.734 0.626 0.088 0.894 0.839 0.822 0.065 0.919 0.876 0.880 0.058 GCoNet+ (Ours) Submission Train-1, 3 0.814 0.738 0.637 0.081 0.901 0.843 0.834 0.062 0.924 0.881 0.891 0.056</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Qualitative ablation studies of our GCoNet+ on different modules and their combinations. (a) Source image; (b) Ground truth; (c) GCoNet [1]; (d) Our new baseline; (e) Baseline+RACM; (f) Baseline+RACM+CEM; (g) Baseline+RACM+CEM+GST, the final version of our GCoNet+. To keep consistency with GCoNet, the predicted maps provided here are generated by our GCoNet+ trained on DUTS class only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Qualitative comparisons of our GCoNet+ and other methods. "GT" denotes the ground truth. The predictions in the row of Ours is produced by our GCoNet+ , which is trained with DUTS class dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Peng Zheng is with the College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China, and also with Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, United Arab Emirates. Huazhu Fu is with the Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore. Deng-Ping Fan is with the Computer Vision Lab (CVL), ETH Zurich, Zurich, Switzerland. Qi Fan is with the Hong Kong University of Science and Technology. Jie Qin is with the College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China. Yu-Wing Tai is with the Hong Kong University of Science and Technology, and also with Kuaishou Technology.</figDesc><table><row><cell>10]</cell></row></table><note>??????? Chi-Keung Tang is with the Hong Kong University of Science and Technology.? Luc Van Gool is with the Computer Vision Lab, ETH Zurich, Zurich,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where the distances between features are measured with Euclidean distance. Specifically, L Tri (F 1A</figDesc><table /><note>r , F 1Br , F 2Ar ) can be denoted as follows:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 Runtime comparisons of different methods.</head><label>4</label><figDesc>Batch size is set as 2 for all methods during their inference.</figDesc><table><row><cell>Methods</cell><cell>Inference Time (ms)</cell><cell>Parameters (MB)</cell></row><row><cell>CoEGNet [7]</cell><cell>2300</cell><cell>412.3</cell></row><row><cell>GICD [14]</cell><cell>18.2</cell><cell>1060.7</cell></row><row><cell>ICNet [18]</cell><cell>12.5</cell><cell>70.3</cell></row><row><cell>CoADNet [16]</cell><cell>70.0</cell><cell>113.2</cell></row><row><cell>GCAGC [70]</cell><cell>103</cell><cell>280.7</cell></row><row><cell>CADC [101]</cell><cell>54</cell><cell>1498.7</cell></row><row><cell>DCFM [111]</cell><cell>3.73</cell><cell>542.9</cell></row><row><cell>GCoNet [1] (Ours)</cell><cell>3.3</cell><cell>541.7</cell></row><row><cell>GCoNet+ (Ours)</cell><cell>4.0</cell><cell>70.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Ground truth with problem occurs in DUTS class dataset, COCO-9k and COCO-SEG dataset.</head><label></label><figDesc>shows, objects with wrong categories still exist in the ground truth, which gives the models a wrong optimization direction. Whatsmore, We adopt different datasets to set up experiments to validate the different optimization directions of DUTS class dataset<ref type="bibr" target="#b94">[95]</ref> and COCO-9k/COCO-SEG datasets<ref type="bibr" target="#b14">[15]</ref>,<ref type="bibr" target="#b99">[100]</ref>. "Mix-1, 2" denotes both the DUTS class and COCO-9k are used in training, "Mix-1, 3" denotes both the DUTS class and COCO-SEG are used in training. CoCA is the most complex CoSOD test set and needs more attention to be paid on finding objects of common class. At the same time, CoSal2015 is a relatively simple one that almost only measures the ability of salient object detection in most cases. As the examples given show, salient objects of different classes occur together in one image of DUTS class dataset, whose regions are wrong ground truth in DUTS class. In COCO-9k and COCO-SEG, many objects are not salient, which also plays a bad role in training a CoSOD model.</figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix-1, 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix-1, 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>COCO-9k</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DUTS class</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CoCA</cell><cell>CoSal2015</cell></row><row><cell>x</cell><cell>? x ?</cell><cell>x</cell><cell>?</cell></row><row><cell></cell><cell>DUTS class</cell><cell></cell><cell>COCO-9k and COCO-SEG</cell></row><row><cell cols="2">Fig. 14.</cell><cell></cell><cell></cell></row></table><note>Fig. 13. Qualitative results produced by GCoNet+ trained on different training sets.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Group collaborative learning for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higher-order image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1011" to="1021" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepco3: Deep instance co-segmentation by co-peak search and co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8846" to="8855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7223" to="7233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient video object colocalization with co-saliency activated tracklets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="744" to="755" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No-reference synthetic image quality assessment with convolutional neural network and local image saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="208" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Re-thinking co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a selfpaced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified metric learningbased framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2473" to="2483" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised cnn-based co-saliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A co-saliency model of image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-adaptively weighted co-saliency detection via rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4175" to="4186" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-induced cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="455" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Group-wise deep co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E F</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3041" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coadnet: Collaborative aggregation-and-distribution networks for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="6959" to="6970" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global-and-local collaborative learning for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Icnet: Intra-saliency correlation network for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="18" to="749" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Re-thinking the relations in co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="251" to="268" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unconstrained salient object detection via proposal subset optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5733" to="5742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A shape-based approach for salient object detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="455" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supercnn: A superpixelwise convolutional neural network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Poolnet+: Exploring the potential of pooling for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image. Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8778" to="8787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3080" to="3089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="236" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization and adaptive scale fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Abcnet: Real-time scene text spotting with adaptive bezier-curve network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9806" to="9815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Textmountain: Accurate scene text detection via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107336</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Boundaryaware segmentation network for mobile and web applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sant&amp;apos;anna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04704</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Salient object detection via integrity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cosaliency: Where people look when comparing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM symposium on User interface software and technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a selfpaced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Cosformer: Detecting co-salient object with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14729</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adaptive intra-group aggregation for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Acoust. Speech SP</title>
		<imprint>
			<biblScope unit="page" from="2520" to="2524" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="548" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Co-saliency detection guided by group weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A feature-adaptive semisupervised framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="959" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a selfpaced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Detection of cosalient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised cnn-based co-saliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="502" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A unified multiple graph learning and convolutional network model for co-saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1375" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multiple graph convolutional networks for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="332" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional network with attention graph clustering for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9047" to="9056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Revisiting co-saliency detection: A novel approach based on two-stage multi-view spectral rotation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3196" to="3209" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Detecting robust cosaliency with recurrent co-attention neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="818" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1163" to="1176" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Quality-guided fusionbased co-saliency estimation for image co-segmentation and colocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2466" to="2477" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Image co-saliency detection and co-segmentation via progressive joint optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Co-saliency detection via mask-guided fully convolutional networks with multi-scale label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3095" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep learning intra-image and inter-images features for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. Mach. Vis. Conf</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Co-saliency detection via integration of multi-layer convolutional features and interimage propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page" from="137" to="146" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Co-salient object detection based on deep saliency networks and seed propagation over an integrated graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5866" to="5879" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A feature-adaptive semisupervised framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="959" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Supervision by fusion: Towards unsupervised learning of deep salient object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Co-attention cnns for unsupervised object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Co-saliency detection based on hierarchical consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia, 2019</title>
		<imprint>
			<biblScope unit="page" from="1392" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Cosaliency detection within a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7509" to="7516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">An easy-to-hard learning strategy for within-image co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="166" to="176" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="391" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Self-supervised video representation learning for correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">299</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Mast: A memory-augmented selfsupervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Structuremeasure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3796" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Triplet loss in siamese network for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Detection of cosalient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8917" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7263" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">icoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Deepacg: Co-saliency detection via semantic-aware contrast gromovwasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">A unified transformer framework for group-based segmentation: Co-segmentation, cosaliency detection and video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04708</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Democracy does matter: Comprehensive feature mining for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Co-saliency detection via mask-guided fully convolutional networks with multi-scale label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3090" to="3099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">grabcut&quot;: interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: A benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
