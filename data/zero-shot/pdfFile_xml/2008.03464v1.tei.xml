<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Audio Spoofing Verification using Deep Convolutional Neural Networks by Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">T</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication</orgName>
								<orgName type="institution">NSS College of Engineering</orgName>
								<address>
									<settlement>Palakkad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Aravind</surname></persName>
							<email>aravindpkd1@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication</orgName>
								<orgName type="institution">NSS College of Engineering</orgName>
								<address>
									<settlement>Palakkad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjith</forename><forename type="middle">C</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication</orgName>
								<orgName type="institution">NSS College of Engineering</orgName>
								<address>
									<settlement>Palakkad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usamath</forename><surname>Nechiyil</surname></persName>
							<email>usmathn@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication</orgName>
								<orgName type="institution">NSS College of Engineering</orgName>
								<address>
									<settlement>Palakkad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandakumar</forename><surname>Paramparambath</surname></persName>
							<email>nandakumarpp@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Communication</orgName>
								<orgName type="institution">NSS College of Engineering</orgName>
								<address>
									<settlement>Palakkad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Audio Spoofing Verification using Deep Convolutional Neural Networks by Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Spoofing detection</term>
					<term>Mel-Spectrogram</term>
					<term>Deep- Convolutional Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic Speaker Verification systems are gaining popularity these days; spoofing attacks are of prime concern as they make these systems vulnerable. Some spoofing attacks like Replay attacks are easier to implement but are very hard to detect thus creating the need for suitable countermeasures. In this paper, we propose a speech classifier based on deep-convolutional neural network to detect spoofing attacks. Our proposed methodology uses acoustic time-frequency representation of power spectral densities on Mel frequency scale (Mel-spectrogram), via deep residual learning (an adaptation of ResNet-34 architecture). Using a single model system, we have achieved an equal error rate (EER) of 0.9056% on the development and 5.32% on the evaluation dataset of logical access scenario and an equal error rate (EER) of 5.87% on the development and 5.74% on the evaluation dataset of physical access scenario of ASVspoof 2019.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Biometrics technologies play a critical role in regulating access to informational resources in today's world. One of the reliable approaches for attaining a suitable secure system is speaker verification.</p><p>Automatic speaker verification (ASV) has undergone rapid improvements in the recent decade but continues to show high vulnerability towards different spoofing attacks. Spoofing methods are categorized as speech synthesis (SS), voice conversion (VC), impersonation and replay attacks <ref type="bibr">[1]</ref>. Among these, replay attacks are arguably the most simple yet highly indistinguishable ASV spoofing technique as they do not require the attackers to have any specialized knowledge, and can be mounted with relative ease using consumer devices.</p><p>For the first automatic speaker verification spoofing and counter-measures challenge (ASVspoof 2015 <ref type="bibr" target="#b0">[2]</ref>) even though the best results showed an overall average detection EER of less than 1.5%, the EER of unknown attacks is five times higher than that of known attacks. In addition, while some attacks were easily and consistently detected, others provoked extremely high error rates nearing 50%.</p><p>Systems submitted under ASVspoof 2017 <ref type="bibr" target="#b1">[3]</ref> challenge inspected distinct front-end features and the usage of various classifiers to detect replay attack under conditions. Among them, the best-performing system <ref type="bibr" target="#b2">[4]</ref>, had an equal error rate (EER) of 6.73% and used a light convolutional neural network (LCNN) to extract high-level features from the log power spectrum, together with a Gaussian Mixture Model (GMM) as classifier. Variable length Teager energy operator-energy separation algorithm-instantaneous frequency cosine coefficients (VESA-IFCCs) <ref type="bibr" target="#b3">[5]</ref> were proposed as a single system with the aim of capturing the spectral changes due to the transmission and channel characteristic of replay devices. A single frequency filtering feature with a high spectro-temporal resolution was proposed <ref type="bibr" target="#b4">[6]</ref> to capture the channel information embedded in the low signal to noise ratio region.</p><p>ASVspoof 2019 <ref type="bibr" target="#b5">[7]</ref> Challenge was organized to emphasize the development of reliable countermeasures that could efficiently segregate bona fide and spoofed speech. The initiative aims specifically to encourage the design of generalized countermeasures, i.e. countermeasures that would perform well when encountered with spoofing attacks which are unpredictable in nature. The data used for ASVspoof 2015 <ref type="bibr" target="#b0">[2]</ref> contained spoofed speech samples generated using text-tospeech (TTS) and voice conversion (VC) systems at that time. The ASVspoof 2017 <ref type="bibr" target="#b1">[3]</ref> challenge emphasized on the design of countermeasures strived at detecting replay spoofing attacks that could be implemented easily by anyone using conventional devices. The ASVspoof 2019 <ref type="bibr" target="#b5">[7]</ref> edition was the first to focus on countermeasures for all three major spoofing attack types, namely those originating from TTS, VC and replay attacks. ASVspoof 2019 <ref type="bibr" target="#b5">[7]</ref> focuses to develop next-generation countermeasures for the automatic detection of spoofed or fake audio. The challenge encloses two separate sub-challenges in logical and physical access control and provides a common database of the most advanced spoofing attacks to date. The aim was to study the extremities of spoofing countermeasures with respect to automatic speaker verification and spoofed audio detection.</p><p>In this paper, we propose the use of Mel-spectrograms <ref type="bibr" target="#b6">[8]</ref> which is obtained from the audio frames as a time-frequency representation of power spectral density, for the training of deep-convolutional neural networks for audio spoofing attack detection. The use of Mel-spectrograms provides a timefrequency representation with sufficient spectral resolution, which is required for robust replay attack detection. Our framework is based on adapting the ResNet-34 architecture <ref type="bibr" target="#b7">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Our proposed framework ( <ref type="figure" target="#fig_1">Figure 1</ref>) employs transfer learning <ref type="bibr" target="#b8">[10]</ref> of a pre-trained convolutional neural network (CNN) for fast adaptation to the Mel-spectrograms extracted from speech inputs. Transfer learning is an analysis technique in machine learning that used to solve a problem by reusing knowledge gained from solving another but similar problem. Extracted spectrograms are fed into the deep convolutional neural network in which the speech signals are classified as bona fide or spoofed. In the following subsections, we describe the three major components of our system: Transfer Learning <ref type="bibr" target="#b8">[10]</ref>, Melspectrograms <ref type="bibr" target="#b6">[8]</ref> and ResNet <ref type="bibr" target="#b7">[9]</ref>, followed by a functional overview of our proposed framework.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Transfer Learning</head><p>Transfer Learning is a machine learning technique that is used to solve a problem based on knowledge gained from solving another but related problem. In this approach, the features gained from training a base neural network is reused or adjusted to a target neural network for its training. Transfer learning is usually expressed through the use of pre-trained models. A pre-trained model is trained using a large benchmark dataset to solve a similar problem. The computational cost of training such models is high, So it is common practice to reuse models from published literature (e.g. VGG, Inception, MobileNet, ResNet). Pre-trained models used in transfer learning are usually based on large convolutional neural networks (CNN). High performance and the easiness in training are two of the main factors that makes CNN popular for such applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Mel-spectrograms</head><p>A Mel-spectrogram represents an acoustic time-frequency representation of a sound, i.e. the power spectral density P (f, t). It is sampled into a number of points at equal intervals of time ti and frequencies f j (on a Mel frequency scale).</p><p>The Mel-frequency scale is defined as:</p><formula xml:id="formula_0">M el = 2595log10(1 + Hertz 700 ) (1)</formula><p>And its inverse is:</p><formula xml:id="formula_1">Hertz = 700(10 M el 2595 ? 1)<label>(2)</label></formula><p>STFT (short-time Fourier transform) and Mel-spectrogram have been the most popular input representations for audio classifications <ref type="bibr" target="#b6">[8]</ref>. Mel-spectrograms provide an efficient and perceptually relevant representation compared to STFT <ref type="bibr" target="#b9">[11]</ref> and have been shown to perform well in various tasks <ref type="bibr" target="#b10">[12]</ref>- <ref type="bibr" target="#b14">[16]</ref>. However, an STFT is closer to the original signal and neural networks may be able to learn a representation that is more optimal to the given task than Mel-spectrograms. The latter requires large amount of training data; however as reported in <ref type="bibr" target="#b15">[17]</ref> where using Mel-spectrograms outperformed STFT with a smaller dataset. Mel-spectrogram transforms the input raw sequence to a 2-D feature map where one of the dimensions represents time and the other one represents frequency and the values represent amplitude. The above <ref type="figure" target="#fig_2">Figure 2</ref> shows the Mel-spectrogram of an audio signal. 2-D Mel Model betters the 1-D raw wave model but the average of the two outperforms each individual model significantly <ref type="bibr">[18]</ref>. The Mel-spectrogram is selected since it is psychologically relevant and computationally efficient. It produces a Mel-scaled frequency representation which is a more appropriate conjecture of human auditory understanding <ref type="bibr" target="#b9">[11]</ref> and typically involves compressing the frequency axis of shorttime Fourier transform representation. And the magnitude of Mel-spectrogram is mapped to the decibel scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep Residual Net (ResNet)</head><p>The deeper the Neural Network the more difficult it is to train. It is well known that the depth of a network is a determining factor of the network performance. Deeper neural networks are used in the field of computer vision. However, it is not easy to train a deeper network due to the notorious gradient vanishing problem. This was one of the drawbacks of VGG net <ref type="bibr" target="#b16">[19]</ref> as they started losing generalization capability after some depth. To address this problem, an effective method called residual neural network(ResNet) <ref type="bibr" target="#b7">[9]</ref> is introduced. ResNet provides a training framework to ease the training of networks that are substantially deeper than those used previously. It was motivated by the counter intuitive experimental findings that adding more layers lead to higher training error. Theoretically, as the number of layers increased, the modeling capabilities of the Neural Networks should be better and therefore, the deeper networks should produce no higher training error. Experiments in <ref type="bibr" target="#b7">[9]</ref> showed that ResNet greatly improves training efficiency since the gradients can propagate many layers through the shortcut connection. In addition, ResNet allows deeper networks to be trained, resulting in models that usually perform better. One of the problems ResNets solves is known as the vanishing gradient. This is because when the network is too deep, the gradients from where the loss function is calculated easily shrink to zero after several applications of the chain rule. This results in the weights never getting updated and therefore, no learning process is involved. With ResNets, the gradients flow directly through the identity function.</p><p>Deep residual learning <ref type="bibr" target="#b7">[9]</ref> enables the training of CNNs that have substantially deeper architectures. It introduces skip connections that enable gradient flow across a large number of layers thus relieves the problem of vanishing gradients in deep CNNs. These skip connections enables the outputs to learn a residual mapping. The residual block forms the basic building block of a ResNet <ref type="figure" target="#fig_3">(Figure 3</ref>). If the desired mapping to be learned is H(x), the stacked residual layers learn the residual mapping, F(x) = H(x) ? x. Thus, the original mapping to be learned becomes F(x) + x. ResNet uses the Rectified linear unit (ReLU) activation function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data Conditions: Logical Access</head><p>Logical access includes spoofing attacks generated with textto-speech (TTS) and voice conversion (VC) attacks. Genuine speech is collected from speakers with no significant channel or background noise effects. Spoofed speech is generated from genuine data using a number of possible spoofing algorithms. The full dataset is split into three subsets, the first one is for training, the second is for development and the third is for evaluation. The number of speakers in the former two subsets is illustrated in <ref type="table" target="#tab_0">Table 1</ref>. There is no speaker overlap in these three subsets regarding target speakers used in voice conversion or TTS technique. The voice conversion systems are based on (1) neural-network-based and (2) transfer-function-based methods. The speech synthesis systems were done with (1) waveform concatenation, (2) neural-network-based parametric speech synthesis using source-filter vocoders and (3) neuralnetwork-based parametric speech synthesis using Wavenet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Data Conditions: Physical Access</head><p>The spoofing attacks that at the sensor level are considered as physical access scenario. Both bona fide and spoofed signals propagate through a physical space prior to the acquisition. Spoofing attacks in this scenario are therefore referred to as replay attacks, whereby are cording of a bona fide access attempt is first captured, presumably surreptitiously, before being replayed to the ASV microphone. <ref type="table" target="#tab_0">Table 1</ref> shows the number of speakers and the number of bona fide and replay spoofing access attempts (utterances) in and development set. Both bona fide and replay spoofing access attempts in both training and development data partitions were generated with respect to the same set of randomly selected acoustic and replay configurations.</p><p>In a similar fashion to the logical access scenario, the evaluation set is disjoint in terms of speakers. Evaluation data is generated in the same manner as training and development data, albeit with different, randomly acoustic and replay configurations. Specifically, the set of room sizes, levels of reverberation, speaker-to-ASV microphone distances, attackerto-speaker recording distances, and loudspeaker qualities, while drawn from the same categories, will be different. Full details of the dataset and performance measures of ASVspoof 2019 are presented in evaluation plan <ref type="bibr" target="#b17">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance measures</head><p>Improvements in Countermeasure (CM) technology need not necessarily imply an improved complete system that involves co-operation of CM and ASV. Thus, the main focus is on assessment of tandem systems where the CM act as a 'mediator' to determine whether a given speech input originates from a bona fide (genuine) user, before passing it as the main biometric verifier (the ASV system). The cost function used in this scenario is a recently-proposed tandem detection cost function (t-DCF) <ref type="bibr" target="#b18">[21]</ref> as its primary performance metric. Other than t-DCF, the evaluation of the equal error rate (EER) of each submitted countermeasure is used as a secondary performance metric <ref type="bibr" target="#b17">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Tandem-Detection Cost Function (t-DCF)</head><p>The t-DCF <ref type="bibr" target="#b18">[21]</ref> is mainly consisted of statistical detection theory and involves detailed specification of a specified application. A key feature of t-DCF involves the assessment of a tandem system while on the other hand, keeping the two subsystems (CM and ASV) isolated from each other, i.e. they can be developed independently of each other.</p><p>The t-DCF metric has 6 parameters: (i) false alarm and miss costs for both systems, and (ii) prior probabilities of target and spoof trials (with an implied third, non-target prior).</p><formula xml:id="formula_2">t ? DCF (s) = C1P cm miss (s) + C2P cm f a (s)<label>(3)</label></formula><p>where P cm miss (s) and P cm f a (s) are, respectively, the miss rate and the false alarm rate of the CM system at threshold s. The constants C2 and C1 are dictated by the t-DCF costs, priors, and the ASV system detection errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Equal Error Rate (EER)</head><p>In scoring-based spoofing detection tasks, EER is used as a secondary metric to analyze the performance of different counter measures methods. Let FRR(?) and FAR(?) denote the false rejection rate and false acceptance rate at threshold ?. F AR(?) = count(spoof trials with score) &gt; ? total spoof trials (4)</p><p>F RR(?) = count(human trials with score) &lt; ? total human trials <ref type="bibr" target="#b3">(5)</ref> F RR(?) and F AR(?) are monotonically decreasing and increasing functions of ?. The EER corresponds to the threshold ?EER at which two detection error rates are equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Setup</head><p>Our model was trained only on the training set and validated on the development set of the ASVspoof 2019 dataset. The model was trained in Google Colab with the granted GPU (Tesla K80). Audio preprocessing was performed via the Google Cloud Platform. Python's fastai library was used to train CNN which provides support for vision, text, tabular, and collaborative filtering models. The fastai library is a high-level library build on PyTorch which enables easy prototyping and gives you access to a lot of state-of-the-art methods such as marketing application (horizontal application) and health care application (vertical application). Mel-spectrograms were extracted from the audio files using FFT window of size 2048 and the number of samples between successive frames was taken as 512. The Mel-spectrograms were further resized to 224x224 before feeding into the CNN. ResNet-34 training ran on Google Colab with ADAM optimizer and a learning rate in range of 0.000001, over 8 epochs and a batch size of 64.   <ref type="table" target="#tab_1">Table 2</ref> shows the result of our model in ASVspoof 2019 challenge based on the two performance matrices (t-DCF and EER) on evaluation and development dataset. Comparing the result of ASVspoof 2019 challenge, our single model outperforms the baseline models provided by ASVspoof 2019 organizers given in <ref type="table" target="#tab_2">Table 3</ref>. But when compared to other teams, the result that we obtained in the testing phase was low. We had investigated the reason behind this problem and found that in the test dataset, spoofed data are generated according to diverse unseen spoofing algorithms. However, they are variations of the spoofing algorithms used to generate the development dataset. Among the systems submitted under ASVspoof 2019 challenge, the best-performing system under logical access scenario, had a min-tDCF of 0.2093 and an equal error rate (EER) of 11.40% (while evaluating a single system). By comparing this with our single system, we had obtained a better result in both the performance metrics. Along with that, while analyzing the result and spoofing attack algorithms provided by ASVspoof 2019 committee, our model performed well in detecting spoofing attacks which uses voice conversion techniques by waveform filtering (A17). In both logical and physical access scenario, we are one among the top three teams while considering both primary and single system as same. We had proposed the same methodology, rather than going for different methodologies to counter both logical and physical access scenarios. Also, t-DCF and EER showed correlation of 0.99686 and 0.96886 in physical and logical access scenarios respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we explored the applicability of the transfer learning approach for the solution to the problem of spoofing detection. Our first finding is that, instead of feeding raw audio directly to the model, we use an advanced approach to audio classification using Mel-spectrogram. Mel-spectrogram transforms the input raw sequence to a 2-D feature map in which one dimension represents time and the other represents frequency and the values represent amplitude. A second finding is that residual neural networks can help us build deeper models that outperform conventional deep neural networks.</p><p>Finally, we demonstrated that even with a single model, better system performance can be obtained. By using different features and models system performance can be significantly improved. In future, we would like to explore the effectiveness of using more advanced generative adversarial networks (GAN) for the spoofing detection task. Denoising methods like Adaptive filtering and Autoencoders could also be implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2008.03464v1 [eess.AS] 8 Aug 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the proposed framework for speech spoofing detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Mel-spectrogram of an audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Residual learning: a building block<ref type="bibr" target="#b7">[9]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of non-overlapping target speakers and number of utterances in training and development sets of the ASVspoof 2019 database<ref type="bibr" target="#b17">[20]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell>Speakers</cell><cell></cell><cell cols="2">Utterances</cell><cell></cell></row><row><cell>Subset</cell><cell cols="2">Male Female</cell><cell cols="2">Logical Access</cell><cell cols="2">Physical Access</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Bona fide Spoofed Bona fide Spoofed</cell></row><row><cell>Training</cell><cell>8</cell><cell>12</cell><cell>2,580</cell><cell>22,800</cell><cell>5,400</cell><cell>48,600</cell></row><row><cell>Development</cell><cell>8</cell><cell>12</cell><cell>2,548</cell><cell>22,296</cell><cell>5,400</cell><cell>24,300</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The experiment result obtained from ASVspoof 2019.</figDesc><table><row><cell>Dataset</cell><cell>EER</cell><cell></cell><cell>t-DCF</cell><cell></cell></row><row><cell></cell><cell cols="4">Development Evaluation Development Evaluation</cell></row><row><cell>Logical Access</cell><cell>0.9056</cell><cell>5.32</cell><cell>-</cell><cell>0.1514</cell></row><row><cell>Physical Access</cell><cell>5.87</cell><cell>5.74</cell><cell>-</cell><cell>0.1351</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The experiment result of baseline in ASVspoof 2019.</figDesc><table><row><cell>Baseline</cell><cell cols="4">Logical Access Physical Access</cell></row><row><cell>Implementation</cell><cell>EER</cell><cell>t-DCF</cell><cell>EER</cell><cell>t-DCF</cell></row><row><cell>CQCC -GMM</cell><cell cols="3">9.57 0.2366 11.04</cell><cell>0.2454</cell></row><row><cell>LFCC -GMM</cell><cell cols="3">8.09 0.2116 13.54</cell><cell>0.3017</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors would like to thank ASVspoof 2019 organizers for providing the dataset and detailed analysis of our system. We also acknowledge the technical support from Sachin Dev S, working at Tricodia Inc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hanili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sizov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The ASVspoof 2017 challenge: Assessing the limits of replay spoofing attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio Replay Attack Detection with Deep Learning Frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavrentyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Novoselov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kozlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kudashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shchemelinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="82" to="86" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Novel Variable Length Teager Energy Separation Based Instantaneous Frequency Features for Replay Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Soni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SFF Anti-Spoofer: IIIT-H Submission for Automatic Speaker Verification Spoofing and Countermeasures Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Alluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kadiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Gangashetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Vuppala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="107" to="111" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ASVspoof2019:the automatic speaker verification spoofing and countermeasure</title>
		<ptr target="www.asvspoof.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A tutorial on deep learning for music information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04396</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An introduction to the psychology of hearing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Brill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep contentbased music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning for music audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6964" to="6968" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved musical onset detection with convolutional neural networks, in Acoustics,Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boundary detection in music structure analysis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Society for Music Information Retrieval Conference</title>
		<meeting>the 15th International Society for Music Information Retrieval Conference<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ISMIR 2014</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic tagging using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 17th International Society of Music Information Retrieval Conference</title>
		<meeting><address><addrLine>New York, USA. ISMIR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic tagging using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00298</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ASVspoof2019:the automatic speaker verification spoofing and countermeasure challenge evaluation plan</title>
		<ptr target="http://www.asvspoof.org/asvspoof2019/asvspoof2019evaluation-plan.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">t-DCF: a detection cost function for the tandem assessment of spoofing countermeasures and automatic speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey, Les Sables dOlonne</title>
		<meeting>Odyssey, Les Sables dOlonne<address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
