<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sliced Iterative Normalizing Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwei</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uro?</forename><surname>Seljak</surname></persName>
						</author>
						<title level="a" type="main">Sliced Iterative Normalizing Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop an iterative (greedy) deep learning (DL) algorithm which is able to transform an arbitrary probability distribution function (PDF) into the target PDF. The model is based on iterative Optimal Transport of a series of 1D slices, matching on each slice the marginal PDF to the target. The axes of the orthogonal slices are chosen to maximize the PDF difference using Wasserstein distance at each iteration, which enables the algorithm to scale well to high dimensions. As special cases of this algorithm, we introduce two sliced iterative Normalizing Flow (SINF) models, which map from the data to the latent space (GIS) and vice versa (SIG). We show that SIG is able to generate high quality samples of image datasets, which match the GAN benchmarks, while GIS obtains competitive results on density estimation tasks compared to the density trained NFs, and is more stable, faster, and achieves higher p(x) when trained on small training sets. SINF approach deviates significantly from the current DL paradigm, as it is greedy and does not use concepts such as mini-batching, stochastic gradient descent and gradient back-propagation through deep layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Latent variable generative models such as Normalizing Flows (NFs) <ref type="bibr" target="#b49">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b9">Dinh et al., 2014;</ref><ref type="bibr" target="#b22">Kingma &amp; Dhariwal, 2018)</ref>, Variational Au-toEncoders (VAEs) <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b50">Rezende et al., 2014)</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b47">Radford et al., 2016)</ref> aim to model the distribution p(x) of high-dimensional input data x by introducing a mapping from a latent variable z to x, where z is assumed to follow a given prior distribution ?(z). These models usually parameterize the mapping using neural networks, and the training of these models typically consists of minimizing a dissimilarity measure between the model distribution and the target distribution. For NFs and VAEs, maximizing the marginal likelihood is equivalent to minimizing the Kullback-Leibler (KL) divergence. While for GANs, the adversarial training leads to minimizations of the Jenson-Shannon (JS) divergence <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref>. The performance of these models largely depends on the following aspects: 1) The parametrization of the mapping (the architecture of the neural network) should match the structure of the data and be expressive enough. Different architectures have been proposed <ref type="bibr" target="#b22">(Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b57">van den Oord et al., 2017;</ref><ref type="bibr" target="#b20">Karras et al., 2018;</ref>, but to achieve the best performance on a new dataset one still needs extensive hyperparameter explorations <ref type="bibr" target="#b35">(Lucic et al., 2018)</ref>.</p><p>2) The dissimilarity measure (the loss function) should be appropriately chosen for the tasks. For example, in high dimensions the JS divergence is more correlated with the sample quality than KL divergence <ref type="bibr" target="#b19">(Husz?r, 2015;</ref><ref type="bibr" target="#b55">Theis et al., 2016)</ref>, which is believed to be one of the reasons that GANs are able to generate higher quality samples than VAEs and NFs. However, JS divergence is hard to directly work with, and the adversarial training could bring many problems such as vanishing gradient, mode collapse and non-convergence <ref type="bibr" target="#b59">Wiatrak &amp; Albrecht, 2019)</ref>.</p><p>To avoid these complexities, in this work we adopt a different approach to build the map from latent variable z to data x. We approach this problem from the Optimal Transport (OT) point of view. OT studies whether the transport maps exist between two probability distributions, and if they do, how to construct the map to minimize the transport cost. Even though the existence of transport maps can be proved under mild conditions <ref type="bibr" target="#b58">(Villani, 2008)</ref>, it is in general hard to construct them in high dimensions. We propose to decompose the high dimensional problem into a succession of 1D transport problems, where the OT solution is known. The mapping is iteratively augmented, and it has a NF structure that allows explicit density estimation and efficient sampling. We name the algorithm Sliced Iterative Normalizing Flow (SINF). Our objective function is inspired by the Wasserstein distance, which is defined as the minimal transport arXiv:2007.00674v3 <ref type="bibr">[cs.</ref>LG] 14 Jun 2021 cost and has been widely used in the loss functions of generative models <ref type="bibr" target="#b56">Tolstikhin et al., 2018)</ref>. We propose a new metric, max K-sliced Wasserstein distance, which enables the algorithm to scale well to high dimensions.</p><p>In particular, SINF algorithm has the following properties: 1) The performance is competitive compared to state-ofthe-art (SOTA) deep learning generative models. We show that if the objective is optimized in data space, the model is able to produce high quality samples similar to those of GANs; and if it is optimized in latent space, the model achieves comparable performance on density estimation tasks compared to NFs trained with maximum likelihood, and achieves highest performance on small training sets.</p><p>2) Compared to generative models based on neural networks, this algorithm has very few hyperparameters, and the performance is insensitive to their choices.</p><p>3) The model training is very stable and insensitive to random seeds. In our experiments we do not observe any cases of training failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Normalizing Flows</head><p>Flow-based models provide a powerful framework for density estimation <ref type="bibr" target="#b10">(Dinh et al., 2017;</ref><ref type="bibr" target="#b43">Papamakarios et al., 2017)</ref> and sampling <ref type="bibr" target="#b22">(Kingma &amp; Dhariwal, 2018)</ref>. These models map the d-dimensional data x to d-dimensional latent variables z through a sequence of invertible transformations f = f 1 ? f 2 ? ... ? f L , such that z = f (x) and z is mapped to a base distribution ?(z), which is normally chosen to be a standard Normal distribution. The probability density of data x can be evaluated using the change of variables formula:</p><formula xml:id="formula_0">p(x) = ?(f (x))| det ?f (x) ?x | = ?(f (x)) L l=1 | det ?f l (x) ?x |.<label>(1)</label></formula><p>The Jacobian determinant det( ?f l (x) ?x ) must be easy to compute for evaluating the density, and the transformation f l should be easy to invert for efficient sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Radon Transform</head><p>Let L 1 (X) be the space of absolute integrable functions on X. The Radon transform R :</p><formula xml:id="formula_1">L 1 (R d ) ? L 1 (R ? S d?1 ) is defined as (Rp)(t, ?) = R d p(x)?(t ? x, ? )dx,<label>(2)</label></formula><p>where S d?1 denotes the unit sphere ? 2 1 + ? ? ? ? 2 d = 1 in R d , ?(?) is the Dirac delta function, and ?, ? is the standard inner product in R d . For a given ?, the function (Rp)(?, ?) : R ? R is essentially the slice (or projection) of p(x) on axis ?.</p><p>Note that the Radon transform R is invertible. Its inverse, also known as the filtered back-projection formula, is given by <ref type="bibr" target="#b18">(Helgason, 2010;</ref><ref type="bibr" target="#b27">Kolouri et al., 2019)</ref> </p><formula xml:id="formula_2">R ?1 ((Rp)(t, ?))(x) = S d?1 ((Rp)(?, ?) * h)( x, ? )d?,<label>(3)</label></formula><p>where * is the convolution operator, and the convolution kernel h has the Fourier transform?(k) = c|k| d?1 . The inverse Radon transform provides a practical way to reconstruct the original function p(x) using its 1D slices (Rp)(?, ?), and is widely used in medical imaging. This inverse formula implies that if the 1D slices of two functions are the same in all axes, these two functions are identical, also known as Cram?r-Wold theorem <ref type="bibr" target="#b5">(Cram?r &amp; Wold, 1936)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sliced and Maximum Sliced Wasserstein Distances</head><p>The p-Wasserstein distance, p ? [1, ?), between two probability distributions p 1 and p 2 is defined as:</p><formula xml:id="formula_3">W p (p 1 , p 2 ) = inf ???(p1,p2) E (x,y)?? [ x ? y p ] 1 p ,<label>(4)</label></formula><p>where ?(p 1 , p 2 ) is the set of all possible joint distributions ?(x, y) with marginalized distributions p 1 and p 2 . In 1D the Wasserstein distance has a closed form solution via Cumulative Distribution Functions (CDFs), but this evaluation is intractable in high dimension. An alternative metric, the Sliced p-Wasserstein Distance (SWD), is defined as:</p><formula xml:id="formula_4">SW p (p 1 , p 2 ) = S d?1 W p p (Rp 1 (?, ?), Rp 2 (?, ?))d? 1 p ,<label>(5)</label></formula><p>where d? is the normalized uniform measure on S d?1 . The SWD can be calculated by approximating the high dimensional integral with Monte Carlo samples. However, in high dimensions a large number of projections is required to accurately estimate SWD. This motivates to use the maximum Sliced p-Wasserstein Distance (max SWD):</p><formula xml:id="formula_5">max -SW p (p 1 , p 2 ) = max ??S d?1 W p (Rp 1 (?, ?), Rp 2 (?, ?)),<label>(6)</label></formula><p>which is the maximum of the Wasserstein distance of the 1D marginalized distributions of all possible directions. SWD and max SWD are both proper distances <ref type="bibr" target="#b25">(Kolouri et al., 2015;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sliced Iterative Normalizing Flows</head><p>We consider the general problem of building a NF that maps an arbitrary PDF p 1 (x) to another arbitrary PDF p 2 (x) of the same dimensionality. We first introduce our objective function in Section 3.1. The general SINF algorithm is presented in Section 3.2. We then consider the special cases of p 1 and p 2 being standard Normal distributions in Section 3.3 and Section 3.4, respectively. In Section 3.5 we discuss our patch-based hierarchical strategy for modeling images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Maximum K-sliced Wasserstein Distance</head><p>We generalize the idea of maximum SWD and propose maximum K-Sliced p-Wasserstein Distance (max K-SWD):</p><formula xml:id="formula_6">max -K-SW p (p 1 , p 2 ) = max {?1,??? ,? K } orthonormal 1 K K k=1 W p p ((Rp 1 )(?, ? k ), (Rp 2 )(?, ? k )) 1 p .<label>(7)</label></formula><p>In this work we fix p = 2. The proof that max K-SWD is a proper distance is in the appendix. If K = 1, it becomes max SWD. For K &lt; d, the idea of finding the subspace with maximum distance is similar to the subspace robust Wasserstein distance <ref type="bibr" target="#b45">(Paty &amp; Cuturi, 2019)</ref>. <ref type="bibr" target="#b60">Wu et al. (2019)</ref> and <ref type="bibr" target="#b51">Rowland et al. (2019)</ref> proposed to approximate SWD with orthogonal projections, similar to max K-SWD with K = d. max K-SWD will be used as the objective in our proposed algorithm. It defines K orthogonal axes {? 1 , ? ? ? , ? K } for which the marginal distributions of p 1 and p 2 are the most different, providing a natural choice for performing 1D marginal matching in our algorithm (see Section 3.2).</p><p>The optimization in max K-SWD is performed under the constraints that {? 1 , ? ? ? , ? K } are orthonormal vectors, or equivalently, A T A = I K where A = [? 1 , ? ? ? , ? K ] is the matrix whose i-th column vector is ? i . Mathematically, the set of all possible A matrices is called Stiefel Manifold V K (R d ) = {A ? R d?K : A T A = I K }, and we perform the optimization on the Stiefel Manifold following <ref type="bibr" target="#b54">Tagare (2011)</ref>. The details of the optimization is provided in the appendix, and the procedure for estimating max K-SWD and A is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed SINF Algorithm</head><p>The proposed SINF algorithm is based on iteratively minimizing the max K-SWD between the two distributions p 1 and p 2 . Specifically, SINF iteratively solves for the orthogonal axes where the marginals of p 1 and p 2 are most different (defined by max K-SWD), and then match the 1D marginalized distribution of p 1 to p 2 on those axes. This is motivated by the inverse Radon Transform <ref type="table" target="#tab_4">(Equation 3)</ref> and Cram?r-Wold theorem, which suggest that matching the high dimensional distributions is equivalent to matching the 1D slices on all possible directions, decomposing the high dimensional problem into a series of 1D problems. See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration of the SINF algorithm. Given a set Algorithm 1 max K-SWD</p><formula xml:id="formula_7">Input: {x i ? p 1 } N i=1 , {y i ? p 2 } N i=1 , K, order p, max iteration J maxiter Randomly initialize A ? V K (R d ) for j = 1 to J maxiter do Initialize D = 0 for k = 1 to K do ? k = A[:, k] Computex i = ? k ? x i and? i = ? k ? y i for each i Sortx i andx j in ascending order s.t.x i[n] ? x i[n+1] and? j[n] ?? j[n+1] D = D + 1 KN N i=1 |x i[n] ?? j[n] | p end for G = [? ?D ?Ai,j ], U = [G, A] , V = [A, ?G] Determine learning rate ? with backtracking line search A = A ? ? U (I 2K + ? 2 V T U ) ?1 V T A if A has converged then Early stop end if end for Output: D 1 p ? max -K-SW p , A ? [? 1 , ? ? ? , ? K ] of i.i.d. samples X drawn from p 1 , in each iteration, a set of 1D marginal transformations {? k } K k=1 (K ? d where d</formula><p>is the dimensionality of the dataset) are applied to the samples on orthogonal axes {? k } K k=1 to match the 1D marginalized PDF of p 2 along those axes. Let A = [? 1 , ? ? ? , ? K ] be the matrix derived from max K-SWD optimization (algorithm 1), that contains K orthogonal axes(A T A = I K ). Then the transformation at iteration l of samples X l can be written as</p><formula xml:id="formula_8">1 X l+1 = A l ? l (A T l X l ) + X ? l ,<label>(8)</label></formula><p>where X ? l = X l ? A l A T l X l contains the components that are perpendicular to ? 1 , ..., ? K and is unchanged in iteration l. ? l = [? l1 , ? ? ? , ? lK ] T is the marginal mapping of each dimension of A T l X l , and its components are required to be monotonic and differentiable. The transformation of Equation 8 can be easily inverted:</p><formula xml:id="formula_9">X l = A l ? ?1 l (A T l X l+1 ) + X ? l ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">X ? l = X l ? A l A T l X l = X l+1 ? A l A T l X l+1 .</formula><p>The Jacobian determinant of the transformation is also efficient to calculate (see appendix for the proof): At iteration l, the SINF objective can be written as:</p><formula xml:id="formula_11">det( ?X l+1 ?X l ) = K k=1 d? lk (x) dx .<label>(10)</label></formula><formula xml:id="formula_12">F l = min {? l1 ,??? ,? lK } max {? l1 ,??? ,? lK } orthonormal 1 K K k=1 W p p (? lk ((Rp 1,l )(?, ? lk )), (Rp 2 )(?, ? lk )) 1 p .<label>(11)</label></formula><p>The algorithm first optimizes ? lk to maximize the objective, with ? lk fixed to identical transformations (equivalent to Equation 7). Then the axes ? lk are fixed and the objective is minimized with marginal matching ? l . The samples are updated, and the process is repeated until convergence.</p><p>Let p 1,l be the transformed p 1 at iteration l. The kth component of ? l , ? l,k , maps the 1D marginalized PDF of p 1,l to p 2 and has an OT solution:</p><formula xml:id="formula_13">? l,k (x) = F ?1 k (G l,k (x)),<label>(12)</label></formula><p>where G l,k (x) =</p><p>x ?? (Rp 1,l )(t, ? k )dt and F k (x) =</p><p>x ?? (Rp 2 )(t, ? k )dt are the CDFs of p 1,l and p 2 on axis ? k , respectively. The CDFs can be estimated using the quantiles of the samples (in SIG Section 3.3), or using Kernel Density Estimation (KDE, in GIS Section 3.4). Equation 12 is monotonic and invertible. We choose to parametrize it with monotonic rational quadratic splines <ref type="bibr" target="#b15">(Gregory &amp; Delbourgo, 1982;</ref><ref type="bibr" target="#b11">Durkan et al., 2019)</ref>, which are continuouslydifferentiable and allows analytic inverse. More details about the spline procedure are given in the appendix. We summarize SINF in Algorithm 2.</p><p>The proposed algorithm iteratively minimizes the max K-SWD between the transformed p 1 and p 2 . The orthonomal vectors {? 1 , ? ? ? , ? K } specify K axes along which the marginalized PDF between p 1,l and p 2 are most different, thus maximizing the gain at each iteration and improving the efficiency of the algorithm. In the appendix we show empirically that the model is able to converge with two orders of magnitude fewer iterations than random axes, and it also leads to better sample quality. This is because as the dimensionality d grows, the number of slices (Rp)(?, ?) required to approximate p(x) using inverse Radon formula scales as L d?1 <ref type="bibr" target="#b25">(Kolouri et al., 2015)</ref>, where L is the number of slices needed to approximate a similar smooth 2D distribution. Therefore, if ? are randomly chosen, it takes a Algorithm 2 Sliced Iterative Normalizing Flow</p><formula xml:id="formula_14">Input: {x i ? p 1 } N i=1 , {y i ? p 2 } N i=1 , K, number of iteration L iter for l = 1 to L iter do A l = max K-SWD(x i , y i , K) for k = 1 to K do ? k = A l [:, k] Computex i = ? k ? x i and? i = ? k ? y i for each ? x m = quantiles(PDF(x i )) y m = quantiles(PDF(? i )) ? l,k = RationalQuadraticSpline(x m ,? m ) end for ? l = [? l1 , ? ? ? , ? lK ] Update x i = x i ? A l A T l x i + A l ? l (A T l x i ) end for</formula><p>large number of iterations to converge in high dimensions due to the curse of dimensionality. Our objective function reduces the curse of dimensionality in high dimensions by identifying the most relevant directions first.</p><p>K is a free hyperparameter in our model. In the appendix we show empirically that the convergence of the algorithm is insensitive to the choice of K, and mostly depends on the total number of 1D transformations L iter ? K.</p><p>Unlike KL-divergence, which is invariant under the flow transformations, max K-SWD is different in data space and in latent space. Therefore the direction of building the flow model is of key importance. In the next two sections we discuss two different ways of building the flow, which are good at sample generation and density estimation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sliced Iterative Generator (SIG)</head><p>For Sliced Iterative Generator (SIG) p 1 is a standard Normal distribution, and p 2 is the target distribution. The model iteratively maps the Normal distribution to the target distribution using 1D slice transformations. SIG directly minimizes the max K-SWD between the generated distribution and the target distribution, and is able to generate high quality samples. The properties of SIG are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Specifically, one first draws a set of samples from the standard Normal distribution, and then iteratively updates the samples following Equation 8. Note that in the NF framework, Equation 8 is the inverse of transformation f l in Equation 1. The ? transformation and the weight matrix A are learned using Equation 12 and Algorithm 1. In Equation <ref type="formula" target="#formula_0">12</ref> we estimate the CDFs using the quantiles of the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Gaussianizing Iterative Slicing (GIS)</head><p>For Gaussianizing Iterative Slicing (GIS) p 1 is the target distribution and p 2 is a standard Normal distribution. The Advantage Good samples Good density estimation model iteratively gaussianizes the target distribution, and the mapping is learned in the reverse direction of SIG. In GIS the max K-SWD between latent data and the Normal distribution is minimized, thus the model performs well in density estimation, even though its learning objective is not log p. The comparison between SIG and GIS is shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We add regularization to GIS for density estimation tasks to further improve the performance and reduce overfitting. The regularization is added in the following two aspects: 1) The weight matrix A l is regularized by limiting the maximum number of iterations J maxiter (see Algorithm 1). We set J maxiter = N/d. Thus for very small datasets (N/d ? 1) the axes of marginal transformation are almost random. This has no effect on datasets of regular size.</p><p>2) The CDFs in Equation 12 are estimated using KDE, and the 1D marginal transformation is regularized with:</p><formula xml:id="formula_15">? l,k (x) = (1 ? ?)? l,k (x) + ?x,<label>(13)</label></formula><p>where ? ? [0, 1) is the regularization parameter, and? l,k is the regularized transformation. In the appendix we show that as ? increases, the performance improves, but more iterations are needed to converge. Thus ? controls the tradeoff between performance and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Patch-Based Hierarchical Approach</head><p>Generally speaking, the neighboring pixels in images have stronger correlations than pixels that are far apart. This fact has been taken advantage by convolutional neural networks, which outperform Fully Connected Neural Networks (FCNNs) and have become standard building blocks in computer vision tasks. Like FCNNs, vanilla SIG and GIS make no assumption about the structure of the data and cannot model high dimensional images very well. <ref type="bibr" target="#b38">Meng et al. (2020)</ref> propose a patch-based approach, which decomposes an S ? S image into p ? p patches, with q ? q neighboring pixels in each patch (S = pq). In each iteration the marginalized distribution of each patch is modeled separately without considering the correlations between different patches. This approach effectively reduces the dimensionality from S 2  <ref type="figure">Figure 2</ref>. Illustration of the patch-based approach with S = 4, p = 2 and q = 2. At each iteration, different patches are modeled separately. The patches are randomly shifted in each iteration assuming periodic boundaries.</p><p>to q 2 , at the cost of ignoring the long range correlations. <ref type="figure">Figure 2</ref> shows an illustration of the patch-based approach.</p><p>To reduce the effects of ignoring the long range correlations, we propose a hierarchical model. In SIG, we start from modeling the entire images, which corresponds to q = S and p = 1. After some iterations the samples show correct structures, indicating the long range correlations have been modeled well. We then gradually decrease the patch size q until q = 2, which allows us to gradually focus on the smaller scales. Assuming a periodic boundary condition, we let the patches randomly shift in each iteration. If the patch size q does not divide S, we set p = S/q and the rest of the pixels are kept unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Iterative normalizing flow models called RBIG <ref type="bibr" target="#b3">(Chen &amp; Gopinath, 2000;</ref><ref type="bibr" target="#b30">Laparra et al., 2011)</ref>   Wasserstein distance between samples and data, and has a NF structure. We compare their samples in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Density Estimation p(x) of Tabular Datasets</head><p>We perform density estimation with GIS on four UCI datasets <ref type="bibr" target="#b32">(Lichman et al., 2013)</ref> and BSDS300 <ref type="bibr" target="#b36">(Martin et al., 2001)</ref>, as well as image datasets <ref type="bibr">MNIST (LeCun et al., 1998)</ref> and Fashion-MNIST <ref type="bibr" target="#b61">(Xiao et al., 2017)</ref>. The data preprocessing of UCI datasets and BSDS300 follows <ref type="bibr" target="#b43">Papamakarios et al. (2017)</ref>. In <ref type="table" target="#tab_3">Table 2</ref> we compare our results with RBIG <ref type="bibr" target="#b30">(Laparra et al., 2011)</ref> and GF <ref type="bibr" target="#b38">(Meng et al., 2020)</ref>. The former can be seen as GIS with random axes to apply 1D gaussianization, while the latter can be seen as training non-iterative GIS with MLE training on p(x).</p><p>We also list other NF models Real NVP <ref type="bibr" target="#b10">(Dinh et al., 2017)</ref>, Glow <ref type="bibr" target="#b22">(Kingma &amp; Dhariwal, 2018)</ref>, FFJORD <ref type="bibr" target="#b14">(Grathwohl et al., 2019)</ref>, MAF <ref type="bibr" target="#b43">(Papamakarios et al., 2017)</ref> and RQ-NSF (AR) <ref type="bibr" target="#b11">(Durkan et al., 2019)</ref> for comparison.</p><p>We observe that RBIG performs significantly worse than current SOTA. GIS outperforms RBIG and is the first iterative algorithm that achieves comparable performance compared  to maximum likelihood models. This is even more impressive given that GIS is not trained on p(x), yet it outperforms GF on p(x) on GAS, BSDS300 and Fashion-MNIST.</p><p>The transformation at each iteration of GIS is well defined, and the algorithm is very stable even for small training sets. To test the stability and performance we compare the density estimation results with other methods varying the size of the training set N (from 10 2 to 10 5 ). For GIS we consider two hyperparameter settings: large regularization ? (Equation 13) for better log p performance, and small regularization ? for faster training. For other NFs we use settings recommended by their original paper, and set the batch size to min(N/10, N batch ), where N batch is the batch size suggested by the original paper. All the models are trained until the validation log p val stops improving, and for KDE the kernel width is chosen to maximize log p val . Some non-GIS NF models diverged during training or used more memory than our GPU, and are not shown in the plot. The results in <ref type="figure" target="#fig_1">Figure 3</ref> show that GIS is more stable compared to other NFs and outperforms them on small training sets. This highlights that GIS is less sensitive to hyper-parameter optimization and achieves good performance out of the box. GIS training time varies with data size, but is generally lower than other NFs for small training sets. We report the training time for 100 training data in <ref type="table" target="#tab_4">Table 3</ref>. GIS with small regularization ? requires significantly less time than other NFs, while still outperforming them at 100 training size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Generative Modeling of Images</head><p>We evaluate SIG as a generative model of images using the following 4 datasets: MNIST, Fashion-MNIST, CIFAR-10 ( <ref type="bibr" target="#b28">Krizhevsky et al., 2009</ref>) and Celeb-A (cropped and interpolated to 64 ? 64 resolution) <ref type="bibr" target="#b33">(Liu et al., 2015)</ref>. In <ref type="figure" target="#fig_3">Figure  5</ref> we show samples of these four datasets. For MNIST, Fashion-MNIST and CelebA dataset we show samples from the model with reduced temperature T = 0.85 (i.e., sampling from a Gaussian distribution with standard deviation T = 0.85 in latent space), which slightly improves the sample quality <ref type="bibr" target="#b44">(Parmar et al., 2018;</ref><ref type="bibr" target="#b22">Kingma &amp; Dhariwal, 2018)</ref>. We report the final FID score (calculated using temperature T=1) in <ref type="table" target="#tab_6">Table 4</ref>, where we compare our results with similar algorithms SWF and Flow-Gan (ADV). We also list the FID scores of some other generative models for comparison, including models using slice-based distance SWAEs (two different models with the same name) <ref type="bibr" target="#b60">(Wu et al., 2019;</ref><ref type="bibr" target="#b26">Kolouri et al., 2018)</ref>   expect it to outperform most NF models in terms of sample quality due to the use of AEs. We notice that previous iterative algorithms are unable to produce good samples on high dimensional image datasets (see <ref type="table" target="#tab_6">Table 4</ref> and <ref type="figure" target="#fig_5">Figure 7</ref> for SWF samples; see <ref type="figure" target="#fig_4">Figure 6</ref> and 7 of <ref type="bibr" target="#b38">Meng et al. (2020)</ref> for RBIG samples). However, SIG obtains the best FID scores on MNIST and Fashion-MNIST, while on CIFAR-10 and CelebA it also outperforms similar algorithms and AE-based models, and gets comparable results to GANs. In <ref type="figure" target="#fig_2">Figure 4</ref> we show samples at different iterations. In <ref type="figure" target="#fig_4">Figure  6</ref> we display interpolations between SIG samples, and the nearest training data, to verify we are not memorizing the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Improving the Samples of Other Generative Models</head><p>Since SIG is able to transform any base distribution to the target distribution, it can also be used as a "Plug-and-Play" tool to improve the samples of other generative models.</p><p>To demonstrate this, we train SWF, Flow-GAN(ADV) and MAF(5) on Fashion-MNIST with the default architectures in their papers, and then we apply 240 SIG iterations (30% of   the total number of iterations in Section 5.2) to improve the sample quality. In <ref type="figure" target="#fig_5">Figure 7</ref> we compare the samples before and after SIG improvement. Their FID scores improve from 207.6, 216.9 and 81.2 to 23.9, 21.2 and 16.6, respectively. These results can be further improved by adding more SIG iterations. OoD detection with generative models has recently attracted a lot of attention, since the log p estimates of NF and VAE have been shown to be poor OoD detectors: different generative models can assign higher probabilities to OoD data than to In Distribution (InD) training data <ref type="bibr" target="#b40">(Nalisnick et al., 2019)</ref>. One combination of datasets for which this has been observed is Fashion-MNIST and MNIST, where a model trained on the former assigns higher density to the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Out of Distribution (OoD) Detection</head><p>SINF does not train on the likelihood p(x), which is an advantage for OoD. Likelihood is sensitive to the smallest variance directions <ref type="bibr" target="#b48">(Ren et al., 2019)</ref>: for example, a zero variance pixel leads to an infinite p(x), and noise must be added to regularize it. But zero variance directions contain little or no information on the global structure of the image. SINF objective is more sensitive to the meaningful global structures that can separate between OoD and InD. Because the patch based approach ignores the long range correlations and results in poor OoD, we use vanilla SINF without patch based approach. We train the models on F-MNIST, and then evaluate anomaly detection on test data of MNIST and OMNIGLOT <ref type="bibr" target="#b29">(Lake et al., 2015)</ref>. In <ref type="table" target="#tab_7">Table 5</ref> we compare our results to maximum likelihood p(x) models PixelCNN++ <ref type="bibr" target="#b52">(Salimans et al., 2017;</ref><ref type="bibr" target="#b48">Ren et al., 2019)</ref>, and IWAE <ref type="bibr" target="#b4">(Choi et al., 2018)</ref>. Other models that perform well include VIB and WAIC <ref type="bibr" target="#b4">(Choi et al., 2018)</ref>, which achieve 0.941, 0.943 and 0.766, 0.796, for MNIST and OMNIGLOT, respectively (below our SIG results). For the MNIST case <ref type="bibr" target="#b48">Ren et al. (2019)</ref> obtained 0.996 using the likelihood ratio between the model and its perturbed version, but they require fine-tuning on some additional OoD dataset, which may not be available in OoD applications. Lower dimensional latent space PAE <ref type="bibr" target="#b2">(B?hm &amp; Seljak, 2020)</ref> achieves 0.997 and 0.981 for MNIST and OMNIGLOT, respectively, while VAE based likelihood regret <ref type="bibr" target="#b63">(Xiao et al., 2020)</ref> achieves 0.988 on MNIST, but requires additional (expensive) processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduce sliced iterative normalizig flow (SINF) that uses Sliced Optimal Transport to iteratively transform data distribution to a Gaussian (GIS) or the other way around (SIG). To the best of our knowledge, SIG is the first greedy deep learning algorithm that is competitive with the SOTA generators in high dimensions, while GIS achieves comparable results on density estimation with current NF models, but is more stable, faster to train, and achieves higher p(x) when trained on small training sets, even though it does not train on p(x). It also achieves better OoD performance. SINF is very stable to train, has very few hyperparameters, and is very insensitive to their choice (see appendix). SINF has deep neural network architecture, but its approach deviates significantly from the current DL paradigm, as it does not use concepts such as mini-batching, stochastic gradient descent and gradient back-propagation through deep layers. SINF is an existence proof that greedy DL without these ingredients can be state of the art for modern high dimensional ML applications. Such approaches thus deserve more detailed investigations that may have an impact on the theory and practice of DL.</p><p>Since R l is an orthogonal matrix with determinant ?1, and the Jacobian of the marginal transformation? l is diagonal, the Jacobian determinant of the above equation can be written as</p><formula xml:id="formula_16">det( ?X l+1 ?X l ) = K k=1 d? lk (x) dx ? d?K k=1 d(id k (x)) dx = K k=1 d? lk (x) dx .<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Monotonic Rational Quadratic Spline</head><p>Monotonic Rational Quadratic Splines <ref type="bibr" target="#b15">(Gregory &amp; Delbourgo, 1982;</ref><ref type="bibr" target="#b11">Durkan et al., 2019)</ref> approximate the function in each bin with the quotient of two quadratic polynomials. They are monotonic, contineously differentiable, and can be inverted analytically. The splines are parametrized by the coordinates and derivatives of M knots: {(x m , y m , y m )} M m=1 , with x m+1 &gt; x m , y m+1 &gt; y m and y m &gt; 0. Given these parameters, the function in bin m can be written as <ref type="bibr" target="#b11">(Durkan et al., 2019)</ref> </p><formula xml:id="formula_17">y = y m + (y m+1 ? y m ) s m ? 2 + y m ?(1 ? ?) s m + ? m ?(1 ? ?) ,<label>(20)</label></formula><p>where</p><formula xml:id="formula_18">s m = (y m+1 ? y m )/(x m+1 ? x m ), ? m = y m+1 + y m ? 2s m and ? = (x ? x m )/(x m+1 ? x m ).</formula><p>The derivative is given by</p><formula xml:id="formula_19">dy dx = s 2 m [y m+1 ? 2 + 2s m ?(1 ? ?) + y m (1 ? ?) 2 ] [s m + ? m ?(1 ? ?)] 2 .<label>(21)</label></formula><p>Finally, the inverse can be calculated with</p><formula xml:id="formula_20">x = x m + (x m+1 ? x m ) 2c ?b ? ? b 2 ? 4ac ,<label>(22)</label></formula><p>where a = (s m ? y m ) + ?? m , b = y m ? ?? m , c = ?s m ? and ? = (y ? y m )/(y m+1 ? y m ). The derivation of these formula can be found in Appendix A of <ref type="bibr" target="#b11">Durkan et al. (2019)</ref>.</p><p>In our algorithm the coordinates of the knots are determined by the quantiles of the marginalized PDF (see Algorithm 2). The derivative y m (1 &lt; m &lt; M ) is determined by fitting a local quadratic polynomial to the neighboring knots (x m?1 , y m?1 ), (x m , y m ), and (x m+1 , y m+1 ):</p><formula xml:id="formula_21">y m = s m?1 (x m+1 ? x m ) + s m (x m ? x m?1 ) x m+1 ? x m?1 .<label>(23)</label></formula><p>The function outside [x 1 , x M ] is linearly extrapolated with slopes y 1 and y M . In SIG, y 1 and y M are fixed to 1, while in GIS they are fitted to the samples that fall outside [x 1 , x M ].</p><p>We use M = 400 knots in SIG to interpolate each ? l,k , while in GIS we allow M to vary between <ref type="bibr">[50,</ref><ref type="bibr">200]</ref>, depending on the dataset size M = ? N train . The performance is insensitive to these choices, as long as M is large enough to fully characterize the 1D transformation ? l,k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization on the Stiefel Manifold</head><p>The calculation of max K-SWD (Equation 7) requires optimization under the constraints that {? 1 , ? ? ? , ? K } are orthonormal vectors, or equivalently, A T A = I K where A = [? 1 , ? ? ? , ? K ] is the matrix whose i-th column vector is ? i . As suggested by <ref type="bibr" target="#b54">Tagare (2011)</ref>, the optimization of matrix A can be performed by doing gradient ascent on the Stiefel Manifold:</p><formula xml:id="formula_22">A (j+1) = I d + ? 2 B (j) ?1 I d ? ? 2 B (j) A (j) ,<label>(24)</label></formula><p>where A (j) is the weight matrix at gradient descent iteration j (which is different from the iteration l of the algorithm), ? is the learning rate, which is determined by backtracking line search, B = GA T ? AG T , and G is the negative gradient matrix G = [? ?F ?Ap,q ] ? R d?K . Equation 24 has the properties that A (j+1) ? V K (R d ), and that the tangent vector</p><formula xml:id="formula_23">dA (j+1) d? | ? =0 is the projection of gradient [ ?F ?Ap,q ] onto T A (j) (V K (R d )) (the tangent space of V K (R d ) at A (j)</formula><p>) under the canonical inner product <ref type="bibr" target="#b54">(Tagare, 2011)</ref>.</p><p>However, Equation 24 requires the inverse of a d ? d matrix, which is computationally expensive in high dimensions.</p><p>The matrix inverse can be simplified using the Sherman-Morrison-Woodbury formula, which results in the following equation <ref type="bibr" target="#b54">(Tagare, 2011)</ref>: For high dimensional data (e.g. images), we use a relatively small K to avoid the inverse of large matrices. A large K leads to faster training, but one would converge to similar results with a small K using more iterations. In Appendix D we show that the convergence is insensitive to the choice of K.</p><formula xml:id="formula_24">A (j+1) = A (j) ? ? U (j) (I 2K + ? 2 V T (j) U (j) ) ?1 V T (j) A (j) ,<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameter study and ablation analysis</head><p>Here we study the sensitivity of SINF to hyperparameters and perform ablation analyses.</p><p>D.1. Hyperparameter K, objective function, and patch based approach</p><p>We firstly test the convergence of SIG on MNIST dataset with different K choices. We measure the SWD (Equation 5) and max SWD <ref type="formula" target="#formula_5">(Equation 6</ref>) between the test data and model samples for different iterations (without patch based hierarchical modeling). The results are presented in <ref type="figure" target="#fig_7">Figure  8</ref>. The SWD is measured with 10000 Monte Carlo samples and averaged over 10 times. The max SWD is measured with Algorithm 1 (K = 1) using different starting points in order to find the global maximum. We also measure the SWD and max SWD between the training data and test data, which gives an estimate of the noise level arising from the finite number of test data. For the range of K we consider (1 ? K ? 128), all tests we perform converges to the noise level, and the convergence is insensitive to the choice of K, but mostly depends on the total number of 1D transformations (N iter ? K). As a comparison, we also try running SIG with random orthogonal axes per iteration, and for MNIST, our greedy algorithm converges with two orders of magnitude fewer marginal transformations than random orthogonal axes <ref type="figure" target="#fig_7">(Figure 8</ref>).</p><p>For K = 1, the objective function (Equation 11) is the same as max SWD, so one would expect that the max SWD between the data and the model distribution keep decreasing as the iteration number increases. For K &gt; 1, the max K-SWD is bounded by max SWD <ref type="figure" target="#fig_0">(Equation 15</ref> and 16) so one would also expect similar behavior. However, from <ref type="figure" target="#fig_7">Figure 8</ref> we find that max SWD stays constant in the first 400 iterations. This is because SIG fails to find the global maximum of the objective function in those iterations, i.e.,  the algorithm converges at some local maximum that is almost perpendicular to the global maximum in the high dimensional space, and therefore the max SWD is almost unchanged. This suggests that our algorithm does not require global optimization of A at each iteration: even if we find only a local maximum, it can be compensated with subsequent iterations. Therefore our model is insensitive to the initialization and random seeds. This is very different from the standard non-convex loss function optimization in deep learning with a fixed number of layers, where the random seeds often make a big difference <ref type="bibr" target="#b35">(Lucic et al., 2018)</ref>.</p><p>In <ref type="figure" target="#fig_8">Figure 9</ref> we show the samples of SIG of random axes, optimized axes and hierarchical approach. On the one hand, the sample quality of SIG with optimized axes is better than that of random axes, suggesting that our proposed objective max K-SWD improves both the efficiency and the accuracy of the modeling. On the other hand, SIG with optimized axes has reached the noise level on both SWD and max SWD at around 2000 marginal transformations <ref type="figure" target="#fig_7">(Figure 8</ref>), but the samples are not good at that point, and further increasing the number of 1D transformations from 2000 to 200000 does not significantly improve the sample quality. At this stage the objective function of Equation 11 is dominated by the noise from finite sample size, and the optimized axes are nearly random, which significantly limits the efficiency of our algorithm. To better understand this noise, we do a simple experiment by sampling two sets of samples from the standard normal distribution N (0, I) and measuring the max SWD using the samples. The true distance should be zero, and any nonzero value is caused by the finite number of samples. In <ref type="figure" target="#fig_0">Figure 10</ref> we show the measured max SWD as a function of sample size and dimensionality. For small number of samples and high dimensionality, the measured max SWD is quite large, suggesting that we can easily find an axis where the marginalized PDF of the two sets of samples are significantly different, while their underlying distribution are actually the same. Because of this sample noise, once the generated and the target distribution are close to each other (the max K-SWD reached the noise level), the optimized axes becomes random and the algorithm becomes inefficient. To reduce the noise level, one needs to either increase the size of training data or decrease the dimensionality of the problem. The former can be achieved with data augmentation. In this study we adopt the second approach, i.e., we effectively reduce the dimensionality of the modeling with a patch based hierarchical approach. The corresponding samples are shown in the bottom panel of <ref type="figure" target="#fig_8">Figure 9</ref>. We see that the sample quality keeps improving after 2000 marginal transformations, because the patch based approach reduces the effective noise level. To explore the effect of regularization parameter ?, we train GIS on POWER dataset with different ?. We keep adding iterations until the log-likelihood of validation set stops im- proving. The final test log p and the number of iterations are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. We see that with a larger ?, the algorithm gets better density estimation performance, at the cost of taking more iterations to converge. Setting the regularization parameter ? is a trade-off between performance and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental details</head><p>The hyperparameters of GIS include the number of axes per iteration K, the regularization ?, and the KDE kernel width factor b. We have two different ? values: ? = (? 1 , ? 2 ), where ? 1 regularizes the rational quadratic splines, and ? 2 regularizes the linear extrapolations. The KDE kernel width ? is determined by the Scott's rule <ref type="bibr" target="#b53">(Scott, 2015)</ref>:</p><formula xml:id="formula_25">? = bN ?0.2 ? data ,<label>(26)</label></formula><p>where N is the number of training data, and ? data is the standard deviation of the data marginalized distribution.</p><p>The hyperparameters for density-estimation results in <ref type="table" target="#tab_3">Table  2</ref> are shown in <ref type="table" target="#tab_8">Table 6</ref>. K is determined by K = min(8, d).</p><p>For BSDS300 we first whiten the data before applying GIS. For high dimensional image datasets MNIST and Fashion-MNIST, we add patch-based iterations with patch size q = 4 and q = 2 alternately. Logit transformation is used as data preprocessing. For all of the datasets, we keep adding iterations until the validation log p stops improving.</p><p>For density estimation of small datasets, we use the following hyperparameter choices for large regularization setting: b = 1, K = min(8, d), ? = (1 ? 0.02 log 10 (N train ), 1 ? 0.001 log 10 (N train )). While for low regularization setting we use b = 2 and ? = (0, 1 ? 0.01 log 10 (N train )). The size of the validation set is 30% of the training set size. All results are averaged over 5 different realizations.</p><p>The hyperparameters of SIG include the number of axes per iteration K, and the patch size for each iteration, if the patch-based approach is adopted. We show the SIG hyperparameters for modeling image datasets in <ref type="table">Table 7</ref>. As discussed in Section 3.5, the basic idea of setting the architecture is to start from the entire image, and then gradually decrease the patch size until q = 2. An illustration of the patch-based hierarchical approach is shown in <ref type="figure" target="#fig_0">Figure 12</ref>. We set K = q or K = 2q, depending on the datasets and the depth of the patch. For each patch size we add 100 or 200 iterations.</p><p>For OOD results in Section 5.4, we train SIG and GIS on Fashion-MNIST with K = 56. GIS is trained with b = 1 and ? = 0.9 (the results are insensitive to all these hyperparameter choices). We do not use logit transformation preprocessing, as it overamplifies the importance of pixels with low variance. The number of iterations are determined by optimizing the validation log p. For SIG, which cannot produce good log p, the results shown in <ref type="table" target="#tab_7">Table 5</ref> use 100 iterations, but we verify they do not depend on this choice and are stable up to thousands of iterations.</p><p>Sliced Iterative Normalizing Flows 8 (q = 4) 4 (q = 2) ? = (? 1 , ? 2 ) (0.9,0.9) (0.9,0.9) (0.95, 0.99) (0.95, 0.999) (0.95, 0.95) (0.9, 0.99) (0.9, 0.99) b 2 1 1 2 5 1 1 <ref type="table">Table 7</ref>. The architectures of SIG for modeling different image datasets in Section 5.2. The architecture is reported in the format of (q 2 ? c, K) ? L, where q is the side length of the patch, c is the depth of the patch, K is the number of marginal transformations per patch, and L is the number of iterations for that patch size. MNIST and Fashion-MNIST share the same architecture.</p><p>MNIST / Fashion-MNIST CIFAR-10 CelebA architecture (28 2 ? 1, 56) ? 100 (32 2 ? 3, 64) ? 200 (64 2 ? 3, 128) ? 200 (14 2 ? 1, 28) ? 100 (16 2 ? 3, 32) ? 200 (32 2 ? 3, 64) ? 200 (7 2 ? 1, 14) ? 100 (8 2 ? 3, 16) ? 200 (16 2 ? 3, 32) ? 200 (6 2 ? 1, 12) ? 100 (8 2 ? 1, 8) ? 100 (8 2 ? 3, 16) ? 200 (5 2 ? 1, 10) ? 100 (7 2 ? 3, 14) ? 200 (8 2 ? 1, 8) ? 100 (4 2 ? 1, 8) ? 100 (7 2 ? 1, 7) ? 100 (7 2 ? 3, 14) ? 200 (3 2 ? 1, 6) ? 100 (6 2 ? 3, 12) ? 200 (7 2 ? 1, 7) ? 100 (2 2 ? 1, 4) ? 100 (6 2 ? 1, 6) ? 100 (6 2 ? 3, 12) ? 200 (5 2 ? 3, 10) ? 200 (6 2 ? 1, 6) ? 100 (5 2 ? 1, 5) ? 100 (5 2 ? 3, 10) ? 200 (4 2 ? 3, 8) ? 200 (5 2 ? 1, 5) ? 100 (4 2 ? 1, 4) ? 100 (4 2 ? 3, 8) ? 200 (3 2 ? 3, 6) ? 200 (4 2 ? 1, 4) ? 100 (3 2 ? 1, 3) ? 100 (3 2 ? 3, 6) ? 200 (2 2 ? 3, 4) ? 200</p><p>(3 2 ? 1, 3) ? 100 (2 2 ? 1, 2) ? 100</p><p>(2 2 ? 3, 6) ? 100</p><p>Total number of iterations L iter 800 2500 2500</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of 1 iteration of SINF algorithm with K = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Density estimation on small training sets. The legends in panel (a) and (b) apply to other panels as well. At 100 training data GIS has the best performance in all cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Gaussian noise (first column), Fashion-MNIST (top panel) and CelebA (bottom) samples at different iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Random samples from SIG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Middle: interpolations between CelebA samples from SIG. Left and right: the corresponding nearest training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Fashion-MNIST samples before (left panel) and after SIG improvement (right panel). Top: SWF. Middle: Flow-GAN (ADV). Bottom: MAF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where U = [G, A] (the concatenation of columns of G and A) and V = [A, ?G]. Equation 25 only involves the inverse of a 2K ? 2K matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Sliced Wasserstein Distance (SWD, top panel) and Max-Sliced Wasserstein Distance (max SWD, bottom panel) between the MNIST test data and model samples as a function of total number of marginal transformations. The legend in the top panel also applies to the bottom panel.The SWD and max SWD between the training data and test data is shown in the horizontal solid black lines. The lines with "random" indicate that the axes are randomly chosen (like RBIG) instead of using the axes of max K-SWD. We also test K = 2, 4, 8, 32, and 64. Their curves overlap with K = 1, 16 and 128 and are not shown in the plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Top panel: SIG samples with random axes (K = 64). Middle panel: SIG samples with optimized axes (K = 64). Bottom panel: SIG samples with optimized axes and patch based hierarchical approach. The numbers above each panel indicate the number of marginal transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>The measured maximum sliced Wasserstein distance between two Gaussian datasets as a function of number of samples. 10 different starting points are used to find the global maximum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>D. 2 .Figure 11 .</head><label>211</label><figDesc>Effects of regularization parameter ? in density estimation Test log-likelihood (left panel) and number of iterations (right panel) as a function of regularization parameter ? on POWER dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Illustration of the hierarchical modeling of an S = 8 image. The patch size starts from q = 8 and gradually decreases to q = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between SIG and GIS</figDesc><table><row><cell>Model</cell><cell>SIG</cell><cell>GIS</cell></row><row><cell>Initial PDF p 1</cell><cell>Gaussian</cell><cell>p data</cell></row><row><cell>Final PDF p 2</cell><cell>p data</cell><cell>Gaussian</cell></row><row><cell>Training</cell><cell cols="2">Iteratively maps Gaussian to p data p data to Gaussian Iteratively maps</cell></row><row><cell>NF structure</cell><cell>Yes</cell><cell>Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are simplified versions of GIS, as they are based on a succession of rotations followed by 1D marginal Gaussianizations. Iterative Distribution Transfer (IDT)<ref type="bibr" target="#b46">(Piti? et al., 2007)</ref> is a similar algorithm but does not require the base distribution to be a Gaussian. These models do not scale well to high dimensions because they do not have a good way of choosing the axes (slice directions), and they are not competitive against modern NFs trained on p(x)<ref type="bibr" target="#b38">(Meng et al., 2020)</ref>.<ref type="bibr" target="#b37">Meng et al. (2019)</ref> use a similar algorithm called Projection Pursuit Monge Map (PPMM) to construct OT maps. They propose to find the most informative axis using Projection Pursuit (PP)<ref type="bibr" target="#b12">(Freedman, 1985)</ref> at each iteration, and show that PPMM works well in low-dimensional bottleneck settings (d = 8). PP scales as O(d 3 ), which makes PPMM scaling to high dimensions prohibitive. A DL, non-iterative version of these models is Gaussianization Flow (GF)<ref type="bibr" target="#b38">(Meng et al., 2020)</ref>, which trains on p(x) and achieves good density estimation results in low dimensions, but does not have good sampling properties in high dimensions. RBIG, GIS and GF have similar architectures but are trained differently. We compare their density estimation results in Section 5.1. Another iterative generative model is Sliced Wasserstein Flow (SWF)<ref type="bibr" target="#b34">(Liutkus et al., 2019)</ref>. Similar to SIG, SWF tries to minimize the SWD between the distributions of samples and the data, and transforms this problem into solving a d dimensional PDE. The PDE is solved iteratively by doing a gradient flow in the Wasserstein space, and works well for low dimensional bottleneck features. However, in each iteration the algorithm requires evaluating an integral over the d dimensional unit sphere approximated with Monte Carlo integration, which does not scale well to high dimensions.Another difference with SIG is that SWF does not have a flow structure, cannot be inverted, and does not provide the likelihood. We compare the sample qualities between SWF and SIG in Section 5.2. DSW), which tries to find the optimal axes distribution by parametrizing it with a neural network. They apply DSW to the training of GANs, and we will refer to their model as DSWGAN in this paper. We compare the sample qualities between SIG, SWAE, ms-DRAE, DSWGAN and other similar models in Section 5.2.<ref type="bibr" target="#b16">Grover et al. (2018)</ref> propose Flow-GAN using a NF as the generator of a GAN, so the model can perform likelihood evaluation, and allows both maximum likelihood and adver-</figDesc><table /><note>SWD, max SWD and other slice-based distance (e.g. Cram?r-Wold distance) have been widely used in training generative models (Deshpande et al., 2018; 2019; Wu et al., 2019; Kolouri et al., 2018; Knop et al., 2018; Nguyen et al., 2020b;a; Nadjahi et al., 2020). Wu et al. (2019) propose a differentiable SWD block composed of a rotation fol- lowed by marginalized Gaussianizations, but unlike RBIG, the rotation matrix is trained in an end-to-end DL fashion. They propose Sliced Wasserstein AutoEncoder (SWAE) by adding SWD blocks to an AE to regularize the latent variables, and show that its sample quality outperforms VAE and AE + RBIG. Nguyen et al. (2020b;a) generalize the max-sliced approach using parametrized distributions over projection axes. Nguyen et al. (2020b) propose Mix- ture Spherical Sliced Fused Gromov Wasserstein (MSSFG), which samples the slice axes around a few informative direc- tions following Von Mises-Fisher distribution. They apply MSSFG to training of Deterministic Relational regularized AutoEncoder (DRAE) and name it mixture spherical DRAE (ms-DRAE). Nguyen et al. (2020a) go further and propose Distributional Sliced Wasserstein distance (sarial training. Similar to our work they find that adversarial training gives good samples but poor p(x), while training by maximum likelihood results in bad samples. Similar to SIG, the adversarial version of Flow-GAN minimizes the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Negative test log-likelihood for tabular datasets measured in nats, and image datasets measured in bits/dim (lower is better).</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="7">POWER GAS HEPMASS MINIBOONE BSDS300 MNIST Fashion</cell></row><row><cell>iterative</cell><cell>RBIG GIS (this work)</cell><cell>1.02 -0.32</cell><cell>0.05 -10.30</cell><cell>24.59 19.00</cell><cell>25.41 14.26</cell><cell>-115.96 -155.75</cell><cell>1.71 1.34</cell><cell>4.46 3.22</cell></row><row><cell></cell><cell>GF</cell><cell>-0.57</cell><cell>-10.13</cell><cell>17.59</cell><cell>10.32</cell><cell>-152.82</cell><cell>1.29</cell><cell>3.35</cell></row><row><cell></cell><cell>Real NVP</cell><cell>-0.17</cell><cell>-8.33</cell><cell>18.71</cell><cell>13.55</cell><cell>-153.28</cell><cell>1.06</cell><cell>2.85</cell></row><row><cell>maximum</cell><cell>Glow</cell><cell>-0.17</cell><cell>-8.15</cell><cell>18.92</cell><cell>11.35</cell><cell>-155.07</cell><cell>1.05</cell><cell>2.95</cell></row><row><cell>likelihood</cell><cell>FFJORD</cell><cell>-0.46</cell><cell>-8.59</cell><cell>14.92</cell><cell>10.43</cell><cell>-157.40</cell><cell>0.99</cell><cell>-</cell></row><row><cell></cell><cell>MAF</cell><cell>-0.30</cell><cell>-10.08</cell><cell>17.39</cell><cell>11.68</cell><cell>-156.36</cell><cell>1.89</cell><cell>-</cell></row><row><cell></cell><cell>RQ-NSF (AR)</cell><cell>-0.66</cell><cell>-13.09</cell><cell>14.01</cell><cell>9.22</cell><cell>-157.31</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Averaged</figDesc><table><row><cell></cell><cell cols="5">training time of different NF models on small</cell></row><row><cell cols="6">datasets (Ntrain = 100) measured in seconds. All the models</cell></row><row><cell cols="6">are tested on both a cpu and a K80 gpu, and the faster results are</cell></row><row><cell cols="6">reported here (the results with * are run on gpus.). P: POWER, G:</cell></row><row><cell cols="5">GAS, H: HEPMASS, M: MINIBOONE, B: BSDS300.</cell><cell></cell></row><row><cell>Method</cell><cell>P</cell><cell>G</cell><cell>H</cell><cell>M</cell><cell>B</cell></row><row><cell>GIS (low ?)</cell><cell>0.53</cell><cell>1.0</cell><cell>0.63</cell><cell>3.5</cell><cell>7.4</cell></row><row><cell>GIS (high ?)</cell><cell>6.8</cell><cell>9.4</cell><cell>7.3</cell><cell>44.1</cell><cell>69.1</cell></row><row><cell cols="2">GF 113  MAF 18.4</cell><cell>-1</cell><cell>10.2</cell><cell>-1</cell><cell>32.1</cell></row><row><cell>FFJORD</cell><cell cols="5">1051 1622 1596 499  *  4548  *</cell></row><row><cell cols="2">RQ-NSF (AR) 118</cell><cell>127</cell><cell cols="2">55.5 38.9</cell><cell>391</cell></row></table><note>* 539* 360* 375* 122*1 Training failures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>FID scores on different datasets (lower is better). The errors are generally smaller than the differences.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell>MNIST Fashion CIFAR-10 CelebA</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">iterative</cell><cell></cell><cell cols="5">SWF SIG (T = 1) (this work)</cell><cell>225.1 4.5</cell><cell>207.6 13.7</cell><cell>-66.5</cell><cell>-37.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Flow-GAN (ADV)</cell><cell>155.6</cell><cell>216.9</cell><cell>71.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">adversarial training</cell><cell></cell><cell></cell><cell cols="3">DSWGAN WGAN WGAN GP</cell><cell></cell><cell>-6.7 20.3</cell><cell>-21.5 24.5</cell><cell>56.4 55.2 55.8</cell><cell>66.9 41.3 30.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Best default GAN</cell><cell>? 10</cell><cell>? 32</cell><cell>? 70</cell><cell>? 48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">SWAE(Wu et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>107.9</cell><cell>48.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">SWAE(Kolouri et al., 2018)</cell><cell>29.8</cell><cell>74.3</cell><cell>141.9</cell><cell>53.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">AE based</cell><cell></cell><cell></cell><cell cols="3">CWAE ms-DRAE</cell><cell></cell><cell>23.6 43.6</cell><cell>57.1 -</cell><cell>120.0 -</cell><cell>49.7 46.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PAE</cell><cell></cell><cell></cell><cell>-</cell><cell>28.0</cell><cell>-</cell><cell>49.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">two-stage VAE</cell><cell></cell><cell>12.6</cell><cell>29.3</cell><cell>96.1</cell><cell>44.4</cell></row><row><cell>Iteration:</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>500</cell><cell>800</cell></row><row><cell>Iteration:</cell><cell>1</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>500</cell><cell>1000</cell><cell>2500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>OoD detection accuracy quantified by the AUROC of data p(x) trained on Fashion-MNIST.</figDesc><table><row><cell>Method</cell><cell cols="2">MNIST OMNIGLOT</cell></row><row><cell>SIG (this work)</cell><cell>0.980</cell><cell>0.993</cell></row><row><cell>GIS (this work)</cell><cell>0.824</cell><cell>0.891</cell></row><row><cell>PixelCNN++</cell><cell>0.089</cell><cell>-</cell></row><row><cell>IWAE</cell><cell>0.423</cell><cell>0.568</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>GIS hyperparameters for density-estimation results inTable 2.</figDesc><table><row><cell cols="2">Hyperparameter POWER</cell><cell>GAS</cell><cell cols="2">HEPMASS MINIBOONE</cell><cell>BSDS300</cell><cell>MNIST</cell><cell>Fashion</cell></row><row><cell>K</cell><cell>6</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8 (q = 4) 4 (q = 2)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Notation definition: In this paper we use l, k, j and m to represent different iterations of the algorithm, different axes ? k , different gradient descent iterations of max K-SWD calculation (see Algorithm 1), and different knots in the spline functions of 1D transformation, respectively.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank He Jia for providing his code on Iterative Gaussianization, and for helpful discussions. We thank Vanessa Boehm and Jascha Sohl-Dickstein for comments on the manuscript. This material is based upon work supported by the National Science Foundation under Grant Numbers 1814370 and NSF 1839217, and by NASA under Grant Number 80NSSC18K1274.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sliced Iterative Normalizing Flows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY DOCUMENT</head><p>A. Proofs Proposition 1. Let P p (?) be the set of Borel probability measures with finite p'th moment on metric space (?, d).</p><p>The maximum K-sliced p-Wasserstein distance is a metric over P p (?).</p><p>Proof. We firstly prove the triangle inequality. Let ? 1 , ? 2 and ? 3 be probability measures in P p (?) with probability density function p 1 , p 2 and p 3 , respectively. Let</p><p>where the first inequality comes from the triangle inequality of Wasserstein distance, and the second inequality follows Minkowski inequality. Therefore max -K-SW p satisfies the triangle inequality.</p><p>Now we prove the identity of indiscernibles. For any probability measures ? 1 and ? 2 in P p (?) with probability density function p 1 and p 2 , let</p><p>On the other hand, let {?,? 2 , ? ? ? ,? K } be a set of orthonormal vectors in S d?1 where the first element is?, we have max -K-SW p (p 1 , p 2 )</p><p>Therefore we have ( 1 K ) 1 p max -SW p (p 1 , p 2 ) ? max -K-SW p (p 1 , p 2 ) ? max -SW p (p 1 , p 2 ).</p><p>Thus max -K-SW p (p 1 , p 2 ) = 0 ? max -SW p (p 1 , p 2 ) = 0 ? Sliced Iterative Normalizing Flows ? 1 = ? 2 , where we use the non-negativity and identity of indiscernibles of max -SW p .</p><p>Finally, the symmetry of max -K-SW p can be proven using the fact that p-Wasserstein distance is symmetric:</p><p>Proof of Equation <ref type="formula">10</ref>. Let {? 1 , ? ? ? , ? K , ? ? ? , ? d } be a set of orthonormal basis in R d where the first K vectors are ? 1 , ? ? ? , ? K , respectively. Let R l = [? 1 , ? ? ? , ? d ] be an orthogonal matrix whose i-th column vector is ? i , </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk4_qw5xe" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserstein Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>B?hm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Seljak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05479</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Probabilistic auto-encoder. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaussianization</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS)</title>
		<editor>Leen, T. K., Dietterich, T. G., and Tresp, V.</editor>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Waic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">but why? generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Some theorems on distribution functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the London Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="290" to="294" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diagnosing and enhancing VAE models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1e0X3C9tQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative modeling using the sliced wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00367</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Deshpande_Generative_Modeling_Using_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Max-sliced wasserstein distance and its use for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pyrros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01090</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Deshpande_Max-Sliced_Wasserstein_Distance_and_Its_Use_for_GANs_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10648" to="10656" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nice</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkpbnH9lx" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">; H M</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploratory projection pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journsl of American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">397</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">; Z</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>Ghahramani,</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FFJORD: free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJxgknCcK7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piecewise rational quadratic interpolation to monotonic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Delbourgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA Journal of Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining maximum likelihood and adversarial learning in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flow-Gan</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17409" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>McIlraith, S. A. and Weinberger, K. Q.</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3069" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C. ; I</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
	<note>Guyon</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Integral geometry and Radon transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Helgason</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How (not) to train your generative model: Scheduled sampling, likelihood, adversary?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05101</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk99zCeAb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00453</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<idno>De- cember 3-8</idno>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
	<note>Generative flow with invertible 1x1 convolutions</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Knop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Podolak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cramer-Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoencoder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09235</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The radon cumulative distribution transform and its application to image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="920" to="934" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sliced-wasserstein autoencoder: An embarrassingly simple generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01947</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized sliced wasserstein distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nadjahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Simsekli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K. ; H M</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Fox, E. B., and Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iterative gaussianization: from ica to random rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="549" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.425</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.425" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Simsekli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>St?ter</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/liutkus19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Are gans created equal? A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Sliced Iterative Normalizing Flows Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Cesa-Bianchi, N., and Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale optimal transport map estimation using projection pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Wallach, H. M., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E. B., and Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8116" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gaussianization flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v108/meng20b.html" />
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics</title>
		<editor>Chiappa, S. and Calandra, R.</editor>
		<meeting><address><addrLine>Palermo, Sicily, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-28" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4336" to="4345" />
		</imprint>
	</monogr>
	<note type="report_type">Online</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Statistical and topological properties of sliced probability divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nadjahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shahrampour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Simsekli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>G?r?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1xwNhCcYm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07367</idno>
		<title level="m">Distributional sliced-wasserstein and applications to generative modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improving relational regularized autoencoders with spherical sliced fused gromov wasserstein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01787</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">; I</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
	<note>Guyon,</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/parmar18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<editor>Dy, J. G. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsm?ssan, Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
	<note type="report_type">Sweden</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Subspace robust wasserstein distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/paty19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5072" to="5081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Automated colour grading using colour distribution transfer. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="123" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06434" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Wallach, H. M., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E. B., and Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="14680" to="14691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/rezende15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F. R. and Blei, D. M.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/rezende14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Orthogonal estimation of wasserstein distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v89/rowland19a.html" />
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<editor>Chaudhuri, K. and Sugiyama, M.</editor>
		<meeting><address><addrLine>Naha, Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-18" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pix-elcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJrFC6ceg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Multivariate density estimation: theory, practice, and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Notes on optimization on stiefel manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Tagare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical report</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.01844" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Bengio, Y. and Le-Cun, Y.</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkL7n1-0b" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">; I</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
	<note>Guyon,</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Stabilizing generative adversarial network training: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiatrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00927</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sliced wasserstein generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00383</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Generative latent flow: A framework for non-adversarial image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10485</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Likelihood regret: An out-ofdistribution detection score for variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02977</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
