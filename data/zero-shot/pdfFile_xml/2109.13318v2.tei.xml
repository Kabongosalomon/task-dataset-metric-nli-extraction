<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Voskou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cyprus University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">P</forename><surname>Panousis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cyprus University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kosmopoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Patras</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country>New Jersey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Chatzis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cyprus University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automating sign language translation (SLT) is a challenging real-world application. Despite its societal importance, though, research progress in the field remains rather poor. Crucially, existing methods that yield viable performance necessitate the availability of laborious to obtain gloss sequence groundtruth. In this paper, we attenuate this need, by introducing an end-to-end SLT model that does not entail explicit use of glosses; the model only needs text groundtruth. This is in stark contrast to existing end-toend models that use gloss sequence groundtruth, either in the form of a modality that is recognized at an intermediate model stage, or in the form of a parallel output process, jointly trained with the SLT model. Our approach constitutes a Transformer network with a novel type of layers that combines: (i) local winner-takes-all (LWTA) layers with stochastic winner sampling, instead of conventional ReLU layers, (ii) stochastic weights with posterior distributions estimated via variational inference, and (iii) a weight compression technique at inference time that exploits estimated posterior variance to perform massive, almost lossless compression. We demonstrate that our approach can reach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark, but without making use of glosses for model training, and with a memory footprint reduced by more than 70%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Sign Languages (SLs) are the native languages of the Deaf and therefore they are the main communication means within the Deaf communities. The SLs are rich visual languages, that convey information through multiple modalities, which are of complementary nature. Specifically, SLs utilize both manual (hand shape, movement and * ai.voskou@edu.cut.ac.cy pose), as well as non-manual modalities (e.g., facial expressions, lip movements, head movements, shoulders and torso), to convey salient meanings <ref type="bibr" target="#b29">[30]</ref>.</p><p>Exploiting the latest advances in computer vision and machine learning to facilitate the communication of SLspeakers with SL non-speakers is an endeavor of high potential impact to the livelihoods of the Deaf. Automating the process of converting SL video to written language is the goal of SLT (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>). This has proven to be a hard task for computer vision algorithms, as a natural consequence of the syntax, of the complex entailed gestures, and of the multitude of concurrent modalities that are combined to convey a unique meaning.</p><p>Due to these challenges, the computer vision community has traditionally focused on recognizing sequences of sign glosses. These are natural language words that attempt to encode the meaning of SL signs, forming a minimal dictionary of indicative lexical items. Thus, the combination of glosses pertaining to some SL video does not constitute translation in natural language; yet, it can help a non-SL speaker get a feeling of what the SL speaker is talking about. The process of pinpointing glosses in SL videos is usually referred to as sign language recognition (SLR). This distinction is important, as the grammar and the structure of sign and spoken languages are very different. These differences are reflected in the outcome of SLR, whereby there is no simple way of associating recognized glosses to actual words/phrases in natural language. This renders SLR outcomes of limited usefulness in real-world applications.</p><p>In an effort to alleviate the limited usefulness of SLR while, at the same time, improving the translation quality of SLT systems, several researchers have recently considered methods that combine SLR with SLT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4]</ref>. Specifically, existing methods choose among two alternatives: (i) perform SLR and then translate the sequence of detected glosses into natural language (S2G2T); and (ii) train a multitask Deep learning model that jointly performs SLR and SLT, in a way that the representations learned in the intermediate layers are shared among tasks (S2(G+T)). In the most recent works in the field, this is effected by exploiting a state-of-the-art framework for sequential data modeling, namely the Transformer network <ref type="bibr" target="#b33">[34]</ref>.</p><p>Transformer networks <ref type="bibr" target="#b33">[34]</ref> currently constitute the stateof-the-art paradigm for sequential data modeling; this includes both sequence-to-sequence modeling tasks and (autoregressive) density modeling tasks. The main principle of Transformer networks, which sets them apart from all previous deep learning approaches for sequential data, consists in the use of a neural attention-based mechanism, dubbed selfattention; this captures (long) temporal dynamics within a modeled sequence. Specifically, self-attention is a dot product attention <ref type="bibr" target="#b20">[21]</ref> that draws all queries, keys and values from the same sequence. This way, self-attention is the key mechanism that allows for each position within a sequence to attend all the others; this enables capturing long-range dependencies in the data. In addition, it enables high-scale parallelization of computation, which previous approaches (with recurrent connections) cannot afford.</p><p>Existing Transformer network formulations are widely founded upon Dense layers with ReLU activation functions. However, several recent works have shown that, by using activation functions employing some sort of stochasticity in their operation, one can yield a considerable performance improvement, especially in hard machine learning tasks. In this context, <ref type="bibr" target="#b25">[26]</ref> yielded a considerable performance improvement, without increasing the number of trainable model parameters, by: (i) Replacing ReLU units with blocks of stochastically competing local winner-takesall (LWTA) linear units. Specifically, each layer is split into blocks of linear units. At each time, only one of the units within a block passes its activation output to the following layer; that is the winner unit. All the rest are zeroed out, thus passing zero values to the following layer. Winner selection is performed on the basis of a stochastic sampling procedure, whereby the greater the unit activation value the higher the probability of it being sampled as the winner. (ii) Performing an (approximate) Bayesian treatment of the layer parameters (connection weights), whereby the model infers a full variational posterior over layer weights, instead of simple point-estimates.</p><p>In this work, we draw inspiration from these advances, seeking an SLT approach that yields significantly improved SL translation accuracy. Our most important goal is to devise an end-to-end SLT modeling approach that completely obviates the need of using SLR groundtruth information (glosses) as part of the model pipeline; that is, either as an intermediate recognition step (S2G2T paradigm), or as a joint task used to facilitate optimization of the learned intermediate input representations (S2(G+T)). Achieving this goal may greatly facilitate progress in the field, since con-structing gloss sequences for large training data corpora is an extremely costly and time-consuming process. In addition, our goal is to contribute an SLT method with reduced memory requirements at inference time, as this is important for real-world applications of our technology.</p><p>To this end, we devise a novel formulation of Transformer networks, built of Dense layers that comprise the following innovative arguments: (i) LWTA dense layers with stochastic winner sampling, as opposed to conventional ReLU layers; (ii) stochastic connection weights, across the network, with Gaussian posteriors fitted under a variational Bayes rationale; and (iii) a trained network compression scheme, which exploits the estimated variance of the fitted variational posteriors of the layer weights. We employ this novel Transformer network paradigm to formulate an end-to-end SLT model which does not use gloss sequence groundtruth throughout its modeling pipeline. We demonstrate that the proposed method achieves comparable or better results than the state-of-the-art in the most prominent SLT benchmark, namely PHOENIX 2014T. At the same time, our devised model imposes a significantly lower memory footprint compared to the state-of-the-art.</p><p>The remainder of this paper is organized as follows: In Section 2, we briefly present the recent related work in the field of SLT and SLR, putting more emphasis on the latest advances that make use of Transformer networks. In Section 3, we present the proposed SLT method; we first introduce our novel modeling rationale; subsequently, we devise appropriate training and inference algorithms; then, we elaborate on the model compression process, which we eventually use to obtain a scalable, end-to-end trainable SLT model. In Section 4, we perform a thorough experimental evaluation of our proposed approach, combined with a deep ablation study. To this end, we use the PHOENIX 2014T dataset. Finally, in Section 5 we conclude this paper, summarizing our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>SLT has been widely treated as a recognition problem (see <ref type="bibr" target="#b12">[13]</ref> for a detailed list). Initial approaches sought to recognize individual and well-segmented signs, using discriminative or generative methods under a time-series classification framework; examples include hidden Markov models (HMMs), e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b17">18]</ref>, dynamic time warping, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, and conditional random fields, e.g., <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>. These methods involved hand-crafted features; more recently, deep learning methods offered some better representations, such as those stemming from CNNs, e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>This approach to SLT is, however, of limited real-world usefulness, as it yields a set of words with rather incoherent context structure, as opposed to a natural language outcome. Thus, SLT with continuous recognition is a far more realistic framework, but is also much more challeng-ing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>. The challenge is due to epenthesis (insertion of extra visual cues into signs), co-articulation (the ending of one sign affects the start of the next), and spontaneous sign production (which may include slang, special expressions, etc.). To address the problem, <ref type="bibr" target="#b13">[14]</ref> used a model comprising a CNN-LSTM network to generate features, which are then fed to HMMs that perform inference via a variant of the Viterbi algorithm. In a similar fashion, <ref type="bibr" target="#b6">[7]</ref> used a bi-directional LSTM fed with features from a CNN; <ref type="bibr" target="#b22">[23]</ref> used a 3D CNN combined with a penultimate connectionist temporal classification (CTC) layer <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b37">[38]</ref>, a network dubbed SMTC is proposed, which combines multiple cues from pose and image (hands, face, holistic) in multiple scales, fed to a CTC penultimate layer.</p><p>Despite this progress, these works are not capable of scaling to natural language dictionaries of large size. On the contrary, they are typically implemented on either: (i) small dictionaries of relevance to specific real-world scenarios; or (ii) a set of natural language words that attempt to encode the meaning of SL signs in a succinct manner, thus forming a minimal dictionary of indicative lexical items (glosses). Indeed, recognition of glosses is often utilized so as to break the SLT task into two separate tasks of translating signs-toglosses and, then, glosses-to-text.</p><p>These shortcomings have been greatly ameliorated by utilizing Transformer networks. Transformers allow for scaling SLT to real-world natural language dictionaries, while also dramatically increasing the obtained translation performance. This is even more profound when combining SLT with an SLR process, either as an intermediate task, or even in the context of a multitask learning scheme. More specifically, in <ref type="bibr" target="#b4">[5]</ref>, the authors use a Transformer network to perform translation in an end-to-end fashion. In essence, they propose an S2(G+T) architecture: They postulate a Transformer network to perform S2T; in parallel, they use the encoder part of the Transformer to predict the respective gloss sequence groundtruth. The latter SLR task is performed via a penultimate CTC layer over all possible gloss alignments. Training is performed jointly for the whole structure (both tasks). This way, <ref type="bibr" target="#b4">[5]</ref> managed to achieve the then highest BLEU-4 score reported on PHOENIX 2014T, equal to 21.80. In addition, the authors also show that using only the end-to-end trainable Transformer network (with no use of gloss sequence groundtruth), they can obtain an SLT BLEU-4 score of 20.17 on PHOENIX 2014T.</p><p>This important breakthrough has spurred fresh research interest in the field, with many recent works building upon and extending this framework. For instance, <ref type="bibr" target="#b3">[4]</ref> propose to split the visual signal into three different streams: manual, face and body pose. On this basis, they devise a Transformer network with a novel multi-channel attention mechanism, to process the multistream signal. This yielded endto-end SLT BLEU-4 scores of up to 21.32 on PHOENIX 2014T (without use of glosses). Analogous advances have also been reported on hybrid approaches. For instance, <ref type="bibr" target="#b36">[37]</ref> propose an S2G2T hybrid whereby Spatial-Temporal Multi-Cue (STMC) networks <ref type="bibr" target="#b37">[38]</ref> are used for gloss recognition; these subsequently feed the recognized gloss sequences to a 2-layered Transformer. This S2G2T network achieves a BLUE-4 score of 24.00; a score of 25.40 is obtained by using an ensemble of such networks.</p><p>At this point, it is important to note that Transformerbased networks which utilize gloss sequence groundtruth currently yield the best reported BLEU-4 scores. The availability of gloss sequences may also be useful for system explainability, but it comes with significant costs: Training in the case of such models entails segmentation/alignment of glosses (via Viterbi decoding, a CTC layer, or similar methods). This, in turns, requires the availability of the possible gloss sequences to be aligned. The alignment process itself incurs additional computations, which are meaningful when addressing SLR, but not necessarily in the case of SLT. Most importantly, the groundtruth of possible gloss sequences is not trivially obtainable; this is especially the case with realistic unconstrained scenarios, which may involve large vocabularies and complex syntax.</p><p>In all cases, the attention mechanisms are implemented as the multi-headed variant of dot-product attention. That is, considering a set of keys K, queries Q, and values V , attention computes a linear transformation of the form</p><formula xml:id="formula_0">head = softmax KW k QW q ? d V W v<label>(2)</label></formula><p>where d is the dimensionality of the input, and W ? are trainable parameter sets. Rule <ref type="formula" target="#formula_0">(2)</ref> is applied multiple times (as many as the number of heads), with different parameters sets each time. The outcomes are eventually linearly combined to generate the final multihead attention layer output:</p><formula xml:id="formula_1">M ultiHead = Concatenate(head 1 , ...., head i )W m (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stochastic Transformers with linear competing units</head><p>Let us denote as x ? R J an input representation vector fed to some dense ReLU layer of a Transformer network, comprising J features. This layer is presented with a linear combination of the inputs, obtained via a weights matrix W ? R J?K , and produces an output vector y ? R K , which is fed to the subsequent layer. In our approach, this mechanism is replaced by the introduction of LWTA blocks, each containing a set of competing linear units. The layer input is originally presented to each block, via different weights for each unit; thus, the weights of the connections are now organized into a three-dimensional matrix W ? R J?K?U , where K denotes the number of blocks and U is the number of competing units therein.</p><p>Under our approach, within each block these linear units compute their activations; for the uth unit in the kth block, we obtain the sum J j=1 (w j,k,u ) ? x j . Then, the block selects one winner unit on the basis of a competitive random sampling procedure (described next), and sets the rest to zero. This way, we yield a sparse layer output, encoded into the vectors y ? R K?U that are fed to the next layer.</p><p>In the following, we represent the outcome of local competition between the units in each block via the discrete latent vectors ? ? one hot(U) K , where one hot(U) is an one-hot vector with U components. These denote the winning unit out of the U competitors in each of the K blocks of a proposed layer, when presented with some input. Using this notation, the output reads</p><formula xml:id="formula_2">[y] k,u = [?] k,u J j=1 (w j,k,u ) ? x j ? R<label>(4)</label></formula><p>where we denote as [h] l the lth component of a vector h. As we observe, at each time, only one (linear) unit in each LWTA block passes its output to the next layer, while the rest are zeroed out.</p><p>Let us now examine the statistical properties of the latent indicator vector ?. To enable data-driven competition between the units within an LWTA block, we postulate that the probability of a unit being sampled as the winner increases with the value of its (linear) output. In other words, we consider sampling from a Discrete posterior to select the winner at each time. On the basis of this rationale, we postulate that, a posteriori, it holds</p><formula xml:id="formula_3">q([?] k ) = Discrete [?] k softmax J j=1 [w j,k,u ] U u=1 ? x j (5) where [w j,k,u ] U u=1</formula><p>denotes the vector concatenation of the set {w j,k,u } U u=1 . On this basis, we obtain a novel variant of Transformer networks, the main operating principles of which are depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. We observe that the proposed network entails statistical inference arguments, which bring to the fore stochastic activation principles. Drawing from this inspiration, we proceed to derive a full Bayesian treatment of the obtained network, by also considering that the network parameters themselves are governed by statistical principles. Specifically, we postulate that, throughout the network, all trainable weights are random variables; their (posterior) distributions can be estimated in data-driven fashion. For simplicity, we seek to derive (approximate) independent Gaussian posteriors over the set of trainable weights, w:</p><formula xml:id="formula_4">q(w) = N (w|?, diag(? 2 ))<label>(6)</label></formula><p>where ? is the mean and ? 2 is the variance of the Gaussians. This concludes the formulation of the proposed Stochastic Transformer networks with competing linear units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and inference algorithms</head><p>To train the proposed model, we resort to maximization of the resulting evidence lower-bound (ELBO) of the model. To this end, we need to introduce appropriate prior assumptions regarding the distributions of the winner indicator latent variables, ? on each LWTA layer, as well as the trainable weights, w, throughout the network. For convenience, we postulate a priori spherical Gaussian weights of the form p(w) = N (0, I), and a symmetric Discrete prior over the winners: [? n ] k ? Discrete(1/U ).</p><p>Introducing a mean-field (posterior independence) assumption across layers, we yield the following ELBO:</p><formula xml:id="formula_5">L(?) = E q(?) log p(D|{w, ?}) ? KL q({?}) || p({?}) ?KL q({w}) || p({w})<label>(7)</label></formula><p>where ? = {?, ? 2 } is the set of the means and variances of the Gaussian weight posteriors, trained through-  <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11]</ref> for the latent winner indicator variables of the LWTA layers, ?. On this basis, ELBO maximization is performed using standard off-the-shelf, stochastic gradient techniques; specifically, we adopt Adam <ref type="bibr" target="#b11">[12]</ref>. We provide the analytical expression of the ELBO <ref type="bibr" target="#b6">(7)</ref> in the Supplementary.</p><p>Let us now turn to the inference algorithm of our network. At inference time, we directly draw samples from the trained posteriors of the winner selection latent variables, ?, of the LWTA layers, as well as the trained weight posteriors, w, throughout the network. Thus, differently from previous work in the field, the proposed Transformer networks are characterized by a doubly stochastic nature, stemming from two different sampling processes. On the one hand, we implement a data-driven random sampling procedure (by sampling from q(?)) to determine the activations of Dense layers in the network (LWTA layers). In addition, we infer the weight values, throughout the network, again based on sampling from the trained posteriors q(w) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A compression scheme</head><p>According to the current standards <ref type="bibr" target="#b31">[32]</ref>, computers represent real numbers by a set of bits divided into 3 different subsets: a single sign bit, a set of eb exponent bits, and a set of pb significant precision bits. Then, the stored value is expressed as a product of three factors:</p><formula xml:id="formula_6">value = (?1) sign * 2 E?2 eb ?1 * (1 + pb i b pb?i 2 ?i ) (8) where E = eb i=1 b i * 2 i?1 ,</formula><p>and b i is the ith bit. Therein, the second factor determines the maximum and minimum values that can be stored, and the third one determines floating point precision. Typical machine learning implementations (e.g., PyTorch <ref type="bibr" target="#b27">[28]</ref>) employ 8 exponent bits and 23 precision bits (float32 format). Yet, it is now wellestablished that a variational Bayesian treatment of deep network weights allows for significantly reducing the used bits without damaging accuracy <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Specifically, the obtained posterior variance of the network weights, ? 2 , constitutes a measure of uncertainty in their sampled values. The higher the associated uncertainty the more the fluctuation of their values. One can leverage this uncertainty information to assess which precision bits, out of the pb available, are significant, and remove the ones which fluctuate too much under approximate posterior sampling. In addition, combining posterior mean, ?, and variance information allows for estimating a confidence interval, that is an interval that sampled weight values may lie within with high probability. Using this information, we can also reduce the number of used exponent bits, eb.</p><p>In our work, we perform both these reductions on a layer-wise basis. To this end, we consider the minimum posterior variance ? 2 of the weights within a layer, as well as the minimum and maximum ? values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Proposed SLT model</head><p>In our SLT model, the whole Transformer network is trained from scratch. The input modality is a frame-wise feature sequence, obtained from the whole video frames. These frame-wise features stem from a pretrained Inception network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref>, in a fashion similar to <ref type="bibr" target="#b4">[5]</ref>. This input modality is initially fed to an LWTA layer; this yields spatial embeddings that we subsequently feed to the encoder of our proposed Transformer network, illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The output modality, generated from the decoder part of our network, is natural language interpretations. At each time, the decoder is presented with the previous word, which is initially fed to a vanilla linear embedding layer. The whole resulting model is trained in an end-to-end fashion, as described previously. We implement our method considering LWTA blocks of U = 4 units each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this Section, we perform the comparative assessment of our approach. To this end, we use PHOENIX 2014T dataset <ref type="bibr" target="#b2">[3]</ref>; this constitutes the most used benchmark in the recent literature. Hence, this benchmark selection allows for optimal and full comparability of our results with the recent related work in the field. The used dataset contains German SL videos of weather forecasts, and corresponding translations into the German spoken language. They are obtained from 9 different speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>All trained Transformers use embedding sizes of 512 and 8 attention heads. Weight posterior means and variances are initialized by means of Kaiming uniform initialization <ref type="bibr" target="#b9">[10]</ref>. Conventional models, for which we obtain point-estimates (as opposed to weight posteriors), are initialized by employing Xavier normal <ref type="bibr" target="#b7">[8]</ref>. Gumbel-Softmax temperature is set to T = 1.69 for training and T = 0.01 for inference. In all cases, we use Adam <ref type="bibr" target="#b11">[12]</ref> with a learning rate of 0.001 (? 1 = 0.9, ? 2 = 0.998), and a batch size of 32. During training, we evaluate the networks on the validation set every 80 iterations, and decrease the learning rate by 20% if the validation does not improve for 5 consecutive iterations. Training ends when the learning rate falls bellow a minimum of 0.0001. This evaluation process during network training is performed via greedy decoding. At inference time, evaluation on the test set is performed by means of Beam-Search; we perform several runs to determine optimal beam-size in all cases. Our main reference metric for assessing translation quality is the BLEU-4 score.</p><p>Our implementation is developed in Pytorch <ref type="bibr" target="#b27">[28]</ref>, and based on the "JOEYNMT" <ref type="bibr" target="#b16">[17]</ref>, "Sign language transformer" <ref type="bibr" target="#b4">[5]</ref>, "Bayesian Compression for Deep Learning" <ref type="bibr" target="#b19">[20]</ref>, and "nonparametric Bayesian local-winner-takes-all" <ref type="bibr" target="#b25">[26]</ref> frameworks. <ref type="table">Table 1</ref>: State-of-the-art BLEU-4 scores, as of late 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Test S2T <ref type="bibr" target="#b4">[5]</ref> 20.69 20.17 S2(G+T) <ref type="bibr" target="#b4">[5]</ref> 22.12 21.80 G2T <ref type="bibr" target="#b4">[5]</ref> 25.35 24.54 S2G2T-STMC <ref type="bibr" target="#b36">[37]</ref> 22.47 24.00 S2G2T-STMC ensemble <ref type="bibr" target="#b36">[37]</ref> 24.68 <ref type="bibr">25.40</ref> Before discussing our results, we first present some of the latest state-of-the-art methodologies on the considered benchmark, for further reference. <ref type="table">Table 1</ref> summarises the BLEU-4 scores of those models. The first state-of-the-art model we consider in our experimental evaluations is the sign-to-text transformer (S2T) <ref type="bibr" target="#b4">[5]</ref>. Our SLT method presented in this paper largely extends upon this method; thus, we consider this approach as our Baseline.</p><p>In addition, we consider three further Transformer-based models, namely a gloss-to-text (G2T) <ref type="bibr" target="#b4">[5]</ref>, a sign-to-glossand-text (S2(G+T)) <ref type="bibr" target="#b4">[5]</ref>, and a sign-to-gloss-to-text (S2G2T) <ref type="bibr" target="#b36">[37]</ref> model. These methods obtain higher BLEU scores than the basic S2T; the last one actually yields the highest performance reported to-date in the considered benchmark. However, as mentioned in section 2, these networks require the possible gloss sequences which may be hard to obtain for large training datasets. Specifically, S2(G+T) takes advantage of glosses as a parallel task that facilitates the encoder to obtain better representations; S2G2T utilizes them as an intermediate step, while G2T uses gloss input to obtain natural language (this renders it the least relevant to a realworld SLT task, as it assumes availability of a system that allows for perfect gloss recognition). Additionally, we emphasize that S2G2T employs the computationally burdensome STMC 3-channel recognition network <ref type="bibr" target="#b37">[38]</ref>, while we process the whole frame as a single channel.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance results</head><p>In <ref type="table" target="#tab_0">Table 2</ref>, we summarize the performance of our model for network configurations of varying depth. In our setup, an encoder (decoder) of depth H means a module comprising H consecutive submodules of the form depicted on the left (right) hand side of <ref type="figure" target="#fig_0">Fig. 1(a)</ref>.</p><p>Comparing the best performance reported therein with the summary of state-of-the-art results in <ref type="table">Table 1</ref>, we observe that our method outperforms the corresponding S2T baseline approach by 3.48 BLEU-4 scores on the test set. In this context, the best configuration under the proposed modeling approach seems to be the (2-2); this achieves BLEU-4 scores as high as <ref type="bibr" target="#b22">23</ref>.65 on the test set. This performance is superior to the S2(G+T) hybrid network as well, which yields 21.80 BLEU-4 on the test set. This outcome becomes even more prominent if we consider that S2(G+T) imposes much higher computational burden, and most importantly, it requires the possible sequences of glosses as groundtruth.</p><p>Subsequently, we examine network compression. By employing the layerwise compression scheme outlined in Section 3.4, we manage to reduce the average required bits for storing network parameters from 32 to less than 10. This fact implies a memory usage of around 30% that of the baseline SLT Transformer network of <ref type="bibr" target="#b4">[5]</ref>. In <ref type="table" target="#tab_1">Table 3</ref>, we present the average required number of bits throughout the layers of our network. In addition, we show how the compressed network performs in terms of the obtained BLEU-4 scores. These scores are obtained by compressing network parameters, and then re-running inference. Our results show that our compressed network incurs a negligible trade-off in translation accuracy, for massively lower memory needs.</p><p>Finally, we turn to the S2G2T ensemble <ref type="bibr" target="#b36">[37]</ref>, which still performs better than our approach, yielding a BLEU-4 score of 25.40 (c.f., <ref type="table">Table 1</ref>). The key element that renders S2G2T ensembles so potent is the utilization of ensemble decoding. This consists in averaging the predictions of different networks, in order improve the eventual translation quality. Thus, it is worth to examine whether an ensembling scheme can also improve the BLEU-4 scores of our method. To this end, we repeat our experiments with the (2-2)-version, training 10 different network instances with different random seeds. We use the best performing L = 4 or L = 8 of the so-obtained 10 networks to perform ensemble-decoding.</p><p>In <ref type="table" target="#tab_2">Table 4</ref>, we present the obtained BLEU-4 scores. With L = 8, our approach yields a BLEU-4 score of 25.59; this is the best BLEU-4 score ever reported in the literature on the considered dataset. We emphasize that we obtain this performance without making use of any predefined gloss sequences that need alignment in the Transformer network pipeline, contrary to <ref type="bibr" target="#b36">[37]</ref>. Then, we repeat our ensembledecoding experiment using the technique of Section 3.4 to perform parameter compression. We obtain a memory footprint reduction similar to the second line of <ref type="table" target="#tab_1">Table 3</ref>. As we show in <ref type="table" target="#tab_2">Table 4</ref>, for a memory footprint reduced by approximately 70%, our method remains competitive with <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study 4.4.1 How ReLU activations would perform?</head><p>We now scrutinize the proposed stochastic competitionbased activation functions. Specifically, we re-implement our method using ReLU and other popular activation functions in place of the proposed LWTA layers. We continue, though, to perform a full variational Bayesian treatment of the model, by inferring Gaussian weight posteriors. Since the (2-2)-version of the proposed network was shown to be the most accurate, all the following experiments focus on this configuration. <ref type="table" target="#tab_3">Table 5</ref> illustrates the so-obtained experimental outcomes. It is clear that the proposed LWTA activations with 4 units in each block constitute the approach with the best overall performance; in particular, it yields an advantage of more than 1 BLEU-4 units over the commonly used ReLU and the other conventional activation functions. Further, we also examine how our approach performs if we use a different number of competing units (U ) per block. <ref type="table" target="#tab_3">Table 5</ref> makes apparent that for both U = 2 and U = 8 LWTA still yields better scores than ReLU, but larger blocks seem to decrease the performance. Finally, we perform network compression following the rationale of Section 3.4, and repeat our experiments. As shown in <ref type="table" target="#tab_3">Table 5</ref>, ReLU continues to yield approximately 1.0 BLEU-4 units less than our approach (with U = 4); this corroborates the superiority of the proposed activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Does the variational Bayesian treatment of network weights contribute to SLT accuracy?</head><p>Conversely to the experiments of the previous Section, it is also important to examine whether training full variational posteriors over the network weights does actually offer tangible gains in terms of translation accuracy. To this end, we re-implement our method, making full utilization of the proposed stochastic LWTA activations, but obtaining conventional point-estimates over the network weights. Thus, the set of network weights, w, becomes now a parameters set that we optimize during training. Specifically, network training now reduces to maximization of the following ELBO expression:</p><formula xml:id="formula_7">L(w) = E q(?) log p(D|{?}) ? KL q({?}) || p({?})<label>(9)</label></formula><p>In <ref type="table" target="#tab_4">Table 6</ref>, we provide our results, again considering the (2-2)-version of our method, which constitutes its bestperforming configuration. Our findings show that, even with point-estimates, we manage to score 2 BLEU-4 units above the S2T Baseline. This outcome is clearly inferior to our full-fledged model. Therefore, we deduce that the variational Bayesian treatment of connection weights, throughout the proposed network, offers important SLT accuracy gains. This comes in addition to allowing for massive memory savings, by following the rationale of Subection 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative investigation</head><p>From a qualitative perspective, our translations seem to be of acceptable quality <ref type="table" target="#tab_5">(Table 7)</ref>. There is a small number of syntactic and grammar errors; most of them are about locations and dates. Moreover, while in many cases   R: im s?den schwacher wind S: der wind weht meist nur schwach E: der wind weht im s?den schwach bis m??ig R: am freitag insgesamt viele wolken die regen bringen S: am donnerstag viele wolken hier und da schauer E: am freitag gibt es viele wolken und gebietsweise schauer R: ganz?hnliche temperaturen wie heute zwischen sechs und elf grad S: am bodensee heute nacht nur sechs bis elf grad E:?hnliches wetter heute nacht R: im westen und nordwesten fallen einzelne schauer . S: im westen und nordwesten gibt es im westen hier und da schauer . E: im westen und nordwesten gibt es im westen einige schauer .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed an SLT method with the following advantages: (i) no requirement of glossing sequences for training; (ii) state-of-the-art BLEU-4 score on PHOENIX 2014T, competing with methods that require possible gloss sequences and/or multiple streams; and (iii) at least 70% less memory requirements than the state-of-the-art. We achieved this by devising a Transformer network that: (i) replaces ReLU layers with stochastically competing linear units; and (ii) performs variational Bayesian inference over all connection weights, throughout the network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed Approach. (a) The Proposed Transformer network for end-to-end SLT. (b) A graphical illustration of the proposed LWTA layers. Rectangles depict LWTA blocks, while circles therein represent competing linear units. The winner units are denoted with bold contours (? = 1). All edges correspond to Gaussian-distributed weights. out the network in an end-to-end fashion. In this expression, E q(?) log p(D|w, ?) corresponds to the (negative) posterior expectation over the standard categorical crossentropy error, used for training conventional Transformer networks. All the posterior expectations in the ELBO are computed by drawing Monte-Carlo (MC) samples under (i) the standard reparameterization trick for the postulated Gaussian weights, w; and (ii) the Gumbel-Softmax relaxation trick</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Proposed Approach: BLEU scores for varying depths.</figDesc><table><row><cell>Depth</cell><cell></cell><cell>Dev</cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>encoder-decoder</cell><cell cols="8">BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-1 BLEU-2 BLEU-3 BLEU-4</cell></row><row><cell>1 -1</cell><cell>48.67</cell><cell>35.34</cell><cell>27.3</cell><cell>22.03</cell><cell>47.47</cell><cell>34.75</cell><cell>26.8</cell><cell>21.85</cell></row><row><cell>2 -2</cell><cell>49.12</cell><cell>36.29</cell><cell>28.34</cell><cell>23.23</cell><cell>48.61</cell><cell>35.97</cell><cell>28.37</cell><cell>23.65</cell></row><row><cell>3 -3</cell><cell>45.68</cell><cell>32.87</cell><cell>25.72</cell><cell>21.66</cell><cell>45.84</cell><cell>33.40</cell><cell>25.72</cell><cell>21.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Network compression as per Section 3.4: effect on memory requirements and translation quality.</figDesc><table><row><cell>Depth</cell><cell>Average Required</cell><cell>Memory</cell><cell>Dev</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell>encoder-decoder</cell><cell>Bits</cell><cell cols="5">Reduction BLEU-4 change BLEU-4 change</cell></row><row><cell>1 -1</cell><cell>9.4</cell><cell>70.6%</cell><cell>21.66</cell><cell>-1.6%</cell><cell>22.05</cell><cell>+0.9%</cell></row><row><cell>2 -2</cell><cell>8.8</cell><cell>72.3%</cell><cell>23.09</cell><cell>-0.6%</cell><cell>23.52</cell><cell>-0.5%</cell></row><row><cell>3 -3</cell><cell>8.7</cell><cell>73.0%</cell><cell>20.82</cell><cell>-3.8%</cell><cell>20.77</cell><cell>-2.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>BLEU-4 scores with Ensemble-Decoding.</figDesc><table><row><cell></cell><cell cols="2">32 bit</cell><cell cols="2">Reduced</cell></row><row><cell>L</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>4</cell><cell cols="2">24.02 24.84</cell><cell cols="2">24.23 24.52</cell></row><row><cell>8</cell><cell cols="4">24.88 25.59 24.52 25.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Activation function comparison (BLEU-4 scores). 23.23 23.65 23.09 23.52 LWTA -U = 8 22.28 22.96 22.35 22.72 LWTA -U = 16 22.32 22.52 22.00 22.34</figDesc><table><row><cell></cell><cell cols="2">32 bit</cell><cell cols="2">Reduced</cell></row><row><cell>Activation</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>ReLU</cell><cell cols="2">22.42 22.61</cell><cell cols="2">22.17 22.67</cell></row><row><cell>Elu</cell><cell cols="2">22.63 22.56</cell><cell cols="2">22.19 22.32</cell></row><row><cell>SiLU</cell><cell cols="2">22.73 22.33</cell><cell cols="2">22.23 21.99</cell></row><row><cell>LWTA -U = 2</cell><cell cols="2">22.99 22.82</cell><cell cols="2">23.12 22.37</cell></row><row><cell>LWTA -U = 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Comparison of variational Gaussian weights to point-estimates (BLEU-4 scores). .23 23.65 23.09 23.52 the predicted sentence is syntactically different from the groundtruth, the resulting meaning remains similar. C.f. the Supplementary for more examples and English translations.</figDesc><table><row><cell>Weights</cell><cell cols="2">32 bit</cell><cell cols="2">Reduced</cell></row><row><cell>Type</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Point-Estimates</cell><cell cols="2">22.54 22.34</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Variational Gaussian 23</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Reference (R), single model (S), and ensemble (E).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Proposed Approach 3.1. Conventional Transformer networks Before we introduce our proposed approach, we revisit the main principles of Transformer networks. Transformers comprise an encoder module and a decoder module. The encoder is presented with the input sequence, after application of positional encoding (PE), according to the rule PE (pos,2i) = sin( pos 10000 2i d ), PE (pos,2i+1) = cos( pos 10000 2i d )(1) where pos is a position in the sequence, i is an index, and d the total size of the encoding. Then, it learns to extract a higher-level representation that entails salient temporal dynamics that may unfold over long horizons. To this end, the encoder module is built of a stack of self-attention layers, each of which is paired with two immediately succeeding Dense layers, one with ReLU units and a linear one.On the other hand, the decoder module is presented with the so-obtained input sequence encoding, and learns to generate the corresponding output sequence. In this context, the decoder module capitalizes upon (possibly multiple) encoder-decoder attention layers; this allows for capturing the salient correlations between input and output sequence patterns in a continuous manner. These attention layers are interleaved by preceding, decoder-side, self-attention layers, and succeeding pairs of Dense layers, the former with ReLU units and the latter with linear units.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In detail, inference is performed by sampling the q(?) and q(w) posteriors a total of S = 4 times, and averaging the corresponding S = 4 sets of output logits (Bayesian averaging).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified framework for gesture recognition and spatiotemporal gesture segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1685" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding vision-based continuous sign language recognition. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neena</forename><surname>Aloysius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geetha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="22177" to="22209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bowden. Neural sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-channel transformers for multiarticulatory sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Necati Cihan Camg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<editor>Adrien Bartoli and Andrea Fusiello</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12538</biblScope>
			<biblScope unit="page" from="301" to="319" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sign language transformers: Joint endto-end sign language recognition and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Necati Cihan Camg?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10020" to="10030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust sequential data modeling using an outlier tolerant hidden markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sotirios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><forename type="middle">I</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><forename type="middle">A</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varvarigou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1657" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for continuous sign language recognition by staged optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Quantitative survey of the state of the art in sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09918v2</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs.cv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised learning with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2306" to="2320" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="108" to="125" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Pose Gesture</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Re-sign: Re-aligned endto-end sequence modelling with deep recurrent cnn-hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3416" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joey NMT: A minimalist NMT toolkit for novices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmijn</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sign language recognition using kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ra?l</forename><surname>Rojas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Soft Computing</title>
		<editor>Leszek Rutkowski, Marcin Korytkowski, Rafa? Scherer, Ryszard Tadeusiewicz, Lotfi A. Zadeh, and Jacek M. Zurada</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="394" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sign language recognition by combining statistical dtw and independent classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jeroen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><forename type="middle">A</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel J T</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008-11" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2040" to="2046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bayesian compression for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4207" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Moddrop: Adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1692" to="1706" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural sign language translation by learning tokenization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="222" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian deep networks with local competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Panousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergios</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4980" to="4988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational bayesian sequence-to-sequence networks for memoryefficient sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Partaourides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Voskou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="251" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sign language recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 Workshops</title>
		<editor>Lourdes Agapito, Michael M. Bronstein, and Carsten Rother</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="572" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Quer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Cecchetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caterina</forename><surname>Donati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Geraci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meltem</forename><surname>Kelepir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Steinbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01" />
			<publisher>De Gruyter Mouton</publisher>
		</imprint>
	</monogr>
	<note type="report_type">editors. SignGram Blueprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting coarticulation in sign language using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiduo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ieee standard for floating-point arithmetic</title>
	</analytic>
	<monogr>
		<title level="j">IEEE computer society</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>More Sites</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Handshapes and movements: Multiple-channel american sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Vogler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gesture-Based Communication in Human-Computer Interaction</title>
		<editor>Antonio Camurri and Gualtiero Volpe</editor>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="247" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust sign language recognition with hierarchical conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, International Conference on</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="2202" to="2205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Better sign language translation with STMC-transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="5975" to="5989" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial-temporal multi-cue network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13009" to="13016" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
