<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Git: Clustering Based on Graph of Intensity Topology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Gao</surname></persName>
							<email>gaozhangyang@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Research and Innovation Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Lin</surname></persName>
							<email>linhaitao@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Research and Innovation Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tan</surname></persName>
							<email>tancheng@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Research and Innovation Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
							<email>wulirong@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Research and Innovation Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>stan.zq.li@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Research and Innovation Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Git: Clustering Based on Graph of Intensity Topology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accuracy, Robustness to noises and scales, Interpretability, Speed, and Easy to use (ARISE) are crucial requirements of a good clustering algorithm. However, achieving these goals simultaneously is challenging, and most advanced approaches only focus on parts of them. Towards an overall consideration of these aspects, we propose a novel clustering algorithm, namely GIT (Clustering Based on Graph of Intensity Topology). GIT considers both local and global data structures: firstly forming local clusters based on intensity peaks of samples, and then estimating the global topological graph (topo-graph) between these local clusters. We use the Wasserstein Distance between the predicted and prior class proportions to automatically cut noisy edges in the topo-graph and merge connected local clusters as final clusters. Then, we compare GIT with seven competing algorithms on five synthetic datasets and nine real-world datasets. With fast local cluster detection, robust topo-graph construction and accurate edge-cutting, GIT shows attractive ARISE performance and significantly exceeds other non-convex clustering methods. For example, GIT outperforms its counterparts about 10% (F1-score) on MNIST and FashionMNIST. Code is available at https://github.com/gaozhangyang/GIT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the continuous development of the past 90 years <ref type="bibr" target="#b11">(Driver and Kroeber 1932;</ref><ref type="bibr" target="#b42">Zubin 1938;</ref><ref type="bibr" target="#b36">Tryon 1939)</ref>, numerous clustering algorithms <ref type="bibr" target="#b22">(Jain, Murty, and Flynn 1999;</ref><ref type="bibr" target="#b34">Saxena et al. 2017;</ref><ref type="bibr" target="#b16">Gan, Ma, and Wu 2020)</ref> have promoted scientific progress in various fields, such as biology, social science and computer science. As to these approaches, Accuracy, Robustness, Interpretability, Speed, and Easy to use (ARISE) are crucial requirements for wide usage. However, most previous works only show their superiority in certain aspects while ignoring others, leading to sub-optimal solutions. How to boost the overall ARISE performance, especially the accuracy and robustness is the critical problem this paper try to address.</p><p>Existing clustering methods, e.g., center-based, spectralbased and density-based, cannot achieve satisfactory ARISE performance. Typical center-based methods such as k-means <ref type="bibr" target="#b35">(Steinhaus 1956;</ref><ref type="bibr" target="#b25">Lloyd 1982)</ref> and k-means++ <ref type="bibr" target="#b1">(Arthur and Vassilvitskii 2006;</ref><ref type="bibr" target="#b24">Lattanzi and Sohler 2019)</ref> are fast, conve-Preprint nient and interpretable, but the resulted clusters must be convex and depend on the initial state. In the non-convex case, elegant spectral clustering <ref type="bibr" target="#b10">(Dhillon, Guan, and Kulis 2004)</ref> finds clusters by minimizing the edge-cut between them with solid mathematical basis. However, it is challenging to calculate eigenvectors of the large and dense similarity matrix and handle noisy or multiscale data for spectral clustering <ref type="bibr" target="#b30">(Nadler and Galun 2006)</ref>. Moreover, the sensitivity of eigenvectors to the similarity matrix is not intuitive <ref type="bibr" target="#b29">(Meila 2016)</ref>, limiting its interpretability. A more explainable way to detect non-convex clusters is density-based method, which has recently attracted considerable attention. Density clustering relies on the following assumption: data points tend to form clusters in high-density areas, while noises tend to appear in low-density areas. For example, DBSCAN <ref type="bibr" target="#b13">(Ester et al. 1996)</ref> groups closely connected points into clusters and leaves outliers as noises, but it only provides flat labeling of samples; thus, HDBSCAN <ref type="bibr" target="#b2">(Campello, Moulavi, and Sander 2013;</ref><ref type="bibr" target="#b3">Campello et al. 2015;</ref><ref type="bibr" target="#b27">McInnes and Healy 2017)</ref> and <ref type="bibr">DPA (d'Errico et al. 2021</ref>) are proposed to identify hierarchical clusters. In practice, we find that DBSCAN, HDB-SCAN, and DPA usually treat overmuch valid points as outliers, resulting in the label missing issue 1 . In addition, Meanshift, ToMATo, FSFDP and their derivatives <ref type="bibr" target="#b6">(Comaniciu and Meer 2002;</ref><ref type="bibr" target="#b5">Chazal et al. 2013;</ref><ref type="bibr" target="#b33">Rodriguez and Laio 2014;</ref><ref type="bibr" target="#b15">Ezugwu et al. 2021</ref>) are other classic density clustering algorithms with sub-optimum accuracy. Recently, some delicate algorithms appear to improve the robustness to data scales or noises, such as RECOME <ref type="bibr" target="#b17">(Geng et al. 2018)</ref>, Quickshift++ <ref type="bibr" target="#b23">(Jiang, Jang, and Kpotufe 2018)</ref> and SpectACI <ref type="bibr" target="#b19">(Hess et al. 2019</ref>). However, the accuracy gain of these methods is limited, and the computational cost increases sharply on largescale datasets. Another promising direction is to combine deep learning with clustering <ref type="bibr" target="#b18">(Hershey et al. 2016;</ref><ref type="bibr" target="#b4">Caron et al. 2018;</ref><ref type="bibr" target="#b38">Wang, Le Roux, and Hershey 2018;</ref><ref type="bibr" target="#b41">Zhan et al. 2020)</ref>, but these methods are hard to use, time-consuming, and introduce the randomness of deep learning. In summary, as to clustering algotirhms, there is still a large room for improving ARISE performance.</p><p>To improve the overall ARISE performance, we propose a novel algorithm named GIT (Clustering Based on Graph of Intensity Topology), which contains two stages: finding lo-1 Most valid points are identified as noise without proper labels. <ref type="figure">Figure 1</ref>: The pipeline of GIT: after the intensity function in (b) has been estimated from the raw data (two rings) in (a), local clusters (c) are detected through the intensity growing process, seeing Section. 2.3. Due to the limited number of colors, different local clusters may share the same color. (d) shows that each local cluster is represented as a vertex in the topological graph, and edges between them are calculated. In (e), noisy edges are pruned and the real global structure (two rings) is revealed. Finally, the clustering task is finished by merging connected local lusters, as shown in (f). cal clusters, and merging them into final clusters. We detect locally high-density regions through an intensity function and collect internal points as local clusters. Unlike previous works, we take local clusters as basic units instead of sample points and further consider connectivities between them to consititude a topo-graph describing the global data structure. We point out that the key to improving the accuracy is cutting noisy edges in the topo-graph. Differ from thresholdbased edge-cutting, we introduce a knowledge-guidied algorithm to filter noisy edges by using prior class proportion, e.g., 1:0.5:0.1 for a dataset with three unbalanced classes. This algorithm enjoys two advantages. Firstly, it is relatively robust and easy-to-tune when only the number of classes is known, in which case we set the same sample number for all classes. Secondly, it is promising to solve the problem of unbalanced sample distribution given the actual proportion. Treat the balanced proportion as prior knowledge, there is only one parameter (k for kNN searching) need to be tuned in GIT, which is relative easy to use. We further study the robustness of various methods to data shapes, noises and scales, where GIT significantly outperform competitors. We also speed up GIT and its time complexity is O(d s n log(n)), where d s is the dimension of feature channels and n is the number of samples. Finally, we provide visual explanations of each step for GIT to show its interpretability.</p><p>In summary, we propose GIT to achieve better ARISE performance, considering both local and global data structures. Extensive experiments confirm this claim. The number of input feature dimension. r i The root (or intensity peak) of the i-th local cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(x)</head><p>The root of the local cluster containing x. R</p><p>The set of root points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(?, ?)</head><p>Similarity between local clusters. s l (?, ?) Similarity between samples based on local clusters. f (x)</p><p>The intensity of x L + ? The super-level set of the intensity function with threshold ?. C i (t)</p><p>The i-th local cluster at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation, Symbols and Pipeline</head><p>In <ref type="figure" target="#fig_1">Fig. 2</ref>, we illustrate the motivation that local clusters are point sets in high-intensity regions and they are connected by shared boundaries. And GIT aims to 1. determine local clusters via the intensity function;</p><p>2. construct the connectivity graph of local clusters;</p><p>3. cut noisy edges for reading out final clusters, each of which contains several connected local clusters.</p><p>The commonly used symbols and the overall pipeline are shown in <ref type="table" target="#tab_0">Table. 1</ref>   In (a), we estimate the intensity function f (x) from samples and partition data into local clusters {v1, v2, v3} along valleys. Points with the largest intensity are called roots of local clusters, such as r1, r2 and r3. (x1, x2) is a boundary pair connecting v1 and v2. (b) shows the topology graph containing local clusters (nodes) and their connectivity (edges), where e1,2 is stronger than e2,3. In (c), by cutting e2,3, we get final clusters Y1 = {v1, v2} and Y2 = {v3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intensity Function</head><p>To identify local clusters, non-parametric kernel density estimation (KDE) is employed to estimate the data distribution. However, the kernel-based <ref type="bibr" target="#b31">(Parzen 1962;</ref><ref type="bibr" target="#b8">Davis, Lii, and Politis 2011)</ref> or k-Nearest Neighborhood(KNN)-based <ref type="bibr" target="#b26">(Loftsgaarden, Quesenberry et al. 1965</ref>) KDE either suffers from global over-smoothing or local oscillatory <ref type="bibr" target="#b40">(Yip, Ding, and Chan 2006)</ref>, both of which are not suitable for GIT. Instead, we use a new intensity function f (x) for better empirical performance:</p><formula xml:id="formula_0">f (x) = 1 |N x | y?Nx e ?d(x,y) ; d(x, y) = s j=1 (x j ? y j ) 2 ? 2 j ,<label>(1)</label></formula><p>where N x is x's neighbors for intensity estimation and ? j is the standard deviation of the j-th dimension. With the consideration of ? j , d(?, ?) is robust to the absolute data scale. We estimate the intensity for each point in its neighborhood, avoiding the global over-smoothing. The exponent kernel is helpful to avoid local oscillatory, and we show that f (x) is Lipschitz continuous under a mild condition, seeing Appendix. 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Local Clusters</head><p>A local cluster is a set of points belonging to the same highintensity region. Once the intensity function is given, how to efficiently detect these local clusters is the key problem.</p><p>We introduce a fast algorithm that collects points within the same intensity peak by searching along the gradient direction of the intensity function (refer to <ref type="figure">Fig. 8</ref>, Appendix. 5.1). Although similar approaches have been proposed before <ref type="bibr" target="#b5">(Chazal et al. 2013;</ref><ref type="bibr" target="#b33">Rodriguez and Laio 2014)</ref>, we provide comprehensive time complexity analysis and take connected boundary pairs (introduced in Section. 2.4) of adjacent local clusters into considerations. To clarify this approach, we formally define the root, gradient flow and local clusters. Then we introduce a density growing process for an efficient algorithmic implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition. (root, gradient flow, local clusters)</head><p>Given intensity function f (x), the set of roots is R = {r|?f (r) = 0, |? 2 f (r)| &lt; 0, r ? X }, i.e., the local maximum. For any point x ? R ds , there is a gradient flow ? x : [0, 1] ? R ds , starting at ? x (0) = x and ending in ? x (1) = R(x), where R(x) ? R. A local cluster a set of points converging to the same root along the gradient flow, e.g., {x|R(x) ? R, x ? X }. To better understand, please see <ref type="figure">Fig. 8</ref> in Appendix. 5.1.</p><p>Intensity Growing Process. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, points with higher intensities appear earlier and local clusters grow as the time goes on. We formally define the intensity growing process via a series of super-level sets w.r.t. ?:</p><formula xml:id="formula_1">L + ? = {x|f (x) ? ?}.</formula><p>(2) We introduce a time variable t, 1 ? t ? n, such that ? t?1 ? ? t . At time t, the i-th local cluster is</p><formula xml:id="formula_2">C i (t) = {x|R(x) = r i , x ? L + ?t },<label>(3)</label></formula><p>where R(?) is the root function, r i is the i-th root. It is obvious that L + ?t?1 ? L + ?t and L + ?n = X . When t = n, we can get all local clusters C 1 (n), C 2 (n), . . .. Next, we introduce a recursive function to efficiently compute C i (t). Efficient Implementation. For each point x i , to determine whether it creates a new local cluster or belongs to existing one, we need to compute its intensity 2 and its parent point P a(x i ) along the gradient direction. We determine P a(x i ) as the neighbor which has maximum directional derivative from x i to P a(x i ), that is</p><formula xml:id="formula_3">P a(x i ) = arg max xp?L + f (x i ) ?Nx i f (x p ) ? f (x i ) ||x p ? x i || ,<label>(4)</label></formula><p>where N xi is the neighborhood system of x i . We sort and re-index all samples by their intensities. With a slight abuse of notation, the sorted {x 1 , x 2 , . . . ,</p><formula xml:id="formula_4">x n } satisfies f (x i ) ? f (x i+1 ) for all i. If ? t?1 , L + ?t?1 and C i (t ? 1) are known, we can get ? t , L + ?t and C i (t) following ? + t = f (x t ) L + ?t = L + ?t?1 + {x t },<label>(5)</label></formula><p>and</p><formula xml:id="formula_5">Ci(t) = ? ? ? ? ? {xt} i = nt?1, P a(xt) = xt Ci(t ? 1) + {xt} R(P a(xt)) = ri Ci(t ? 1) else,<label>(6)</label></formula><p>where i = 1, 2, . . . , n t?1 3 . Eq. 6 means that 1) if x i 's father is itself, x i is the peak point and creates a new local cluster {x i }, 2) if x i 's father shares the same root with C i (t?1), x i will be absorbed by C i (t ? 1) to generate C i (t), and 3) local clusters which is irrelevant to x i remain unchanged. We treat R(?) as a mapping table and update it on-the-fly. C i (t) is obviously recursive <ref type="bibr" target="#b7">(Davis 2013)</ref>. The time complexity of</p><formula xml:id="formula_6">computing C i (n) is O(n ? cost(P a)) 4 , where cost(P a) is O(kd s log n) for querying k neighbors. Note that the initial state is ? ? ? ? 0 = 1 L + ?0 = ? C i (0) = ?, i = 1.<label>(7)</label></formula><p>2 The intensity can be obtained by searching kNN and applying Eq. 1 with time complexity O(dsn log n) (using kd-tree).</p><p>3 nt?1 is the minimum index of empty local clusters at t ? 1:</p><formula xml:id="formula_7">|Ci(t ? 1)| &gt; 0, if i &lt; nt?1; |Ci(t ? 1)| = 0, if i ? nt?1 4 cost(P a)</formula><p>is complexity of computing function P a(?).</p><p>In summary, by applying Eq. 4-7, we can obtain local clusters C 1 (n), C 2 (n), . . . with time complexity O(d s n log n). For simplicity, we write C i (n) as C i in later sections. The detailed local cluster detection algorithm 1 can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Construct Topo-graph</head><p>Instead of using local clusters as final results <ref type="bibr" target="#b6">(Comaniciu and Meer 2002;</ref><ref type="bibr" target="#b5">Chazal et al. 2013;</ref><ref type="bibr" target="#b33">Rodriguez and Laio 2014;</ref><ref type="bibr" target="#b23">Jiang, Jang, and Kpotufe 2018)</ref>, we further consider connectivities between them for improving the results. For example, local clusters that belong to the same class usually have stronger connectivities. Meanwhile, there is a risk that noisy connectivities may damage performance. We manage local clusters and their connectivities with a graph, namely topo-graph, and the essential problem is cutting noisy edges.</p><p>Connectivity. According to UPGMA standing for unweighted pair group method using arithmetic averages <ref type="bibr" target="#b21">(Jain and Dubes 1988;</ref><ref type="bibr" target="#b16">Gan, Ma, and Wu 2020)</ref>, the similarity S(?, ?) between C i , C j ? C k can be obtained from the Lance-Williams formula:</p><formula xml:id="formula_8">S(C i , C j ? C k ) = |C j | |C j | + |C k | S(C i , C j ) + |C k | |C j | + |C k | S(C i , C k ),<label>(8)</label></formula><p>and</p><formula xml:id="formula_9">S(C i , C j ) = 1 |C i ||C j | x?Ci,y?Cj s l (x, y),<label>(9)</label></formula><p>where Eq. 9 can be derived from Eq. 8. We define the similarity between points as:</p><formula xml:id="formula_10">s l (x, y) = e ?||x?y|| x ? N y , y ? N x , R(x) = R(y) 0 else.</formula><p>(10) Note that s l (x, y) is non-zero only if x and y are mutual neighborhood and belong to different local clusters, in which case (x, y) is a boundary pair of adjacent local clusters, e.g., x 1 and x 2 in <ref type="figure" target="#fig_1">Fig. 2</ref>. Fortunately, all the boundary pairs among local clusters can be obtained from previous intensity growing process, without further computation.</p><p>Topo-graph. We construct the topo-graph as</p><formula xml:id="formula_11">G = (V, E), where v i is the i-th local cluster C i and e i,j = S(C i , C j ).</formula><p>In summary, we introduce a well-defined connectivity between local clusters using connected boundary pairs. The detailed topo-graph construction algorithm 3 can be found in the Appendix, whose time complexity is O(kn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Edges Cutting and Final Clusters</head><p>To capture global data structures, connected local clusters are merged as final clusters, as shown in <ref type="figure">Fig. 1</ref>(e, f). However, the original topo-graph is usually dense, indicating redundant (or noisy) edges which lead to trivial solutions, e.g., all the local clusters are connected, and there is one final cluster. In this case, cutting noisy edges is necessary and the simplest way is to set a threshold to filter weak edges or using spectral clustering. However, we find that these methods are either sensitive, hard to tune or inaccurate, which severely limits the widely usage of similar approaches. Is there a robust and accurate way to filter noisy edges using available prior knowledge?</p><p>Auto Edge Filtering. To make GIT more easy to use and accurate, we automatically filter edges with the help of pior class proportion. Firstly, we define a metric about final clusters by comparing the predicted and pior proportions, such that the higher the proportion score, the better the result (seeing the next paragraph). We sort edges with connectivity strengths from high to low, in which order we merge two end local clusters for each edge if this operation gets a higher score. In this process, edges that cannot increase the metric score are regarded as noisy edges and will be cutted.</p><p>Metric about Final Clusters. Let p = [p 1 , p 2 , . . . , p m ] and q = [q 1 , q 2 , . . . , q m ] be predicted and predefined class proportions, where</p><formula xml:id="formula_12">m i=1 p i = 1, m i q i = 1.</formula><p>We take the similarity between p and q as the aforementioned metric score. Because m and m may not be equal and the order of elements in p and q is random, it is not feasible to use conventional Lp-norm. Instead, we use Wasserstein Distance to measure the similarity and define the metric M(p, q) as:</p><formula xml:id="formula_13">M(p, q) = Exp(? min ?i,j ?0 i,j ? i,j ) s.t. j ? i,j = p i ; i ? i,j = q j ,<label>(11)</label></formula><p>where the original Wasserstein Distance is min ?i,j ?0 i,j ? i,j d i,j , ? i,j is the transpotation cost from i to j , and we set d i,j = 1. Because dim(p i ) = dim(q j ) = 1, Eq. 11 can be efficiently calculated by linear programming.</p><p>In summary, we introduce a new metric of class proportion and use it to guide the process of edge filtering, then merge connected local clusters as final clusters. Compared to threshold-based or spectral-based edge cutting, this algorithm considers available prior knowledge to reduce the difficulty of parameter tuning or provide higher accuracy. By default, we set the same proportion for each category if only the number of classes is known. Besides, it is promising for dealing with sample imbalances if we know the actual proportion, seeing Section. 3.1 for experimetal evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we conduct experiments on a series of synthetic, small-scale and large-scale datasets to study the Accuracy, Robustness to shapes, noises and scales, Speed and Easy to use, while the Interpretability of GIT has been demonstrated in previous sections. As an extension, we further investigate how PCA and AE (autoencoder) work with GIT to further improve the accuracy.</p><p>Baselines. GIT is compared to density-based methods, such as FSFDP <ref type="bibr" target="#b33">(Rodriguez and Laio 2014)</ref>, HDBSCAN (McInnes and Healy 2017), Quickshift++ (Jiang, Jang, and Kpotufe 2018), SpectACl <ref type="bibr" target="#b19">(Hess et al. 2019) and</ref><ref type="bibr">DPA (d'Errico et al. 2021)</ref>. Besides, classical Spectral Clustering and k-means++ <ref type="bibr">(Pedregosa et al. 2011) 5</ref> . Since there is little work comprehensively comparing recent clustering algorithms, our results can provide a reliable baseline for subsequent researches. For simplicity, we use these abbreviations: FSF (FSFDP), HDB (HDBSCAN), QSP (Quick-shiftPP), SA (SpectACI), DPA (DPA), SC (Spectral Clustering) and KM (k-means++).</p><p>Metrics. We report F1-score <ref type="bibr" target="#b14">(Evett and Spiehler 1999)</ref>, Adjusted Rand Index (ARI) <ref type="bibr" target="#b20">(Hubert and Arabie 1985)</ref> and Normalized Mutual Information (NMI) <ref type="bibr" target="#b37">(Vinh, Epps, and Bailey 2010)</ref> to measure the clustering results. F1-score evaluates both each class's accuracy and the bias of the model, ranging from 0 to 1. ARI is a measure of agreement between partitions, ranging from ?1 to 1, and if it is less than 0, the model does not work in the task. NMI is a normalization of the Mutual Information (MI) score, where the result is scaled to range of 0 (no mutual information) and 1 (perfect correlation). Besides, because HDBSCAN and DPA perfer to drop some valid points as noises, we also consider the fraction of samples assigned to clusters, namely cover rate. If the cover rate is less than 0.8, we will ignore this result and mark it in gray. The mathmatical formulas of these metrics can be found in the Appendix. 5.2. For each methods, we carefully tune the hyperparameters (which can be found in the open-source code), and report the best results.</p><p>Datasets. We evaluate algorithms on synthetic, smallscale and large-scale datasets. In <ref type="table">Table.</ref> 2, we count the number of samples, feature dimensions, number of classes and balance rate for real-world datasets. The balance rate is the sample ratio of the smallest calss to the largest class. All these datasets are available online (Asuncion and Newman 2007; Deng 2012; Xiao, Rasul, and Vollgraf 2017). Platform. The platform for our experiment is ubuntu 18.04, with a AMD Ryzen Threadripper 3970X 32-Core cpu and 256GB memory. We fairly compare all algorithms on this platform for research convenience.</p><p>5 Some classical algorithms are ignored because they usually perform worse than the latest ones, including OPTICS <ref type="bibr" target="#b0">(Ankerst et al. 1999)</ref>, DBSCAN <ref type="bibr" target="#b13">(Ester et al. 1996)</ref> and mean-shift <ref type="bibr" target="#b6">(Comaniciu and Meer 2002)</ref>. Few recent works are also overlooked due to the lack of open-source python code, such as RECOME <ref type="bibr" target="#b17">(Geng et al. 2018</ref>) and better k-means++ (Lattanzi and Sohler 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Accuracy</head><p>Objective and Setting. To study the accuracy of various algorithms, we: 1) firstly compare GIT with density-based FSF, HDB, QSP, SA, and DPA on small-scale datasets to determine their priority order, 2) and secondly choose the top-3 (F1-score) density-based algorithms and k-means++ as baselines to further compare with GIT on large-scale datasets. With the exception of Frogs and Codon, which are extremely unbalanced, all the class proportions are set to be the same in GIT, e.g. 1:1:1 for three classes.</p><p>Result and Analysis. Comparisons on small-scale and large-scale datasets are shown in <ref type="table" target="#tab_2">Table. 3 and Table.</ref> 4. We notice that GIT outperforms other approaches, with the top-3 ARI, top-2 NMI and top-1 F1-score in all cases. GIT also exceeds competitors up to 6% and 8% F1-score in the highly unbalanced case (Frogs and Condon) by specifying the actual prior class proportions. In addition, we find that the performance gains from GIT seem to be negatively correlated with the feature dimension, indicating the potential dimension curse. And we will introduce how to mitigate this problem through dimension reduction in Section. 3.5. Finally, the runtime of GIT is acceptable, especially on large scale datasets, where GIT is much faster than recent density-based clustering methods, such as HDB, QSP and SA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Easy to Use</head><p>We introduce some experience for parameter tuning and auxiliary tool for data structure analysis.</p><p>Parameters Settings. Apart from the prior class proportion, there is only one hyper-parameter k in GIT for kNN searching, which is usually less than 100. On large-scale datasets, we choose k from <ref type="bibr">[30,</ref><ref type="bibr">40,</ref><ref type="bibr">50,</ref><ref type="bibr">60,</ref><ref type="bibr">70,</ref><ref type="bibr">80,</ref><ref type="bibr">90,</ref><ref type="bibr">100]</ref>. When only the number of classes is known, simply setting the ratio of all classes to be the same can generally yield good results. We admit that GIT cannot determine the number of classes from scratch and leave it for future work. If the actual proportion is known, GIT can obtain better re- sults, even in highly unbalanced classes, seeing Frogs and Codon in <ref type="table">Table.</ref> 4.</p><p>Topo-graph Visualization. The topo-graph describes the global structure of the dataset, and GIT uses it to discover final clusters. We also provide API for users to visualize these topo-graphs to understand the inner structure better (d' <ref type="bibr">Errico et al. 2021</ref>). One of the example of topo-graph can be found in <ref type="figure">Fig. 1(e</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Speed</head><p>Objective and Setting. To study the scalability of GIT, we generate artificial data from the mixture of two Gaussian with class proportion 1:1. We examine how dimension (d s ), sample number (n) and hyper-parameter (k) affect the runtime, where n ? [10 4 , 10 6 ], d s ? [10, 10 3 ] and k ? [10, 10 2 ]. By default, d s = 10, n = 10 4 and k = 50.</p><p>Result and Analysis. The runtime impact of n,d s and k is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. According to previous analysis, the time complexity of GIT is O(d s n log n), which is indeed confirmed by the experimental results. Since k is limited within 100, seeing Section. 3.2, its effect on time complexity can be treated as a constant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Robustness</head><p>Objective and Setting. To study the robustness to shapes, noises and scales of various algorithms, we: 1) firstly compare all algorithms on synthetic datasets with complex shapes, such as Circles and Moons, and show the top-3 (F1score) results, 2) secondly compare GIT with top-3 competitors on Moons (with Gaussian noise) and Impossible (with Uniform noises), and 3) thirdly evaluate these methods under the mixed multi-scale data, e.g. a new dataset containing original cricles and enlarged circles in <ref type="figure" target="#fig_4">Fig. 6</ref>. We report F1scores on different noise or scaling levels in <ref type="figure">Fig. 5</ref>  Result and Analysis. For shapes, we evaluate all baselines but only visualize the top-3 results in <ref type="table">Table.</ref> 5. GIT is the best one to get the resonable clusters on all synthetic datasets and sup-optimum methods are SA, QSP, HDB and SC in turns. As expected, many baseline methods cannot deal with complex distributions (e.g., the Impossible dataset) for the lack of ability to identify global data structures, while GIT handles it well. For noises and scales, we compare GIT with SA and QSP in <ref type="table">Table.</ref> 6, from which we find that GIT is more robust against noise and data scales, whereas SA and QSP both fail. What's more, we plot the F1-scores of GIT, SA, QSP and HDB under different noise levels and scaling factors <ref type="figure">(Fig. 5)</ref>, where GIT achieve the highest F1-score with smallest variance on the extreme situations. We believe the robustness comes from two aspects: On the one hand, the well-designed intensity function helps handle the multiscale issue because the kNN is invariant to scales, as proved in the Appendix (Theorem 1). On the other hand, the local and global clustering mechanism is helpful to anti-noise. Firstly, the process of local clustering detection is reliable because noises usually have a limited effect on the relative point intensities. Secondly, connectivity strength between two local clusters is also robust because it depends on all the adjacent boundary points, not just a few noisy points. As both nodes and edges of the topo-graph are robust to noise, the clustering results are undoubtedly robust.</p><p>Data GIT SA QSP <ref type="table">Table 6</ref>: Comparing GIT with SA and QSP on noisy and multiscale datasets. The first column is the raw data and others are results of GIT, SA and QSP. We add 0.2 Gaussian noise on Circles, 0.1 Uniform noise on Impossible and mix two Circles as the multiscale data, where one Circles is 100 times larger than the other. <ref type="figure">Figure 5</ref>: Performance comparison under different noise levels and scales. In (a), we change the (Gaussian) noise on Moons from 0.02 to 0.26 with step 0.02, and plot the average F1-score and its variance over 10 random seeds. In (b), we show the F1-score for each scaling factor ranging from 1 to 100 on the mixed Circles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dimension Reduction + GIT</head><p>Objective and Setting. To alleviate the problem of dimension curse, dimension reduction methods are often used together with clustering algorithms, such as PCA and Autoencoder. We study how far GIT outperforms competitors under these settings. As to PCA, we directly use scikit-learn's API to project raw data into h-dimensional space. For the autoencoder, we construct MLP using PyTorch with the following structure: 784-512-256-128-h (enc) and h-128-256-512-784 (dec). We use Adam to optimize the autoencoder up to 100 epochs with the learning rate 0.001. In both of these settings, we project 60k samples (training set, dim=784) to h-dimensional space, where h ? {5, 10, 20, 30}. Finally, we use the same embeddings as the inputs of various clustering methods and report the F1-score.</p><p>Result and Analysis. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, both PCA and AE bring consistent improvement of F1-score for clustering methods excluding SA. It is worth pointing out that GIT consistently outperforms competitors on all settings, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. We further visualize the results of AE+MNSIT and AE+FMNIST (h=5) via UMAP <ref type="bibr" target="#b28">(McInnes, Healy, and Melville 2018)</ref>, and <ref type="figure" target="#fig_5">Fig. 7</ref> shows that GIT generates more reasonable clusters than other algorithms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel clustering algorithm GIT to achieve better Accuracy, Robustness to noises and scales, Interpretability with accaptable Speed and Easy to use (ARISE), considering both local and global data structures. Compared with previous works, the proper usage of global structure is the key to GIT's accuracy gain. Both the intensity-based local cluster detection and well-designed topo-graph connectivity make it robust. We believe that GIT will promote the development of cluster analysis in various scientific fields. </p><formula xml:id="formula_14">= {x i } k i=1 , N y = {y i } k i=1</formula><p>can be re-indexed to form sequences (x 1 , x 2 , . . . , x k ) and (y 1 , y 2 , . . . , y k ), satisfying sup 1?i?k |d(x i , y i )| ? d(x, y), we call that N xi ? N xj .</p><p>Theorem 1. f (x) is invariant to the scale of the data.</p><p>Proof 1. Denote s as the transform scale of dataset and x = sx, y = sy, we know that ? j = s? j for all 0 &lt; j &lt; s. According to Eq.1, d(x , y ) = s j=1</p><formula xml:id="formula_15">(x j ?y j ) 2 ? 2 j = s j=1 (s?xj ?s?yj ) 2 (s? j ) 2 = s j=1 (xj ?yj ) 2 ? 2 j = d(x, y). Thus d(x, y) is invariant to the data scale s =? N x is invariant to s. Finally, f (x) is invariant to s.</formula><p>Theorem 2. f (x) is Lipschitz continuous under the fol-</p><formula xml:id="formula_16">lowing condition: (a) if x i ? x j , then N xi ? N xj . Proof 2. Denote d ij = d(x i , x j ), d i (y) = d(x i , y) and |N xi | = |N xj | = k.</formula><p>The goal is to prove there are exit a constant c, such that |f (</p><formula xml:id="formula_17">x i ) ? f (x j )|/d ij ? c. In case 1, ? constant a &gt; 0, such that d ij ? a. Because |f (x i ) ? f (x j )| ? [0, 1] and a ? d ij , we know that 0 ? |f (x i ) ? f (x j )|/d ij ? 1/a = constant.</formula><p>In case 2, ?a &gt; 0, d ij &lt; a, which means x i ? x j . By the condition (a), we have N xi ? N xj . From Eq.1 (Intensity Function), we derive that |f (</p><formula xml:id="formula_18">x i ) ? f (x j )|/d ij = 1 kdij | y?Nx i e ?d(xi,y) ? z?Nx j e ?d(xj ,z) |.</formula><p>According to the definition 1, we can find two sequence (y 1 , y 2 , . . . , y k ) and (z 1 , z 2 , . . . , z k ), such that y t ? z t , and</p><formula xml:id="formula_19">k|f (xi) ? f (xj)| = | y?Nx i e ?d(x i ,y) ? z?Nx j e ?d(x j ,z) | (12) = | 1?t?k (e ?d(x i ,y t ) ? e ?d(x j ,z t ) )| (13) ? 1?t?k |e ?d(x i ,y t ) ? e ?d(x j ,z t ) | (14) &lt; 1?t?k |d(xj, zt) ? d(xi, y t )|<label>(15)</label></formula><p>where we use the definition 1 in (13), basic arithmetic in (14) and the property of the exponent function in (15). Using the triangle inequality in metric space (R ds , d) , we know that</p><formula xml:id="formula_20">d(xi, zt) ? d(y t , zt) ? d(xi, y t ) ? d(xi, zt) + d(y t , zt)<label>(16)</label></formula><formula xml:id="formula_21">hence k|f (xi) ? f (xj)| &lt; 1?t?k |d(xj, zt) ? d(xi, y t )| (17) ? 1?t?k (|d(xj, zt) ? d(xi, zt)| + d(y t , zt)) (18) ? 1?t?k d(xi, xj) + 1?t?k d(y t , zt) (19) ? k ? dij + k ? dij (20) = 2kdij<label>(21)</label></formula><p>Note that we apply (16) in <ref type="formula" target="#formula_0">(18)</ref>, the triangle inequality and Definition 1 in <ref type="bibr">(19)</ref>. Finally, we have |f (</p><formula xml:id="formula_22">x i )?f (x j )|/d ij &lt; 2.</formula><p>In summary, we can find a constant c = max{1/a, 2}, such that |f (x i ) ? f (x j )|/d ij ? c. <ref type="figure">Figure 8</ref>: Algorithm realization of intensity growth process. Several data points and the intensity contour are presented. As the intensity threshold gradually decreases from 1 to 0, points with higher intensity appear earlier. For a newborn point xi, if there is a neighbor xj with the greatest gradient starting at xi and f (xj) ? f (xi), we connect xj with xi, and call xj as the parent of xi.Otherwise, there is no valid parent for xi, and we treat xi as the root of a new local cluster. Finally, points sharing the same root form local clusters. The number of true positive samples of class i F P i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metrics</head><p>The number of false positive samples of class i F N i</p><p>The number of false negtive samples of class i F i</p><p>The F1-score of class i F score</p><p>The weighted F1-score overall classes C i</p><p>The i-th class n</p><p>The number of samples. i |C i | = n F1-score. The F1-score is a measure of a model's accuracy on a dataset. It is the harmonic mean of precision and recall for each class. In the multi-class case, the overall metric is the average F1 score of each class weighted by support (the number of true instances for each label). The higher F1score, the better the result. The mathematical definition can be found in Eq. 22.</p><formula xml:id="formula_23">? ? ? ? ? ? ? ? ? F score = 1 n i |C i | ? F i ; F i = 2 P reci?Reci P reci+Reci ; P rec i = T Pi T Pi+F Pi ; Rec i = T Pi T Pi+F Ni .<label>(22)</label></formula><p>ARI. The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings:</p><formula xml:id="formula_24">RI = T P + T N T P + T N + F P + F N ,<label>(23)</label></formula><p>where T P is the number of true positives, T N is the number of true negatives, F P is the number of false positives, and F N is the number of false negatives. The raw RI score is then "adjusted for chance" into the ARI score using the following scheme:</p><formula xml:id="formula_25">ARI = RI ? E[RI] max(RI) ? E(RI)</formula><p>.</p><p>The higher the ARI, the better the clusterings. By introducing a contingency <ref type="table">Table.</ref> 8, the original Adjusted Rand Index value is:</p><formula xml:id="formula_27">ARI = ij nij 2 ? [ i ai 2 j bj 2 ]/ n 2 [ i ai 2 + j bj 2 ]/2 ? [ i ai 2 j bj 2 ]/ n 2 .</formula><p>NMI. Normalized Mutual Information (NMI) is a normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation). Using notations in <ref type="table">Table.</ref> 8, NMI can be computed by <ref type="bibr" target="#b17">(Geng et al. 2018)</ref>: (25)  Note that N i is the set of neighborhoods' index of x i .</p><formula xml:id="formula_28">N M I = r i=1<label>s</label></formula><formula xml:id="formula_29">idx ? arg sort({?f (x i )} n?1 i=0 ) O(n log n) 9: while idx = ? do O(nk 2 ) 10: i ? idx[0], ? ? f (x i ) 11: J ? {j|j ? N i , f (x i ) ? ?} O(k) 12: j ? arg max j?J f (xj )?f (xi)</formula><p>Auxiliary functions. During the process of topo-graph pruning (Algorithm 3), we use the following auxiliary functions for efficiency concerns:</p><p>(1) Mapping local cluster index to final cluster index</p><formula xml:id="formula_30">C[k] = C[i] k ? S j C[k] else<label>(26)</label></formula><p>(2) Mapping final cluster index to local cluster index</p><formula xml:id="formula_31">M [k] = S k ? S M [k] else<label>(27)</label></formula><p>(3) Recording the number of samples for each final class: </p><formula xml:id="formula_32">N [k] = N [i] + N [j] k ? S N [k] else<label>(28)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sensitive analysis</head><p>Objective and Setting. As mentioned before, GIT has one hyperparameter k which needs to be tuned. To study how sensitive the results are to this hyperparameter, we change k from 30 to 100, and compare GIT with k-mean++ on MNIST and FMNIST.     Results and Analysis. Generally, we cannot ensure the results of GIT are robust to k. However, we can claim that GIT can significantly outperform the baseline method in the vast majority of cases. Thanks to this property, it is easy to select a proper parameter for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Potential questions</head><p>Q1: Do the authors use labels to tune hyperparameters? R1: Yes, but it is fair and reasonable for all algorithms. Concretely, we perform clustering from artificially specified hyperparameters and evaluate the F1-score using labels after clustering. For each algorithm, we repeat this process to find better results. We do this because we think it is not reasonable to report results derived from randomly selected hyperparameters for different algorithms with different hyperparameters. Thus, we use some labels to guide hyperparameter searching and fairly report the best results that we can find for each algorithm.</p><p>Q2: In <ref type="table">Table.</ref> 5, why different baselines are selected for different data sets?</p><p>R2: There is some misunderstanding here. We evaluated all baselines mentioned in this paper, but only visualize part of them in <ref type="table">Table.</ref> 5 due to page limitations. More specifically, we choose results with top-3 F1-score for visualization.</p><p>Q3: How do the authors treat 'uncovered' points in metrics computation?</p><p>R3: All the metrics are calculated based on 'covered' points without considering 'uncovered' points. If the cover rate is less than 80%, the corresponding results will be ignored and marked in gray.</p><p>Q4: How do the authors visualize results using UMAP in <ref type="figure">Figure.</ref> 7? R4: We use UMAP to project original data (dimension=5) into 2-dimensional space for visualization convenience. Then, we color each point with real label (ground truth) or a predicted label (generated from a clustering algorithm). Due to the information loss caused by dimension reduction, points in different classes may overlap.</p><p>Q5: Why not report averagy results under different random seeds?</p><p>R5: There are three cases:</p><p>? When adding noises to study the robustness, we report the average results due to the varying noises.</p><p>? As to accuracy, GIT is deterministic which means the accuracy does not fluctunate with the change of random seeds under the fixed hyperparameters. Thus, there is no need to report average results of different seeds.</p><p>? As to speed, the reported running time is close to the average based on our experimental experience. Since the running time of different algorithms varies greatly and random seed will not cause the change of the magnitude order, our results are sufficient to distinguish them.</p><p>Q6: Why different baselines are chosen in different experiments? Do authors prioritize favorable baselines?</p><p>R6: We don't prioritize favorable baselines because that would be cheating. We choose classical k-means++ and Spectral Clustering as they are widely used. By comparing GIT with them, readers can extend their understanding to a wider range of situations. We also choose recent HDB-SCAN (McInnes and Healy 2017), Quickshift++ <ref type="bibr" target="#b23">(Jiang, Jang, and Kpotufe 2018)</ref>, SpectACl <ref type="bibr" target="#b19">(Hess et al. 2019) and</ref><ref type="bibr">DPA (d'Errico et al. 2021)</ref> as SOTA methods. In each experiment, we have evaluated all baseline algorithms along with GIT. However, we cannot present all the results due to the page limitations, although we would like to. As a compromise, we present the results of the most representative and effective algorithms in each experiment. If you have carefully read this paper, you would discover that we:</p><p>? compare {DPA, FSF, HDB, QSP, SA} with GIT in Table. 3 (accuracy on small-scale datasets) ? select the most competitve SOTA methods {HDB,QSP,SA} for further comparison. ? compare {KM, SC} ? {HDB,QSP,SA} with GIT in <ref type="table">Table.</ref> 4 (accuracy on large-scale datasets) ? the most accurate classical algorithm is {KM} and the top-2 accurate SOTA algorithms are {QSP,SA}.</p><p>? compare {QSP,SA} with GIT in terms of robustness in <ref type="table">Table.</ref> 6 and <ref type="figure">Fig. 5</ref>. ? compare {KM,QSP,SA} with GIT along with dimension reduction in <ref type="figure" target="#fig_5">Fig. 7</ref>. ? compare all algorithms in <ref type="table">Table.</ref> 5.</p><p>The above is the logic of our experiments. We only show the most representative results and ignore others due to page limitations. Perhaps we may miss some interesting open source baselines, but we are willing to provide further comparisons. Thanks for your reading.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>andFig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Motivation with 1D data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Example of the intensity growth process. (a) shows the estimated intensities; and (b), (c), (d), (e) show the growing process of local clusters as ?t gradually decays. We mark the unborn points in light gray and highlight local clusters. Limited by the number of colors, different local clusters may share the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The actual running time when changing sample number (n), feature dimension (ds)and hyper-parameter (k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Dimension reduction + Clustering. The x-axis and yaxis are the projected dimension h and F1-score. GIT outperforms competitors by up to 12.5% (on PCA+MNIST), 17.1% (on PCA+FMNIST), 9.5% (on AE+MNSIT) and 10.4% (on AE+FMNIST).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of results (h = 5) on MNIST and FMNIST with UMAP. GIT provide more accurate clusters than competitors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>F1-score of GIT on the raw dataset (dim=784) under different k. The x-axis and y-axis are k and F1-score, respectively. Dashed lines represent baseline results of k-means++, and solid lines represent results of GIT. Results of different data sets are colored differently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>PCA+MNIST, changing k. The x-axis and y-axis are k and F1-score. GIT MNIST(PCA5) indicates that we use PCA to project the original MNIST data into a 5-dimensional space, and then reported GIT's accuracy on it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>PCA+FMNIST, changing k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>AE MNIST, changing k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>AE FMNIST, changing k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Commonly used symbols</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>X</cell><cell>Dataset, containing x 1 , x 2 , . . . , x n .</cell></row><row><cell>n</cell><cell>The number of samples.</cell></row><row><cell>k</cell><cell>The hyper-parameter, used for kNN searching.</cell></row><row><cell>d s</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of real-world datasets.</figDesc><table><row><cell></cell><cell>dataset</cell><cell cols="4">#samples #dim #class balance rate</cell></row><row><cell>small</cell><cell>Iris Wine Hepatitis</cell><cell>150 178 154</cell><cell>4 13 19</cell><cell>3 3 2</cell><cell>1.00 0.68 0.26</cell></row><row><cell></cell><cell>Cancer</cell><cell>569</cell><cell>30</cell><cell>2</cell><cell>0.59</cell></row><row><cell></cell><cell>Olivetti face</cell><cell>400</cell><cell>4096</cell><cell>40</cell><cell>1.00</cell></row><row><cell>large</cell><cell>MNIST FMNIST Frogs</cell><cell>60000 60000 7195</cell><cell>784 784 22</cell><cell>10 10 10</cell><cell>1.00 1.00 0.02</cell></row><row><cell></cell><cell>Codon</cell><cell>13028</cell><cell>24</cell><cell>11</cell><cell>0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>GIT vs density-based algorithms on small-scale datasets. The best and sub-optimum results are highlighted in bold and underline, respectively. Gray results are ignored because their cover rate is less than 0.8.DPA FSF HDB QSP SA GIT</figDesc><table><row><cell>Iris</cell><cell cols="2">F1-score 0.83 0.56 0.57 0.80 0.78 0.88 ARI 0.57 0.57 0.58 0.56 0.56 0.71</cell></row><row><cell></cell><cell>NMI</cell><cell>0.68 0.73 0.74 0.58 0.63 0.76</cell></row><row><cell>Wine</cell><cell cols="2">F1-score 0.38 0.62 0.54 0.73 0.70 0.90 ARI 0.05 0.31 0.30 0.39 0.36 0.71 NMI 0.15 0.38 0.42 0.44 0.36 0.76</cell></row><row><cell>Hepatitis</cell><cell cols="2">F1-score 0.42 0.77 0.71 0.71 0.72 0.78 ARI -0.05 0.50 0.05 0.02 0.06 0.23 NMI 0.14 0.46 0.02 0.00 0.01 0.12</cell></row><row><cell>Cancer</cell><cell cols="2">F1-score 0.70 0.72 0.78 0.78 0.92 0.93 ARI 0.00 0.00 0.40 0.41 0.69 0.73 NMI 0.00 0.00 0.34 0.34 0.57 0.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>GIT vs SOTA algorithms on large-scale datasets.</figDesc><table><row><cell></cell><cell></cell><cell>KM</cell><cell>SC</cell><cell>HDB</cell><cell>QSP</cell><cell>SA</cell><cell>GIT</cell></row><row><cell></cell><cell cols="2">F1-score 0.52</cell><cell>0.37</cell><cell>0.34</cell><cell>0.60</cell><cell>0.34</cell><cell>0.62</cell></row><row><cell>Face</cell><cell>ARI NMI</cell><cell>0.38 0.74</cell><cell>0.19 0.66</cell><cell>0.08 0.61</cell><cell>0.38 0.79</cell><cell>0.21 0.61</cell><cell>0.45 0.78</cell></row><row><cell></cell><cell>time</cell><cell>2.6s</cell><cell>0.4s</cell><cell>0.9s</cell><cell>0.8s</cell><cell>1.1s</cell><cell>2.1s</cell></row><row><cell>MNIST</cell><cell cols="7">F1-score 0.50 ARI 0.36 NMI 0.45 time 76.7s 407.7s 2037.0s 3384.0s 4096s 422.1s 0.41 0.99 0.45 0.40 0.59 0.33 0.99 0.13 0.17 0.42 0.44 0.99 0.45 0.33 0.53</cell></row><row><cell>FMNIST</cell><cell cols="7">F1-score 0.39 ARI 0.35 NMI 0.51 time 54.6s 397.7s 1647.9s 3832.6s 4684s 444.5s 0.43 0.06 0.42 0.47 0.56 0.34 0.01 0.16 0.29 0.32 0.49 0.07 0.41 0.45 0.51</cell></row><row><cell>Frogs</cell><cell cols="2">F1-score 0.47 ARI 0.40 NMI 0.61</cell><cell>0.60 0.41 0.60</cell><cell>0.95 0.96 0.93</cell><cell>0.50 0.21 0.45</cell><cell>0.60 0.49 0.45</cell><cell>0.66 0.69 0.66</cell></row><row><cell></cell><cell>time</cell><cell>0.18</cell><cell>3.7s</cell><cell>1.2s</cell><cell>0.5s</cell><cell>2.9s</cell><cell>3.2s</cell></row><row><cell>Codon</cell><cell cols="2">F1-score 0.25 ARI 0.19 NMI 0.33</cell><cell>0.37 0.24 0.37</cell><cell>0.21 0.05 0.24</cell><cell>0.24 0.04 0.21</cell><cell>0.19 0.02 0.02</cell><cell>0.45 0.31 0.39</cell></row><row><cell></cell><cell>time</cell><cell>1.1s</cell><cell>11.6s</cell><cell>10.2s</cell><cell>7.9s</cell><cell>82s</cell><cell>16s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Data</cell><cell>Top1</cell><cell>Top2</cell><cell>Top3</cell></row><row><cell>Circles</cell><cell>GIT</cell><cell>SA</cell><cell>QSP</cell></row><row><cell>Moons</cell><cell>GIT, QSP, SA</cell><cell>HDB</cell><cell>FSF</cell></row><row><cell>Impossible</cell><cell>GIT</cell><cell>HDB</cell><cell>QSP</cell></row><row><cell>S-sets</cell><cell>GIT, FSF, QSP, KM</cell><cell>DPA, HDB</cell><cell>SA</cell></row><row><cell>Smiles</cell><cell>GIT, HDB, SA, SC</cell><cell>DPA</cell><cell>KM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Top-3 (F1-score) results on synthetic datasets with com-</cell></row><row><cell>plex shapes. The first column is the ground truth data. Other</cell></row><row><cell>columns show the clustering results from top-1 to top-3. We mark</cell></row><row><cell>the corresponding method abbreviation under each result.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>If |N x | = |N y | = k and points in N x</figDesc><table><row><cell>5 Appendix</cell></row><row><cell>5.1 Itensify function</cell></row><row><cell>Definition 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Important symbols used in the metric definition.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>P rec i</cell><cell>Precision of class i</cell></row><row><cell>Rec i</cell><cell>Recall of class i</cell></row><row><cell>T P i</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The contingency table. Given a set of n elements, and two partitions (e.g. clusterings) of these elements, namely X = {X1, X2, . . . , Xr} and Y = {Y1, Y2, . . . , Ys}, the overlap between X and Y can be summarized in a contingency table[nij]   where each entry nij denotes the number of objects in common between Xi and Yj: nij = |Xi ? Yj|.Input: sample set X , neighborhood size k; Output: local clusters V , boundary pair B, intensity f (x); 1: Init:r i ? x i for each i, wherer i is the root index of x i</figDesc><table><row><cell></cell><cell>Y 1</cell><cell cols="2">Y 2 . . .</cell><cell cols="2">Y s sums</cell></row><row><cell>X 1</cell><cell cols="4">n 11 n 12 . . . n 1s</cell><cell>a 1</cell></row><row><cell>X 2</cell><cell cols="4">n 21 n 22 . . . n 2s</cell><cell>a 2</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>X r</cell><cell cols="4">n r1 n r2 . . . n rs</cell><cell>a r</cell></row><row><cell>sums</cell><cell>b 1</cell><cell>b 2</cell><cell>. . .</cell><cell>b s</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Input: boundary pairs B, intensity f (x), threshold ? Output: set of edge E 1: Init: E i,j = 0 for all i, jS i = M prev [i]; S j = M prev [j]</figDesc><table><row><cell cols="4">Algorithm 2: Topo-graph construction</cell></row><row><cell cols="3">Complexity: O(kn)</cell></row><row><cell>2:</cell><cell></cell><cell></cell></row><row><cell cols="4">3: Step 1: Construct topo-graph (line 4-6)</cell></row><row><cell cols="3">4: for (i, j,r i ,r j ) ? B do</cell><cell>O(|B|) &lt; O(kn)</cell></row><row><cell>5:</cell><cell>s ?</cell><cell cols="2">(f (xi)+f (xj )) 2 4|vr i |?|vr j | , Eq.9 and Eq.10.</cell></row><row><cell>6:</cell><cell cols="3">Er i,rj + = s, Er j ,ri ? Er i,rj</cell></row><row><cell cols="4">Algorithm 3: Topo-graph pruning (connectivity)</cell></row><row><cell cols="4">Complexity: min{O(|E||V | log |V |), O(|E| log |E|)}</cell></row><row><cell cols="4">Input: local clusters V , edge E, class number n =</cell></row><row><cell></cell><cell cols="2">[n 0 , n 1 , . . . , n c ]</cell></row><row><cell cols="4">Output: the label mapping of each local clusters C[?1]</cell></row><row><cell cols="3">1: Init: Initialization (line 2-8)</cell></row><row><cell cols="4">2: Create a list E = [(i, j, E i,j )for allE i,j &gt; 0], such that</cell></row><row><cell></cell><cell cols="2">E k,2 ? E k+1,2</cell><cell>O(|E| log |E|)</cell></row><row><cell cols="4">3: C = [{i : i for i in V.keys()}]</cell><cell>node index ? class</cell></row><row><cell></cell><cell>index</cell><cell></cell></row><row><cell cols="4">4: M = [{i : [i] for i in V.keys()}]</cell><cell>class index ?</cell></row><row><cell></cell><cell cols="2">member node indexes</cell></row><row><cell cols="4">5: N = [{i : len(V [i]) for i in V.keys()}]</cell><cell>node index</cell></row><row><cell></cell><cell cols="3">? number of nodes of the same class</cell></row><row><cell cols="3">6: score = [9999]</cell><cell>intermediate dissimilarity</cell></row><row><cell cols="3">7: c = len(n)</cell></row><row><cell>8:</cell><cell></cell><cell></cell></row><row><cell cols="4">9: Step 1: Merge nodes by the descending order of edge</cell></row><row><cell></cell><cell cols="3">strength, generate intermediate class ratios seq[:,c], get</cell></row><row><cell></cell><cell cols="3">the dissimilarity sc, and determine whether accept this</cell></row><row><cell></cell><cell cols="2">merge operation (line 11-28)</cell></row><row><cell cols="3">10: for i, j, v in E do</cell></row><row><cell>11:</cell><cell cols="2">C prev = C[?1]</cell></row><row><cell>12:</cell><cell cols="2">M prev = M [?1]</cell></row><row><cell>13:</cell><cell cols="2">N prev = N [?1]</cell></row><row><cell>14:</cell><cell cols="3">if C prev [i] = C prev [j] then</cell><cell>merge different</cell></row><row><cell></cell><cell>classes</cell><cell></cell></row><row><cell>15:</cell><cell></cell><cell></cell></row><row><cell>16:</cell><cell cols="3">S = M prev [i] + M prev [j]</cell></row><row><cell>17:</cell><cell cols="3">Create C now by Eq. 26 for all k.</cell><cell>O(|V |)</cell></row><row><cell>18:</cell><cell cols="3">Create M now by Eq. 27 for all k.</cell><cell>O(|V |)</cell></row><row><cell>19:</cell><cell cols="3">Create N now by Eq. 28 for all k.</cell><cell>O(|V |)</cell></row><row><cell>20:</cell><cell cols="3">seq = sort( [N now [c] for c in set(C now .values())]</cell></row><row><cell></cell><cell cols="2">), descending order.</cell><cell>O(|V | log |V |)</cell></row><row><cell>21:</cell><cell cols="2">if len(seq) &lt; c then</cell></row><row><cell>22:</cell><cell></cell><cell>break</cell></row><row><cell>23:</cell><cell cols="2">sc = S(C now , ?)</cell></row><row><cell>24:</cell><cell cols="3">if sc ? score[-1] then</cell><cell>if accept merging</cell></row><row><cell>25:</cell><cell></cell><cell>score.append(sc)</cell></row><row><cell>26:</cell><cell></cell><cell>C.append(C now )</cell></row><row><cell>27:</cell><cell></cell><cell>M.append(M now )</cell></row><row><cell>28:</cell><cell></cell><cell>N.append(N now )</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OPTICS: ordering points to identify the clustering structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmod record</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Stanford. Asuncion, A</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>UCI machine learning repository</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Densitybased clustering based on hierarchical density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia conference on knowledge discovery and data mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="160" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical density estimates for data clustering, visualization, and outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Persistence-based clustering in riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chazal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Oudot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skraba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Computability and unsolvability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Remarks on some nonparametric estimates of a density function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Lii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Politis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected Works of Murray Rosenblatt</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel k-means: spectral clustering and normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Quantitative expression of cultural relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Driver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Kroeber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1932" />
			<publisher>University of California Press</publisher>
			<biblScope unit="volume">31</biblScope>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic topography of high-dimensional data sets by nonparametric Density Peak clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Errico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Facco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">560</biblScope>
			<biblScope unit="page" from="476" to="492" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kdd</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast and effective text mining using linear-time document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Evett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Spiehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic clustering algorithms: a systematic review and bibliometric analysis of relevant literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Ezugwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Agbaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">N</forename><surname>Oyelade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jose-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Agushaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6247" to="6306" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Data clustering: theory, algorithms, and applications. SIAM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RECOME: A new density-based clustering algorithm using relative KNN kernel density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">436</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The SpectACl of nonconvex clustering: a spectral approach to density-based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Duivesteijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honysz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Morik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3788" to="3795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Journal of Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
		</imprint>
	</monogr>
	<note>Comparing partitions</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice-Hall, Inc. ISBN 013022278X</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quickshift++: Provably good initializations for sample-based mean shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kpotufe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2294" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A better k-means++ algorithm via local search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lattanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3662" to="3671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A nonparametric estimate of a multivariate density function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Loftsgaarden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Quesenberry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1049" to="1051" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accelerated hierarchical density based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining Workshops (ICDMW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Spectral Clustering: a Tutorial for the 2010&apos;s. Handbook of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fundamental limitations of spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Clustering by fast search and find of density peaks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A review of clustering techniques and developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bharill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="664" to="681" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sur la division des corps mat?riels en parties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steinhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Acad. Polon. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">804</biblScope>
			<biblScope unit="page">801</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cluster analysis: correlation profile and orthometric analysis for the isolation of unities in mind and personality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Tryon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1939" />
			<publisher>Edward Brothers</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q.; Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic cluster formation using level set methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="877" to="889" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A technique for measuring like-mindedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Abnormal and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
