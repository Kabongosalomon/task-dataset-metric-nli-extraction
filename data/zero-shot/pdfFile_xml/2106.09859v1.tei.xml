<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RSG: A Simple but Effective Module for Learning Imbalanced Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<email>thomas.lukasiewicz@cs.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
							<email>xlhu@tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
							<email>jianfei.cai@monash.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Hebei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
							<email>zhenghua.xu@hebut.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RSG: A Simple but Effective Module for Learning Imbalanced Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Imbalanced datasets widely exist in practice and are a great challenge for training deep neural models with a good generalization on infrequent classes. In this work, we propose a new rare-class sample generator (RSG) to solve this problem. RSG aims to generate some new samples for rare classes during training, and it has in particular the following advantages: (1) it is convenient to use and highly versatile, because it can be easily integrated into any kind of convolutional neural network, and it works well when combined with different loss functions, and (2) it is only used during the training phase, and therefore, no additional burden is imposed on deep neural networks during the testing phase. In extensive experimental evaluations, we verify the effectiveness of RSG. Furthermore, by leveraging RSG, we obtain competitive results on Imbalanced CIFAR and new state-of-the-art results on Places-LT, ImageNet-LT, and iNaturalist 2018. The source code is available at https://github.com/Jianf-Wang/RSG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer vision research has made great progress in the past few years, driven by the development of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref> as well as large-scale datasets of high quality <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>. However, these large-scale datasets are usually well-designed, and the number of instances in each class is balanced artificially, which is inconsistent with the data distributions in real-world scenaries. It is common that the images of some categories are difficult to be collected, resulting in a dataset with an imbalanced data distribution. In general, imbalanced datasets can be classified into two categories in terms of data distributions: long-tailed imbalanced distributions <ref type="bibr" target="#b5">[6]</ref> and step imbalanced distributions <ref type="bibr" target="#b1">[2]</ref>, which will both be the focus of this work.</p><p>Generating new samples for rare classes during training is a good solution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>, which is regarded as a data * Corresponding author. augmentation method. However, these methods have different drawbacks, which limit their performance. Firstly, some frameworks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> were not trained in an end-to-end manner, so that the gradients cannot be backpropagated from the top to the bottom of CNNs. But it is well known that deep models can usually benefit from end-to-end training. Secondly, some methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> utilized variation information, such as different poses or lighting, among samples from the same frequent class to generate new rare-class samples. However, these methods did not introduce any mechanism to ensure that the variation information obtained from frequent classes is class-irrelevant. As a result, if the variation information (which still contains the class-relevant information) is directly combined with real rare-class samples to generate new rare-class ones for training the classifier and reshaping decision boundaries, the performance will be hurt due to the aliasing of different class-relevant information. Finally, Wang et al. <ref type="bibr" target="#b35">[36]</ref> use noise vectors to encode the variation information mentioned above. But using such noise vectors for generation can possibly generate unstable or low-quality samples, since noise vectors are too random to reflect the true variations among real images. <ref type="bibr" target="#b0">1</ref> To alleviate the above drawbacks, in this paper, we propose a simple but efficient fully parameterized generator, called rare-class sample generator (RSG), which can be trained end-to-end with any backbone. RSG directly uses the variation information, which usually reflects different poses or lighting, among the real samples from the same frequent class to generate new samples rather than using random vectors to encode such information, and therefore, RSG can generate more reasonable and stable samples. Besides, RSG introduces a new module that is designed to further filter out the frequent-class-relevant information that possibly exists in the variation information, solving the aliasing problem mentioned above. <ref type="figure" target="#fig_0">Figure 1</ref> shows how it is integrated into a simple CNN for imbalanced datasets. RSG only requires the feature maps of samples from any specific layer, and it generates some new samples during training to impact on rare classes in order to adjust their decision boundaries and enlarging their feature space. In the testing phase, RSG is removed, so that no additional computational burden is imposed on the network. Note that we only show a simple CNN in <ref type="figure" target="#fig_0">Fig. 1</ref>, but RSG can be used in any network architecture, such as ResNet <ref type="bibr" target="#b10">[11]</ref>, DenseNet <ref type="bibr" target="#b14">[15]</ref>, ResNeXt <ref type="bibr" target="#b34">[35]</ref>, and Inception <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent existing solutions for dealing with imbalanced datasets can be largely classified into approaches based on resampling and reweighting, new loss functions, meta-learning, utilizing unlabeled data, and sample generation.</p><p>Resampling techniques include oversampling the minority classes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref> and undersampling the majority classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>, which aims to balance the data distribution. Reweighting methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref> also try to balance the data distribution by assigning different weights to frequent-class and rare-class samples. Some approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b3">4]</ref> designed new loss functions by directly adding constraints to affect the decision boundaries for frequent and rare classes. Some meta-learning-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref> were also proposed to solve the data imbalance problem. Very recently, Yang and Xu <ref type="bibr" target="#b36">[37]</ref> analyzed the value of imbalanced labels, and utilized unlabeled data to boost classimbalanced learning via semi-supervised and self-supervised strategies.</p><p>Previous sample generation methods are more relevant to this work than other approaches. A hallucinator <ref type="bibr" target="#b35">[36]</ref> was designed to generate new samples for rare classes. It uses real instances from rare classes and noise vectors to produce new hallucinated instances for rare classes. A ?-encoder framework <ref type="bibr" target="#b7">[8]</ref> was proposed for generating new samples. It is first trained to reconstruct the pre-computed feature vector of input images from frequent classes. Thereafter, it is used to generate new samples by combining the real rare-class samples, and the newly generated ones are further used to train the classifier. A feature transfer learning (FTL) framework <ref type="bibr" target="#b37">[38]</ref> was recently proposed, which consists of an auto-encoder, a feature filter, and fully-connected (FC) layers. The auto-encoder is initially pre-trained on a largescale dataset for several epochs to converge to learn the latent representations. Then, principal component analysis (PCA) is leveraged to transfer the intra-class variance from frequent classes to rare classes by generating some new rare-class samples. A two-stage alternating training strategy was also proposed to jointly optimize the encoder, the feature filter, and FC layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rare-Class Sample Generator (RSG)</head><p>The rare-class sample generator (RSG) is composed of a center estimation module, a contrastive module, and a vector transformation module (see <ref type="figure" target="#fig_2">Fig. 2</ref>). To optimize the parameters of RSG, two loss functions are used, namely, center estimation with sample contrastive (CESC) loss and maximized vector (MV) loss.</p><p>RSG assumes that samples from a class follow a unimodal distribution or a multi-modal distribution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>, and thus there can be a center or a set of centers in each category to fit the distribution. In this paper, we define the notion of feature displacement, which indicates the displacement of a sample to its corresponding center in a class, caused by the same object with different conditions (e.g., angles, poses, or light conditions) in input images. Therefore, under ideal circumstances, feature displacement should not contain class-relevant information.</p><p>Given a mini-batch of samples consisting of both frequentclass and rare-class instances, RSG takes their feature maps as input and forwards them to these modules. The center estimation module aims to estimate a set of centers in each class, which is used as "anchors" for obtaining the feature displacement of each sample. The contrastive module is used to ensure that the feature displacement does not contain any frequent-class-relevant information during the sample generation process. The vector transformation module calculates the feature displacement of each frequent-class sample based on the estimated centers and uses it for generating new samples for rare classes. Intuitively, generating some new samples with such feature displacement that comes from abundant classes for rare classes may alleviate the problem caused by imbalanced datasets, as rare classes usually lack input variations.</p><p>The center estimation module is formulated as:</p><formula xml:id="formula_0">? l = f (A l ave(x l ) + b l ),<label>(1)</label></formula><p>where x l ? R D?W ?H is the feature map of an input sample, and we assume that the channel dimension, width, and height are D, W , and H, respectively. l is the class label of the sample, ave(?) denotes global average pooling across width  and height, A l and b l are the parameters of this module performing a linear transformation on the input, and f is the softmax function that outputs a probability distribution (? l ) for assigning the sample to the closest center in its corresponding class. The center estimation module is designed to estimate a set of centers instead of only one center for each class, since the intra-class data distribution is unknown. If the intra-class data distribution is a multi-modal distribution, using a set of centers is better than using a single center. On the contrary, if it is a uni-modal distribution, those centers can be very close or overlapping, which is similar to using a single center.</p><p>The contrastive module is formulated as:</p><formula xml:id="formula_1">? * = f (A * ave(h(cat[x 1 , x 2 ])) + b * ),<label>(2)</label></formula><p>where x 1 ? R D?W ?H and x 2 ? R D?W ?H are the feature maps of any two input samples from a given mini-batch, and cat(?) denotes the concatenation operation, which performs along the channel dimension. h(?) is implemented by stacking two 3 ? 3 convolutional layers with 256 channels interleaved with a ReLU activation layer throughout the paper. A * and b * are the parameters of the linear layer, resulting in a probability distribution ? * to show whether two samples come from the same class.</p><p>The vector transformation module is responsible for generating new rare-class samples through combining the feature displacement from real frequent-class samples with real rare-class samples. As <ref type="figure" target="#fig_0">Fig. 1</ref> shows, an imbalanced dataset causes a bias in the decision boundary, resulting in a smaller feature space for rare classes than for frequent classes. Thus, we propose to use the vector transformation module to generate new samples for rare classes to enlarge the feature space and "push away" the decision boundaries.</p><p>To generate new samples, we first need to obtain the feature displacement from frequent classes, which is implemented by using the frequent-class samples and their corresponding centers estimated by the center estimation module:</p><formula xml:id="formula_2">x fd-freq = x l freq ? up(C l K ),<label>(3)</label></formula><p>where x l freq ? R D?W ?H denotes a sample in a frequent class l. We use C l i ? R D to denote the i-th center in class l with dimension D, and K is the index of the closest center to x l freq , i.e., K = arg max f (A l ave(x l freq ) + b l ). up(?) denotes the upsampling operation implemented by repeating the values of C l i along the width and height, forming feature maps of a center in the same size as the x l freq . After we subtract the corresponding center feature maps from x l freq , most of the class-relevant information is removed from x l freq ; thus, we use x fd-freq to represent the feature displacement of the frequent-class sample.</p><p>Then, the second step is to generate new samples for rare classes by using x fd-freq and the real rare-class samples. Intuitively, x fd-freq can be added to the centers of rare classes, but we directly add x fd-freq to the real rare-class samples for two reasons: Firstly, the length of some x fd-freq may be smaller than the original variance of the feature space in rare classes. If we add x fd-freq to the centers, the new samples may have no impact on decision boundaries. Secondly, due to the limited sample size of rare classes, most rare-class samples can directly determine the decision boundaries, and adding x fd-freq to rare-class samples has a more straightforward impact on the decision boundaries.</p><p>So, the generation process of new rare-class samples is:</p><formula xml:id="formula_3">x l new = T (x fd-freq ) + x l rare ,<label>(4)</label></formula><p>where x l rare ? R D?W ?H denotes a sample in a rare class l , x l new is a newly generated sample in that class, and T is a linear transformation defined as T (z) = conv(z), where conv denotes a single convolutional layer containing a set of convolutional filters with the kernel size 3, the stride 1, and the padding size 1, whose number is the same as the number of channels of input feature maps.</p><p>The center estimation with sample contrastive loss (L CESC ) aims to update centers of each class and to optimize the contrastive module as well as the center estimation Training: for j in range(0, T):</p><p>Compute</p><formula xml:id="formula_4">L CESC with {x (i) } s i=1 . Compute gradient ? CESC . Update: ? CESC ? CE ? , ? CESC ? C. if j &lt;T th : Compute L cls with {x (i) } s i=1 . Compute gradient ? cls . Update: ? cls ? ?, ? CESC ? CM ? . else:</formula><p>Generate new samples with ? and ?: {x</p><formula xml:id="formula_5">(i) new } snew i=1 . Concat: {x (i) aug } s+snew i=1 = [{x (i) } s i=1 , {x (i) new } snew i=1 ]. Compute L MV with C, {x (i) new } snew i=1 , and {x (i) } s i=1 . Compute gradient ? MV . Compute L cls with {x (i) aug } s+snew i=1 . Compute gradient ? cls . Update: ? MV + ? cls ? V T ? , ? cls ? ?. end if end for Direct Addition</formula><p>The new overall feature displacement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector Transformation Module</head><p>The new overall feature displacement module. Therefore, it is composed of two classical loss terms, which can be written as:</p><formula xml:id="formula_6">LCESC = K?1 i=0 ? l i d,j,k ||x l (d,j,k) ? up(C l i ) (d,j,k) || 2 s ? (ylog? * + (1 ? y)log(1 ? ? * )) s 2 ,<label>(5)</label></formula><p>where d, j, and k denote the indices of the feature maps along the channel, width, and height. ? l i is the probability of the sample belonging to the i-th center obtained from Eq. (2), K is the number of centers in each class, s is the batch size. Considering a mini-batch with batch size s, s 2 sample pairs are formed by randomly picking samples from the mini-batch during training for the contrastive module. We denote by y ? {0, 1} the ground-truth showing whether the samples in each input pair come from the same class.</p><p>? s and ? s 2 denote that the first term and the second term of L CESC are calculated over s instances and s 2 pairs on average, respectively.</p><p>The maximized vector loss (L MV ) optimizes the parameter of the vector transformation module (namely, T ) and ensures that newly generated samples can enlarge the feature space of rare classes, where the basic idea is to maximize the feature displacement of newly generated samples relative to their centers (i.e., the new overall feature displacement in <ref type="figure" target="#fig_4">Fig. 3</ref>). Here, we treat the feature displacement of a sample as a vector starting from a center to the sample (see <ref type="figure" target="#fig_4">Fig. 3</ref>). To generate a new rare-class sample, one can directly add x fd-freq to a rare-class sample <ref type="figure" target="#fig_4">(Fig. 3, left)</ref>, but the direction of x fd-freq is usually uncertain and the new overall feature displacement typically does not always have the largest length, because of the triangle inequality. Thus, we design the MV loss to make the transformed vector co-linear with the feature displacement of the rare-class sample in the same direction, and leave the length of the transformed vector unchanged <ref type="figure" target="#fig_4">(Fig. 3, right)</ref>, to maximally impact the decision boundary. For example, if the direct addition is used, the newly generated samples may not impact the decision boundary due to the limited overall length. But leveraging the vector transformation module and MV loss ensures that the newly generated samples are widely distributed in the feature space of rare classes, because of the larger displacement relative to the centers, and it improves the probability that newly generated samples can appear around decision boundaries in each batch during training.</p><p>Moreover, as for a given frequent-class sample, although the frequent-class-relevant information has been largely removed when the feature displacement of a sample is calculated via Eq. (3), we still use the contrastive module to ensure that the feature displacement does not contain frequent-classrelevant information in order to further alleviate the possible class-relevant information aliasing problem when new rareclass samples are generated. W.l.o.g., ? * is the probability that the two input samples of the contrastive module do not belong to the same category, and the MV loss is:</p><formula xml:id="formula_7">L MV = j,k (| T (x fd-freq ) (j,k) ? x (j,k) fd-rare ||T (x fd-freq ) (j,k) || 2 ||x (j,k) fd-rare || 2 ? 1|) snew + j,k (| ||T (x fd-freq ) (j,k) || 2 ? ||x (j,k) fd-freq || 2 |) snew ? log? * snew ,<label>(6)</label></formula><p>where j and k denote the indices of the feature maps along the width and height, | ? | takes the absolute value, and x fd-rare represents the feature displacement obtained from a sample and its closest center in a rare class via Eq. (3). The two input samples of the contrastive module are T (x fd-freq ) and x l freq , respectively. The first term of L MV is essentially to minimize the cosine angle of T (x fd-freq ) and x fd-rare in order to make them co-linear in the same direction, the second term is to keep the length of T (x fd-freq ) unchanged compared with x fd-freq , and the third term makes T (x fd-freq ) and x l freq not belong to the same category, ensuring that T (x fd-freq ) will not have any frequent-class-relevant information. Given a mini-batch of samples, ? snew denotes that L MV is calculated over newly generated samples on averages, where s new is the number of newly generated samples.</p><p>Note that minimizing L MV may encourage to generate some new samples with very large overall feature displacement, which can hurt the performance on frequent classes. Thus, the vector transformation module also receives the gradients from the classification loss function L cls , reaching a trade-off between L cls and the second term of L MV , to generate more reasonable new samples for rare classes.</p><p>The training procedure and overall loss function of RSG are summarized in Algorithm 1 and given as follows, respectively:</p><formula xml:id="formula_8">L total = L cls + ? 1 L CESC + ? 2 L MV ,<label>(7)</label></formula><p>where L cls denotes any classification loss, such as softmax with cross-entropy loss, focal loss <ref type="bibr" target="#b22">[23]</ref>, AM-Softmax <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, and LDAM <ref type="bibr" target="#b3">[4]</ref>, and ? 1 and ? 2 denote coefficients. The epoch threshold T th is set to the index of epoch in which the learning rate is decayed to 0.001 in this paper.</p><p>The workflow of RSG is as follows (see <ref type="figure" target="#fig_2">Fig. 2</ref>). Before the epoch threshold T th , given a mini-batch of samples, RSG splits them into two parts according to a manually set constant frequent-class ratio ? = n freq /n cls , where n freq and n cls denote the number of frequent classes and the total number of classes, respectively. For example, for a training set of 10 classes and ? = 0.3, the three classes with the largest number of samples are frequent classes, and the other classes are rare classes. (Note that for simplicity, only a frequent-class and a rare-class are plotted in <ref type="figure" target="#fig_2">Fig. 2.)</ref> Then, the data are forwarded to the center estimation module to update centers for each class and optimize the parameters of the center estimation module. In addition, those data are also forwarded to the contrastive module to optimize its parameters. After the epoch threshold T th , RSG starts to generate new samples and the parameters of the contrastive module are not further updated. The feature displacement of each sample in frequent classes is calculated by the vector transformation module, which is then transformed with T and randomly added to the data in rare classes with a manually set parameter transfer strength ?, resulting in newly generated samples. The contrastive module propagates gradients to the T in the vector transformation module to optimize T and filter out frequent-class-relevant information. In general, the number of samples in frequent classes is not smaller than that in rare classes in a given mini-batch. We define the transfer strength ? as the number of samples in frequent classes involved in calculating the feature displacement and generating new samples for rare classes. Specifically, the number of newly generated samples is s new = max{ ??s freq /s rare , 1}?s rare , where s freq and s rare are the numbers of samples in frequent and rare classes in a mini-batch, respectively, and ? is the floor function. Finally, the feature maps of newly generated samples are concatenated with the original input feature maps along the batch dimension and forwarded to subsequent layers to calculate the loss and to optimize the whole framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>Datasets. The experimental evaluation focuses on the Imabalanced CIFAR, the iNaturalist 2018, the Places-LT, and the ImageNet-LT datasets. Imbalanced CIFAR is based on the original CIFAR dataset, which is constructed by reducing the training samples per class, and the validation set is not changed. An imbalance ratio ? is defined as the ratio between sample sizes of the most frequent class and the least frequent class, i.e., ? = N max /N min . We conducted experiments on the long-tailed imbalance <ref type="bibr" target="#b5">[6]</ref> and step imbalance <ref type="bibr" target="#b1">[2]</ref> settings. The imbalance factors (?) that we used in our experiments are 50 and 100. The iNaturalist species classification dataset <ref type="bibr" target="#b28">[29]</ref> is a large-scale imbalanced dataset of 437,513 training images classified into 8142 species in its 2018 version. The official training and validation set has a long-tailed distribution and a balanced distribution, respectively. Places-LT has 365 categories, with the maximum of 4980 images per class and the minimum of 5 images per class, while ImageNet-LT has 1000 categories, with the maximum of 1280 images per class and the minimum of 5 images per class. As for the evaluation on these two datasets, the classes are further categorized into three splits: many-shot (more than 100 samples), medium-shot (between 20 to 100), and few-shot (less than 20) in order to better examine performance variations across classes with different numbers of samples seen during training. We follow the experimental setting of these datasets in previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> for evaluation.</p><p>Implementation details. The training details on the four datasets are summarized as follows:</p><p>? Imbalanced CIFAR: We followed the basic data augmentation method <ref type="bibr" target="#b10">[11]</ref> for training: 4 pixels are padded, and a 32 ? 32 patch is randomly cropped from the image or its horizontal flip. The framework was trained with a batch size of 128 for 200 epochs. The learning rate was initially set to 0.1, and then it was decayed by 0.01 at the 160-th epoch and again at the 180-th epoch. The network was optimized by using stochastic gradient descend with a momentum of 0.9.</p><p>? iNaturalist 2018: We followed standard practice and performed data augmentation with random-size cropping <ref type="bibr" target="#b26">[27]</ref> to 224 ? 224 from images or their horizontal flip. The network was trained from scratch for 90 epochs with a batch size of 256. The learning rate was set to 0.1 initially, and then it was decayed by 0.1 at the 50-th epoch, the 70-th epoch, and the 85-th epoch, respectively. Besides, for a fair comparison, we followed Kang et al. <ref type="bibr" target="#b18">[19]</ref> and also trained the model for the 2? schedular (180 epochs). In our 2? schedular experiment, the learning rate was decayed by 0.1 at the 100-th epoch, the 140-th epoch, and the 170-th epoch, respectively. During validation, images were center-cropped to 224 ? 224 without further augmentation.</p><p>? Places-LT: We followed previous work <ref type="bibr" target="#b23">[24]</ref> to perform the data augmentation and to fine-tune ResNet-152, which is pre-trained on the full ImageNet-2012 dataset. The network was trained with a batch size of 256 for 30 epochs. The initial learning rate was set to 0.01, and it was decayed by 0.1 at every 10 epoch, and the training was stopped after 30 epochs.</p><p>? ImageNet-LT: We followed previous work <ref type="bibr" target="#b18">[19]</ref> to use ResNeXt-50-32x4d, which was trained with a batch size of 256 for 100 epochs. The initial learning rate was set to 0.1, and it was decayed by 0.1 at the 60-th epoch, the 80-th epoch, and the 95-th epoch, respectively.</p><p>Ablation studies. We performed ablation studies on Imbalanced CIFAR with ? = 50. The mean error rates that are taken from three independent runs are reported. We comprehensively searched the hyperparameters of RSG and explored which level of feature is the most suitable for RSG to generate new samples by conducting experiments on ResNet-32 <ref type="bibr" target="#b10">[11]</ref> with LDAM-DRW <ref type="bibr" target="#b3">[4]</ref>, where "DRW" denotes a deferred re-weighting training strategy proposed by Cao et al. <ref type="bibr" target="#b3">[4]</ref>. Based on our exploration, in the following experiments,  we set the number of centers to 15, the frequent-class ratio to 0.2 and 0.5 for long-tailed and step imbalanced distributions, the transfer strength to 1.0 and 0.01 for long-tailed and step imbalanced distributions, and ? 1 and ? 2 to 0.1 and 0.01, respectively. The search process can be found in the supplementary material. Note that RSG was initially used before the second-to-last down-sampling layer. Firstly, we fixed the network architecture to ResNet-32 <ref type="bibr" target="#b10">[11]</ref> and tested RSG relative to different L cls . By <ref type="table">Table 1</ref>, the deep model equipped with RSG consistently performs better than the one without RSG when combined with different loss functions. RSG significantly improves the performance when the model is combined with standard softmax with cross-entropy loss (denoted ERM, i.e., empirical risk minimization). This is reasonable, as standard softmax does not have any mechanism against imbalanced datasets. As for focal loss, AM-softmax, and LDAM, although they are well-designed to tackle imbalanced datasets, RSG can still further improve the performance.</p><p>Secondly, we set L cls to LDAM with DRW <ref type="bibr" target="#b3">[4]</ref>    and . Note that the used networks were built according to the experiments on CIFAR in their original papers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b14">15]</ref>. As <ref type="table" target="#tab_1">Table 2</ref> shows, when RSG is integrated into the networks, all the models are consistently improved.</p><p>Thirdly, we did a comprehensive ablation study on MV loss and the vector transformation module, and we obtain the following conclusions based on <ref type="table" target="#tab_3">Table 3</ref>: (1) Every subterm of MV loss is important and useful, since once we remove any subterm of it, an increase can be observed with regard to the error rate. (2) Adding the feature displacement to the centers of rare classes leads to an increase in terms of the error rate. This fact verifies what we have mentioned in Section 3, i.e., adding the feature displacement to real rare-class samples is a better choice than adding it to the centers of rare classes. (3) Using the vector transformation module with MV loss performs better than directly adding the feature displacement to the samples in rare classes, which thus verifies their effectiveness.</p><p>Moreover, RSG is compared with previous sample generation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. As <ref type="table" target="#tab_4">Table 4</ref> shows, RSG has outperformed previous methods with different margins, showing that RSG can solve the drawbacks in previous generation methods and improve the performance.</p><p>Finally, we leveraged RSG before different pooling layers of ResNet-32 to explore which level of feature is the most suitable for generating new samples. As <ref type="table" target="#tab_6">Table 5</ref> shows, RSG achieves the best result when it was used before the secondto-last down-sampling layer. Therefore, in the remaining experiments, RSG was still used before the second-to-last down-sampling layer.</p><p>Comparison with state of the art. For each of the following experiments, we report mean error rates or mean</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-Tailed</head><p>Step CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 1st down-sampling <ref type="bibr" target="#b17">18</ref>   <ref type="table" target="#tab_10">Table 7</ref> shows the top-1 error rate of different methods using ResNet-50 <ref type="bibr" target="#b10">[11]</ref> as the backbone on iNaturalist 2018, and we followed Kang et al. <ref type="bibr" target="#b18">[19]</ref> to conduct experiments in two training settings, namely, the 1? schedular and the 2? schedular. In the 1? schedular experiment, we compare LDAM-DRW-RSG and LDAM-DRS-RSG with previous LDAM-DRW and LDAM-DRS, separately. Here, "DRS" denotes a deferred class-balanced resampling strategy proposed by Cao et al. <ref type="bibr" target="#b3">[4]</ref>. Note that we cannot reproduce the result on iNaturalist 2018 reported in the original paper (32.0%) <ref type="bibr" target="#b3">[4]</ref> by using LDAM-DRW. So, we report our reproduced results of LDAM-DRW and LDAM-DRS <ref type="bibr" target="#b3">[4]</ref> based on their publicly available code. The results in <ref type="table" target="#tab_10">Table 7</ref> show that we can obtain better results by leveraging the proposed generator, which directly demonstrates the effectiveness of RSG. Moreover, as for the 2? schedular setting, the top-1 error rate of LDAM-DRS-RSG is further decreased. Thus, it can be seen that RSG helps the model achieve new stateof-the-art results in both training schedular settings, which demonstrates that RSG is capable of dealing with imbalanced datasets effectively. <ref type="table" target="#tab_12">Table 8</ref> shows the top-1 accuracy on Places-LT. The results show that the performance can be further improved when RSG is combined with LDAM-DRS, showing that RSG is useful. Moreover, when compared with the recent two popular methods, namely, ? -normalized <ref type="bibr" target="#b18">[19]</ref> and BBN <ref type="bibr" target="#b0">[1]</ref>, RSG can improve the performance of the model on medium-shot and few-shot classes with less accuracy loss on many-shot classes, resulting in a higher overall accuracy and a new state-of-the-art result.   <ref type="bibr" target="#b5">[6]</ref> 38.88 ERM-DRW <ref type="bibr" target="#b3">[4]</ref> 36.27 ERM-DRS <ref type="bibr" target="#b3">[4]</ref> 36.44 BBN <ref type="bibr" target="#b0">[1]</ref> 33.71 ? -normalized <ref type="bibr" target="#b18">[19]</ref> 34.40 LDAM-DRW <ref type="bibr" target="#b3">[4]</ref> 34.00 LDAM-DRS <ref type="bibr" target="#b3">[4]</ref> 32.73 LDAM-DRW-SSP <ref type="bibr" target="#b36">[37]</ref> 33.70 LDAM-DRW-RSG (ours) <ref type="bibr" target="#b32">33</ref>  <ref type="bibr" target="#b18">[19]</ref> 30.70 cRT <ref type="bibr" target="#b18">[19]</ref> 32.40 LWS <ref type="bibr" target="#b18">[19]</ref> 30.50 LDAM-DRS-RSG (ours) 29.74 When compared with LDAM-DRW, LDAM-DRW-RSG can achieve a higher accuracy, verifying that RSG is able to alleviate the problem caused by imbalanced datasets. RSG can enhance the model and greatly improve its generality on medium-shot and few-shot classes. In addition, by equipping RSG, we can also obtain a new state-of-the-art result on ImageNet-LT. Since all hyperparameters of RSG were fixed after the hyperparameter searching process, we can conclude that the hyperparameters and RSG are quite robust to new datasets (i.e., Places-LT, ImageNet-LT, and iNaturalist 2018). If hyperparameters are further tuned on the new datasets, even better results might be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Outlook</head><p>We have introduced a rare-class sample generator (RSG), which is a general building block to mitigate the issue of training on imbalanced datasets. RSG is simple yet effective, since it is an architecture-agnostic and loss-agnostic plugin module, and it does not bring any additional burdens</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Many Medium Few All Lifted Loss <ref type="bibr" target="#b25">[26]</ref> 41.   to the backbone network during the inference phase. In extensive experiments, we have verified the effectiveness of RSG, which has achieved excellent results on four public benchmarks. Since RSG is flexible and orthogonal to most previous methods, future research can focus on improving the RSG module directly by designing more elegant ways to generate higher-quality rare-class samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>RSG in a simple CNN. The part in the dotted box is only used during training. RSG learns to generate new rare-class samples, which are used to reshape the decision boundary and enlarge the feature space of rare classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>A diagram of RSG with samples' feature maps as input. The blue dashed line denotes a decision boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Training Procedure of RSG Input: Batch size: s; feature maps of training data: {x (i) } s i=1 ; epoch threshold: T th ; centers: C; training epochs: T; transfer strength: ? ? (0, 1]; center estimation module: CE ? ; contrastive module: CM ? ; vector transformation module: V T ? ; weights of the backbone network: ?; frequent-class ratio: ? ? (0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The objective and principle of the vector transformation module and MV loss. The triangles and circles in the figure have the same meaning as those inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top-1 error rates of different network architectures combined with LDAM-DRW [4] on Imbalanced CIFAR for ? = 50.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on MV loss and the vector transformation module. Top-1 error rates of ResNet-32 combined with RSG and LDAM-DRW [4] on Imbalanced CIFAR for ? = 50 are reported.</figDesc><table><row><cell></cell><cell cols="2">Long-Tailed</cell><cell>Step</cell><cell></cell></row><row><cell></cell><cell cols="4">CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100</cell></row><row><cell>?-Encoder [8]</cell><cell>23.76</cell><cell>54.91</cell><cell>27.70</cell><cell>57.85</cell></row><row><cell>Imaginary [36]</cell><cell>23.99</cell><cell>55.08</cell><cell>28.23</cell><cell>58.46</cell></row><row><cell>FTL [38]</cell><cell>23.56</cell><cell>55.24</cell><cell>27.83</cell><cell>58.03</cell></row><row><cell>ERM-RSG (ours)</cell><cell>20.25</cell><cell>54.44</cell><cell>26.07</cell><cell>56.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison with other sample generation methods on</cell></row><row><cell>Imbalanced CIFAR (? = 50). All of them are based on ResNet-32</cell></row><row><cell>combined with ERM for a fair comparison.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study (top-1 error rates) with regard to the different layers, where RSG was used on Imbalanced CIFAR (? = 50). RSG was used before the three down-sampling layers in ResNet-32. ResNet-32 combined with LDAM-DRW was used, and GAP denotes global average pooling.</figDesc><table><row><cell>accuracies, which are taken from three independent runs. Ta-</cell></row><row><cell>ble 6 shows the results on Imbalanced CIFAR with ? ? {50,</cell></row><row><cell>100}. We first compare our LDAM-DRW-RSG with LDAM-</cell></row><row><cell>DRW, as this comparison directly shows the improvement</cell></row><row><cell>brought by RSG. After combining LDAM-DRW with RSG,</cell></row><row><cell>we obtain a remarkable improvement for both long-tailed</cell></row><row><cell>and step imbalanced distributions, which shows the power</cell></row><row><cell>of RSG for handling imbalanced datasets. As a result, with</cell></row><row><cell>the help of RSG, LDAM-DRW-RSG achieves superior re-</cell></row><row><cell>sults on Imbalanced CIFAR when compared with previous</cell></row><row><cell>methods.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9</head><label>9</label><figDesc>shows the top-1 accuracy on ImageNet-LT. 25.19 36.70 28.88 61.68 56.15 61.43 59.32 Focal loss [23] 29.62 23.28 36.09 28.70 61.59 55.68 61.65 58.50 CB Focal [6] 25.43 20.73 39.73 39.65 63.98 54.83 80.24 85.10 CB RW [6] 27.63 21.95 38.06 30.38 66.01 57.54 78.69 69.63 M-DRW [4] 24.94 20.44 27.67 21.05 59.49 56.06 58.91 56.26 BBN [1] 20.18 17.82 22.34 18.33 57.44 52.98 54.14 50.49 LDAM-DRW [4] 22.97 18.97 23.08 18.67 57.96 53.38 54.64 50.97 LDAM-DRW-SSP [37] 22.17 17.87 22.95 18.38 56.57 52.89 54.28 50.47 LDAM-DRW-RSG (ours) 20.45 17.20 21.65 17.90 55.45 51.50 53.00 49.43</figDesc><table><row><cell>Dataset</cell><cell cols="3">Imbalanced CIFAR-10</cell><cell></cell><cell cols="3">Imbalanced CIFAR-100</cell><cell></cell></row><row><cell>Imbalance Type</cell><cell cols="2">Long-Tailed</cell><cell>Step</cell><cell></cell><cell cols="2">Long-Tailed</cell><cell>Step</cell><cell></cell></row><row><cell>Imbalance Ratio (?)</cell><cell>100</cell><cell>50</cell><cell>100</cell><cell>50</cell><cell>100</cell><cell>50</cell><cell>100</cell><cell>50</cell></row><row><cell>ERM</cell><cell>29.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Top-1 error rates of ResNet-32 on Imbalanced CIFAR.</figDesc><table><row><cell>Training Schedular</cell><cell>Method</cell><cell>Error Rate</cell></row><row><cell></cell><cell>ERM</cell><cell>42.86</cell></row><row><cell></cell><cell>CB Focal Loss</cell><cell></cell></row><row><cell>1? schedular</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Top-1 error rates of ResNet-50 on iNaturalist 2018.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Top-1 accuracy of ResNet-152 on Places-LT.</figDesc><table><row><cell>Method</cell><cell cols="3">Many Medium Few All</cell></row><row><cell>Focal Loss [23]</cell><cell>63.3</cell><cell>37.4</cell><cell>7.7 43.2</cell></row><row><cell>OLTR [24]</cell><cell>52.1</cell><cell>39.7</cell><cell>20.3 41.2</cell></row><row><cell>Joint [19]</cell><cell>65.9</cell><cell>37.5</cell><cell>7.7 44.4</cell></row><row><cell>NCM [19]</cell><cell>56.6</cell><cell>45.3</cell><cell>28.1 47.3</cell></row><row><cell>cRT [19]</cell><cell>61.8</cell><cell>46.2</cell><cell>27.4 49.6</cell></row><row><cell>? -normalized [19]</cell><cell>59.1</cell><cell>46.9</cell><cell>30.7 49.4</cell></row><row><cell>LWS [19]</cell><cell>60.2</cell><cell>47.2</cell><cell>30.3 49.9</cell></row><row><cell>LDAM-DRS [4]</cell><cell>63.7</cell><cell>47.6</cell><cell>30.0 51.4</cell></row><row><cell cols="2">LDAM-DRS-RSG (ours) 63.2</cell><cell>48.2</cell><cell>32.3 51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Top-1 accuracy of ResNeXt-50 on ImageNet-LT.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that<ref type="bibr" target="#b37">[38]</ref> has proposed to avoid sampling random vectors due to their randomness, and<ref type="bibr" target="#b7">[8]</ref> also has conducted experiments and verified that using random vectors to generate new samples for training classifiers can degrade the performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by the National Natural Science Foundation of China under the grant 61906063, by the Natural Science Foundation of Tianjin City, China, under the grant 19JCQNJC00400, and by the "100 Talents Plan" of Hebei Province, China, under the grant E2019050017. This work was also supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1 and by the AXA Research Fund. We also acknowledge the use of the Tier 2 facility JADE (EP/P020275/1) and GPU computing support by Scan Computers International Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BBN: Bilateralbranch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is the effect of importance weighting in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Label distribution learning on auxiliary label space graphs for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classbalanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2845" to="2855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep imbalanced learning for face recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta-Weight-Net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="429" to="449" />
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internation Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient harmonized singlestage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8577" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Largescale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The iNaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CosFace: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for OCR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">Convolutional neural networks with gated recurrent connections. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the value of labels for improving class-imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Feature transfer learning for deep face recognition with underrepresented data. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
