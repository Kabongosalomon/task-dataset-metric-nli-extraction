<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Kun</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE.</roleName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON MULTIMEDIA</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Question Answering</term>
					<term>Multi-step Reason- ing</term>
					<term>Graph Neural Network</term>
					<term>Multi-Modal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video question answering is a challenging task, which requires agents to be able to understand rich video contents and perform spatial-temporal reasoning. However, existing graph-based methods fail to perform multi-step reasoning well, neglecting two properties of VideoQA: (1) Even for the same video, different questions may require different amount of video clips or objects to infer the answer with relational reasoning; (2) During reasoning, appearance and motion features have complicated interdependence which are correlated and complementary to each other. Based on these observations, we propose a Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an end-to-end fashion. The first contribution of our DualVGR is the design of an explainable Query Punishment Module, which can filter out irrelevant visual features through mutiple cycles of reasoning. The second contribution is the proposed Video-based Multi-view Graph Attention Network, which captures the relations between appearance and motion features. Our DualVGR network achieves state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is available at https://github.com/MM-IR/DualVGR-VideoQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>VideoQA is a challenging and high-level multimedia task <ref type="bibr" target="#b0">[1]</ref>, which requires the agents to understand videos and perform relational reasoning according to questions based on visual, textual, as well as spatial-temporal contents. Since the input of VideoQA is a sequence of frames, there are two differences between ImageQA and VideoQA: <ref type="bibr" target="#b0">(1)</ref> In addition to appearance information, VideoQA also needs to understand the motion information to answer the questions. (2) VideoQA requires to perform spatio-temporal reasoning over the objects, while ImageQA only requires spatial reasoning over the objects. Therefore, scene graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and neural-symbolic reasoning frameworks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, which are used in ImageQA, are hard to be implemented in VideoQA as the agents have <ref type="figure">Fig. 1</ref>. Answering questions requires agents to utilize both appearance and motion information, and usually requires them to be capable of multiple steps of spatio-temporal reasoning <ref type="bibr" target="#b17">[18]</ref>. (a) Question1: Not all video shots or objects are necessary to answer the question. (b) Question2: In each reasoning cycle, appearance and motion features are usually associated and complementary to each other.</p><p>to address the issues of comprehensive representation (e.g. appearance and motion information) and multi-step reasoning.</p><p>In order to solve the above challenges, this work focuses on performing multi-step reasoning via Graph Networks for VideoQA. Previous reasoning-based methods can be divided into four categories based on their frameworks. The first group <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> implements spatial and temporal attention mechanism to iteratively select useful information to answer the questions. The second group <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> focuses on memorybased network, which is quite popular in TextQA. However, these methods neglect the visual relation information when performing multi-step reasoning. The third group <ref type="bibr" target="#b14">[15]</ref> aims to perform relational reasoning via a simple module, like relation network. However, this module can only model a limited number of objects. The fourth group <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> aims to use graph neural networks to integrate relation information into their frameworks by considering GNN's powerful representation ability on relation modeling. GNN is a novel relation encoder that captures the inter-object relations beyond static object/region detection, thus enabling reasoning with rich relational information. Compared with relation network, graph neural network is more flexible and powerful in relational reasoning, thus we follow this group in our work.</p><p>However, existing graph-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> neglect two key attributes of VideoQA task when performing reasoning:</p><p>(1) Not all video shots or objects are correlated to the question. As illustrated in <ref type="figure">Fig. 1</ref>., to answer Question1: Who is a dog that looks like a panda walking with?, only question-related video clips are needed to infer the answer. <ref type="bibr" target="#b1">(2)</ref> Appearance and motion features are associated and complementary to each other in each reasoning step. Using <ref type="figure">Fig. 1</ref> as an example again, when answering Question2: What is a small dog which arXiv:2107.04768v1 [cs.MM] 10 Jul 2021 looks exactly like a panda running rapidly on?, it is necessary for agents to utilize both appearance and motion information. Specifically, in order to understand a small dog which looks exactly like a panda, agents need to rely on appearance information to find the dog. Then, they have to analyze the motion information to infer the action of the dog: running rapidly on the road. In short, The appearance information could provide clues to pay attention to certain motion information to answer the questions, vice versa.</p><p>Motivated by these observations, we devise a novel graphbased reasoning unit named Dual-Visual Graph Reasoning Unit (DualVGR), which is stacked in an iterative manner to perform multi-step reasoning. At first, we design Query Punishment Module to generate query-guided masks to allow a limited number of question-related video features into relational reasoning. Then, in order to fully capture the multiview visual relation information, we propose Video-based Multi-view Graph Attention Network to reveal the relation between appearance and motion channels. The proposed graph network includes two graphs for each visual channel. The first graph aims to learn the underlying complementary relation within each specific visual space, and the other is to learn the concomitant and correlated relation between appearance and motion features. The losses of video-based multi-view graph network are consistency constraint loss and disparity constraint loss. Consistency constraint loss is to enhance the commonality between appearance and motion features, while disparity constraint loss is to enhance the heterogeneity between them.</p><p>The main contributions of this work can be summarized as follows. (1) We propose a DualVGR unit, a multimodal reasoning unit enabling the represention of rich interactions between question and video clips. This unit includes two components. One is an explainable Query Punishment Module to filter out irrelevant visual-based information. It has been demonstrated good results in both short and long compositional questions during multiple cycles of reasoning. The other is a Video-based Multi-view Graph Attention Network to perform spatial-temporal relational reasoning such that the relations between appearance and motion can be adaptively captured; <ref type="bibr" target="#b1">(2)</ref> We incorporate our DualVGR unit into a full DualVGR network by stacking it in an iterative manner. Through multi-step reasoning, our method achieves state-ofthe-art or competitive results on several mainstream datasets: MSVD-QA <ref type="bibr" target="#b12">[13]</ref>, MSRVTT-QA <ref type="bibr" target="#b17">[18]</ref> and SVQA <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review the recent studies related to visual question answering, which can be divided into two sub-categories: image question answering and video question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Question Answering</head><p>There are three research directions about image question answering. The first line, namely monolithic method <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>, locates the most relevant visual region of images based on attention mechanism, then projects visual features and textual features together into a common latent space through a single step. Antol et al. <ref type="bibr" target="#b20">[21]</ref> combine the visual features and the textual features via multimodal pooling, such as addition and concatenation, then map them into a unified space. However, multimodal pooling methods do not well capture the complex associations between two modalities due to their different distributions. Therefore, some approaches <ref type="bibr" target="#b21">[22]</ref> related to Bilinear Pooling, which has been used to integrate different CNN features for fine-grained image recognition <ref type="bibr" target="#b22">[23]</ref>, have been proposed. However, Bilinear Pooling needs a huge number of parameters and the dimensionality of output feature is usually too high. Some extended versions are proposed to handle these issues, such as Multimodal Compact Bilinear Pooling (MCB) <ref type="bibr" target="#b23">[24]</ref>, Hadamard Product for Low-rank Bilinear Pooling (MLB) <ref type="bibr" target="#b24">[25]</ref>, Multimodal Factorized Bilinear Pooling (MFB) <ref type="bibr" target="#b25">[26]</ref> and Multimodal Factorized High-order pooling (MFH) <ref type="bibr" target="#b26">[27]</ref>. Monolithic methods have been demonstrated useful in some cases, but failed to perform well in long and compositional questions.</p><p>The second line focuses on modeling the multi-step interaction process through a recurrent cell <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Nguyen and Okatani <ref type="bibr" target="#b27">[28]</ref>, Yang et al. <ref type="bibr" target="#b28">[29]</ref> stack the attention layers to model the multi-step interaction process. Xiong et al. <ref type="bibr" target="#b30">[31]</ref> introduce a novel dynamic memory network to iteratively retrieve meaningful visual contents. Meanwhile, some approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> aim to integrate relational reasoning in each step, especially graph-based methods. Li et al. <ref type="bibr" target="#b2">[3]</ref> model multi-type inter-object relations via a graph attention mechanism to learn question-adaptive relation representations.</p><p>The third line is neural-symbolic reasoning. This kind of methods decompose the whole task into some subtasks, and design several Neural Module Networks (NMN) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref> to solve those subtasks. Andreas et al. <ref type="bibr" target="#b33">[33]</ref> firstly utilize an off-the-shelf parser to decompose the compositional question into logical expressions, then construct a monolithic network for each subtask. Hu et al. <ref type="bibr" target="#b4">[5]</ref> claim that a brittle off-the-shelf semantic parser would lead to bad performance, hence they design a seq-to-seq RNN to end-to-end predict instance-specific layout. These kinds of methods receive good performances on synthetic datasets <ref type="bibr" target="#b38">[38]</ref>, as the questions are easy to parse into subtasks. Besides, Yi et al. <ref type="bibr" target="#b36">[36]</ref> propose a CLEVRER dataset for exploring the problem of temporal and causal reasoning in videos. This dataset further promotes the development of the neuro-symbolic reasoning research in video-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Video Question Answering</head><p>Different from ImageQA, VideoQA is more challenging as videos contain much more complex patterns than a single image. To tackle this problem, agents have to fully comprehend the temporal structure of videos. Jang et al. <ref type="bibr" target="#b39">[39]</ref> encode appearance feature and motion feature with ResNet <ref type="bibr" target="#b40">[40]</ref> and C3D <ref type="bibr" target="#b41">[41]</ref> respectively, then design a spatial and temporal attention mechanism to select different regions during multi-step reasoning. Later, with the success of Dynamic Memory Network <ref type="bibr" target="#b30">[31]</ref> in ImageQA, some work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> applies memory-based network to VideoQA tasks. Xu et al. <ref type="bibr" target="#b12">[13]</ref> propose an Attention <ref type="figure">Fig. 2</ref>. DualVGR architecture for VideoQA. First, we divide the whole video into certain clips, and extract the appearance and motion features from each clip. Meanwhile, Question is embed with BiLSTM. Then, with these representations, we send them to our stacked DualVGR unit to iteratively model rich interactions between video clips. Next, we fuse appearance and motion features of each clip by Bilinear Fusion, and utilize attention mechanism to fuse features of all clips to obtain the final visual vector. Finally, we concatenate the visual vector with our question vector to predict the answers.</p><p>Memory Unit (AMU), which gradually refines its attention over the appearance and motion features by using question as guidance. Considering that motion and appearance features are correlated in the reasoning process, Gao et al. <ref type="bibr" target="#b13">[14]</ref> propose a motion-appearance co-memory network and use a temporal convolutional and deconvolutional neural network to generate multi-level contextual facts. However, their methods do not consider the relations between different objects.</p><p>Relation information is obviously an important clue in the reasoning process. Le et al. <ref type="bibr" target="#b14">[15]</ref> propose a Conditional Relation Network (CRN) to model the relations between visual objects, and stack CRNs to perform multi-step relational reasoning. However, the limitation of relation network-based methods is that they can only process a limited number of objects at a time. Therefore, graph neural network comes into researchers' sight. Huang et al. <ref type="bibr" target="#b15">[16]</ref> point out that previous work neglects the interactions among objects in each frame and design a location-aware graph convolutional network to learn a relation-enhanced visual feature encoder. Jiang et al. <ref type="bibr" target="#b16">[17]</ref> propose an undirected heterogeneous graph with each video snippet and question word as node to integrate correlations of both inter-and intra-modality in an uniform module. In these graph-based methods, all the objects, even those irrelevant to the question, are engaged into graph construction without discrimination, which could bring lots of uninformative noises into relational reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>The task of VideoQA can be described as follow. Given a video V and the corresponding question q, agents aim to infer the answer a correctly. Formally, the prediction? is given by classification scores:</p><formula xml:id="formula_0">a = argmax a?A P ? (a|q, V ) ,<label>(1)</label></formula><p>where ? is our trainable model parameters.</p><p>The proposed DualVGR framework is depicted in <ref type="figure">Fig. 2</ref>. Firstly, each video is divided into several clips, and we extract appearance and motion features for each clip. Meanwhile, the corresponding question is embed with BiLSTM. Secondly, appearance features, motion features and the corresponding question features are input into the our stacked DualVGR unit iteratively. We use each DualVGR unit to determine the attention within video guided by the question, and model the relational reasoning by stacking the DualVGR units. Thirdly, we fuse appearance and motion features of each clip by Multimodal Factorized Bilinear pooling (MFB) <ref type="bibr" target="#b25">[26]</ref>, and utilize attention mechanism to fuse features of all clips to obtain the final visual vector. Finally, we concatenate the visual vector with our question vector to predict the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual and Linguistic Representation</head><p>We firstly divide the whole video V of L frames into N equal length clips C = (C 1 , . . . , C N ). Therefore, each video clip C i contains F frames, where F = L/N . We extract two sources of information from each video clip, which are appearance features and motion features. The appearance features of each video clip C i are denoted as</p><formula xml:id="formula_1">V i a = {v i a,1 , v i a,2 , . . . , v i a,F } ? R 2048?F ,</formula><p>where the subscript a indicates appearance. The motion features of each clip are represented as V i m ? R 2048?1 , where the subscript m indicates motion. For fair comparison with the state-of-the-art methods (in Section IV) <ref type="bibr" target="#b14">[15]</ref>, V i a are the pool5 output of ResNet101 and V i m are extracted from ResNeXt-101. We follow the previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">42]</ref> to process questions, which embed the words of each question into fixedlength vectors initialized with 300-dimension GloVe <ref type="bibr" target="#b43">[43]</ref> as</p><formula xml:id="formula_2">W = {w i : 1 i L q , w i ? R 300?1 },</formula><p>where L q is the length of each question. Then, we pass these word-embeddings into a BiLSTM Network to get context-aware embedding vectors. Different from the previous methods which just use the last hidden state of LSTM to represent the questions without deep analysis, we process all the hidden state sequences generated by BiLSTM Q ? R d?Lq in each step, which will be described latter in Section B. Furthermore, in order to further perform graph-based reasoning, appearance features should have the same dimension as motion features. Therefore, we further pass the appearance features of each video clip V i a into a BiLSTM network, and project the motion features V i m ? R d?1 of each video clip C i into a fixed-size vector with dimension d. The generated video subclip-based appearance features V i a ? R d?1 and question features Q ? R d?Lq denote as:</p><formula xml:id="formula_3">? ? ? {V i a , O} = BiLST M V i a , ? (v) BiLST M , {E Q , Q} = BiLST M W, ? (q) BiLST M ,<label>(2)</label></formula><p>where O ? R d?Lq is the hidden state of appearance-based BiLSTM, and E Q ? R d?1 is the output of the last hidden units and represents the global features of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The proposed DualVGR</head><p>To answer the question based on the given video, the designed agent needs to have the ability of locating on the question-related video clips, understanding the video with both appearance and motion channels, and performing multistep reasoning. The proposed DualVGR Unit determines the question-related video clips, mines the relation within/between appearance and motion, and combines complementing relation features with corresponding visual features, then stacks the DualVGR units for multi-step relational reasoning. It consists of two modules, Query Punishment Module and Video-based Multi-view Graph Attention Network, as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Query Punishment Module is designed to filter out the irrelevant video clips by employing query-guided mask on video features. Video-based Multi-view Graph Attention Network is to reveal the underlying complementary relations within appearance and motion by constructing appearance-specific and motion-specific graphs, and reveal the correlated relations between appearance and motion by constructing appearancemotion correlation and motion-appearance correlation graphs.</p><p>1) Query Punishment Module: Only query-related information is needed to infer the correct answer. Moreover, when perform multi-step reasoning, people have a propensity to pay attention to different textual parts of the question, especially the long and compositional questions. Therefore, we propose a Query Punishment Module to mimic human's step-to-step reasoning behavior, and filter out irrelevant visual features in each reasoning step. Specifically, based on previous word-level contextual word embedding Q = Q 1 , Q 2 , Q 3 , . . . , Q Lq and initialized word embedding W = w 1 , w 2 , w 3 , . . . , w Lq , we employ a self-attention mechanism to obtain the current step's question feature q</p><formula xml:id="formula_4">(t) w . ? ? ? ? ? ? ? ? ? ? ? w i q = L2N orm W 1 Q i ? i q = sof tmax(W 2 w i q ) q (t) w = Lq i=1 ? i q w i ,<label>(3)</label></formula><p>where W 1 ? R d?d and W 2 ? R d?1 are learnable parameters. After obtaining the question representation q t w at the t-th iteration, we use it to filter out query-irrelevant visual features. Specifically, we first transform our question into the visual space with a fully-connected network, then dot-product operation is used to model the relevance of the current question and visual clip features. With gate mechanism, we obtain the query-guided mask value for each clip feature. Formally, the punishment mechanism is given by: First, query punishment module is implemented to filter out the irrelevant clips' features. Then, a multi-view graph network is used to provide a context-aware feature representation. With skip-connection mechanism, we get a combination of raw visual feature and visual relation feature. The unit's output is sent to another unit as the input feature representation.</p><formula xml:id="formula_5">? ? ? ? ? ? ? ? ? ? (t) a,i =? i a W (t) a q (t) w ? (t) a = ? ? (t) a,1 , . . . , ? (t) a,N ?(t) a = ? (t) a,1? 1 a , . . . , ? (t) a,N? N a ,<label>(4)</label></formula><formula xml:id="formula_6">? ? ? ? ? ? ? ? ? ? (t) m,i =? i m W (t) m q (t) w ? (t) m = ? ? (t) m,1 , . . . , ? (t) m,N ?(t) m = ? (t) m,1? 1 m , . . . , ? (t) m,N? N m ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">W (t) a ? R 300?d and W (t)</formula><p>m ? R 300?d are learnable parameters, and ? is the sigmoid function. After multiplying the masks and corresponding clip-based appearance and motion features, we filter out current irrelevant visual information at each step.</p><p>2) Video-based Multi-view Graph Attention Network: As mentioned before, both appearance and motion channels are crucial for video understanding. In order to fully reveal the complemented information from these two channels, we need not only to extract the appearance and motion features from video clip itself, but also to consider the relations among video clips within each channel as well as the relations between two channels for each video clip. To this end, our work tries to aggregate the within-channel and between-channel relations into appearance and motion features. Inspired by GAT <ref type="bibr" target="#b44">[44]</ref>, which updates node representation over its neighbors with self-attention mechanisms, and has the ability to deal with both transductive learning and inductive learning problems, especially arbitrarily structured graph learning problems, we construct appearance graph with node as appearance feature of each clip, and motion graph with node as motion feature of each clip, then follow the proposed self-attention strategy in GAT to encode the neighborhood-relation into node representation for these two graphs. In this way, the withinchannel relations are aggregated into appearance and motion features respectively. For between-channel relations, we follow the idea of AM-GCN <ref type="bibr" target="#b45">[45]</ref>, which is a new type of GCNs for graph classification task that can optimally integrate node features and topological structures by adaptively fusing specific and common embeddings from node features, topological structures and their combinations rather than simple GNN whose capability in extracting deep correlation information between topological structures and node features is distant from optimal, to seek one specific embedding and one common embedding for each channel by performing graph convolution operation. The specific embedding of one channel remains the specific characters of this channel, which should be disparity from that of the other channel. Two common embeddings model the correlated information from both channels, thus they should be consistency with each other.</p><p>For appearance channel, we construct two undirected complete graph networks, including appearance independent graph (AIG) and appearance-motion correlation graph (AMC), with each query punished clip-based appearance feature as a node. AIG aims to learn specific spatial-temporal contextual embeddings Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(t)</head><p>A within appearance channel. AMC is designed to extract the correlated relation features Z (t) CA shared by appearance and motion channels. For motion channel, we also utilize two graph networks to learn the contextual embeddings, including motion independent graph (MIG) and motionappearance correlated graph (MAC). Their goals are the same with graphs in appearance channel. According to these two motion graphs, we can get the specific embedding Z CM . Each graph is implemented with a multi-head Graph Attention Network (GAT) to model the relations between clip-features. Specifically, the attention score of each head ? between two nodes is given by:</p><formula xml:id="formula_8">? ? ? ? ? ? ? h t i = U t? (t) i + b t ? (t) i,j = exp LeakyReLU W t h t i ||h t j k?Ni exp (LeakyReLU (W t [h t i ||h t k ])) ,<label>(6)</label></formula><p>where U t ? R d?d1 and W t ? R 2d1?1 are learnable parameters, || is the concatenation operation, and N i is the first-order neighborhood of node i in the graph.</p><p>Once the multi-head attention scores for each node are obtained, we update the representation of each node by:</p><formula xml:id="formula_9">h t i = K || k=1 ? ? ? j?Ni ? k,t i,j h j ? ? ,<label>(7)</label></formula><p>where || represents concatenation, ? k i,j are the normalized attention coefficients from the k-th attention mechanism ? k . The final shape of each node feature is R Kd1?1 .</p><p>Finally, we propose two losses to enhance the multi-view representation ability of our graphs, that is, consistency constraint and disparity constraint. Consistency constraint is to enhance the commonality between the correlated contextual information of appearance and motion spaces. Disparity constraint to enhance the independence of specific embeddings and correlated embeddings. The formulations of these two constraints are listed as follows.</p><p>Consistency Constraint: For two output embeddings Z CA and Z CM , we design a consistency constraint for our multiview task, which could enhance their commonality. We first normalize the final embedding matrix Z Then, the similarity of N embeddings can be calculated by:</p><formula xml:id="formula_10">S (t) A = Z CAnor ? Z T CAnor S (t) M = Z CM nor ? Z T CM nor (8)</formula><p>The consistency indicates that the two similarity matrices should be similar as much as possible. The consistency loss of a single unit can be represented as:</p><formula xml:id="formula_11">L (t) c = S (t) A ? S (t) M 2 .<label>(9)</label></formula><p>Then the consistency loss of the whole network is:</p><formula xml:id="formula_12">L c = 1 T t=1 T L (t) c ,<label>(10)</label></formula><p>where T is the number of iteration steps. Disparity Constraint: Since specific embeddings and common embeddings, such as Z A and Z CA , are learned from graphs with the same topology, to make sure that these two embeddings could capture different information, we employ the Hilbert-Schmidt Independence Criterion (HSIC) <ref type="bibr" target="#b46">[46]</ref> to enhance the independence of these two embeddings. HSIC, a simple yet effective measure of independence, has been implemented to several machine learning tasks <ref type="bibr" target="#b47">[47]</ref>. For appearance, we use Z A to represent Z CA in a single unit is defined as:</p><formula xml:id="formula_13">HSIC (t) (Z A , Z CA ) = (n ? 1) ?2 tr(RK A RK CA ),<label>(11)</label></formula><p>where K A and K CA are the Gram matrices with k A,ij = k A z i A , z j A and k CA,ij = k CA z i CA , z j CA . R = I ? 1 n ee T , where I is an identity matrix and e is an all-one column vector. Follow AM-GCN <ref type="bibr" target="#b45">[45]</ref>, we use the inner product kernel function for K A and K CA .</p><p>Similarly, the disparity constraint of embeddings Z (t) M and Z (t) CM in motion space can be given by:</p><formula xml:id="formula_14">HSIC (t) (Z M , Z CM ) = (n ? 1) ?2 tr(RK M RK CM ).<label>(12)</label></formula><p>Finally, the disparity constraint for the whole network is given by:</p><formula xml:id="formula_15">L t d = HSIC (t) (Z A , Z CA ) + HSIC (t) (Z M , Z CM ) ,<label>(13)</label></formula><formula xml:id="formula_16">L d = 1 T t=1 T L t d ,<label>(14)</label></formula><p>where T is the number of iteration steps.</p><p>3) Attention: Now we have two embeddings Z A and Z CA in appearance space, and two embeddings Z M and Z CM in motion space. In order to adaptively capture the contextual information for each space, we utilize attention mechanism to fuse two embeddings in appearance space, as well as those in motion space. Their corresponding attention importance scores (? a , ? ca ) and (? m , ? cm ) are given by:</p><formula xml:id="formula_17">(? a , ? ca ) = Attn (Z A , Z CA ) ,<label>(15)</label></formula><formula xml:id="formula_18">(? m , ? cm ) = Attn (Z M , Z CM ) .<label>(16)</label></formula><p>The embedding of the i-th clip in appearance space in Z A is z (i,t) A ? R Kd1?1 and the embedding of the i-th clip in </p><formula xml:id="formula_19">motion space in Z M is z (i,t) M ? R Kd1?1 . First,</formula><formula xml:id="formula_20">v (i,t) A = U (t) a tanh(w (t) a z (i,t) A + b a ), v (i,t) M = U (t) m tanh(w (t) m z (i,t) M + b m ).<label>(17)</label></formula><p>Similarly, we can get the attention score v (i,t)</p><p>CA for Z CA and the attention score v (i,t) CM for Z CM . Then, we normalize the attention values with softmax function to get the final score:</p><formula xml:id="formula_21">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (i,t) A = exp v (i,t) A exp v (i,t) A + exp v (i,t) CA , ? (i,t) M = exp v (i,t) M exp v (i,t) M + exp v (i,t) CM .<label>(18)</label></formula><p>Finally, we obtain the final embedding z (i,t) a in appearance space by combining these two embeddings z </p><formula xml:id="formula_22">z (i,t) a = ? (i,t) A ? z (i,t) A + ? (i,t) CA ? z (i,t) CA , z (i,t) m = ? (i,t) M ? z (i,t) M + ? (i,t) CM ? z (i,t) CM .<label>(19)</label></formula><p>Then, a residual connection <ref type="bibr" target="#b48">[48]</ref> is used to avoid the vanishing gradient problem. This operation can also be considered as an amalgam of complementing factors including appearance, motion and query-related visual relation information. Each clip-based feature is updated by:</p><formula xml:id="formula_23">? i a =? i a + z (i,t) a , V i m =? i m + z (i,t) m .<label>(20)</label></formula><p>4) Multi-step Reasoning: Finally, the DualVGR unit is stacked as a chain to perform the final DualVGR network:</p><formula xml:id="formula_24">? i a ,? i m = DualV GR(? i a ;? i m ; Q; W ).<label>(21)</label></formula><p>Through multiple steps of iteration, our DualVGR's final visual representation contains complementary information about the question-related clips, including appearance, motion and the corresponding relations between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Video Representation Fusion</head><p>At the final step, we obtain the appearance and motion features for each clip? i a and? i m , we use Multimodal Factorized Bilinear pooling (MFB) <ref type="bibr" target="#b25">[26]</ref> to fuse them to get the final visual representation of each clip V i f . An attention mechanism called Graph Readout Operation <ref type="bibr" target="#b49">[49]</ref> is used to fuse the clip features. The details are illustrated as follows:  <ref type="table" target="#tab_1">Type  what  who  how  when where  Train  1,200  30,933  19,485 10,469  736  161  72  Val  250  6,415  3,995  2,168  185  51  16  Test  520  13,157  8,149  4,552  370  58  28  All  1,970  50,505  31,629 17,199 1,291  270  116   TABLE II</ref>  where? ? R d?1 is the output visual feature. The attention mechanism is implemented as Eqn. (15) -Eqn. <ref type="bibr" target="#b18">(19)</ref>.</p><formula xml:id="formula_25">V i f = M F B(? i a ,? i m ),<label>(22)</label></formula><formula xml:id="formula_26">o = Attn V i f ,<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Answer Decoder</head><p>Following <ref type="bibr" target="#b14">[15]</ref>, we adopt the same answer decoders of these open-ended questions for the fair comparison:</p><formula xml:id="formula_27">y = ELU W o ?, W Q E Q + b + b ,<label>(24)</label></formula><formula xml:id="formula_28">y = ELU (W y y + b) ,<label>(25)</label></formula><formula xml:id="formula_29">p = sof tmax (W y y + b) ,<label>(26)</label></formula><p>where W Q ? R d?d and W o ? R 2d?d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Total Loss</head><p>We cast our open-ended VideoQA task as a classification task. Hence, we use cross-entropy loss L t for this task. Then, the total loss is:</p><formula xml:id="formula_30">L = L t + ?L c + ?L d ,<label>(27)</label></formula><p>where ? and ? are parameters of the consistency and disparity constraint terms. The consistency constraint is implemented as Eqn. <ref type="bibr" target="#b9">(10)</ref> and disparity constraint is implemented as Eqn. <ref type="bibr" target="#b13">(14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Datasets 1) MSVD-QA <ref type="bibr" target="#b12">[13]</ref>: There are 1970 trimmed videos collected from the Microsoft Research Video Description (MSVD) Corpus <ref type="bibr" target="#b50">[50]</ref>. It contains 50,500 QA pairs automatically generated by the NLP algorithm in total, which contains five general types of questions, including what, how, when, where and who. The average video length is approximately 10 seconds, and the average question length is approximately 6 words. Therefore, it is a small-scale dataset with short questions. We conduct experiments on this dataset to test our model's generalization ability in short videos in real worlds. The details of MSVD-QA dataset are illustrated in <ref type="table" target="#tab_1">Table I.</ref> 2) MSRVTT-QA <ref type="bibr" target="#b17">[18]</ref>: Compared with MSVD-QA, MSR-VTT contains longer videos and more complex scenes. We conduct experiments on this dataset to test our model's performance for longer videos of real datasets. It contains 10,000  <ref type="table" target="#tab_1">Table II</ref>.</p><p>3) SVQA <ref type="bibr" target="#b18">[19]</ref>: It is a large-scale synthetic dataset which contains 12,000 synthetic videos and around 120K QA pairs. Specifically, videos are generated from Unity3D, and each video length is the same as 10 seconds. Meanwhile, QA pairs are generated from question templates automatically, and the questions are generated exclusively long with an average length of 20 words. Furthermore, each question can be decomposed into human readable logical chain or tree layout easily. The goal of this dataset is to test the reasoning ability of VideoQA systems. <ref type="table" target="#tab_1">Table III</ref> illustrates the statistics of SVQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For each video in MSVD-QA, we segment the video into 8 clips. For MSRVTT-QA, each video is divided into 16 clips. Besides, in SVQA dataset, each video is splitted into 20 clips. The numbers of divided clips are determined by grid search method from {4, <ref type="bibr">8, 12, 16, 20, 24}</ref>. For all the datasets, each clip contains 16 frames by default. The video appearance and question encoders are one-layer BiLSTMs. The dimension d = 768, d1 = 192, and the number of the multi-heads k is 4. The iteration step t for MSVD-QA is set to 1, the iteration step t for MSRVTT-QA is 6, and the iteration step t for SVQA is 4. For loss function, we use the grid search method to select the best coefficients. Specifically, the value set of ? is {0, 1e ? 4, 1e ? 3, 1e ? 2, 1e ? 1, 1, 10, 100, 1000}, while the value set of the remaining coefficient ? is {0, 1e?8, 1e?6, 1e?4, 1e? 2, 1, 10, 100}. After searching, we obtain the best coefficients for all the datasets: ? = 100, ? = 1e ? 6 in MSVD-QA, ? = 100, ? = 1e ? 6 in MSRVTT-QA and ? = 1, ? = 1e ? 6 in SVQA. Our framework is implemented in PyTorch, and the network is trained by Adam optimizer with a fixed learning rate 1e ? 4. The batch size is set to 256. All experiments are terminated after 25 epochs and the results are reported at the epoch which has the best validation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the State-of-the-art</head><p>We compare our proposed model with state-of-the-art methods (SOTA) on aforementioned datasets. For MSVD-QA and MSRVTT-QA, we compare with most resent SOTA, including HME <ref type="bibr" target="#b51">[51]</ref>, HGA <ref type="bibr" target="#b16">[17]</ref>, HCRN <ref type="bibr" target="#b14">[15]</ref> and TSN <ref type="bibr" target="#b42">[42]</ref>.</p><p>? HME is a model equipped with memory network. It uses a redesigned question memory to improve the question representation in each step. The visual representations, including appearance and motion features, are mapped into the heterogeneous memory. Then, multi-step reasoning is performed with self-updated attention in this memory network. ? HGA is a model with graph network. It represents all video shots and question words as the graph to perform cross-modal reasoning. ? HCRN is a model with stacked clip-based relation networks. The CRN takes input as an array of tensorial objects and a conditioning feature, and output as an array of relation information within them. By hierarchically stacking these blocks, HCRN performs multi-step relational reasoning. ? TSN is a model containing several modules to perform multi-step reasoning. For example, since appearance and motion play different roles in multi-step reasoning, a switch module has been proposed to adaptively choose appearance or motion channel as the primary channel, guiding the reasoning process. The results are summarized in <ref type="table" target="#tab_1">Table IV</ref> for MSVD-QA and MSRVTT-QA. It is clear that our framework consistently outperforms or is competitive with SOTA models on all tasks for short questions (average question length ? 7 words). For MSVD-QA dataset, our DualVGR achieves 39.03% overall accuracy, which is 2.33% improvement over previous SOTA methods. It also achieves quite high performance in each question types. For instance, as shown in <ref type="table" target="#tab_1">Table IV</ref>, DualVGR achieves approximately 3% improvement for the question of "what" and "who". The improvement of DualVGR compared with HME and TSN confirms the effectiveness of relational reasoning in VideoQA tasks. Furthermore, the improvement of DualVGR compared with HGA shows that the critical role of query punishment module in our framework. For MSRVTT-QA, our model achieves 35.52% accuracy, which is only 0.08% lower than the current state-of-the-art performance. Compared with SOTA method HCRN which conducts the question-aware frame-level feature and the question-aware clip-level feature in a hierarchical structure, our method only extracts question-aware clip-level feature, which reduces the computational cost, with only a minimum drop of performance even in dataset with complicated scenes. To sum up, MSVD-QA and MSRVTT-QA results imply that our model can handle real-world videos well.</p><p>We further compare our methods with SOTA methods on synthetic dataset, SVQA. This dataset contains many compositional questions, which require agents to be able to perform multi-step relational reasoning to infer the answer. For SVQA, three video question answering models as well as ours are used for comparisons:</p><p>? Unified-Attn <ref type="bibr" target="#b52">[52]</ref> is a model with two attention mechanisms, including sequential video attention and temporal question attention mechanisms. ? SA+TA-GRU <ref type="bibr" target="#b18">[19]</ref> is a model with a refined GRU whose hidden state transfer process is associated with temporal attention to strengthen long-term temporal dependency. ? STRN <ref type="bibr" target="#b53">[53]</ref> is a model with spatio-temporal relational network, which aims to model temporal changes in both the interactions among different objects and the motiondynamics of individual objects.</p><p>The results are summarized in <ref type="table">Table V</ref>. From the results, we can observe that our proposed model DualVGR outperforms the state-of-the-art methods. Specifically, our framework achieves the best overall accuracy 50.38%, leading to approximately 2.8% improvement over the best compared method STRN. Our method achieves promising results on all question types in SVQA dataset, especially 4.88% improvement on "Count" questions. On the contrary, previous methods perform poorly on "Count" questions, which indicates the powerful generalization ability of our method for multi-step reasoning. The aforementioned results indicate the effectiveness of the DualVGR for long and compositional questions. Furthermore, the method STRN is implemented with relation network, which means that DualVGR unit with graph network may be more suitable for relational reasoning task in VideoQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>To prove the effectiveness of the essential components of DualVGR unit and to provide more detailed parameter analysis, we conduct extensive ablation studies on MSVD-QA test set and SVQA test set with a wide range of configurations. The results of the component analysis are reported in <ref type="table" target="#tab_1">Table  VI.</ref> 1) The Effectiveness of Essential Components: The whole architecture of our DualVGR unit mainly incorporates two essential components: Query Punishment Module and Videobased Multi-view Graph Attention Network. We consider the following ablation models to verify the importance of each component in VideoQA:</p><p>? AG: this model extracts appearance feature from video clips, and constructs them as the appearance graph. Then, multi-head GAT is implemented to get the contextual representation. At last, graph attention readout mechanism is used to get the final visual vector, and fuse it with the question vector to infer the answer. The results are shown in <ref type="table" target="#tab_1">Table VI</ref>, and the key observations and conclusions are as follows: (1) Two-stream Visual   graphs in the unit, we obtain better results than one shared graph. (4) Self-Attention for Questions: simpleDualVGR is the DualVGR framework without self-attention mechanism in each reasoning step. We can observe that with self-attention mechanism, our DualVGR obtains 1.36% performance gain in SVQA dataset. It verify the effectiveness of self-attention mechanism when performing multi-step reasoning for long and compositional questions.</p><p>2) The detailed analysis of iteration steps T : We perform the detailed analysis of the iterative process in <ref type="figure" target="#fig_6">Fig. 4</ref>. We train our DualVGR network on all datasets with the configuration as our best model. First, questions in MSVD-QA dataset are usually very short, which implies that agents do not need to perform very complex relational reasoning to fully understand the questions. Consequently, the overall accuracy on MSVD-QA decreases when performing more steps of relational reasoning. Next, the test accuracy on MSRVTT-QA dataset increases from 35.29% to 35.52% when the number of reasoning iteration increases from 1 to 6. Since MSRVTT-QA dataset has more complex questions and longer videos than MSVD-QA, it could benefit from a higher number of reasoning iterations over our DualVGR unit. Then, when the iteration step is set to 7, the model's performance becomes very stable. At last, SVQA is a dataset designed for testing the relational reasoning ability of models. Therefore, the questions are compositional and complex which require agents to perform multi-step relational reasoning. The results reveal that the accuracy of SVQA dataset also benefits from a higher number of iteration steps. Network with four steps provides a gain of +1.71% in overall test accuracy on SVQA over the  network with a single step. Then, when the step increases by 5, the model's performance becomes very stable.</p><p>3) The detailed analysis of the number of clips N : Since questions in SVQA are compositional, requiring agents to understand the whole video contents and spatial-temporal relations between relevant objects in videos, the key information of visual facts may distribute evenly in the videos. Therefore, we also perform a detailed analysis of the number of divided clips N of SVQA dataset. We train several networks in three iteration steps on SVQA, the results are illustrated in <ref type="figure" target="#fig_8">Fig. 6</ref>.</p><p>Networks with 16 and 20 clips respectively provide a gain of +0.84% and +0.93% in overall accuracy on SVQA test set over the network with 4 clips. This implies that the performance of spatial-temporal relational modeling via Graph Neural Network could benefit from a larger number of divided clips. With more divided clips, the key information in videos would distribute more evenly, that is, we explore relational reasoning of video content in a more fine-grained manner. Finally, since each video in SVQA has the same length of 300 frames, network with 24 clips would contain much noise in each clip representation. Therefore, their performance degrades as the number N increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Analysis</head><p>To better understand the contributions of our Query Punishment Module in DualVGR, we provide the visualization examples of the attention results of query features and the queryguided mask values on MSVD-QA and SVQA datasets in <ref type="figure" target="#fig_7">Fig.  5</ref>. In the first example of one step in real dataset MSVD-QA, this question intends to ask who is showing a gun in a box? Our model mainly focuses on the right part of the question "showing a gun". This question requires agents to focus on the motion information to infer the answer. We can observe from the example that the motion-based mask values are higher than the appearance-based mask values in all clips, which proves the effectiveness of our Query Punishment Module in real datasets. Besides, the motion-based mask values of the first two clips are higher than others. It further demonstrates the correctness of our model to pay more attention to the relevant video clips to predict the answer. For the second example of SVQA dataset, this video is extremely simple as it only contains four objects. The gray cube is rotating all the time. Then, the cyan object starts to rotate for a while. At the same time, the green cylinder begins to move upwards. In iterative step t = 1 , the question pays more attention to "rotating present", then the motion-based mask values are higher than the appearance-based mask values. In iterative step t = 2, the model pays much attention to "cyan objects", then appearancebased mask values are higher than the motion-based mask values to learn the contextual representations of cyan objects. Finally, in iterative step t = 3, the model considers both of them to infer the final answer. The mask values are becoming closer to each other than previous steps. Through multi-step message passing, our DualVGR progressively finds out much more question-related visual semantics to infer the answer. The visualization results further verify the effectiveness of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a Dual-Visual Graph Reasoning Unit (DualVGR) for Video Question Answering, which could be stacked iteratively to model the question-related rich spatial-temporal interactions between video clips. Specifically, in our DualVGR unit, a Query Punishment Module is proposed to filter out irrelevant visual information through multiple cycles of reasoning. Then, a Video-based Multi-view Graph Attention Network is designed to learn the contextual representations of visual features to perform relational reasoning. With multi-step iterations, our DualVGR network achieves stateof-the-art or competitive performance in three mainstream VideoQA datasets, MSVD-QA, MSRVTT-QA and SVQA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>DualVGR Unit. DualVGR efficiently represents videos as amalgam of complementing information, including appearance, motion and relation features. The cell separately processes the appearance features and the motion features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>CM nor respectively. For simplicity, here we useZ CAnor to represent Z (t) CA , and Z CM nor to represent Z (t) CM .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A</head><label></label><figDesc>and Z CA to represent Z (t)CA for simplicity. The HSIC constraint of Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>? MG: compared with AG, this model extracts motion feature from video clips, and constructs them as the motion graph. ? FG: compared with AG, this model extracts both appearance and motion features from video clips, and fuses them into a new clip-based visual vector with MLP. Then, the visual graph is constructed with them. ? Bigraph: compared with AG, this model constructs both appearance graph and motion graph. Then, we apply two multi-head GAT to capture contextual representations of both visual spaces in each unit. Next, appearance and motion features of each clip are fused with MFB. ? MVgraph: compared with Bigraph, this model uses Video-based Multi-view Graph Attention Network to learn the contextual representations of appearance and motion features. ? PFG: compared with FG, this model implements Query Punishment Module to keep the relevant visual features. Then, multi-head GAT is utilized to represent the contextual information. ? DualVGR: this is our proposed model. Compared with MVgraph, this model adds Query Punishment Module in our unit. ? sharedVGR: compared with DualVGR, this model use one shared multi-head GAT to learn the contextual representations of both visual spaces, including appearance and motion spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Number of iterations. Impact of the number of steps in the iterative process on MSVD, MSRVTT-QA and SVQA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of the attention scores in each step's query representation and the generated query-guided visual mask of each visual feature in each step on MSVD-QA (the top row) and SVQA (the bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Number of divided clips. Impact of the number of divided clips in the iterative process on SVQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>we transform the embeddings through a nonlinear transformation w</figDesc><table><row><cell></cell><cell></cell><cell>(t) a</cell><cell>?</cell></row><row><cell cols="2">R Kd1?Kd1 and w</cell><cell>(t) m ? R Kd1?Kd1 respectively, then use a</cell></row><row><cell>weight vector U</cell><cell cols="2">(t) a ? R Kd1?1 and U m ? R Kd1?1 to get the (t)</cell></row><row><cell cols="3">attention value v i A and v i M for the i-th clip as follows:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF THE MSVD-QA DATASET.</figDesc><table><row><cell>Video</cell><cell>QA pair</cell><cell>Question</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>STATISTICS OF THE MSRVTT-QA DATASET.</figDesc><table><row><cell></cell><cell>Video</cell><cell>QA pair</cell><cell>what</cell><cell cols="2">Question Type who how</cell><cell>when</cell><cell>where</cell></row><row><cell>Train</cell><cell>6,513</cell><cell>158,581</cell><cell>108,792</cell><cell cols="3">43,592 4,067 1,626</cell><cell>504</cell></row><row><cell>Val</cell><cell>497</cell><cell>12,278</cell><cell>8,337</cell><cell>3,439</cell><cell>344</cell><cell>106</cell><cell>52</cell></row><row><cell>Test</cell><cell>2,990</cell><cell>72,821</cell><cell>49,869</cell><cell>20,385</cell><cell>1,640</cell><cell>677</cell><cell>250</cell></row><row><cell>All</cell><cell>10,000</cell><cell>243,680</cell><cell>166,998</cell><cell cols="3">67,416 6,051 2,409</cell><cell>806</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III STATISTICS</head><label>III</label><figDesc>OF THE SVQA DATASET.</figDesc><table><row><cell>Question Category</cell><cell>Sub Category</cell><cell>Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell>Count</cell><cell>/</cell><cell>19,320</cell><cell>2,760</cell><cell>5,520</cell></row><row><cell>Exist</cell><cell>/</cell><cell>6,720</cell><cell>960</cell><cell>1,920</cell></row><row><cell></cell><cell>Color</cell><cell>7,560</cell><cell>1,056</cell><cell>2,160</cell></row><row><cell></cell><cell>Size</cell><cell>7,560</cell><cell>1,056</cell><cell>2,160</cell></row><row><cell>Query</cell><cell>Action Type</cell><cell>6,720</cell><cell>936</cell><cell>1,920</cell></row><row><cell></cell><cell>Direction</cell><cell>7,560</cell><cell>1,056</cell><cell>2,160</cell></row><row><cell></cell><cell>Shape</cell><cell>7,560</cell><cell>1,056</cell><cell>2,160</cell></row><row><cell>Integer Comparison</cell><cell>More Equal Less</cell><cell>2,520 2,520 2,520</cell><cell>600 600 600</cell><cell>720 720 720</cell></row><row><cell></cell><cell>Color</cell><cell>2,520</cell><cell>216</cell><cell>720</cell></row><row><cell>Attribute Comparison</cell><cell>Size Action Type Direction</cell><cell>2,520 2,520 2,520</cell><cell>216 216 216</cell><cell>720 720 720</cell></row><row><cell></cell><cell>Shape</cell><cell>2,520</cell><cell>216</cell><cell>720</cell></row><row><cell>Total QA pairs</cell><cell></cell><cell>83,160</cell><cell cols="2">11,880 23,760</cell></row><row><cell>Total Videos</cell><cell></cell><cell>8,400</cell><cell>1,200</cell><cell>2,400</cell></row><row><cell cols="5">trimmed videos from MSR-VTT dataset [18] and 243,000</cell></row><row><cell cols="5">QA pairs generated by the NLP algorithm. The average video</cell></row><row><cell cols="5">length is approximately 15 seconds, and the average question</cell></row><row><cell cols="5">length is approximately 7 words. The details of MSRVTT-QA</cell></row><row><cell>are illustrated in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON TO THE STATE-OF-THE-ART METHODS ON MSVD-QA AND MSRVTT-QA.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSVD-QA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSRVTT-QA</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>what</cell><cell>who</cell><cell>how</cell><cell>when</cell><cell cols="2">where</cell><cell></cell><cell>ALL</cell><cell>what</cell><cell>who</cell><cell>how</cell><cell>when</cell><cell>where</cell><cell>ALL</cell><cell></cell></row><row><cell cols="2">HME [51]</cell><cell>22.4</cell><cell>50.1</cell><cell>73.0</cell><cell>70.7</cell><cell cols="2">42.9</cell><cell></cell><cell>33.7</cell><cell>26.5</cell><cell>43.6</cell><cell>82.4</cell><cell>76.0</cell><cell>28.6</cell><cell>33.0</cell><cell></cell></row><row><cell cols="2">HGA [17]</cell><cell>23.5</cell><cell>50.4</cell><cell>83.0</cell><cell>72.4</cell><cell cols="2">46.4</cell><cell></cell><cell>34.7</cell><cell>29.2</cell><cell>45.7</cell><cell>83.5</cell><cell>75.2</cell><cell>34.0</cell><cell>35.5</cell><cell></cell></row><row><cell cols="2">HCRN [15]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell></cell><cell></cell><cell>36.1</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>35.6</cell><cell></cell></row><row><cell cols="2">TSN [42]</cell><cell>25.0</cell><cell>51.3</cell><cell>83.8</cell><cell>78.4</cell><cell cols="2">59.1</cell><cell></cell><cell>36.7</cell><cell>27.9</cell><cell>46.1</cell><cell>84.1</cell><cell>77.8</cell><cell>37.6</cell><cell>35.4</cell><cell></cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">28.65 53.82</cell><cell cols="2">80.00 70.69</cell><cell cols="2">46.43</cell><cell cols="3">39.03 29.40</cell><cell cols="3">45.56 79.76 76.66</cell><cell>36.40</cell><cell>35.52</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="11">PERFORMANCE COMPARISON TO THE STATE-OF-THE-ART METHODS ON SVQA.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Exist</cell><cell>Count</cell><cell cols="3">Integer Comparison</cell><cell></cell><cell cols="4">Attribute Comparison</cell><cell></cell><cell></cell><cell></cell><cell>Query</cell><cell></cell><cell></cell><cell>All</cell></row><row><cell></cell><cell></cell><cell></cell><cell>More</cell><cell>Equal</cell><cell>Less</cell><cell>Color</cell><cell>Size</cell><cell></cell><cell>Type</cell><cell>Dir</cell><cell cols="2">Shape Color</cell><cell>Size</cell><cell>Type</cell><cell>Dir</cell><cell>Shape</cell></row><row><cell>Unified-Attn [52]</cell><cell>54.17</cell><cell>35.23</cell><cell>67.05</cell><cell>52.98</cell><cell>50.00</cell><cell>51.85</cell><cell cols="2">54.26</cell><cell>50.14</cell><cell>50.28</cell><cell>49.43</cell><cell>22.01</cell><cell>53.73</cell><cell>56.51</cell><cell>34.70</cell><cell>38.20</cell><cell>41.69</cell></row><row><cell cols="2">SA+TA-GRU [19] 52.03</cell><cell>38.20</cell><cell>74.28</cell><cell>57.67</cell><cell>61.60</cell><cell>55.96</cell><cell cols="4">55.90 53.40 57.50</cell><cell>52.98</cell><cell>23.39</cell><cell>63.30</cell><cell cols="2">62.90 43.20</cell><cell>41.69</cell><cell>44.90</cell></row><row><cell>STRN [53]</cell><cell>54.01</cell><cell>44.67</cell><cell>72.22</cell><cell>57.78</cell><cell>62.92</cell><cell>56.39</cell><cell cols="2">55.28</cell><cell>50.69</cell><cell>50.14</cell><cell>50.00</cell><cell>24.31</cell><cell>59.68</cell><cell cols="2">59.32 28.24</cell><cell>44.49</cell><cell>47.58</cell></row><row><cell>Ours</cell><cell>54.15</cell><cell>49.55</cell><cell>80.89</cell><cell>57.63</cell><cell>59.71</cell><cell>56.87</cell><cell cols="2">51.90</cell><cell cols="2">49.79 49.78</cell><cell>51.53</cell><cell>27.96</cell><cell>60.65</cell><cell cols="2">64.19 34.46</cell><cell>47.48</cell><cell>50.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Features: AG and MG just consider one type of visual information, such as appearance and motion features. FG combines the appearance and motion features into a new visual space, and learn the contextual representations of this space. After considering the appearance and motion features, it increases by 1.91% and 0.26% on SVQA dataset, and 0.45% and 3.83% on MSVD-QA dataset. This illustrates the effectiveness of utilizing two-stream visual features. Furthermore, as we can see, AG outperforms MG in MSVD-QA and MG outperforms AG in SVQA. This can be attributed to the differences between MSVD-QA and SVQA. For instance, questions in SVQA always require agents to understand the spatial-temporal relationships between all clips, hence motion features might be more important than appearance features. Questions in MSVD-QA are usually very simple, which only require agents to analyze certain clips to answer the questions. Therefore, motion features would be less important in MSVD-QA. (2) Multi-view Graph Attention Network: Bigraph utilizes two multi-head GAT to learn the contextual representations of visual features. MVgraph implements our Multi-view Graph Attention Network to learn the contextual representations. The results imply that our Multi-view Graph Attention Network is more suitable for this task, especially multiview relation learning. However, we observe that MVgraph underperforms the FG in both datasets. The detailed analysis will be illustrated later. (3) Query Punishment Module: PFG further considers Query Punishment Module in our FG. DualVGR is our whole unit with Query Punishment Module and Video-based Multi-view Graph Attention Network. After the consideration of involving Query Punishment Module to our unit, it increases 0.51% and 1.53% on SVQA dataset, and 0.29% and 1.9% on MSVD-QA dataset, which proves the advantage of our Query Punishment Module to filter our irrelevant information. Moreover, DualVGR outperforms PFG, which is distinct from what was discussed above. This could be putted down to that MVgraph and FG just represent all video clips as useful node to propogate information with GAT which could lead to much noise. Consequently, with more graphs in MVgraph, it contains more noise than FG, which leads to worse performance. With Query Punishment Module,</figDesc><table /><note>our DualVGR successfully achieves better performance than PFG. shareDVGR only uses one shared Multi-head GAT to extract the common contextual information in both visual spaces. We can observe from the results that with two common</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDIES ON MSVD-QA AND SVQA ABOUT ESSENTIAL COMPONENTS OF DUALVGR.</figDesc><table><row><cell>Model</cell><cell>Step T</cell><cell>SVQA</cell><cell>what</cell><cell>who</cell><cell cols="2">MSVD-QA how when</cell><cell>where</cell><cell>ALL</cell></row><row><cell>Two-stream</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AG</cell><cell>T = 1</cell><cell>46.54</cell><cell cols="2">26.97 52.17</cell><cell>79.46</cell><cell>74.14</cell><cell>50.00</cell><cell>37.42</cell></row><row><cell>MG</cell><cell>T = 1</cell><cell>48.19</cell><cell cols="2">23.11 49.56</cell><cell>77.30</cell><cell>70.69</cell><cell>46.43</cell><cell>34.04</cell></row><row><cell>FG</cell><cell>T = 1</cell><cell>48.45</cell><cell cols="2">27.77 51.85</cell><cell>82.97</cell><cell>67.24</cell><cell>50.00</cell><cell>37.87</cell></row><row><cell>Multi-view</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bigraph</cell><cell>T = 1</cell><cell>47.53</cell><cell cols="2">26.09 53.10</cell><cell>77.57</cell><cell>70.69</cell><cell>35.71</cell><cell>37.10</cell></row><row><cell>MVgraph</cell><cell>T = 1</cell><cell>47.59</cell><cell cols="2">26.75 52.07</cell><cell>75.68</cell><cell>72.41</cell><cell>46.43</cell><cell>37.13</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimedia intelligence: When multimedia meets artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1823" to="1835" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relation-aware graph attention network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10313" to="10322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language-conditioned graph networks for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10294" to="10303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural-symbolic vqa: Disentangling reasoning from vision and language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1031" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual visual reasoning with knowledge propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="530" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond rnns: Positional self-attention with co-attention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8658" to="8665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open-ended video question answering via multi-modal conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chujie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3859" to="3870" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiturn video question answering via hierarchical attention context reinforced networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghua</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3860" to="3872" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-form video question answering via dynamic hierarchical reinforced networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5939" to="5952" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motionappearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6576" to="6585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Thao Minh Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9972" to="9981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Location-aware graph convolutional networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11021" to="11028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reasoning with heterogeneous graph alignment for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11109" to="11116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explore multi-step reasoning in video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond text qa: multimedia answer generation by harvesting web information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="426" to="441" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond bilinear: Generalized multimodal factorized highorder pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6087" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<pubPlace>Richang Hong, Meng Wang, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal dialog system: Generating responses via adaptive decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1098" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1811" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Clevrer: Collision events for video representation and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grounding physical concepts of objects and events through dynamic visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee Kenneth</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Question-aware tube-switch network for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1184" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Am-gcn: Adaptive multi-channel graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised feature selection via dependence estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="823" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Measuring statistical dependence with hilbertschmidt norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ALT</title>
		<meeting>ALT</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unifying the video and question attentions for open-ended video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5656" to="5666" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Spatio-temporal relational reasoning for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gursimran</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of British Columbia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Jianyu Wang received the bachelor&apos;s degree in electronic and information engineering from Nanjing University of Posts and Telecommunications</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<pubPlace>Nanjing, China; San Diego, La Jolla, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Master of Computer Science Degree in University of California</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include multimedia analysis and information retrieval, especially multimodal intelligence and recommendation system</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Her current research interests include cross-media cross-modal image search, social event detection, image classification and annotation, and sparse/low rank representation. She was the recipient of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing-Kun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications and Applications (ACM TOMM) Nicolas D. Georganas Best Paper Award, IEEE Multimedia 2017 Best Paper Award, the Best Paper Award from ICIMCS&apos;09, and MMM2019 Best Paper Runner-Up Award. She is an Associate Editor of Multimedia Systems Journal, and a member of IEEE Multimedia Systems &amp; Applications Technical Committee. She received Outstanding Area Chair in ICME 2020</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>College of Telecommunications &amp; Information Engineering, Nanjing University of Posts and Telecommunications, China</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">He has hold 50 granted/pending patents and published over 400 refereed research papers in these areas. Dr. Xu has served as editor-in-chief, associate editor, guest editor, general chair, program chair, area/track chair, and TPC member for over 20 IEEE and ACM prestigious multimedia journals, conferences and workshops, including</title>
	</analytic>
	<monogr>
		<title level="m">Changsheng Xu (M&apos;97-SM&apos;99-F&apos;14) is a Professor in National Lab of Pattern Recognition, Institute of Automation</title>
		<imprint/>
	</monogr>
	<note>IEEE Trans. on Multimedia, ACM Trans. on Multimedia Computing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
