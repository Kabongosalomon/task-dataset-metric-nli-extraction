<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimation by Orthographic Projection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Yuanpei College</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Chunyu</roleName><forename type="first">Wang</forename><surname>4+</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujie</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhou.wang@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center on Frontiers of Computing Studies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Inst. for Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Estimation by Orthographic Projection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Faster VoxelPose: Real-time 3D Human Pose</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D Human Pose Estimation, Multi-view Multi-person</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While the voxel-based methods have achieved promising results for multi-person 3D pose estimation from multi-cameras, they suffer from heavy computation burdens, especially for large scenes. We present Faster VoxelPose to address the challenge by re-projecting the feature volume to the three two-dimensional coordinate planes and estimating X, Y, Z coordinates from them separately. To that end, we first localize each person by a 3D bounding box by estimating a 2D box and its height based on the volume features projected to the xy-plane and z-axis, respectively. Then for each person, we estimate partial joint coordinates from the three coordinate planes separately which are then fused to obtain the final 3D pose. The method is free from costly 3D-CNNs and improves the speed of VoxelPose by ten times and meanwhile achieves competitive accuracy as the state-of-the-art methods, proving its potential in real-time applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating 3D human pose from RGB images is a fundamental problem in computer vision. It not only paves the way for some important downstream tasks such as action recognition <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref> and human-computer interaction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, but also enables a wide range of applications, e.g. sports analysis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref> and virtual avatar animation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>While many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62]</ref> address monocular 3D pose estimation, their application in serious scenarios is limited because of the degraded accuracy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. In addition, monocular human pose estimation struggles when occlusion occurs which is ubiquitous in natural images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. As a result, the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Detection Joint Localization</head><p>Feature Volume 3D Bounding Box 3D Pose <ref type="figure" target="#fig_4">Fig. 1</ref>: Multi-view 3D Pose Estimation. Given multi-view images and camera parameters, the task aims to estimate the 3D poses of all people in the world coordinates. Similar to <ref type="bibr" target="#b38">[39]</ref>, our approach is based on the volumetric representation and detects 3D box as an intermediate step.</p><p>state-of-the-art 3D human pose estimation results are usually obtained via multicamera systems which consist of a group of synchronized and calibrated widebaseline cameras <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. Simple triangulation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b51">52]</ref> can achieve accurate 3D pose estimates if the 2D poses in all views are accurate. However, 2D pose estimates may have errors in practice especially when occlusion occurs. To address the problem, voxel-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref> have been proposed which inversely project 2D features or heatmaps in each view to the 3D space and then fuse the multi-view features. The resulting feature volume is more robust to occlusions in individual cameras. Then they apply a 3D-CNN to estimate the 3D positions of the body joints from the feature volume. While these methods achieve very accurate results, the computation complexity increases cubically with space size. As a result, they cannot support real-time inference for large scenes such as sports stadiums or retail stores.</p><p>In this work, we present Faster VoxelPose which is about ten times faster than VoxelPose on the common benchmarks and more importantly scales gracefully to large spaces. Inspired by technical drawing where a 3D object is often unambiguously represented by three 2D orthographic projections, i.e. plan, elevation and section, we re-project the previously fused 3D volumetric features to the three 2D coordinate planes by orthographic projection and estimate partial coordinates, e.g. xy, xz and yz, of a 3D pose from each of the 2D planes, which are then fused by a tiny network to predict xyz. The main advantage of the method is that we can replace the expensive 3D-CNNs with 2D-CNNs which reduces the computation cost from O(n 3 ) to O(n 2 ) where n is the spatial resolution. However, the factorization brings two new challenges. First, people that are far away in the 3D space could overlap in some planes after re-projection, which may bring severe ambiguity to the corresponding features. Second, the estimation results may be inconsistent across planes so we need a strategy to aggregate the contradictory predictions.</p><p>We address the challenges from two aspects. Firstly, as shown in <ref type="figure" target="#fig_4">Fig. 1</ref>, we present Human Detection Networks (HDN) to estimate a tight 3D box for each person which is used to filter out the features of other people. By contrast, VoxelPose <ref type="bibr" target="#b38">[39]</ref> use a loose fixed-size 3D bounding box. In particular, we re-project the 3D feature volume to the xy plane by max-pooling along the z axis (bird'seye view), and apply a 2D-CNN to localize people by a 2D box in the xy plane. Then, for each bounding box, we obtain a 1D "column" feature representation from the volume at the box center along the z axis, and apply a 1D-CNN to estimate the vertical position of the box center.</p><p>Then we present Joint Localization Networks to estimate a 3D pose for each 3D box. We first mask out the features in the volume which are outside the box to reduce the impact of other people, obtaining person-specific feature volume. We re-project the masked volume to the three coordinate planes and estimate the X, Y and Z coordinates, respectively. For each coordinate, we have two predictions from two planes. It is probable that the two predictions are contradictory so we propose a fusion network to learn a weight for each prediction and aggregate them to obtain the final 3D pose.</p><p>Our approach achieves competing results as the baseline method which uses 3D-CNN. But ours is about 10 times faster than it (speed improvement is larger for larger scenes). Our contributions are four-fold: 1) We design a lightweight framework for efficient training and inference of the multi-view multi-person 3D pose estimation problem. Our approach demonstrates that 3D human detection and pose estimation can be resolved on the re-projected 2D feature maps with careful design. 2) We propose a novel 3D human detector that disentangles ground plane localization and height estimation. 3) We utilize 3D bounding box for feature masking, which contributes to person-specific feature volume and improves joint localization accuracy. 4) We deploy the confidence regression networks to adaptively fuse the estimates on the re-projected planes to compensate for their individual accuracy loss. While we focus on pose estimation in this work, the idea may also benefit other voxel-based tasks such as object detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b60">61]</ref> and shape completion <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-view 3D Pose Estimation</head><p>For the single-person case, the key is to handle 2D pose estimation errors in individual planes. Iskakov et al . <ref type="bibr" target="#b16">[17]</ref> designed differentiable triangulation which uses joint detection confidence in each camera view to learn the optimal triangulation weights. Pavlakos et al . <ref type="bibr" target="#b26">[27]</ref> applied CNN with 3D PSM for markerless motion capture. Qiu et al . <ref type="bibr" target="#b28">[29]</ref> used epipolar lines to guide cross-view feature fusion followed by a recurrent PSM. Epipolar transformer <ref type="bibr" target="#b14">[15]</ref> extended <ref type="bibr" target="#b28">[29]</ref> to handle dynamic cameras. Generally speaking, single-person 3D pose estimation has achieved satisfactory results when there are sufficient cameras to guarantee that every body joint can be seen from at least two cameras.</p><p>Multi-person 3D human pose estimation is more challenging because it needs to solve two additional sub-tasks: 1) Identifying joint-to-person association in different views. 2) Handling mutual occlusions among the crowd. To address the first challenge, various association strategies are proposed based on re-id features <ref type="bibr" target="#b8">[9]</ref>, dynamic matching <ref type="bibr" target="#b1">[2]</ref>, 4D graph cut <ref type="bibr" target="#b56">[57]</ref>, and plane sweep stereo <ref type="bibr" target="#b22">[23]</ref>. However, in crowded scenes, noisy 2D pose estimates would harm their accuracy. To address the second challenge, Belagiannis et al . <ref type="bibr" target="#b1">[2]</ref> extended PSM for multiperson. Wang et al . <ref type="bibr" target="#b44">[45]</ref> propose a transformer-based direct regression model with projective attention.</p><p>Recently, voxel-based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref> are proposed to avoid making decisions in each camera view. Instead, they fuse multi-view features in the 3D space and only make the decision there. Such methods are free from pairwise reasoning of camera views and enable learning human posture knowledge in a data-driven way. However, the computation-intensive 3D convolutions prevent these approaches from being real-time and applicable to large spaces. Our method enjoys the benefit of volumetric feature aggregation, meanwhile being significantly faster and more scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Human Pose Estimation</head><p>Designing efficient human pose estimators has been intensively studied for practical usage. For extracting 2D pose from images, state-of-the-art methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref> have achieved real-time inference speed. In terms of multiview 3D pose estimation, Bultman et al . <ref type="bibr" target="#b5">[6]</ref> explores an efficient system using edge sensors. Remelli et al . <ref type="bibr" target="#b32">[33]</ref> and Fabbri et al . <ref type="bibr" target="#b11">[12]</ref> adopt encoder-decoder networks to reduce computation, but they are not applicable to the multi-person setting. Most recently, Lin et al . <ref type="bibr" target="#b22">[23]</ref> and Wang et al . <ref type="bibr" target="#b44">[45]</ref> present alternative solutions to volumetric methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref> and show some speed improvement. Nevertheless, these methods are capped in terms of scalability, which prevents them from being deployed to large scenes. Our method is complementary to state-of-theart lightweight 2D pose estimators, and can further improve the speed of other volumetric methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Without loss of generality, we explain our motivation with a simple case in which there is only one person. As shown in <ref type="figure" target="#fig_1">Fig. 2 (A)</ref>, the input to our approach is a 3D feature volume V ? R K?L?W ?H which is constructed by back-projecting the 2D pose heatmaps in multiple cameras to the 3D voxel space <ref type="bibr" target="#b38">[39]</ref>. The 2D pose heatmaps are extracted from the images using an off-the-shelf pose estimation model <ref type="bibr" target="#b37">[38]</ref>. L?W ?H represents the number of voxels that are used to discretize the space and K represents the number of joint types. The volume approximately encodes the per-voxel likelihood of body joints.</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref> (A), we show a 3D joint of interest, e.g. a shoulder joint, as P = (X, Y, Z). In general, the corresponding feature volume should have a distinctive pattern around P so that it can be localized by expensive 3D-CNNs <ref type="bibr" target="#b38">[39]</ref>.  To reduce the computation cost, we re-project the volume to the three coordinate planes (i.e. the xy, yz, xz planes), respectively, resulting in three 2D feature maps. We can imagine that there are also distinctive patterns at the corresponding locations of each 2D feature map, e.g. (X, Y ) at the xy plane, which can be similarly detected by 2D-CNNs. Then the 3D position of P can be assembled from the estimated coordinates in the three planes. However, when we apply the idea to the multi-person scenario, we are confronted with new challenges. The features of different people may be mixed together after being projected to the coordinate planes even when they are far away from each other in the 3D space. This may corrupt the pose estimation accuracy. Inspired by top-down 2D pose estimation <ref type="bibr" target="#b12">[13]</ref>, the problem can be alleviated by "cropping" the person from the overall 3D space and only projecting features belonging to the person to the planes. So the remaining task is to detect each person in the 3D space efficiently. We utilize the prior that people barely overlap along the z axis, therefore they can be easily detected in the bird's-eye view as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> </p><formula xml:id="formula_0">(B).</formula><p>We take a two-phase approach to address the challenges. In the first phase, we present Human Detection Networks (Section 3.2) which efficiently detects all people from the bird's-eye view by 3D bounding boxes, ensuring that only the person-of-interest features are passed to the next phase. The second phase conducts fine-grained pose estimation for each person with Joint Localization Networks (Section 3.3), which is greatly eased since occlusion and distraction are mostly eliminated in the first phase. Importantly, all the operators in the networks are on 2D and 1D features, which boosts the speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Human Detection Networks</head><p>We first apply HRNet <ref type="bibr" target="#b37">[38]</ref> to estimate 2D pose heatmaps from the multiview images, and construct an aggregated feature volume V ? R K?L?W ?H by backprojecting the heatmaps to the 3D voxel space. Since people are usually on the ground plane and it is less probable that one person is right on top of another, it inspires us to construct a 2D bird's-eye view representation from the feature volume for efficiently detecting people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection in xy Plane</head><p>We re-project the aggregated feature volume to the ground plane (xy) by performing max-pooling along the z direction and obtain F (xy) ? R K?L?W . Then we feed F (xy) to a 2D fully convolutional network to detect the locations of people in the xy plane. The positions of all people in the plane are encoded by a 2D confidence map? (xy) ? [0, 1] L?W whose value?</p><formula xml:id="formula_1">(xy) i,j</formula><p>represents the likelihood of human presence at the location (i, j). For training supervision, we generate the ground-truth (GT) 2D confidence map H <ref type="bibr">(xy)</ref> . Its values are computed by the distance between the GT center point and each grid point using a Gaussian kernel. Specifically, the confidence value of grid point (i, j) is computed by:</p><formula xml:id="formula_2">H (xy) i,j = max 1?n?N exp{? (i ?? n ) 2 + (j ?j n ) 2 2? 2 }</formula><p>where N denotes the number of persons and (? n ,j n ) represents the corresponding GT position for person n. We just keep the largest scores in the presence of multiple people. The mean squared error (MSE) loss is computed by:</p><formula xml:id="formula_3">L 2d = L i=1 W j=1 ?H (xy) i,j ?? (xy) i,j ? 2<label>(1)</label></formula><p>We further estimate a 2D box size for each person instead of assuming a loose constant size as in the previous work <ref type="bibr" target="#b38">[39]</ref>. The height of the box is simply set to be 2000mm. This is critical to isolate the interference of multiple people, especially in crowded scenes. Our model generates a box size embedding at all grid points, denoted as? ? R 2?L?W . But only those at the locations with large confidences are meaningful. We compute a ground-truth size embedding S based on box annotations.</p><p>During training, we only compute losses on the grid points which are adjacent to the ground-truth box centers. Specifically, for a 2D GT box center (x,?), we only add supervision on the discretized grid points (?x l ?, ?? w ?), where l represents the length of a single voxel and w denotes the width. Let U denote the set of the neighboring points mentioned above and suppose N is the number of persons in the image. We compute an L 1 loss at each center point in U:</p><formula xml:id="formula_4">L size = 1 N (i,j)?U ?S i,j ?? i,j ? 1<label>(2)</label></formula><p>In addition, to reduce the quantization error, we estimate the local offset for each root joint on the horizontal plane. Similar to size estimation, the model outputs an offset prediction at each grid point, denoted as? ? R 2?L?W . We generate a GT offset prediction O and use an L 1 loss on the neighboring points:</p><formula xml:id="formula_5">L of f = 1 N (i,j)?U ?O i,j ?? i,j ? 1<label>(3)</label></formula><p>Inspired by <ref type="bibr" target="#b62">[63]</ref>, we use a simple network structure with three parallel branches to estimate the heatmap, offset and size respectively. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the 2D bird's-eye features are passed through a fully-convolutional backbone network and then fed into three separate branches with identical designs, which consist of a 3 ? 3 convolution, ReLU, and another 1 ? 1 convolution.</p><p>Detection in z Axis The remaining task is to estimate the center height for each proposal. Firstly, we obtain the proposals with P largest confidences on the 2D heatmap? (xy) after applying non-maximum suppression (NMS). We set P = 10 in our experiments. Subsequently, we extract the corresponding 1D "columns" for each proposal from the aggregated feature volume V, denoted as F (z) ? R P ?K?H , which is then fed into a 1D fully convolutional network to regress the height. Similar to 2D detection, our model generates 1D heatmap estimation? For each person, we first construct its local feature volume V ? . The person-specific feature volume V s is obtained by masking V ? with the detected 3D box. We re-project V s to three orthogonal coordinate planes to get the 2D feature maps P (t) . A shared 2D pose estimator regresses the joint locations J (t) for each plane, and a confidence network computes the corresponding weights W (t) . Finally, the 3D poseJ is computed by weighting J (t) with W (t) in a pairwise manner. (t ? {xy, xz, yz})</p><p>based on its center height using the Gaussian distribution. Likewise, we use an MSE loss here:</p><formula xml:id="formula_6">L 1d = 1 P P p=1 H k=1 ?H (z) p,k ?? (z) p,k ? 2<label>(4)</label></formula><p>Finally, we select the height with maximum confidence and by combining it with the 2D box center, offset and size, we can obtain the 3D bounding box. The overall confidence score for each box is computed by multiplying the scores of the 2D heatmap and 1D outputs. According to the exponential property of the Gaussian function, it can be regarded as an approximate of the 3D Gaussian distribution. We set a threshold for confidence scores to select the valid proposals. To sum up, the overall training objective is as follows:</p><formula xml:id="formula_7">L HDN = L 2d + ? size L size + ? of f L of f + ? 1d L 1d<label>(5)</label></formula><p>where we set ? size = 0.02, ? of f = 0.1 and ? 1d = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Localization Networks</head><p>Person-specific Feature Volume. With the bounding box of each person, we construct its fine-grained feature volume to predict the final 3D pose. We first crop a smaller feature volume V ? from V centered at the box center with a fixed size (i.e. 2m ? 2m ? 2m). It suffices to cover arbitrary poses and maintains the relative scale of the motion space. The space is then divided into L ? ?W ? ?H ? voxels.</p><p>Now the key step is to zero out the features outside the estimated bounding box to get the person-specific feature volume V s . This masking mechanism reduces the distraction of other people and enables safe volume re-projection in the following stage.</p><p>Joint Localization. To reduce the computational cost, we re-project V s onto three orthogonal 2D planes, i.e. the xy plane, xz plane and yz planes in the world coordinate systems. Let P (xy) ? R K?L ? ?W ? , P (xz) ? R K?L ? ?H ? and P (yz) ? R K?W ? ?H ? denote the re-projected feature maps corresponding to the three planes, respectively. Again, we use max-pooling for feature projection. Subsequently, they are concatenated as a batch and fed to a 2D CNN for joint localization, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Note that we set the same granularity of voxels on different axes to enable parallel estimation, i.e. L ? = W ? = H ? . The 2D CNN produces a joint-wise heatmap estimation for each re-projection plane, denoted as? (t) (t ? {xy, xz, yz}) in the same shape of P (t) . To reduce the quantization error, we compute the center of mass of? (t) instead of taking the maximum responses. Specifically, the estimated positions? (t) ? R K?2 are computed by:</p><formula xml:id="formula_8">J (xy) = L i=1 W j=1 (i, j) ?? (xy) i,? J (xz) = L i=1 H k=1 (i, k) ?? (xz) i,k J (yz) = W j=1 H k=1 (j, k) ?? (yz) j,k<label>(6)</label></formula><p>We supervise the estimations with the ground-truth 2D location J (t) ? R K?2 on each plane. An L 1 loss is computed by:</p><formula xml:id="formula_9">L hm = t K k=1 ?J (t) k ?? (t) k ? 1<label>(7)</label></formula><p>Adaptive Weighted Fusion. The quality of P (t) and the difficulty of pose estimation naturally vary with the re-projection plane and human pose, thus we hope the model could learn to discriminate and balance the estimations from different planes automatically. To achieve this, we introduce a lightweight confidence regression network. We assume that the pattern of? (t) could reflect the quality of 2D pose estimation. Therefore, the estimated heatmaps? (t) are fed into a shared confidence regression network. Inspired by <ref type="bibr" target="#b58">[59]</ref>, we adopt a simple design for the confidence regression network, consisting of a convolutional layer, a global average pooling layer and one fully-connected layer. The network generates joint-wise fusion weight for each plane, denoted as W (t) ? R K . We then use the Softmax function for normalization in a pair-wise manner and obtain the final 3D predictionJ ? R K?3 . Specifically, for the joint k, the final estimations can be computed by: , namely the component on the x -axis, and the other notations have similar interpretations. Let J denote the GT 3D pose, we use an L 1 loss to train the confidence regression network:</p><formula xml:id="formula_10">J k,1 = softmax(W (xy) k , W (xz) k ) ? (? (xy) k,1 ,? (xz) k,1 ) J k,2 = softmax(W (xy) k , W (yz) k ) ? (? (xy) k,2 ,? (yz) k,1 ) J k,3 = softmax(W (xz) k , W (yz) k ) ? (? (xz) k,2 ,? (yz) k,2 )<label>(8)</label></formula><formula xml:id="formula_11">L conf = K k=1 ?J k ?J k ? 1<label>(9)</label></formula><p>Now we get the overall training objective of JLN as follows. In our experiments, we set ? conf = 1.</p><formula xml:id="formula_12">L JLN = L hm + ? conf L conf<label>(10)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Datasets. The Shelf <ref type="bibr" target="#b1">[2]</ref> dataset captures four people disassembling a shelf using five cameras. We select the frames of test set following previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>. The Campus <ref type="bibr" target="#b1">[2]</ref> dataset captures multiple people interacting with each other in an outdoor environment shot by three cameras. The CMU Panoptic <ref type="bibr" target="#b18">[19]</ref> dataset captures multiple people engaging in social activities. We use the same training and testing sequences captured by five HD cameras as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Training Strategies. Due to incomplete annotations of Shelf and Campus, we use synthetic 3D poses to train the model for the two datasets, following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>. For the Panoptic dataset, we first finetune the 2D heatmap estimation network. Then we fix the 2D network and train the 3D networks following <ref type="bibr" target="#b38">[39]</ref>.</p><p>Evaluation Metrics. Following the common practice, we compute the Percentage of Correct Parts (PCP3D) metric on Shelf and Campus. Specifically, we pair each GT pose with the closest estimation and calculate the percentage of correct parts. For the Panoptic dataset, we adopt the Average Precision (AP K ) and Mean Per Joint Position Error (MPJPE) as metrics, which reflect the quality of multi-person 3D pose estimation more comprehensively. In addition, we measure the inference time and frame per second (FPS) on the Panoptic dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation and Comparison</head><p>Evaluation of HDN. We first evaluate the performance of the Human Detection Networks qualitatively. As <ref type="figure" target="#fig_5">Fig.5</ref> shows, our model is able to detect the human centers and estimate the 3D bounding boxes as intended, despite the fact that severe occlusion occurs in all views. Accurate 3D bounding boxes help to isolate the persons for the fine-grained joint localization. In addition, we quantitatively measure the performance of HDN in terms of both center position and bounding box. As Tab.1 shows, our HDN localizes the root joint well, and the regressed bounding boxes overlap with GT mostly. The mean center error is larger than the MPJPE of JLN because JLN involves detailed pose estimation on a finer voxel granularity. Still, the center precision suffices to provide a reasonable 3D bounding box for joint localization.   Evaluation of JLN. We compare the 3D pose estimation performance with the state-of-the-art (SOTA) multi-view multi-person 3D pose estimation methods on Shelf and Campus <ref type="bibr" target="#b1">[2]</ref>. While the proposed method is primarily optimized for inference efficiency and makes several approximations, it performs competitively with SOTA as shown in Tab.2. On the Shelf dataset, it outperforms the SOTA volumetric approach VoxelPose <ref type="bibr" target="#b38">[39]</ref> which features fully 3D convolutional architecture. We also train and test on the Panoptic <ref type="bibr" target="#b18">[19]</ref> dataset following the most recent works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>. As shown in Tab. 3, our method receives an extra per-joint error of about 2mm. We argue that the error margin is within an acceptable range given the speed-accuracy trade-off in real-time applications.</p><p>Efficiency. We first compare the inference speed of our method to the SOTA methods, and then conduct an in-depth efficiency analysis. The speed results of other methods are obtained using their official codes on the same hardware as ours. For a fair comparison, we set the batch size to be one for all methods during inference following <ref type="bibr" target="#b44">[45]</ref> to simulate the real-time use case where data arrives frame by frame. The batch size of PlaneSweepPose <ref type="bibr" target="#b22">[23]</ref> was set to be 64 in the original paper so their reported speed is different from the one reported in this paper. For all the methods, the off-the-shelf 2D pose estimator time is not measured following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. The results on the Panoptic dataset are shown in Tab. 3. Our approach shows a considerable advantage in terms of inference speed and supports real-time inference. The inference time broken down per module is shown in <ref type="figure">Fig. 6</ref>. The "others" parts mainly consist of data preparation and feature volume construction. For HDN, the time cost is independent of the number of cameras and persons. For JLN, the theoretical computation complexity is linear to the number of persons. In practice, feature maps of different persons are concatenated as a batch and inferred in a single feedforward. As we only use the re-projected 2D feature maps, the batch size could be large enough to cover very crowded scenes. In general, the time cost of our method is mainly determined by voxel granularity. By using 2? coarser voxel, its computation complexity could be reduced to <ref type="bibr">1 4</ref> . The voxel granularity selection serves as a trade-off between speed and accuracy.</p><p>Finally, we analyze the scalability of our method and compare it with the existing methods. Consider applying the algorithms to a challenging scenario that is much larger and more crowded than the current datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>. In order to retain a reasonable coverage, the number of cameras needs to grow proportionally <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54]</ref>. VoxelPose <ref type="bibr" target="#b38">[39]</ref> uses massive 3D convolution operations that are computation-intensive, and its efficiency disadvantage would be enlarged when scaling. PlaneSweepPose <ref type="bibr" target="#b22">[23]</ref> needs to enumerate the depth planes for every pair of camera views and persons. As a result, the computation complexity increases in polynomials regarding the number of cameras and persons. For example, simply shifting from Campus (3 persons, 3 cameras) to Shelf (4 persons, 5 cameras) slows PlaneSweepPose by 2.6? according to <ref type="bibr" target="#b22">[23]</ref> (1.3? for our method). MvP <ref type="bibr" target="#b44">[45]</ref> uses projective attention to integrate the multi-view information, and its time cost also grows quadratically as camera number increases. As previously analyzed, our method does not involve explicit view-person association, and its speed is mainly affected by the granularity of space division. We argue that the above characteristics make our method more scalable to large, crowded scenes than the previous methods. We deployed our model to a basketball court and a retail store where the space size is 16m?16m with 12 cameras and 10 people. Our inference time increases by 28.8% compared to that on Panoptic (8m ? 8m, 5 cameras, 3.4 persons).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We train some ablated models to study the impact of the individual factors. All the ablation experiments are evaluated on CMU Panoptic <ref type="bibr" target="#b18">[19]</ref>, and the results are shown in Tab. 4.</p><p>Feature Masking. In (b), we remove the masking step and directly use the local feature volume V ? in JLN. This is equivalent to using a fixed bounding box size as <ref type="bibr" target="#b38">[39]</ref>. The degraded performance indicates that the masking mechanism indeed reduces the ambiguity and helps joint localization.</p><p>Adaptive Weighted Fusion. In (c), we simply take the mean of the estimated coordinates from different planes to compute the final result. The performance gap suggests that the learned confidence weights emphasize the more reliable estimations as intended.</p><p>Number of Cameras. In (d)-(e), we compare the performance under different camera numbers. The accuracy drops with fewer camera views as the feature volume coverage is weakened.</p><p>Granularity of Voxels. We study the impact of voxel granularity on both efficiency and accuracy. Tab. 5. shows models trained with different JLN voxel sizes. By reducing the number of voxels (effectively increasing the individual voxel size), the error increases slightly while the inference efficiency improves. It inspires us to balance the trade-off between speed and accuracy in real usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a novel method for 3D human pose estimation from multi-view images. Our pipeline uniquely integrates the feature volume re-projection to both human detection and joint localization, which substitutes the computationintensive 3D convolutions. Experiment results prove the effectiveness of the proposed HDN and JLN. The accelerated inference demonstrates the potential of our method in real-time applications, especially for large scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Implementation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Human Detection Networks</head><p>Following <ref type="bibr" target="#b38">[39]</ref>, we discretize the overall motion space into L ? W ? H voxels. In our experiments, we set L = W = 80 and H = 20. Inspired by <ref type="bibr" target="#b38">[39]</ref>, we adopt a similar Encoder-Decoder architecture in the Human Detection Networks. The key difference is that we replace all expensive 3D convolutions with 2D and 1D convolutions. The basic components of our fully-convolutional networks include vanilla convolutional block and residual convolution block. The former is comprised of one convolutional layer, one batch-norm layer and ReLU while the latter consists of two consecutive basic convolutional blocks with residual connection. At the initial stage, the feature volume is fed into a 7 ? 7 convolutional layer. In the subsequent Encoder structure, the feature representation is downsampled through three 3 ? 3 residual convolutional blocks with maxpooling. The Decoder adopts a symmetric design, but with deconvolution operations. Finally, the network generates the results through a 1 ? 1 convolutional layer. Following <ref type="bibr" target="#b62">[63]</ref>, the outputs of 2D networks are fed into three branches to estimate the feature map, the local offset and the size of the bounding box respectively. They share an identical design, which consists of a 3 ? 3 convolution, ReLU and another 1 ? 1 convolution.</p><p>The 1D convolutional network shares the same architecture with its 2D counterpart except for two aspects: 1) all convolutional operations are replaced with 1D convolutions 2) we just maintain the branch for estimating feature maps along the z axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Joint Localization Networks</head><p>The architecture of Joint Localization Networks is essentially the same as one 2D CNN branch of HDN. The outputs of 2D estimators are further fed into a shared confidence network, which consists of one convolutional layer, one global average pooling layer plus a fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training</head><p>We train HDN and JLN jointly to convergence. On the CMU Panoptic dataset, our model is trained 10 epochs with batch size 8. On the Shelf and Campus datasets, we train our model for 30 epochs with the same batch size. The learning rate is set to be ? = 0.0001 using Adam <ref type="bibr" target="#b19">[20]</ref> optimizer. The parameters above are empirically determined.</p><p>In the bounding box regression branch of HDN, we add a safety margin ? = 200mm to GT, as missing information of body joints will be fatal to the subsequent prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head><p>B.1 Dataset CMU Panoptic <ref type="bibr" target="#b18">[19]</ref> This dataset captures multiple people engaging in social activities in an indoor setting. It contains massive sequences in various scenarios. We use the sequences captured by five HD cameras <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23)</ref>. The training and testing split is identical with <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Shelf <ref type="bibr" target="#b1">[2]</ref> This dataset captures four people disassembling a shelf using five cameras. We follow previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> in evaluating only three of the four persons on the test set frames 300-600 since one person is severely occluded. Due to the lack of complete annotations of ground-truth poses, we train with synthetic heatmaps following previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Campus [2]</head><p>This dataset captures multiple people interacting with each other in an outdoor environment by three cameras. We follow previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> and perform evaluation on the test set frames 350-470, 650-750. Similar to the Shelf dataset, we also conduct training on synthetic heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Evaluation Metrics</head><p>PCP For the Percentage of Correct Parts, we pair each GT pose with the closest estimation and calculate the percentage of correct parts. Specifically, the match is counted as correct if their distance is within a threshold T . Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>, we set T to be half of the corresponding limb length. Note that PCP does not penalize false positive results.</p><p>AP K In order to evaluate the results more comprehensively, we follow <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> to measure the Average Precision (within Kmm). Specifically, a predicted joint is considered as correct if there is a corresponding GT joint within distance threshold K.</p><p>MPJPE We first pair the nearest GT for each predicted joint, then calculate the corresponding Mean Per Joint Position Error in millimeters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head><p>We present additional qualitative results in <ref type="figure" target="#fig_7">Fig. 7</ref>. Please refer to the attached video for more results.  In addition, we study the influence of the number of persons on inference time. The results are shown in <ref type="table">Table.</ref> 6. The time increase is mainly on the feature construction phase of JLN. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Problem Decomposition. (A): Considering a single person, we reproject its feature volume to the coordinate planes with orthographic projection. The partial coordinates can be estimated by 2D CNN and assembled to 3D estimation. (B): Multi-person brings the extra challenge of ambiguity and occlusion. Nonetheless, people can be easily isolated from the bird's-eye view of the aggregated feature volume. Based on the intuitive ideas, we develop the lightweight Joint Localization Networks and Human Detection Networks respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Human Detection Networks. We first construct the feature volume V from the multi-view images. It is then projected to the xy plane to obtain the feature map F (xy) (bird's-eye view). A Multi-branch 2D CNN estimates three feature maps encoding each person's center position, bounding box size, and center offset, respectively. We then select the 1D columns feature F (z) from the positions with high confidence values on?(xy)  . Then a 1D CNN estimates the heatmap? (z) of the vertical position of the 3D box center. Finally, HDN outputs the combined 3D bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>(z) ? [0, 1] P ?H , indicating the likelihood of human presence at every possible height. We compute a GT 1D heatmap H (z) for each proposal Joint Localization Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>where? (xy) k, 1</head><label>1</label><figDesc>denotes taking the first component of the 2D estimated coordinates of? (xy) k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative Results on the CMU Panoptic Dataset. The first row illustrates the estimated root joints in HDN. The second row shows the estimated 2D poses on the three orthogonal re-projection planes and the fused 3D pose in JLN. The last row shows the 2D back-projection of the estimated 3D pose to each camera view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Results on sequence Band.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Additional Results on the CMU Panoptic Dataset. We present the results on three different action sequences. For each figure, the first row illustrates the estimated root joints in HDN. The second row shows the estimated 2D poses on three orthogonal re-projection planes and the fused 3D pose in JLN. The last row shows the 2D back-projection of the estimated 3D pose to each camera view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Evaluation of HDN. We measure the mean center error, precision and recall rate to evaluate the quality of human center detection and offset regression. The IoU score is computed between the estimated horizontal bounding box and GT, which additionally reflects the precision of bounding box size estimation.</figDesc><table><row><cell>Mean Center Error (mm)</cell><cell>Precision</cell><cell>Recall</cell><cell>IoU</cell></row><row><cell>53.73</cell><cell>0.9982</cell><cell>0.9985</cell><cell>0.757</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with SOTA on Campus and Shelf. We compute the PCP3D (Percentage of Correct Parts) metrics following previous work. A part is considered correct if its distance with GT is at most half of the limb length.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Shelf</cell><cell></cell><cell></cell><cell cols="2">Campus</cell><cell></cell></row><row><cell>Method</cell><cell>Actor1</cell><cell>Actor2</cell><cell>Actor3</cell><cell>Average</cell><cell>Actor1</cell><cell>Actor2</cell><cell>Actor3</cell><cell>Average</cell></row><row><cell>Belagiannis et al . [2]</cell><cell>66.1</cell><cell>65.0</cell><cell>83.2</cell><cell>71.4</cell><cell>82.0</cell><cell>72.4</cell><cell>73.7</cell><cell>75.8</cell></row><row><cell>Belagiannis et al . [4]</cell><cell>75.0</cell><cell>67.0</cell><cell>86.0</cell><cell>76.0</cell><cell>83.0</cell><cell>73.0</cell><cell>78.0</cell><cell>78.0</cell></row><row><cell>Belagiannis et al . [3]</cell><cell>75.3</cell><cell>69.7</cell><cell>87.6</cell><cell>77.5</cell><cell>93.5</cell><cell>75.7</cell><cell>84.4</cell><cell>84.5</cell></row><row><cell>Ershadi-Nasab et al . [11]</cell><cell>93.3</cell><cell>75.9</cell><cell>94.8</cell><cell>88.0</cell><cell>94.2</cell><cell>92.9</cell><cell>84.6</cell><cell>90.6</cell></row><row><cell>Dong et al . [9]</cell><cell>98.8</cell><cell>94.1</cell><cell>97.8</cell><cell>96.9</cell><cell>97.6</cell><cell>93.3</cell><cell>98.0</cell><cell>96.3</cell></row><row><cell>Huang et al . [16]</cell><cell>98.8</cell><cell>96.2</cell><cell>97.2</cell><cell>97.4</cell><cell>98.0</cell><cell>94.8</cell><cell>97.4</cell><cell>96.7</cell></row><row><cell>Tu et al . [39]</cell><cell>99.3</cell><cell>94.1</cell><cell>97.6</cell><cell>97.0</cell><cell>97.6</cell><cell>93.8</cell><cell>98.8</cell><cell>96.7</cell></row><row><cell>Lin et al . [23]</cell><cell>99.3</cell><cell>96.5</cell><cell>98.0</cell><cell>97.9</cell><cell>98.4</cell><cell>93.7</cell><cell>99.0</cell><cell>97.0</cell></row><row><cell>Wang et al . [45]</cell><cell>99.3</cell><cell>95.1</cell><cell>97.8</cell><cell>97.4</cell><cell>98.2</cell><cell>94.1</cell><cell>97.4</cell><cell>96.6</cell></row><row><cell>Ours</cell><cell>99.4</cell><cell>96.0</cell><cell>97.5</cell><cell>97.6</cell><cell>96.5</cell><cell>94.1</cell><cell>97.9</cell><cell>96.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>AP25</cell><cell>AP50</cell><cell>AP100</cell><cell>AP150</cell><cell>MPJPE</cell><cell>Time</cell><cell>FPS</cell></row><row><cell>VoxelPose [39]</cell><cell>83.59</cell><cell>98.33</cell><cell>99.76</cell><cell>99.91</cell><cell>17.68mm</cell><cell>316.0ms</cell><cell>3.2</cell></row><row><cell>PlaneSweepPose [23]</cell><cell>92.12</cell><cell>98.96</cell><cell>99.81</cell><cell>99.84</cell><cell>16.75mm</cell><cell>234.3ms</cell><cell>4.3</cell></row><row><cell>MvP [45]</cell><cell>92.28</cell><cell>96.60</cell><cell>97.45</cell><cell>97.69</cell><cell>15.76mm</cell><cell>278.8ms</cell><cell>3.6</cell></row><row><cell>Ours</cell><cell>85.22</cell><cell>98.08</cell><cell>99.32</cell><cell>99.48</cell><cell>18.26mm</cell><cell>32.2ms</cell><cell>31.1</cell></row></table><note>Comparison with SOTA on Panoptic. For efficiency metrics, We measure the average per-sample inference time on Panoptics test set (5 camera views, 3.41 person per frame). The measurement is done on a Linux machine with GPU GeForce RTX 2080 Ti and CPU Intel(R) Xeon(R) CPU E5-2699A v4 @ 2.40GHz. Batch size is set to be 1 for all methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Time Cost Visualization. We compute the average inference time cost for each module on the Panoptic test set in milliseconds. It takes 32.2ms in total.</figDesc><table><row><cell>2D CNN</cell><cell>1D CNN</cell><cell>Others</cell><cell>2D CNN</cell><cell>Others</cell></row><row><cell>7.4</cell><cell>6.9</cell><cell>3.0</cell><cell>7.5</cell><cell>7.4</cell></row><row><cell cols="2">HDN (17.3ms)</cell><cell></cell><cell cols="2">JLN (14.9ms)</cell></row><row><cell>Fig. 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study Results. Our full approach is (a). From (b) to (e), we study the effect of volume feature masking, weighted fusion and camera views respectively.</figDesc><table><row><cell cols="5">Method #Views Mask Weighted AP25 AP50 AP100 AP150 MPJPE</cell></row><row><cell>(a)</cell><cell>5</cell><cell>?</cell><cell>?</cell><cell>85.22 98.08 99.32 99.48 18.26mm</cell></row><row><cell>(b)</cell><cell>5</cell><cell></cell><cell>?</cell><cell>72.05 96.75 99.10 99.39 21.07mm</cell></row><row><cell>(c)</cell><cell>5</cell><cell>?</cell><cell></cell><cell>77.23 97.61 99.18 99.48 20.11mm</cell></row><row><cell>(d)</cell><cell>4</cell><cell>?</cell><cell>?</cell><cell>73.95 97.02 99.21 99.35 21.12mm</cell></row><row><cell>(e)</cell><cell>3</cell><cell>?</cell><cell>?</cell><cell>53.68 91.89 97.40 98.30 26.13mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Influence of Voxel Granularity. We additionally report the MACs (Multiply-Accumulate Operations) and number of parameters of the networks.</figDesc><table><row><cell>JLN Voxels</cell><cell>AP25</cell><cell>AP50</cell><cell>AP100</cell><cell>AP150</cell><cell>MPJPE</cell><cell>MACs</cell><cell>Parameters</cell></row><row><cell>64 ? 64 ? 64</cell><cell>85.22</cell><cell>98.08</cell><cell>99.32</cell><cell>99.48</cell><cell>18.26mm</cell><cell>8.670G</cell><cell>1.236M</cell></row><row><cell>48 ? 48 ? 48</cell><cell>78.76</cell><cell>97.14</cell><cell>98.99</cell><cell>99.14</cell><cell>19.66mm</cell><cell>4.877G</cell><cell>1.210M</cell></row><row><cell>32 ? 32 ? 32</cell><cell>73.20</cell><cell>97.37</cell><cell>98.93</cell><cell>99.08</cell><cell>20.47mm</cell><cell>2.167G</cell><cell>1.190M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Experiment of scalability. We measure the average inference time cost of each module in milliseconds (ms) while varying the number of persons present in the synthetic scene. HDN 18.27 17.90 17.71 18.22 18.28 18.50 17.37 17.86 18.30 18.45 JLN 13.16 13.67 14.22 15.03 16.72 18.40 20.70 21.22 24.01 25.88 Total 31.43 31.57 31.93 33.25 35.00 36.90 38.07 39.08 42.31 44.33</figDesc><table><row><cell>Num.</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by MOST-2018AAA0102004 and NSFC-62061136001.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coolmoves: User motion accentuation in virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gonzalez-Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMWUT</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiple human pose estimation with temporally consistent 3d pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time multi-view 3d human pose estimation using semantic feedback to smart edge sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bultmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Occlusion-Aware networks for 3D human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Motion capture from internet videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple human 3d pose estimation from multiview images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Compressed volumetric heatmaps for multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA, 2 edn</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7779" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">End-to-end dynamic matching network for multi-view multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Traish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<title level="m">Learnable triangulation of human pose</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How relevant are incidental power poses for hci</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CHI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Online knowledge distillation for efficient pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-view multi-person 3d pose estimation with plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Voxel-based 3d detection and reconstruction of multiple objects from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Context modeling in 3d human pose estimation: A unified perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<title level="m">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pischulini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15190" to="15200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lightweight multi-view 3d pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rukhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorontsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards fast and accurate multi-person pose estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>IJCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing actions in 3d using actionsnippets and activated simplices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Robust 3d human pose estimation from single images or video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1227" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mining 3d key-pose-motifs for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2639" to="2647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Direct multi-view multi-person 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Voxel-based network for shape completion by leveraging edge generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H A J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Photo wake-up: 3d character animation from a single photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph-Based 3D Multi-Person pose estimation using Multi-View images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning multi-agent coordination for enhancing target coverage in directional sensor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Vipnas: Efficient video pose estimation via neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Humbi: A large multiview dataset of human body expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Fast human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A camera network tracking (camnet) dataset and performance baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Staudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Faltemier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Voxeltrack: Multi-person 3d human pose estimation and tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02452</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Voxeltrack: Multi-person 3d human pose estimation and tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">4d association graph for realtime multi-person motion capture using multiple video cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fusing wearable imus with multi-view images for human pose estimation: A geometric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2200" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adafuse: Adaptive multiview fusion for accurate human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="703" to="718" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<title level="m">3d human pose estimation with spatial and temporal transformers. ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">VIN: voxel-based implicit network for joint 3d object detection and segmentation for lidars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Reconstructing nba players</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
