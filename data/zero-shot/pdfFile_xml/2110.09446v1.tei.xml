<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Squeezing Backbone Feature Distributions to the Max for Efficient Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange Labs, Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Lab-STICC</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6285</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<postCode>F-29238</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange Labs, Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Lab-STICC</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6285</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<postCode>F-29238</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Squeezing Backbone Feature Distributions to the Max for Efficient Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few-Shot learning</term>
					<term>Inductive and Transductive Learning</term>
					<term>Transfer Learning</term>
					<term>Optimal Transport</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot classification is a challenging problem due to the uncertainty caused by using few labelled samples. In the past few years, many methods have been proposed with the common aim of transferring knowledge acquired on a previously solved task, what is often achieved by using a pretrained feature extractor. Following this vein, in this paper we propose a novel transfer-based method which aims at processing the feature vectors so that they become closer to Gaussian-like distributions, resulting in increased accuracy. In the case of transductive few-shot learning where unlabelled test samples are available during training, we also introduce an optimal-transport inspired algorithm to boost even further the achieved performance. Using standardized vision benchmarks, we show the ability of the proposed methodology to achieve state-of-the-art accuracy with various datasets, backbone architectures and few-shot settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Thanks to their outstanding performance, Deep Learning methods have been widely considered for vision tasks such as image classification and object detection. In order to reach top performance, these systems are typically trained using very large labelled datasets that are representative enough of the inputs to be processed afterwards.</p><p>However, in many applications, it is costly to acquire or to annotate data, resulting in the impossibility to create such large labelled datasets. Under this condition, it is challenging to optimize Deep Learning architectures considering the fact they typically are made of way more parameters than the dataset can efficiently tune. This is the reason why in the past few years, few-shot learning (i.e. the problem of learning with few labelled examples) has become a trending research subject in the field. In more details, there are two settings that authors often consider: a) "inductive few-shot", where only a few labelled samples are available during training and prediction is performed on each test input independently, and b) "transductive few-shot", where prediction is performed on a batch of (non-labelled) test inputs, allowing to take into account their joint distribution.</p><p>Many works in the domain are built based on a "learning to learn" guidance, where the pipeline is to train an optimizer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> with different tasks of limited data so that the model is able to learn generic experience for novel tasks. Namely, the model learns a set of initialization parameters that are in an advantageous position for the model to adapt to a new (small) dataset. Recently, the trend evolved towards using well-thought-out transfer architectures (called backbones) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> trained one time on the same training data, but seen as a unique large dataset.</p><p>A main problem of using features extracted using a backbone pretrained architecture is that their distribution is likely to be unorthodox, as the problem the backbone has been optimized for most of the time differs from that it is then used upon. As such, methods that rely on strong assumptions about the feature distributions tend to have limitations on leveraging their quality. In this paper, we propose an efficient feature preprocessing methodology that allows to boost the accuracy in few-shot transfer settings. In the case of transductive few-shot learning, we also propose an optimal transport based algorithm that allows reaching even better performance. Using standardized benchmarks in the field, we demonstrate the ability of the proposed method to obtain state-of-the-art accuracy, for various problems and backbone architectures. hj(k) PEME <ref type="figure" target="#fig_3">Figure 1</ref>: Illustration of the proposed method. First we train a feature extractor f? using D base that has a large number of labelled data. Then we extract feature vectors of all the inputs (support set S and query set Q) in D novel (the considered few-shot dataset). We preprocess them with proposed PEME, which contains power transform that has the effect of mapping a skewed feature distribution into a gaussian-like distribution (h j (k) denotes the histogram of feature k in class j). The result feature vectors are denoted by f S ? f Q . In the case of transductive learning, we introduce another step called Boosted Min-size Sinkhorn (BMS), where we perform a modified Sinkhorn algorithm with class weight parameters w j initialized on labelled feature vectors f S to obtain the class allocation matrix P for the inputs, and we update the weight parameters for the next iteration. After nsteps we evaluate the accuracy on f Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>A large volume of works in few-shot classification is based on meta learning <ref type="bibr" target="#b2">[3]</ref> methods, where the training data is transformed into few-shot learning episodes to better fit in the context of few examples. In this branch, optimization based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> train a well-initialized optimizer so that it quickly adapts to unseen classes with a few epochs of training. Other works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> apply data augmentation techniques to artificially increase the size of the training data in order for the model to generalize better to unseen data.</p><p>In the past few years, there have been a growing interest in transfer-based methods. The main idea consists in training feature extractors able to efficiently segregate novel classes it never saw before. For example, in <ref type="bibr" target="#b5">[6]</ref> the authors train the backbone with a distance-based classifier <ref type="bibr" target="#b14">[15]</ref> that takes into account the interclass distance. In <ref type="bibr" target="#b6">[7]</ref>, the authors utilize self-supervised learning techniques <ref type="bibr" target="#b15">[16]</ref> to co-train an extra rotation classifier for the output features, improving the accuracy in few-shot settings. Aside from approaches focused on training a more robust model, other approaches are built on top of a pre-trained feature extractor (backbone). For instance, in <ref type="bibr" target="#b16">[17]</ref> the authors implement a nearest class mean classifier to associate an input with a class whose centroid is the closest in terms of the 2 distance. In <ref type="bibr" target="#b17">[18]</ref> an iterative approach is used to adjust the class prototypes. In <ref type="bibr" target="#b7">[8]</ref> the authors build a graph neural network to gather the feature information from similar samples. Generally, transfer-based techniques often reach the best performance on standardized benchmarks.</p><p>Although many works involve feature extraction, few have explored the features in terms of their distribution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7]</ref>. Often, assumptions are made that the features in a class align to a certain distribution, even though these assumptions are seldom experimentally discussed. In our work, we analyze the impact of the features distributions and how they can be transformed for better processing and accuracy. We also introduce a new algorithm to improve the quality of the association between input features and corresponding classes in typical few-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. Let us highlight the main contributions of this work. (1)</head><p>We propose to preprocess the raw extracted features in order to make them more aligned with Gaussian assumptions. Namely we introduce transforms of the features so that they become less skewed. (2) We use a Wasserstein-based method to better align the distribution of features with that of the considered classes.</p><p>(3) We show that the proposed method can bring large increase in accuracy with a variety of feature extractors and datasets, leading to state-of-the-art results in the considered benchmarks. This work is an extended version of <ref type="bibr" target="#b8">[9]</ref>, with the main difference that here we consider the broader case where we do not know the proportion of samples belonging to each considered class in the case of transductive few-shot, leading to a new algorithm called Boosted Min-size Sinkhorn. We also propose more efficient preprocessing steps, leading to overall better performance in both inductive and transductive settings. Finally, we introduce the use of Logistic Regression in our methodology instead of a simple Nearest Class Mean classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section we introduce the problem statement. We also discuss the various steps of the proposed method, including training the feature extractors, preprocessing the feature representations, and classifying them. Note that we made the code of our method available at https://github.com/yhu01/BMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem statement</head><p>We consider a typical few-shot learning problem. Namely, we are given a base are unlabelled. In the case of inductive few-shot, the prediction is performed independently on each one of the query samples. In the case of transductive few-shot <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref>, the prediction is performed considering all unlabelled samples together. Contrary to our previous work <ref type="bibr" target="#b8">[9]</ref>, we do not consider knowing the proportion of samples in each class in the case of transductive few-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature extraction</head><p>The first step is to train a neural network backbone model using only the base dataset. In this work we consider multiple backbones, with various training procedures. Once the considered backbone is trained, we obtain robust embeddings that should generalize well to novel classes. We denote by f ? the backbone function, obtained by extracting the output of the penultimate layer from the considered architecture, with ? being the trained architecture parameters. Thus considering an input vector x, f ? (x) is a feature vector with d dimensions that can be thought of as a simpler-to-manipulate representation of x. Note that importantly, in all backbone architectures used in the experiments of this work, the penultimate layers are obtained by applying a ReLU function, so that all feature components coming out of f ? are nonnegative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature preprocessing</head><p>As mentioned in Section 2, many works hypothesize, explicitly or not, that the features from the same class are aligned with a specific distribution (often Gaussian-like). But this aspect is rarely experimentally verified. In fact, it is very likely that features obtained using the backbone architecture are not Gaussian.</p><p>Indeed, usually the features are obtained after applying a ReLU function <ref type="bibr" target="#b21">[22]</ref>, and exhibit a positive and yet skewed distribution mostly concentrated around 0 (more details can be found in the next section).</p><p>Multiple works in the domain <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> discuss the different statistical methods (e.g. batch normalization) to better fit the features into a model. Although these methods may have provable assets for some distributions, they could worsen the process if applied to an unexpected input distribution. This is why we propose to preprocess the obtained raw feature vectors so that they better align with typical distribution assumptions in the field. Denote f ? (x) =</p><formula xml:id="formula_0">[f 1 ? (x), ..., f h ? (x), ..., f d ? (x)] ? (R + ) d ,</formula><p>x ? D novel as the obtained features on D novel , and f h ? (x), 1 ? h ? d denotes its value in the h th position. The preprocessing methods applied in our proposed algorithms are as follows:</p><p>Euclidean normalization. Also known as L2-normalization that is widely used in many related works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8]</ref>, this step scales the features to the same area so that large variance feature vectors do not predominate the others.</p><p>Euclidean normalization can be given by:</p><formula xml:id="formula_1">f ? (x) ? f ? (x) f ? (x) 2<label>(1)</label></formula><p>Power transform. Power transform method <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref> simply consists of taking the power of each feature vector coordinate. The formula is given by:</p><formula xml:id="formula_2">f h ? (x) ? (f h ? (x) + ) ? , ? = 0<label>(2)</label></formula><p>where = 1e ? 6 is used to make sure that f ? (x) + is strictly positive in every position, and ? is a hyper-parameter. The rationale of the preprocessing above is that power transform, often used in combination with euclidean normalization, has the functionality of reducing the skew of a distribution and mapping it to a close-to-gaussian distribution, adjusted by ?. After experiments, we found that ? = 0.5 gives the most consistent results for our considered experiments, which corresponds to a square-root function that has a wide range of usage on features <ref type="bibr" target="#b23">[24]</ref>. We will analyse this ability and the effect of power transform in more details in Section 4. Note that power transform can only be applied if considered feature vectors contain nonnegative entries, which will always be the case in the remaining of this work.</p><p>Mean subtraction. With mean subtraction, each sample is translated using m ? (R + ) d , the projection center. This is often used in combination with euclidean normalization in order to reduce the task bias and better align the feature distributions <ref type="bibr" target="#b17">[18]</ref>. The formula is given by:</p><formula xml:id="formula_3">f ? (x) ? f ? (x) ? m<label>(3)</label></formula><p>The projection center is often computed as the mean values of feature vectors related to the problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. In this paper we compute it either as the mean feature vector of the base dataset (denoted as M b ) or the mean vector of the novel dataset (denoted as M n ), depending on the few-shot settings. Of course, in both of these cases, the rationale is to consider a proxy to what would be the exact mean value of feature vectors on the considered task.</p><p>In our proposed method we deploy these preprocessing steps in the following order: Power transform (P) on the raw features, followed by an Euclidean normalization (E). Then we perform Mean subtraction (M) followed by another Euclidean normalization at the end. For simplicity we denote PEME as our proposed preprocessing order, in which M can be either M b or M n as mentioned above. In our experiments, we found that using M b in the case of inductive fewshot learning and M n in the case of transductive few-shot learning consistently led to the most competitive results. More details on why we used this methodology are available in the experiment section.</p><p>When facing an inductive problem, a simple classifier such as a Nearest-Class-Mean classifier (NCM) can be used directly after this preprocessing step. The resulting methodology is denoted PEM b E-NCM. But in the case of transductive settings, we also introduce an iterative procedure, denoted BMS for Boosted</p><p>Min-size Sinkhorn, meant to leverage the joint distribution of unlabelled samples.</p><p>The resulting methodology is denoted PEM n E-BMS. The details of the BMS procedure are presented thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Boosted Min-size Sinkhorn</head><p>In the case of transductive few-shot, we introduce a method that consists in iteratively refining estimates for the probability each unlabelled sample belong to any of the considered classes. This method is largely based on the one we </p><formula xml:id="formula_4">L(?) = i log P (l(x i ) = j|x i ; ?) = i log P (x i , l(x i ) = j; ?) P (x i ; ?) ? i log P (x i |l(x i ) = j; ?) P (x i ; ?) ,<label>(4)</label></formula><p>here l(x i ) denotes the class label for sample x i ? Q ? S, P (x i ; ?) denotes the marginal probability, and ? represents the model parameters to estimate.</p><p>Assuming a gaussian distribution on the input features for each class, here we define ? = w j , ?j where w j ? R d stand for the weight parameters for class j.</p><p>We observe that Eq. 4 can be related to the cost function utilized in Optimal Transport <ref type="bibr" target="#b24">[25]</ref>, which is often considered to solve classification problems, with constrains on the sample distribution over classes. To that end, a well-known</p><p>Sinkhorn <ref type="bibr" target="#b25">[26]</ref> mapping method is proposed. The algorithm aims at computing a class allocation matrix among novel class data for a minimum Wasserstein distance. Namely, an allocation matrix P ? R</p><formula xml:id="formula_5">(l+u)?n + is defined where P[i, j]</formula><p>denotes the assigned portion for sample i to class j, and it is computed as follows:</p><formula xml:id="formula_6">P = Sinkhorn(C, p, q, ?) = argmi? P?U(p,q) ijP [i, j]C[i, j] + ?H(P),<label>(5)</label></formula><p>where U(p, q) ? R (l+u)?n + is a set of positive matrices for which the rows sum to p and the columns sum to q, p denotes the distribution of the amount that each sample uses for class allocation, and q denotes the distribution of the amount of samples allocated to each class. Therefore, U(p, q) contains all the possible ways of allocation. In the same equation, C can be viewed as a cost matrix that is of the same size as P, each element in C indicates the cost of its corresponding position in P. We will define the particular formula of the cost function for each position C[i, j], ?i, j in details later on in the section. As for the second term on the right of 5, it stands for the entropy ofP:</p><formula xml:id="formula_7">H(P) = ? ijP [i, j] logP[i, j],</formula><p>regularized by a hyper-parameter ?. Increasing ? would force the entropy to become smaller, so that the mapping is less diluted. This term also makes the objective function strictly convex <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> and thus a practical and effective computation. From lemma 2 in <ref type="bibr" target="#b25">[26]</ref>, the result of Sinkhorn allocation has the typical form P = diag(u) ? exp(?C/?) ? diag(v). It is worth noting that here we assume a soft class allocation, meaning that each sample can be "sliced" into different classes. We will present our proposed method in details in the next paragraphs.</p><p>Given all that are presented above, in this paper we propose an Expectation-Maximization (EM ) <ref type="bibr" target="#b27">[28]</ref> based method which alternates between updating the allocation matrix P and estimating the parameter ? of the designed model, in order to minimize Eq. 5 and maximize Eq. 4. For a starter, we define a weight matrix W with n columns (i.e one per class) and d rows (i.e one per dimension of feature vectors), for column j in W we denote it as the weight parameters w j ? R d for class j in correspondence with Eq. 4. And it is initialized as follows:</p><formula xml:id="formula_8">w j = W[:, j] = c j / c j 2 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">c j = 1 s x?S, (x)=j f ? (x).<label>(7)</label></formula><p>We can see that W contains the average of feature vectors in the support set for each class, followed by a L2-normalization on each column so that w j 2 = 1, ?j.</p><p>Then, we iterate multiple steps that we describe thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Computing costs</head><p>As previously stated, the proposed algorithm is an EM -like one that iterately updates model parameters for optimal estimates. Therefore, this step along with</p><p>Min-size Sinkhorn presented in the next step, is considered as the E -step of our proposed method. The goal is to find membership probabilities for the input samples, namely, we compute P that minimizes Eq. 5.</p><p>Here we assume gaussian distributions, features in each class have the same variance and are independent from one another (covariance matrix ? = I? 2 ).</p><p>We observe that, ignoring the marginal probability, Eq. 4 can be boiled down to negative L2 distances between extracted samples f ? (x i ), ?i and w j , ?j, which is initialized in Eq. 6 in our proposed method. Therefore, based on the fact that w j and f ? (x i ) are both normalized to be unit length vectors (f ? (x i ) being preprocessed using PEME introduced in the previous section), here we define the cost between sample i and class j to be the following equation:</p><formula xml:id="formula_10">C[i, j] ? (f ? (x i ) ? w j ) 2 = 1 ? w T j f ? (x i ),<label>(8)</label></formula><p>which corresponds to the cosine distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Min-size Sinkhorn</head><p>In <ref type="bibr" target="#b8">[9]</ref>, we proposed a Wasserstein distance based method in which the Sinkhorn algorithm is applied at each iteration so that the class prototypes are for iter = 1 to 50 do</p><formula xml:id="formula_11">P[i, :] ? p[i] ? P[i,:] j P[i,j] , ?i P[:, j] ? q[j] ? P[:,j] i P[i,j] if i P[i, j] &lt; q[j]</formula><p>, ?j end for return P updated iteratively in order to find their best estimates. Although the method showed promising results, it is established on the condition that the distribution of the query set is known, e.g. a uniform distribution among classes on the query set. This is not ideal given the fact that any priors about Q should be supposedly kept unknown when applying a method. The methodology introduced in this paper can be seen as a generalization of that introduced in [9] that does not require priors about Q.</p><p>In the classical settings, Sinkhorn algorithm aims at finding the optimal matrix P, given the cost matrix C and regulation parameter ? presented in Eq. 4). Typically it initiates P from a softmax operation over the rows in C, then it iterates between normalizing columns and rows of P, until the resulting matrix becomes close-to doubly stochastic according to p and q. However, in our case we do not know the distribution of samples over classes. To address this, we firstly introduce the parameter k, initialized so that k ? s, meant to track an estimate of the cardinal of the class containing the least number of samples in the considered task. Then we propose the following modification to be applied to the matrix P once initialized: we normalize each row as in the classical case, but only normalize the columns of P for which the sum is less than the previously computed min-size k <ref type="bibr" target="#b17">[18]</ref>. This ensures at least k elements allocated for each class, but not exactly k samples as in the balanced case.</p><p>The principle of this modified Sinkhorn solution is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. Updating weights</head><p>This step is considered as the M -step of the proposed algorithm, in which we use a variant of the Logistic Regression algorithm in order to find the model parameter ? in the form of weight parameters w j for each class. Note that w j , if normalized, is equivalent to the prototype for class j in this case. Given the fact that in Eq. 4 we also take into account the marginal probability, which can be further broken down as:</p><formula xml:id="formula_12">P (x i ; ?) = j P (x i |l(x i ) = j; ?)P (l(x i ) = j),<label>(9)</label></formula><p>we observe that Eq. 4 corresponds to applying a softmax function on the negative logits computed through a L2-distance function between samples and class prototypes (normalized). This fits the formulation of a linear hypothesis between f ? (x i ) and w j for logit calculations, hence the rationale for utilizing Logistic Regression in our proposed method.</p><p>The procedure of this step is as follows: now that we have a polished allocation matrix P, we firstly initialize the weights w j as follows:</p><formula xml:id="formula_13">w j ? u j / u j 2 ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_14">u j ? i P[i, j]f ? (x i )/ i P[i, j].<label>(11)</label></formula><p>We can see that elements in P are used as coefficients for feature vectors to linearly adjust the class prototypes <ref type="bibr" target="#b8">[9]</ref>. Similar to Eq. 6, here w j is the normalized newly-computed class prototype that is a vector of length 1.</p><p>Next we further adjust weights by applying a logistic regression, the optimization is performed by minimizing the following loss:</p><formula xml:id="formula_15">1 l + u ? i j ?log( exp (S[i, j]) n ?=1 exp (S[i, ?]) ) ? P[i, j],<label>(12)</label></formula><p>where S ? R (l+u)?n contains the logits, each element is computed as:</p><formula xml:id="formula_16">S[i, j] = ? ? w T j f ? (x i ) w j 2 .<label>(13)</label></formula><p>Note that ? is a scaling parameter, it can also be seen as a temperature parameter that adjusts the confidence metric to be associated to each sample. And it is learnt jointly with W.</p><p>The deployed Logistic Regression comes with hyperparameters on its own.</p><p>In our experiments, we use an SGD optimizer with a gradient step of 0.1 and 0.8 as the momentum parameter, and we train over e epochs. Here we point out that e ? 0 is considered an influential hyperparameter in our proposed algorithm, e = 0 indicates a simple update of W as the normalized adjusted class prototypes (Eq. 10) computed from P in Eq. 11, without further adjustment of logistic regression. And also note that when e &gt; 0 we project columns of W to the unit hypersphere at the end of each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Estimating the class minimum size</head><p>We can now refine our estimate for the min-size k for the next iteration. To this end, we firstly compute the predicted label of each sample as follows:</p><formula xml:id="formula_17">(x i ) = arg max j (P[i, j]),<label>(14)</label></formula><p>which can be seen as the current (temporary) class prediction.</p><p>Then, we compute:</p><formula xml:id="formula_18">k = min j {k j },<label>(15)</label></formula><p>where k j = #{i,? (x i ) = j}, #{?} representing the cardinal of a set.</p><p>Summary of the proposed method. All steps of the proposed method are summarized in Algorithm 2. In our experiments, we also report the results obtained when using a prior about Q as in <ref type="bibr" target="#b8">[9]</ref>. In this case, k does not have to be estimated throughout the iterations and can be replaced with the actual exact targets for the Sinkhorn. We denote this prior-dependent version PEM n E-BMS * (with an added * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate the performance of the proposed method using standardized fewshot classification datasets: miniImageNet <ref type="bibr" target="#b28">[29]</ref>, tieredImageNet <ref type="bibr" target="#b29">[30]</ref>, CUB [31] </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>In order to stress the genericity of our proposed method with regards to the chosen backbone architecture and training strategy, we perform experiments using WRN <ref type="bibr" target="#b32">[33]</ref>, ResNet18 and ResNet12 <ref type="bibr" target="#b33">[34]</ref>, along with some other pretrained backbones (e.g. DenseNet <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17]</ref>). For each dataset we train the feature extractor with base classes and test the performance using novel classes.</p><p>Therefore, for each test run, n classes are drawn uniformly at random among novel classes. Among these n classes, s labelled examples and q unlabelled examples per class are uniformly drawn at random to form D novel . The WRN and</p><p>ResNet are trained following <ref type="bibr" target="#b6">[7]</ref>. In the inductive setting, we use our proposed preprocessing steps PEM b E followed by a basic Nearest Class Mean (NCM) classifier. In the transductive setting, the preprocessing steps are denoted as PEM n E in that we use the mean vector of novel dataset for mean subtraction, followed by BMS or BMS * depending on whether we have prior knowledge on the distribution of query set Q among classes. Note that we perform a QR decomposition on preprocessed features in order to speed up the computation for the classifier that follows. All our experiments are performed using n = 5, q = 15, s = 1 or 5. We run 10,000 random draws to obtain mean accuracy score and indicate confidence scores (95%) when relevant. For our proposed PEM n E-BMS,</p><p>we train e = 0 epoch in the case of 1-shot and e = 40 epochs in the case of 5-shot. As for PEM n E-BMS * we set e = 20 for 1-shot and e = 40 for 5-shot.</p><p>As for the regularization parameter ? in Eq. 5, it is fixed to 8.5 for all settings.</p><p>Impact of these hyperparameters is detailed in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art methods</head><p>Performance on standardized benchmarks. In the first experiment, we conduct our proposed method on different benchmarks and compare the performance with other state-of-the-art solutions. The results are presented in <ref type="table" target="#tab_0">Table 1</ref> and 2, we observe that our method reaches the state-of-the-art performance in both inductive and transductive settings on all the few-shot classification benchmarks. Particularly, the proposed PEM n E-BMS * brings important gains in both 1-shot and 5-shot settings, and the prior-independent PEM n E-BMS also obtains competitive results on 5-shot. Note that for tieredImageNet we implement our method based on a pre-trained DenseNet121 backbone following the procedure described in <ref type="bibr" target="#b16">[17]</ref>. From these experiments we conclude that the proposed method can bring an increase of accuracy with a variety of backbones and datasets, leading to state-of-the-art performance. In terms of execution time, we measured an average of 0.004s per run.</p><p>Performance on cross-domain settings. In this experiment we test our method in a cross-domain setting, where the backbone is trained with the base classes in miniImageNet but tested with the novel classes in CUB dataset. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the proposed method gives the best accuracy both in the case of 1-shot and 5-shot, for both inductive and transductive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>Generalization to backbone architectures. To further stress the interest of the ingredients on the proposed method reaching top performance, in <ref type="table" target="#tab_3">Table 4</ref> we investigate the impact of our proposed method on different backbone architectures and benchmarks in the transductive setting. For comparison purpose we also replace our proposed BMS algorithm with a standard K-Means algorithm where class prototypes are initialized with the available labelled samples for each class. We can observe that: 1) the proposed method consistently achieves the best results for any fixed backbone architecture, 2) the feature extractor trained on WRN outperforms the others with our proposed method on different benchmarks, 3) there are significant drops in accuracy with K-Means, which stresses the interest of BMS, and 4) the prior on Q (BMS vs BMS * ) has major interest for 1-shot, boosting the performance by an approximation of 1% on all tested feature extractors.</p><p>Preprocessing impact. In <ref type="table" target="#tab_4">Table 5</ref> we compare our proposed PEME with other preprocessing techniques such as Batch Normalization and the ones being used in <ref type="bibr" target="#b16">[17]</ref>. The experiment is conducted on miniImageNet (backbone: WRN).</p><p>For all that are put into comparison, we run either a NCM classifier or BMS after preprocessing, depending on the settings. The obtained results clearly show the interest of PEME compared with existing alternatives, we also observe that the power transform helps increase the accuracy on both inductive and transductive settings. We will further study its impact in details.</p><p>Effect of power transform. We firstly conduct a Gaussian hypothesis test    Pearson's methodology <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> and p = 1e ? 3, only one of the 640 ? 20 = 12800 tests return positive, suggesting a very low pass rate for raw features. However, after applying the power transform we record a pass rate that surpasses 50%, suggesting a considerably increased number of positive results for Gaussian tests.</p><p>This experiment shows the effect of power transform being able to adjust feature distributions into more gaussian-like ones.</p><p>To better show the effect of this proposed technique on feature distributions, we depict in <ref type="figure" target="#fig_4">Figure 2</ref> the distributions of an arbitrarily selected feature for 3 randomly selected novel classes of miniImageNet when using WRN, before and after applying power transform. We observe quite clearly that 1) raw features exhibit a positive distribution mostly concentrated around 0, and 2) power transform is able to reshape the feature distributions to close-to-gaussian distributions. We observe similar behaviors with other datasets as well. Moreover, in order to visualize the impact of this technique with respect to the position of feature points, in <ref type="figure">Figure 3</ref> we plot the feature vectors of randomly selected 3 classes from D novel . Note that all feature vectors in this experiment are reduced to a 3-dimensional ones corresponding to their largest eigenvalues. From <ref type="figure">Figure 3</ref> we can observe that power transform, often followed by a L2-normalization, can help shape the class distributions to become more gathered and Gaussian-like <ref type="bibr" target="#b45">[46]</ref>.</p><p>Influence of the number of unlabelled samples. In order to better understand the gain in accuracy due to having access to more unlabelled samples, we depict in <ref type="figure">Figure 4</ref> the evolution of accuracy as a function of q, when the number of classes n = 5 is fixed. Interestingly, the accuracy quickly reaches a close-to-asymptotical plateau, emphasizing the ability of the method to quickly exploit available information in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of hyperparameters.</head><p>In order to test how much impact the hyperparameters could have on our proposed method in terms of prediction  that pre-trains the backbone with the help of external illumination data for augmentation, followed by PT+MAP in <ref type="bibr" target="#b8">[9]</ref> for class center estimation. Here we use the same backbones as <ref type="bibr" target="#b51">[52]</ref>, and replace PT+MAP with our proposed BMS * at the same conditions. Results are presented in <ref type="table" target="#tab_5">Table 6</ref>. Note that we also show the re-implemented results of <ref type="bibr" target="#b51">[52]</ref>, and our method reaches superior performance on all tested benchmarks using external data in <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed method on Few-Shot Open-Set Recognition. Few-Shot</head><p>Open-Set Recognition (FSOR) as a new trending topic deals with the fact that there are open data mixed in query set Q that do not belong to any of the  <ref type="table" target="#tab_6">Table 7</ref> we apply our proposed PEME for feature preprocessing, followed by an NCM classifier, and compare the results with other state-of-the-art alternatives. We observe that our proposed method is able to surpass the others in terms of Accuracy and AUROC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Proposed method on merged features</head><p>In this section we investigate the effect of our proposed method on merged features. Namely, we perform a direct concatenation of raw feature vectors extracted from multiple backbones at the beginning, followed by BMS. In <ref type="table" target="#tab_7">Table 8</ref> we chose the feature vectors from three backbones (WRN, ResNet18</p><p>and ResNet12) and evaluated the performance with different combinations. We observe that 1) a direct concatenation, depending on the backbones, can bring about 1% gain in both 1-shot and 5-shot settings compared with the results in <ref type="table" target="#tab_3">Table 4</ref> with feature vectors extracted from one single feature extractor. 2) BMS * reached new state-of-the-art results on few-shot learning benchmarks with feature vectors concatenated from WRN, ResNet18 and ResNet12, given that no external data is used.</p><p>To further study the impact of the number of backbones on prediction accuracy, in <ref type="figure" target="#fig_7">Figure 6</ref> we depict the performance of our proposed method as a function of the number of backbones. Note that here we operate on feature vectors of 6 WRN backbones (dataset: miniImageNet) concatenated one after another, which makes a total of 6 slots corresponding to a 640 ? 6 = 3840 feature size. Each of them is trained the same way as in <ref type="bibr" target="#b6">[7]</ref>, and we randomly select the multiples of 640 coordinates within the slots to denote the number of concatenated backbones used. The performance result is the average of 100 random selections and we test with both BMS and BMS * for 1-shot, and BMS * for 5-shot. From <ref type="figure" target="#fig_7">Figure 6</ref> we observe that, as the number of backbones increases, there is a relatively steady growth in terms of accuracy in multiple settings of our proposed method, indicating the interest of BMS in merged features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we introduced a new pipeline to solve the few-shot classification problem. Namely, we proposed to firstly preprocess the raw feature vectors to better align to a Gaussian distribution and then we designed an optimaltransport inspired iterative algorithm to estimate the class prototypes for the  transductive setting. Our experimental results on standard vision benchmarks reach state-of-the-art accuracy, with important gains in both 1-shot and 5-shot classification settings. Moreover, the proposed method can bring gains with a variety of feature extractors, with few extra hyperparameters. Thus we believe that the proposed method is applicable to many practical problems. We also provide two versions of our proposed method, one being prior-dependent and one that does not require any knowledge on unlabelled data, and they both are able to bring important gains in accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>dataset D base and a novel dataset D novel such that D base ? D novel = ?. D base contains a large number of labelled examples from K different classes and can be used to train a generic feature extractor. D novel , also referred to as a task or episode in other works, contains a small number of labelled examples (support set S), along with some unlabelled ones (query set Q), all from n new classes that are distinct from the K classes in D base . Our goal is to predict the classes of unlabelled examples in the query set. The following parameters are of particular importance to define such a few-shot problem: the number of classes in the novel dataset n (called n-way), the number of labelled samples per class s (called s-shot) and the number of unlabelled samples per class q. Therefore, the novel dataset contains a total of l + u samples, where l = ns are labelled, and u = nq</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>introduced in [ 9 ]</head><label>9</label><figDesc>, except it does not require priors about samples distribution in each of the considered class. Denote i ? [1, ..., l + u] as the sample index in D novel and j ? [1, ..., n] as the class index, the goal is to maximize the following log post-posterior function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Min-size SinkhornInputs: C, p = 1 l+u , q = k1 n , ? Initializations: P = Sof tmax(??C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2</head><label>2</label><figDesc>Boosted Min-size Sinkhorn (BMS) Parameters: ?, e Inputs: Preprocessed f ? (x), ?x ? D novel = Q ? S Initializations: W as normalized mean vectors over the support set for each class (Eq. 6); Min-size k ? s. for iter = 1 to 20 do Compute cost matrix C using W (Eq. 8). # E-step Apply Min-size Sinkhorn to compute P (Algorithm 1). # E-step Update weights W using P with logistic regression (Eq. 10-13). # M -step Estimate class predictions? and min-size k using P (Eq. 14-15). end for return? and CIFAR-FS [12]. The miniImageNet dataset contains 100 classes randomly chosen from ILSVRC-2012 [32] and 600 images of size 84 ? 84 pixels per class. It is split into 64 base classes, 16 validation classes and 20 novel classes. The tieredImageNet dataset is another subset of ImageNet, it consists of 34 high-level categories with 608 classes in total. These categories are split into 20 meta-training superclasses, 6 meta-validation superclasses and 8 meta-test superclasses, which corresponds to 351 base classes, 97 validation classes and 160 novel classes respectively. The CUB dataset contains 200 classes of birds and has 11,788 images of size 84 ? 84 pixels in total, it is split into 100 base classes, 50 validation classes and 50 novel classes. The CIFAR-FS dataset has 100 classes, each class contains 600 images of size 32 ? 32 pixels. The splits of this dataset are the same as those in miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Distributions of an arbitrarily chosen feature for 3 novel classes before (left) and after (right) power transform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Plot of feature vectors (extracted from WRN) from randomly selected 3 classes. (left) Naive features. (right) Preprocessed features using power transform. accuracy, here we select two important hyperparameters that are used in BMS and observe their impact. Namely the number of training epochs e in logistic regression and the regulation parameter ? used for computing the prediction matrix P. In Figure 5 we show the accuracy of our proposed method as a function of e (top) and ? (bottom). Results are reported for BMS * in 1-shot settings, and for BMS in 5-shot settings. From the figure we can see a slight uptick of accuracy as e or ? increases, followed by a downhill when they become larger, implying an overfitting of the classifier.Proposed method on backbones pre-trained with external data. In this experiment, we compare our proposed method BMS * with the work in [Accuracy of 5-way, 1-shot classification setting on miniImageNet, CUB and CIFAR-FS as a function of q. Accuracy of proposed method on miniImageNet (backbone: WRN) as a function of training epoch e (top) and regulation parameter ? (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Accuracy of proposed method in different settings as a function of the number of backbones (dataset: miniImageNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>1-shot and 5-shot accuracy of state-of-the-art methods in the literature on miniIma-geNet and tieredImageNet, compared with the proposed solution.</figDesc><table><row><cell>miniImageNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>1-shot and 5-shot accuracy of state-of-the-art methods on CUB and CIFAR-FS.</figDesc><table><row><cell>CUB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>1-shot and 5-shot accuracy of state-of-the-art methods when performing cross-domain classification (backbone: WRN).</figDesc><table><row><cell>Setting</cell><cell>Method</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell>Baseline++ [6]</cell><cell>40.44 ? 0.75%</cell><cell>56.64 ? 0.72%</cell></row><row><cell></cell><cell>Manifold Mixup [48]</cell><cell>46.21 ? 0.77%</cell><cell>66.03 ? 0.71%</cell></row><row><cell>Inductive</cell><cell>S2M2 R [7]</cell><cell>48.24 ? 0.84%</cell><cell>70.44 ? 0.75%</cell></row><row><cell></cell><cell>PT+NCM [9]</cell><cell>48.37 ? 0.19%</cell><cell>70.22 ? 0.17%</cell></row><row><cell></cell><cell cols="3">PEM b E-NCM (ours) 50.71 ? 0.19% 73.15 ? 0.16%</cell></row><row><cell></cell><cell>LaplacianShot [39]</cell><cell>55.46%</cell><cell>66.33%</cell></row><row><cell></cell><cell>Transfer+SGC [8]</cell><cell>58.63 ? 0.25%</cell><cell>73.46 ? 0.17%</cell></row><row><cell>Transductive</cell><cell>PT+MAP [9]</cell><cell>63.17 ? 0.31%</cell><cell>76.43 ? 0.19%</cell></row><row><cell></cell><cell>PEM n E-BMS (ours)</cell><cell>62.93 ? 0.28%</cell><cell>79.10 ? 0.18%</cell></row><row><cell></cell><cell>PEM</cell><cell></cell><cell></cell></row></table><note>n E-BMS* (ours) 63.90 ? 0.31% 79.15 ? 0.18%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>1-shot and 5-shot accuracy of proposed method on different backbones and benchmarks. ? 0.25% 89.53 ? 0.13% 91.91 ? 0.18% 94.62 ? 0.09% 87.83 ? 0.22% 91.20 ? 0.15%</figDesc><table><row><cell cols="4">Comparison with k-means algorithm.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">miniImageNet</cell><cell cols="2">CUB</cell><cell cols="2">CIFAR-FS</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell>ResNet12</cell><cell>72.73 ? 0.23%</cell><cell>84.05 ? 0.14%</cell><cell>87.35 ? 0.19%</cell><cell>92.31 ? 0.10%</cell><cell>78.39 ? 0.24%</cell><cell>85.73 ? 0.16%</cell></row><row><cell>K-MEANS</cell><cell>ResNet18</cell><cell>73.08 ? 0.22%</cell><cell>84.67 ? 0.14%</cell><cell>87.16 ? 0.19%</cell><cell>91.97 ? 0.09%</cell><cell>79.95 ? 0.23%</cell><cell>86.74 ? 0.16%</cell></row><row><cell></cell><cell>WRN</cell><cell>76.67 ? 0.22%</cell><cell>86.73 ? 0.13%</cell><cell>88.28 ? 0.19%</cell><cell>92.37 ? 0.10%</cell><cell>83.69 ? 0.22%</cell><cell>89.19 ? 0.15%</cell></row><row><cell></cell><cell>ResNet12</cell><cell>77.62 ? 0.28%</cell><cell>86.95 ? 0.15%</cell><cell>90.14 ? 0.19%</cell><cell>94.30 ? 0.10%</cell><cell>81.65 ? 0.25%</cell><cell>88.38 ? 0.16%</cell></row><row><cell>BMS (ours)</cell><cell>ResNet18</cell><cell>79.30 ? 0.27%</cell><cell>87.94 ? 0.14%</cell><cell>90.50 ? 0.19%</cell><cell>94.29 ? 0.09%</cell><cell>84.16 ? 0.24%</cell><cell>89.39 ? 0.15%</cell></row><row><cell></cell><cell>WRN</cell><cell>82.07 ? 0.25%</cell><cell>89.51 ? 0.13%</cell><cell>91.01 ? 0.18%</cell><cell>94.60 ? 0.09%</cell><cell>86.93 ? 0.23%</cell><cell>91.18 ? 0.15%</cell></row><row><cell></cell><cell>ResNet12</cell><cell>79.03 ? 0.28%</cell><cell>87.01 ? 0.15%</cell><cell>91.34 ? 0.19%</cell><cell>94.32 ? 0.09%</cell><cell>82.87 ? 0.27%</cell><cell>88.43 ? 0.16%</cell></row><row><cell>BMS  *  (ours)</cell><cell>ResNet18</cell><cell>80.56 ? 0.27%</cell><cell>87.98 ? 0.14%</cell><cell>91.39 ? 0.19%</cell><cell>94.31 ? 0.09%</cell><cell>85.17 ? 0.25%</cell><cell>89.42 ? 0.16%</cell></row><row><cell></cell><cell>WRN</cell><cell>83.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>1-shot and 5-shot accuracy on miniImageNet (backbone: WRN) with different preprocessings on the extracted features.</figDesc><table><row><cell></cell><cell cols="2">Inductive (NCM)</cell><cell cols="2">Transductive (BMS)</cell></row><row><cell>Preprocessing</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>None</cell><cell>55.30 ? 0.21%</cell><cell>78.34 ? 0.15%</cell><cell>77.62 ? 0.26%</cell><cell>87.96 ? 0.13%</cell></row><row><cell>Batch Norm [49]</cell><cell>66.81 ? 0.20%</cell><cell>83.57 ? 0.13%</cell><cell>73.74 ? 0.21%</cell><cell>88.07 ? 0.13%</cell></row><row><cell>L2N [17]</cell><cell>65.37 ? 0.20%</cell><cell>83.46 ? 0.13%</cell><cell>73.84 ? 0.21%</cell><cell>88.15 ? 0.13%</cell></row><row><cell>CL2N [17]</cell><cell>63.88 ? 0.20%</cell><cell>80.85 ? 0.14%</cell><cell>73.12 ? 0.28%</cell><cell>86.47 ? 0.15%</cell></row><row><cell>EM b E</cell><cell>68.05 ? 0.20%</cell><cell>83.76 ? 0.13%</cell><cell>80.28 ? 0.26%</cell><cell>88.36 ? 0.13%</cell></row><row><cell>PEM b E</cell><cell cols="2">68.43 ? 0.20% 84.67 ? 0.13%</cell><cell>82.01 ? 0.26%</cell><cell>89.50 ? 0.13%</cell></row><row><cell>EM n E</cell><cell>\</cell><cell>\</cell><cell>80.14 ? 0.27%</cell><cell>88.39 ? 0.13%</cell></row><row><cell>PEM n E</cell><cell>\</cell><cell>\</cell><cell cols="2">82.07 ? 0.25% 89.51 ? 0.13%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Proposed method on backbones pre-trained with external data. Note that -re denotes the re-implementation of an existing method. (ours) 87.83 ? 0.23% 91.49 ? 0.15% supposed classes used for label predictions. Therefore this often requires a robust classifier that is able to classify correctly the non-open data as well as rejecting the open ones. In</figDesc><table><row><cell>Benchmark</cell><cell>Method</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell>Illu-Aug [52]</cell><cell>82.99 ? 0.23%</cell><cell>89.14 ? 0.12%</cell></row><row><cell>miniImageNet</cell><cell>Illu-Aug-re</cell><cell>83.53 ? 0.25%</cell><cell>89.38 ? 0.12%</cell></row><row><cell></cell><cell cols="3">PEM n E-BMS  *  (ours) 83.85 ? 0.25% 90.07 ? 0.12%</cell></row><row><cell></cell><cell>Illu-Aug [52]</cell><cell>94.73 ? 0.14%</cell><cell>96.28 ? 0.08%</cell></row><row><cell>CUB</cell><cell>Illu-Aug-re</cell><cell>94.63 ? 0.15%</cell><cell>96.06 ? 0.08%</cell></row><row><cell></cell><cell cols="3">PEM n E-BMS  *  (ours) 94.78 ? 0.15% 96.43 ? 0.07%</cell></row><row><cell></cell><cell>Illu-Aug [52]</cell><cell>87.73 ? 0.22%</cell><cell>91.09 ? 0.15%</cell></row><row><cell>CIFAR-FS</cell><cell>Illu-Aug-re</cell><cell>87.76 ? 0.23%</cell><cell>91.04 ? 0.15%</cell></row><row><cell></cell><cell>PEM</cell><cell></cell><cell></cell></row></table><note>n E-BMS*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Accuracy and AUROC of Proposed method for Few-Shot Open-Set Recognition.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">miniImageNet</cell><cell></cell><cell></cell><cell cols="2">tieredImageNet</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell cols="2">5-shot</cell><cell></cell><cell>1-shot</cell><cell cols="2">5-shot</cell></row><row><cell>Method</cell><cell>Acc</cell><cell>AUROC</cell><cell>Acc</cell><cell>AUROC</cell><cell>Acc</cell><cell>AUROC</cell><cell>Acc</cell><cell>AUROC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>1-shot and 5-shot accuracy on miniImageNet, CUB and CIFAR-FS on our proposedPEMnE-BMS with multi-backbones (backbone training procedure follows<ref type="bibr" target="#b6">[7]</ref>, '+' denotes a concatenation of backbone features).</figDesc><table><row><cell></cell><cell cols="2">miniImageNet</cell><cell cols="2">CUB</cell><cell cols="2">CIFAR-FS</cell></row><row><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>RN18+RN12</cell><cell>80.32%</cell><cell>89.07%</cell><cell>92.31%</cell><cell>95.62%</cell><cell>85.44%</cell><cell>90.58%</cell></row><row><cell>WRN+RN12</cell><cell>82.63%</cell><cell>90.43%</cell><cell>92.69%</cell><cell>95.96%</cell><cell>87.11%</cell><cell>91.50%</cell></row><row><cell>WRN+RN18</cell><cell>83.05%</cell><cell>90.57%</cell><cell>92.66%</cell><cell>95.79%</cell><cell>87.53%</cell><cell>91.70%</cell></row><row><cell>WRN+RN18+RN12</cell><cell>82.90%</cell><cell>90.64%</cell><cell>93.32%</cell><cell>96.31%</cell><cell>87.62%</cell><cell>91.84%</cell></row><row><cell cols="2">WRN+RN18+RN12  *  84.37%</cell><cell cols="5">90.69% 94.26% 96.32% 88.44% 91.86%</cell></row><row><cell>6?WRN  *</cell><cell cols="2">85.54% 91.53%</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>\</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : BMS* .</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">PEM b E-NCM (ours) 68.43% 72.10% 84.67% 80.04% 71.87% 75.44% 87.09% 83.85%</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJY0-Kcll" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A two-stage approach to few-shot learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2019.2959254</idno>
		<idno>doi:10. 1109/TIP.2019.2959254</idno>
		<ptr target="https://doi.org/10.1109/TIP.2019.2959254" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3336" to="3350" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkxLXnAcFQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph-based interpolation of feature vectors for accurate few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8164" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging the feature distribution in transferbased few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/1707.09835.arXiv:1707.09835</idno>
		<ptr target="http://arxiv.org/abs/1707.09835" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJGven05Y7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxnZh0ct7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Few-shot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2770" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8680" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="488" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/1911.04623.arXiv:1911.04623</idno>
		<ptr target="http://arxiv.org/abs/1911.04623" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tafssl: Taskadaptive feature sub-space learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="522" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving accuracy of nonparametric transfer learning via vector segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Hacene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vermet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2966" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Free lunch for few-shot learning: Distribution calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=JWOiYxMG92s" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyVuRiC5K7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units (relu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/1803.08375.arXiv:1803.08375</idno>
		<ptr target="http://arxiv.org/abs/1803.08375" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<title level="m">Exploratory data analysis</title>
		<meeting><address><addrLine>Reading, Mass.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Approximate fisher kernels of noniid image models for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1084" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Optimal transport: old and new</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional wasserstein distances: Efficient optimal transportation on geometric domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Goes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJcSzz-CZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<ptr target="http://www.bmva.org/bmvc/2016/papers/paper087/index.html" />
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<editor>R. C. Wilson, E. R. Hancock, W. A. P. Smith</editor>
		<meeting>the British Machine Vision Conference<address><addrLine>York, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09-19" />
		</imprint>
	</monogr>
	<note>Wide residual networks</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12203" to="12213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8808" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="741" to="756" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 16</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11660" to="11670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Transductive information maximization for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/2008.11297.arXiv:2008.11297</idno>
		<ptr target="https://arxiv.org/abs/2008.11297" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Transductive few-shot learning with meta-learned confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/2002.12017.arXiv:2002.12017</idno>
		<ptr target="https://arxiv.org/abs/2002.12017" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embedding propagation: Smoother manifold for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJgklhAcK7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Relational embedding for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09666</idno>
		<ptr target="https://arxiv.org/abs/2108.09666" />
		<imprint/>
	</monogr>
	<note type="report_type">CoRR abs/2108.09666.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Transfer learning based few-shot classification using optimal transport mapping from preprocessed latent space of backbone neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chobola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vasata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kord?k</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/2102.05176.arXiv:2102.05176</idno>
		<ptr target="https://arxiv.org/abs/2102.05176" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4136" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An omnibus test of normality for moderate and large sample sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diagostino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="1" to="348" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tests for departure from normality. empirical results for the distributions of b 2 and ? b</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agostino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Sill-net: Feature augmentation with separated illumination representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/2102.03539.arXiv:2102.03539</idno>
		<ptr target="https://arxiv.org/abs/2102.03539" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R M</forename><surname>J?nior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D O</forename><surname>Werneck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nearest neighbors distance ratio open-set classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Pazinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>De Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D S</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="386" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
	<note>Towards open set deep networks</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Few-shot open-set recognition using meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Few-shot open-set recognition by transformation consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12566" to="12575" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
