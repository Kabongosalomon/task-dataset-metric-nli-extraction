<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PUZZLE-CAM: IMPROVED LOCALIZATION VIA MATCHING PARTIAL AND FULL FEATURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><forename type="middle">Jo</forename><surname>Gynetworks</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST School of Computing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In-Jae</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST School of Computing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PUZZLE-CAM: IMPROVED LOCALIZATION VIA MATCHING PARTIAL AND FULL FEATURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image segmentation</term>
					<term>Deep learning</term>
					<term>Neu- ral Networks</term>
					<term>Weakly-supervised semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the gap for semantic segmentation performance from pixel-level supervision to image-level supervision. Most advanced approaches are based on class activation maps (CAMs) to generate pseudo-labels to train the segmentation network. The main limitation of WSSS is that the process of generating pseudo-labels from CAMs that use an image classifier is mainly focused on the most discriminative parts of the objects. To address this issue, we propose Puzzle-CAM, a process that minimizes differences between the features from separate patches and the whole image. Our method consists of a puzzle module and two regularization terms to discover the most integrated region in an object. Puzzle-CAM can activate the overall region of an object using image-level supervision without requiring extra parameters. In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 dataset. Code associated with our experiments is available at https://github.com/ OFRIN/PuzzleCAM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Semantic segmentation is a fundamental approach using convolutional neural networks (CNNs) with the aim of correctly predicting the pixel-wise classification of an image. Recently, fully-supervised semantic segmentation (FSSS) has achieved remarkable progress <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, producing large-scale training datasets with precise pixel-level annotations per image is considerably expensive and requires labor-intensive and time-consuming tasks. To solve this issue, many researchers have focused on weakly supervised semantic segmentation (WSSS), which is used to train networks using image-level annotations, scribbles, bounding boxes, * Corresponding Author and points. Image-level supervision can be more easily conducted than other approaches in a group of weak supervision processes. In this study, we only focused on learning semantic segmentation models using image-level supervision. Most previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> using WSSS are based on the class activation maps (CAMs) <ref type="bibr" target="#b6">[7]</ref> to achieve good performance. However, the generated CAMs are usually focused on small parts of the semantic objects to efficiently classify them, which prevents the segmentation models from learning pixel-level semantic knowledge. Moreover, we can see that the CAMs generated from isolated patches in the tiled image are different from those gained from the original image. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, CAMs of the tiled image comprising tiled patches are significantly inconsistent compared to those of the original image. The differences are factored in by enlarging the supervision gap between FSSS and WSSS by even more.</p><p>The above observations gave us the inspiration to address WSSS issues by using an attention-based feature learning method. Thus, we propose Puzzle-CAM for WSSS training to detect integrated regions of objects. Our method applies reconstructing regularization that corresponds to the generated CAMs from the tiled and original images to provide self-supervision. To improve the network prediction consistency further, we introduced a puzzle module that splits the image and merges CAMs generated from the tiled image.  Puzzle-CAM consists of a Siamese neural network with reconstructing regularization loss that reduces the differences between the original and merged CAMs. Our experiments yielded both quantitative and qualitative results that demonstrate the superiority of our approach. Our main contributions are as follows:</p><p>? We propose Puzzle-CAM that incorporates reconstructing regularization with a puzzle module, to effectively enhance the quality of CAMs without adding layers.</p><p>? Puzzle-CAM outperformed existing state-of-the-art methods with the same level of supervision on the PASCAL VOC 2012 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attention Mechanisms Using CNNs</head><p>These provides fine-grained information on the features learned in CNNs. Simonyan et al. <ref type="bibr" target="#b7">[8]</ref> used the error backpropagation strategy to visualize semantic regions whereas the combined attention model used the global average pooling (GAP) layer in the CNNs to generate the CAMs [7] more efficiently. Last, a final classifier is used to generate attention maps. To the best of our knowledge, which attention mechanism is chosen does not have a great effect on achieving high performance with WSSS, and so we based Puzzle-CAM on the combined attention model because it is more manageable than the other attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Weakly Supervised Semantic Segmentation</head><p>Unlike FSSS, which requires pixel-wise labels for an image, WSSS employs lower level labeling, such as bounding boxes <ref type="bibr" target="#b8">[9]</ref>, scribbles <ref type="bibr" target="#b9">[10]</ref>, and image-level classification labels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Recently, the performance of WSSS has been significantly boosted by incorporating the CAMs. Most previous WSSS methods refine the CAMs generated by the image classifier to approximate the segmentation mask <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref>. Affin-ityNet <ref type="bibr" target="#b3">[4]</ref> trains an additional network to learn similarities between the pixels, which often generates a transition matrix and multiplies with CAM to adjust its activation coverage. IRNet <ref type="bibr" target="#b10">[11]</ref> generates a transition matrix from the boundary activation map and extends the method to achieve weakly supervised instance segmentation and WSSS. SEAM <ref type="bibr" target="#b4">[5]</ref> aims to refine CAMs using a pixel correlation module that captures context appearance information for each pixel and alters the original CAMs by using learned affinity attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>The conventional CAM of a single image highlights the most representative areas of each class. Therefore, when generating the CAM of the same class for image patches, the model focuses on finding the key features of the class using only the part of the object. Thus, the merged CAM of image patches highlights the object area more accurately than the CAM of a single image. To take the aforementioned advantage, we propose Puzzle-CAM that trains a classifier using reconstruction loss to minimize the difference between the CAM of a single image and the merged CAM from image patches. By training a classification network using this reconstruction loss, the CAM covers more precisely the object area. The Puzzle-CAM contains designed loss functions to match the CAMs generated from a tiled image with the original image ( <ref type="figure" target="#fig_1">Fig. 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Employed CAM Method</head><p>We first introduce the CAM method for producing the initial attention map. Given feature extractor F and classifier ?, we can generate CAMs A that are the collection of CAMs for all of the classes. After training the classifier by image-level supervision, we apply the weights of the c-channel classifier as ? c on feature map f = F (I) from input image I to obtain the CAM of class c as follows:</p><formula xml:id="formula_0">A c = ? c f.<label>(1)</label></formula><p>The generated CAM is normalized by using the maximum value of A c . Finally, we obtain the CAMs for all of the classes (A) by concatenating A c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Puzzle Module</head><p>When matching partial and full features, the key is to narrow the gap between FSSS and WSSS. The puzzle module consists of tiling and merging modules. From input image I of size W ? H, the tiling module generates non-overlapping a tiled patches {I 1,1 , I 1,2 , I 2,1 , I 2,2 } of size W/2?H/2. Next, we generate A i,j CAMs for each I i,j . Finally, the merging module attaches all A i,j into a single CAMs A re that has the same shape as A s which is the CAMs of I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The Loss Design for Puzzle-CAM</head><p>We employed a GAP layer at the end of the network to incorporate prediction vector? = ?(G(A c )) for image classification and to adopt multi-label soft margin loss for the classification task. For notational convenience, we define Y t a?</p><formula xml:id="formula_1">Y t = ? , if Y = 1 1 ?? , otherwise (2) cls (? , Y ) = ?log(Y t ).<label>(3)</label></formula><p>The CAMs of the original (A s ) and tiled (A re ) images are converted using the GAP layer with prediction vector? Y s = G(A s ) and? re = G(A re ), respectively. The classification losses for the original and reconstructed images are respectively calculated as</p><formula xml:id="formula_2">L cls = cls (? s , Y ),<label>(4)</label></formula><formula xml:id="formula_3">L p?cls = cls (? re , Y ).<label>(5)</label></formula><p>These two classification losses are used to improve the performance of the image classification. To reinforce the CAMs from the original image, we added reconstructing regularization to correspond with the original and reconstructed CAMs. The reconstruction loss for the original CAM can be easily defined as</p><formula xml:id="formula_4">L re = A s ? A re 1 .<label>(6)</label></formula><p>In summary, the final loss can be written as L = L cls + L p?cls + ?L re . where ? is the balance of the weights for the different losses. The classification losses, L cls and L p?cls , are used to roughly estimate the region of the object. The reconstruction loss, L re , is used to narrow the gaps between the pixel-and image-level supervision processes. We report details of the network training settings and probe into the effectiveness of the proposed module in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We evaluated our method using the PASCAL VOC 2012 dataset <ref type="bibr" target="#b13">[14]</ref>. The dataset was separated into 1,464 images for training, 1,449 for validation, and 1,456 for testing. Following the experimental protocol used in previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, we took additional annotations from the Semantic Boundary Dataset <ref type="bibr" target="#b14">[15]</ref> to build an augmented training set with 10,582 images. The images were randomly re-scaled in the range of [320, 640] and then cropped to 512 ? 512 as the network inputs. For all experiments, we set ? = 4 as the maximum and linearly ramped up ? to its maximum value by half epochs. During inference, we utilized classifiers without the puzzle module. Thus, we adopted multi-scale and horizontal flip to generate pseudo-segmentation labels. We used four TITAN-RTX GPUs for the model to train the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>We conducted ablation studies on the main components of Puzzle-CAM by applying the mean Intersection-Over-Union (mIoU) metric <ref type="table" target="#tab_1">(Table 1)</ref>, for which the baseline was mIoU = 47.82%. With the proposed reconstructing regularization (L re ) of the tiled patches, the baseline was boosted to mIoU = 49.21%, while the proposed classification loss from the tiled patches (L p?cls ) was similar to the baseline. Both L re and L p?cls consistently improved the baseline by a 3.71%.</p><p>We visualized the CAMs by using combinations of loss functions for each of them (see Eq. 3). When the classification losses (L p?cls ) only are employed, the result will show no marginal differences. Meanwhile, when the reconstruction loss (L re ) only is employed, the result will show improved localization ability for some classes compared to the original but the method will fail to predict several classes. When both sets of losses are combined, the result will show improved localization without suffering classification losses.    <ref type="bibr" target="#b13">[14]</ref>. RW, random walk with AffinityNet <ref type="bibr" target="#b3">[4]</ref>; dCRF, dense conditional random field <ref type="bibr" target="#b15">[16]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Existing State-of-the-Art Methods</head><p>To further improve the accuracy of pseudo pixel-level annotations, we followed the approach in <ref type="bibr" target="#b3">[4]</ref> to train AffinityNet based on Puzzle-CAM. We adopted ResNeSt architecture that universally improves the learned feature representations to boost performance across image classification, object detection, instance segmentation and semantic segmentation. In <ref type="table" target="#tab_3">Table 2</ref>, we report the performances with the original CAMs used by the baseline AffinityNet <ref type="bibr" target="#b3">[4]</ref> and Puzzle-CAM. The final synthesized pseudo-labels achieved 74.67% mIoU on the PASCAL VOC 2012 train set. Puzzle-CAM was then used to train the segmentation model DeepLabv3+ <ref type="bibr" target="#b0">[1]</ref> with the ResNeSt-269 <ref type="bibr" target="#b17">[18]</ref> backbone using the pseudolabels in full supervision mode to achieve the final segmentation results. <ref type="table" target="#tab_5">Table 3</ref> reports a comparison of the mIoU values for the proposed method and the previous approaches. Compared to the baseline methods, Puzzle-CAM had remarkably improved performances on both the val and test sets with the same settings for training. <ref type="figure" target="#fig_3">Fig. 4</ref> shows some qualitative results on the val set that illustrate that the proposed method worked well on both large and small objects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we proposed the Puzzle-CAM algorithm to narrow the supervision gap between FSSS and WSSS using image-level labels. To improve the network for generating consistent CAMs, we designed a puzzle module and adopted reconstructing regularization to match partial and full features. Not only did Puzzle-CAM consistently generate features from local tiled patches but it also fitted the shape of the ground truth masks better. The segmentation network trained by our synthesized pixel-level pseudo-labels achieved state-of-the-art performance on the PASCAL VOC 2012 dataset, which proves the effectiveness of our approach. We believe that the concepts of Puzzle-CAM as a training module can be generalized and will benefit other weaklyand semi-supervised tasks, such as semantic and instance segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>CAMs generated from tiled and original images: (a) conventional CAMs from the original image, (b) generated CAMs from the tiled images, and (c) predicted CAMs by the proposed Puzzle-CAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The overall architecture of the proposed Puzzle-CAM showing the integration of reconstructing regularization and the puzzle module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of the prediction tags and CAMs by using combinations of loss functions. In (d), the final CAMs not only suppressed over-activation but also expanded the CAMs into complete object activation coverage .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative segmentation results on the PASCAL VOC 2012 val set. Top: original images. Middle: ground truth. Bottom: prediction of the segmentation model trained using the pseudo-labels from Puzzle-CAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Sep 2021 Tiling module Tiling module Backbone Network Backbone Network Merging module Merging module Reconstruction Loss</head><label></label><figDesc></figDesc><table /><note>arXiv:2101.11253v4 [cs.CV] 23</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of the Puzzle-CAM loss functions using ResNet-50 as the backbone.</figDesc><table><row><cell>L cls</cell><cell>L p?cls</cell><cell>Lre</cell><cell>mIoU (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>47.82</cell></row><row><cell></cell><cell></cell><cell></cell><cell>47.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell>49.21</cell></row><row><cell></cell><cell></cell><cell></cell><cell>51.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quality of the pseudo semantic segmentation labels in mIoU, evaluated on the PASCAL VOC 2012 training set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Puzzle-CAM and existing state-ofthe-art methods on the PASCAL VOC 2012 val and test datasets. I, image-level labels; S, external saliency models.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Supervision</cell><cell>val</cell><cell>test</cell></row><row><cell>AffinityNet [4]</cell><cell>Wide-ResNet-38</cell><cell>I</cell><cell cols="2">61.7 63.7</cell></row><row><cell>DSRG [12]</cell><cell>ResNet-101</cell><cell>I + S</cell><cell cols="2">61.4 63.2</cell></row><row><cell>SeeNet [13]</cell><cell>ResNet-101</cell><cell>I + S</cell><cell cols="2">63.1 62.8</cell></row><row><cell>IRNet [4]</cell><cell>ResNet-50</cell><cell>I</cell><cell cols="2">63.5 64.8</cell></row><row><cell>FickleNet [6]</cell><cell>ResNet-101</cell><cell>I + S</cell><cell cols="2">64.9 65.3</cell></row><row><cell>ICD [17]</cell><cell>ResNet-101</cell><cell>I</cell><cell cols="2">64.1 64.3</cell></row><row><cell>SEAM [5]</cell><cell>Wide-ResNet-38</cell><cell>I</cell><cell cols="2">64.5 65.7</cell></row><row><cell>Ours (Puzzle-CAM)</cell><cell>ResNeSt-101</cell><cell>I</cell><cell cols="2">66.9 67.7</cell></row><row><cell>Ours (Puzzle-CAM)</cell><cell>ResNeSt-269</cell><cell>I</cell><cell cols="2">71.9 72.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semisupervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with interpixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4283" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
