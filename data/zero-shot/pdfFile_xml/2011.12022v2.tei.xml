<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-DECODER DPRNN: HIGH ACCURACY SOURCE COUNTING AND SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhe</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-DECODER DPRNN: HIGH ACCURACY SOURCE COUNTING AND SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Source separation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an end-to-end trainable approach to singlechannel speech separation with unknown number of speakers. Our approach extends the MulCat source separation backbone with additional output heads: a count-head to infer the number of speakers, and decoder-heads for reconstructing the original signals. Beyond the model, we also propose a metric on how to evaluate source separation with variable number of speakers. Specifically, we cleared up the issue on how to evaluate the quality when the ground-truth has more or less speakers than the ones predicted by the model. We evaluate our approach on the WSJ0-mix datasets, with mixtures up to five speakers. We demonstrate that our approach outperforms state-of-the-art in counting the number of speakers and remains competitive in quality of reconstructed signals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Source separation is the task of decomposing a mixed signal into the original signals prior to the mixing procedure. This is an important task with many downstream applications, e.g., improve the accuracy of automatic speech recognition with multiple speakers <ref type="bibr" target="#b0">[1]</ref>, or separating out singing voices and music <ref type="bibr" target="#b1">[2]</ref>. Due to the recent progress in deep learning, supervised methods have received a lot of interest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. These works formulate the source separation as a regression problem, i.e., given the mixed signal regress the individual components. Various specialized deep-net architectures and losses have been proposed. For example, <ref type="bibr" target="#b7">[8]</ref> proposed a loss which is permutation invariant in the ordering of the speakers, or <ref type="bibr" target="#b6">[7]</ref> presented a dual-path RNN architecture to better capture both short and long-term features. However, these works have focused on the setting where the number of speakers is a priori known.</p><p>Recently, several works have also considered the case with variable number of speakers. For example, <ref type="bibr" target="#b8">[9]</ref> have proposed a method for separating variable number of speakers, where they train a different model for every number of speakers. At test time, they run an activity detector on the largest speaker model to determine to number of speakers and then run the corresponding model for source separation. Another work is <ref type="bibr" target="#b9">[10]</ref> where they have proposed to iteratively separate out one speaker at a time. While straight forward, these methods either require training multiple deep-nets or running multiple forward-passes at test-time, both of which scale linearly with the possible number of speakers.</p><p>To tackle the aforementioned issues, we propose to train a single model with multiple output heads: a count-head to infer the number of speakers, and multiple decoder heads to separate the signals. These output heads share the same backbone feature extractor. Therefore, our method requires a single pass through the network at test time and can be trained from end-to-end. Additionally, we propose a new metric for evaluating the separation of a variable number of speakers. In particular, our metric considers how to evaluate the quality of the reconstruction when the number of speakers differs between prediction and ground-truth.</p><p>We evaluate our approach on WSJ0-mix dataset, with up to five speaker mixtures. Our approach surpasses all existing approaches in terms of source counting and achieves similar performance to state-of-the-art models in source separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">APPROACH</head><p>We present a single model approach to source separation with a variable number of speakers, illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. In particular, we augment the standard source separation backbone with additional count-head and decoder-heads to support prediction of variable number of speakers in a single pass. In the following, we describe our approach in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Formulation</head><p>Let x denote the mixed input audio, and Y = {y n } denote the set of separated audios from each speaker. The goal is to learn a parametric function,</p><formula xml:id="formula_0">F ? (x) ? Y,<label>(1)</label></formula><p>with trainable parameters ?. One of the challenges is how to construct a model to handle variable number of outputs? For example, a standard deep-net has a fixed number of output dimensions and does not change between examples.</p><p>To mitigate this problem, we assume that the maximum number of speakers, K ? |Y| ?(x, Y) ? D, is known. In this case, we can model a deep-net to count the number of speakers and model a decoder-head for each number of speakers.  An overview of our proposed approach for handling variable number of speakers for source separation. Given a mixed signal x, our model predicts the number of speakers from the mixed signal and uses the corresponding decoder-head to separate the signal. In this case, decoder 2 is selected, hence a reconstruction of two speakers.</p><p>This allow us to dynamically select which decoder-head to run and output the correct number of speakers.</p><p>We propose a single end-to-end trainable deep-net to accomplish this. Our deep-net contains a count-head, which counts the number of speakers in the mixed-audio, and a list of decoder-heads to reconstruct audios for the corresponding number of speakers. These heads share input features extracted from a backbone network <ref type="bibr" target="#b8">[9]</ref>. In the remainder of this section, we describe the architecture details and training procedure for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Architecture</head><p>Our model contains a mixture encoder to transform waveform into encoding, and a backbone to extract source encoding from mixture encoding following <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b8">[9]</ref>. Instead of using a single decoder head with a fixed number of output channels, we replaced it with a set of decoder heads, each having a different number of output channels, where one channel contains source from one speaker. We also added a count head that chooses which decoder head to use during inference. Encoder &amp; Backbone: As in <ref type="bibr" target="#b8">[9]</ref>, we use convolution with ReLU to encode mixture waveform, then use repeated MulCat blocks as the backbone separation network. Count-Head: We train a speaker count head in as an additional branch in parallel with the decoder heads. Given the output tensor from the backbone network, we first linearly transform the feature dimension, then apply global average pooling and ReLU. We then linearly project the result to the set of possible decoder choices, and apply softmax to the output. Decoder-Heads: We use a list of decoders, as in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b8">[9]</ref>. For the k th decoder, given an input tensor with feature dimension N , we apply PReLU with a channel-independent param-eter, and use 1x1 convolution to project feature dimension to N ? k speakers. We then divide the projected tensor into k tensors, each with feature dimension N , and transform the tensor back to audio with overlap-and-add.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training</head><p>To train the count-head, we formulate it as a classification task, i.e., we minimize the cross-entropy loss,</p><formula xml:id="formula_1">L count(x,Y) = ? K k 1 |Y|=k ? logp(|Y| = k)(x), (2)</formula><p>where 1 denotes the indicator function andp(|Y| = k) denotes the predicted probability that the mixed input audio, x, has k speakers.</p><p>Next, to train the decoder-heads, we utilize the permutation invariant loss, uPit <ref type="bibr" target="#b7">[8]</ref>, on the decoder-head selected by the ground-truth number of speakers, i.e.,</p><formula xml:id="formula_2">L decoders (x, Y) = k 1 |Y|=k ? uPIT(Y,? k ),<label>(3)</label></formula><p>where? k denotes the output from the k th decoder-head and</p><formula xml:id="formula_3">uPIT(Y,? k ) = ? max ? n SI-SNR(y ?(n) ,? n k ),<label>(4)</label></formula><p>where ? denotes a permutation on the speaker channels, and SI-SNR stands for scale-invariant signal-to-noise ratio, as defined in <ref type="bibr" target="#b10">[11]</ref>. Finally, we balance the two losses with a hyper-parameter ?, and train over a dataset of paired mixed inputs and separated audio, i.e., D = {(x, Y)} is as follows, </p><formula xml:id="formula_4">min ? (x,Y)?D ? ? L count (x, Y) + (1 ? ?) ? L decoders (x, Y).<label>(5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Inference</head><p>At test time, the ground-truth number of speakers is not available. In this case, we use the estimated number of speakers from the count-head to select which decoder-head to run, therefore,? =??,? = arg max</p><formula xml:id="formula_5">kp (|Y| = k)<label>(6)</label></formula><p>is the final prediction given x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Evaluation Metric</head><p>Evaluating a system for source separation with variable number of speakers remains an open discussion. It may seem that standard metrics, e.g. SI-SNR, are directly applicable, however these metrics require the number of predicted signals and ground-truth signals to be identical. When the system incorrectly predicts the number of speakers, it is unclear how to compute SI-SNR. Prior work <ref type="bibr" target="#b8">[9]</ref> computes a metric as follows: Let? be the number of predicted speaker and S be the ground-truth. In case (a): When? &gt; S, they compute the correlation between all audio pairs and keep S speakers from the prediction. In case (b): When? &lt; S, they duplicate S ?? speakers with the highest correlation to the ground-truth samples. With the speaker number matched, they compute the standard SI-SNR. We note that this choice of duplication / dropping relies on the ground-truth signal. This is not desirable, as a post-processing procedure should not be dependent on the ground-truth.</p><p>We believe that it is more natural to add "silence" speakers, either ground-truth or the prediction, until the number of speakers between the ground-truth and prediction are identical. Intuitively, a two-speaker mixed signal can be thought of as a three-speaker mixed signal where one of the speakers is silence. However, we run into the issue that SI-SNR is equal to negative infinity if the signal is zero.</p><p>To avoid this, instead of padding with silence, we choose a negative penalty term P ref that would be defined as the approximation to the SI-SNR measured if padded by silence. We name this metric penalized-SI-SNR (P-SI-SNR). </p><formula xml:id="formula_6">Given dataset D = {(x, Y)}, P-SI-SNR is defined as 1 |D| (x,Y)?D 1 max(|Y|, |?|) (L match + L pad ),<label>(7)</label></formula><p>where? = y 1 , ..., y? ,? being the number of predicted sources, and L match and L pad are defined as follows:</p><formula xml:id="formula_7">L match = max ? min(|Y|,|?|) n=1</formula><p>SI-SNR(y ?(n) ,? n )</p><formula xml:id="formula_8">L pad = P ref ? |Y| ? |?| .<label>(8)</label></formula><p>We believe that our proposed metric is intuitive and naturally balances between the reconstruction quality and misclassifications in number of speakers. We will next discuss two possible choices of P ref .</p><p>Measuring P ref from data: One solution is to choose the "silence" as some zero-mean noise distribution. In this case, we measure the SNR empirically based on samples from WSJ0 recordings. We cut out 0.75 second noise segments from their start, repeat those segments to match the length of recordings, and measure the energy ratio between noise files and recordings. Based on the average of our measurements, we set P ref to be -30dB. Setting P ref as average SI-SNR: Another intuitive way to penalize SI-SNR is to have each underestimated or overestimated speaker cancel out the positive contribution to SI-SNR of a correctly predicted speaker. Therefore, we choose P ref to be the negative of the average SI-SNR from oracle source counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We first describe our implementation details for dataset preparation, training, testing, model architecture. Next, we provide quantitative comparisons with baselines and demonstrate that our approach achieves state-of-the-art performance in source counting and comparable performance in source separation.  <ref type="table">Table 3</ref>. P-SI-SNR of each model; For OR-PIT, result is computed by averaging the P-SI-SNR for both 2 and 3 speakers computed with 95.7% recall. Note that models with lower max speaker count generally have higher accuracy, since fewer classes implies a higher P-SI-SNR.</p><formula xml:id="formula_9">P ref = ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We train on WSJ0-2mix and WSJ0-3mix <ref type="bibr" target="#b3">[4]</ref>, in addition to WSJ0-4mix and WSJ0-5mix <ref type="bibr" target="#b8">[9]</ref>. We take 4second chunks of all audios with 2-second overlap, and pad any chunks at the end that are above 2 seconds. We remove all mixtures below 2s. As mixtures have length equal to the shortest source, those with more speakers are shorter and have less chunks. In our training set, 2, 3, 4, 5 speakers all have 20000 audios, and respectively have <ref type="bibr">[24773,</ref><ref type="bibr">19066,</ref><ref type="bibr">15986,</ref><ref type="bibr">13809]</ref> chunks.</p><p>Training Procedure: For each epoch, we use weighted resampling with replacement to ensure that chunks for each speaker number are sampled with equal probability. We set probability of choice for each chunk inversely proportional to number of chunks with the same speaker count. We train our model using Adam <ref type="bibr" target="#b12">[13]</ref> with learning rate 5e ? 4, decay of 0.94 every epoch, and batch size of 4. In total, we train our model for 40 epochs, which is much less than the 100 epochs in most previous papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Testing Procedure: Given an audio signal, We first transform the full audio into chunks. We use the count head to predict which decoder head to use for each chunk and select the decoder head with the highest votes. Using the selected decoder head, we compute separated sources for each chunk, and use the overlap regions to reorder the predicted source channels in a streaming fashion. Lastly, we use overlap-and-add to recover predicted sources for the full audio, and remove the padding at the end chunk.</p><p>Architecture Details: For the encoder, we use a convolution kernel size of 8, stride 4, and 256 feature channels. For backbone, we use LSTM with hidden layer size 256. Similar to <ref type="bibr" target="#b8">[9]</ref>, we use multi-stage loss, but do not use speaker ID loss for simplicity. During training, we train both the decoders and count-heads with multi-loss, with one set of output after each pair of Mulcat blocks. <ref type="bibr" target="#b0">1</ref> Comparisons with Baselines: Many of the systems for variable speaker source separation are not publicly available, therefore we cannot directly compare with them on our proposed P-SI-SNR. To compare, we use the reported numbers from their paper. Note that since we do not have the exact SI-SNR, in the case of speaker mismatch, we compute an <ref type="bibr" target="#b0">1</ref> See project page for more details: https://junzhejosephzhu. github.io/Multi-Decoder-DPRNN/ upper bound for the models using their published statistics on oracle SI-SNR and speaker counting accuracy.</p><p>For computing this upper bound, we assume that each misclassification of speaker number is an overestimate by one, and all the other channels have oracle SI-SNR. This is an upper bound because oracle SNR is always higher than non-oracle SNR, and the ratio of (contribution from correct channels)/penalty is greatest if the error is an overestimate by one channel. For a model with k speakers with oracle SI-SNR x and accuracy a, the upper bound for P-SI-SNR can be computed as</p><formula xml:id="formula_10">P-SI-SNR ? a ? x + (1 ? a) ? (k ? x + P ref ) k + 1<label>(9)</label></formula><p>Quantitative Results: We report quantitative comparisons for source counting performance in Tab. 1, oracle SNR in Tab. 2, and our proposed P-SI-SNR in Tab. 3. We note that models with * are not directly comparable to our model as they train a model for each speaker number, where we have a single model for all speakers.</p><p>As can be seen from Tab. 1, in the source counting task, our model outperforms all other models, even those with fewer possible choices of speaker counts. Our approach remains competitive in source separation when evaluated using Oracle-SNR, see Tab. 2. Lastly, in Tab. 3, when P ref is set to -30 dB, our P-SI-SNR also outperforms all other models in 2-speaker and 4-speaker cases, and achieves similar results to best model in the 3-speaker case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We present a unified approach to single channel speech separation with an unknown number of speakers. With our proposed multi-decoder architecture and count-head, our model requires a single forward-pass at test-time on a single network. In our experiments, we demonstrate that our model achieves state-of-the-part performance in source counting and competitive source separation quality. Additionally, we propose a new evaluation metric for evaluating source separation with an unknown number of speakers, in which we penalize SI-SNR when the number of sources estimated is incorrect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of our proposed approach for handling variable number of speakers for source separation. Given a mixed signal x, our model predicts the number of speakers from the mixed signal and uses the corresponding decoder-head to separate the signal. In this case, decoder 2 is selected, hence a reconstruction of two speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Model-Select(DPRNN)[9]* 81.3 64.4 46.2 85.6 Model-Select(Mulcat)[9]* 84.6 69.0 47.5 92.3 Performance of source counting; Each column is recall for corresponding number of speakers. For OR-PIT, only overall accuracy is provided.</figDesc><table><row><cell>)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Oracle SNR; Each column shows results averaged from all mixtures with corresponding number of speakers. *models above double-line are models with fixed number of speakers.</figDesc><table><row><cell>Model</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Conv-Tasnet[6]*</cell><cell>15.3</cell><cell>12.7</cell><cell>-</cell><cell>-</cell></row><row><cell>DPRNN[7]*</cell><cell>18.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DPRNN[9]*</cell><cell cols="4">18.21 14.71 10.37 8.65</cell></row><row><cell>Mulcat[9]*</cell><cell cols="4">20.12 16.85 12.88 10.56</cell></row><row><cell cols="2">Attractor Network[12] 15.3</cell><cell>14.5</cell><cell>-</cell><cell>-</cell></row><row><cell>OR-PIT[10]</cell><cell>14.8</cell><cell>12.6</cell><cell>10.2</cell><cell>-</cell></row><row><cell>Ours</cell><cell>19.1</cell><cell>14.1</cell><cell>9.3</cell><cell>5.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic speech recognition in cocktail-party situations: A specific training for separated speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximo</forename><surname>Cobos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose J</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1529" to="1535" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Singing-voice separation from monaural recordings using robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">Deeann</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dualpath RNN: efficient long sequence modeling for timedomain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterancelevel permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2623" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive speech separation for unknown number of speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudarsanam</forename><surname>Parthasaarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved source counting and separation for monaural mixture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijian</forename><surname>Zhang</surname></persName>
		</author>
		<idno>10/21/2020</idno>
		<ptr target="https://arxiv.org/abs/2004.00175" />
		<imprint>
			<biblScope unit="volume">03</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
