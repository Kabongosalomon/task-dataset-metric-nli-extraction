<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
							<email>yangsl@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<email>liuwei4@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) are widely used structured models, both of which can be represented as factor graph grammars (FGGs), a powerful formalism capable of describing a wide range of models. Recent research found it beneficial to use large state spaces for HMMs and PCFGs. However, inference with large state spaces is computationally demanding, especially for PCFGs. To tackle this challenge, we leverage tensor rank decomposition (aka. CPD) to decrease inference computational complexities for a subset of FGGs subsuming HMMs and PCFGs. We apply CPD on the factors of an FGG and then construct a new FGG defined in the rank space. Inference with the new FGG produces the same result but has a lower time complexity when the rank size is smaller than the state size. We conduct experiments on HMM language modeling and unsupervised PCFG parsing, showing better performance than previous work. Our code is publicly available at https://github.com/ VPeterV/RankSpace-Models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) are widely used structured models in natural language processing. They can both be represented as factor graph grammars (FGGs) <ref type="bibr" target="#b8">(Chiang and Riley, 2020)</ref>, which are a powerful tool to describe a wide range of models, allowing exact and tractable inference in most situations of interest.</p><p>Over-parameterization has been shown beneficial in facilitating optimization of deep networks <ref type="bibr" target="#b0">(Arora et al., 2018;</ref><ref type="bibr" target="#b45">Xu et al., 2018;</ref><ref type="bibr" target="#b16">Du et al., 2019)</ref>. <ref type="bibr" target="#b4">Buhai et al. (2020)</ref> found that overparameterization is also helpful in learning latent * Equal contributions: Songlin Yang formulated the idea and Wei Liu conducted the experiments. ? Corresponding author.</p><p>variable models by increasing the number of hidden states. <ref type="bibr" target="#b4">Buhai et al. (2020)</ref> argued that it is important to study over-parameterization in structured settings because structured latent variable models are more suitable to model real-word phenomena which exhibit complex dependencies. HMMs and PCFGs are typical structured latent variable models, and recently researchers have found it beneficial to use large state spaces for HMMs and PCFGs <ref type="bibr" target="#b13">(Dedieu et al., 2019;</ref><ref type="bibr" target="#b10">Chiu and Rush, 2020;</ref><ref type="bibr" target="#b50">Yang et al., 2021b;</ref><ref type="bibr" target="#b9">Chiu et al., 2021)</ref>. However, structured inference with large state spaces is computationally demanding, especially for PCFGs, pushing researchers to develop methods to decrease the computational complexities. <ref type="bibr" target="#b10">Chiu and Rush (2020)</ref> propose a neural VL-HMM with 2 15 states for language modeling, narrowing down the performance gap between HMMs and LSTMs. They follow <ref type="bibr" target="#b13">Dedieu et al. (2019)</ref> to impose a strong sparsity constraint (i.e., each hidden state can only generate a small subset of terminal symbols) to decrease the time complexity of the forward algorithm, thus requiring pre-clustering of terminal symbols. <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> use a large state space for neural PCFG induction and achieve superior unsupervised constituency parsing performance. They follow <ref type="bibr" target="#b12">Cohen et al. (2013)</ref> to use tensor rank decomposition (aka. canonical-polyadic decomposition (CPD) <ref type="bibr" target="#b36">(Rabanser et al., 2017)</ref>) to decrease the computational complexity of the inside algorithm, but only scale the state size from tens to hundreds because the resulting complexity is still high. <ref type="bibr" target="#b9">Chiu et al. (2021)</ref> use tensor matricization and low-rank matrix decomposition to accelerate structured inference on chain and tree structure models. However, their method has an even higher complexity than <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> on PCFGs. Recently, <ref type="bibr" target="#b21">Fu and Lapata (2021)</ref> propose a family of randomized dynamic programming algorithms to scale structured models to tens of thousands of states, which is orthogonal to the aforementioned low-rank-based approaches as the former performs approximate inference whereas the latter perform exact inference.</p><p>In this work, we propose a new low-rank-based approach to scale structured inference, which can be described by FGG notations intuitively. We first provide an intuitive and unifying perspective toward the work of <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> and <ref type="bibr" target="#b9">Chiu et al. (2021)</ref>, showing that their low-rank decompositionbased models can be viewed as decomposing large factors in an FGG-e.g., the binary rule probability tensor in PCFGs-into several smaller factors connected by new "rank" nodes. Then we target at a subset of FGGs-which we refer to as B-FGGssubsuming all models considered by <ref type="bibr" target="#b9">Chiu et al. (2021)</ref>, whereby the inference algorithms can be formulated via B-graphs <ref type="bibr" target="#b22">(Gallo et al., 1993;</ref><ref type="bibr" target="#b31">Klein and Manning, 2001)</ref>. We propose a novel framework to support a family of inference algorithms in the rank space for B-FGGs. Within the framework, we apply CPD on the factors of a B-FGG and then construct a new B-FGG defined in the rank space by marginalizing all the state nodes. Inference with the new B-FGG has the same result and a lower time complexity if the rank size is smaller than the state size.</p><p>We conduct experiments in unsupervised PCFG parsing and HMM language modeling. For PCFG induction, we manage to use 20 times more hidden states than <ref type="bibr" target="#b50">Yang et al. (2021b)</ref>, obtaining much better unsupervised parsing performance. For HMM language modeling, we achieve lower perplexity and lower inference complexity than <ref type="bibr" target="#b9">Chiu et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Factor graph grammar</head><p>Factor graphs are fixed-sized and thus incapable of modeling substructures that repeat a variable number of times. <ref type="bibr" target="#b8">Chiang and Riley (2020)</ref> propose factor graph grammars (FGGs) to overcome this limitation, which are expressive enough to subsume HMMs and PCFGs. The main purpose of introducing FGGs in this work is to facilitate more intuitive presentation of our method, and to enable generalization beyond HMMs and PCFGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Basics</head><p>We display necessary notations and concepts of FGGs <ref type="bibr">(Chiang and Riley, 2020, Def. 1,2,5,6,8</ref> ? ? maps node labels to sets of possible values.</p><formula xml:id="formula_0">?(v) ?(lab V (v)). ? F maps edge labels to functions. F (e) F (lab E (e)) is of type ?(v 1 ) ? ? ? ? ? ?(v k ) where att(e) = v 1 ? ? ? v k .</formula><p>In the terminology of factor graphs, a node v with its domain ?(v) is a variable, and an hyperedge e with F (e) is a factor. We typically use T, N, O to denote hidden state, nonterminal state and observation variables for HMMs and PCFGs.</p><formula xml:id="formula_1">Definition 3. A hypergraph fragment is a tuple (V, E, att, lab V , lab E , ext) where ? (V, E, att, lab V , lab E ) is a hypergraph.</formula><p>? ext ? V is a set of zero or more external nodes and each of which can be seen as a connecting point of this hypergraph fragment with another fragment.</p><p>Definition 4. A hyperedge replacement graph grammar (HRG) <ref type="bibr" target="#b14">(Drewes et al., 1997</ref>) is a tuple (N, T, P, S) where</p><p>? N, T ? L E is finite set of nonterminal and terminal symbols. N ? T = ?. ? P is a finite set of rules (X ? R) where X ? N and R is a hypergraph fragment with edge labels in N ? T 1 . ? S ? N is the start symbol.</p><p>Definition 5. A HRG with mapping ?, F (Def. 2) is referred to as an FGG. In particular, F is defined on terminal edge labels T only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations.</head><p>? N : variable node. N : external node.</p><p>? X e : hyperedge e with label X ? N .</p><p>indicates zero or more endpoint nodes. ? F (e) : factor F (e). Generative story. An FGG starts with S , repeatedly selects X e and uses rule X ? R from P to replace e with R, until no X e exists.</p><formula xml:id="formula_2">S ?1 ?? T 1 T1 = bos X 2 , (0) T 1 X, (i ? 1) ?2 ?? T 1 T 2 O 3 p(T2 | T1) p(O3 | T2) X 4 , (i) O3 = wi?1 T 1 X, (n) ?3 ?? T 1 T 2 p(T2 | T1) T2 = eos (a) S ?4 ?? N 1 X 2 , (0, n) N 1 X, (l ? 1, l) ?5 ?? N 1 O 2 p(N1 ? O2) O2 = wl?1 N 1 X, (i, j) ?6 ?? N 1 N 2 N 3 p(N1 ? N2N3) X 4 , (i, k) X 5 , (k, j) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Conjunction</head><p>The conjunction operation <ref type="bibr">(Chiang and Riley, 2020, Sec. 4</ref>) allows modularizing an FGG into two parts, one defining the model and the other defining a query. In this paper, we only consider querying the observed sentence w 0 , ? ? ? , w n?1 , which is exemplified by the red part of <ref type="figure" target="#fig_0">Fig. 1</ref>. We sometimes omit the red part without further elaboration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Inference</head><p>Denote ? as an assignment of all variables, ? D as the set of all assignments of factor graph D, and D(G) as the set of all derivations of an FGG G, i.e., all factor graphs generated by G. an FGG G assigns a score w G (D, ?) to each D ? D(G) along with each ? ? ? D . A factor graph D ? D(G) assigns a score w D (?) to each ? ? ? D :</p><formula xml:id="formula_3">w D (?) = e?D F (e)(?(v 1 ), . . . , ?(v k )) (1) with att(e) = v 1 ? ? ? v k . Notably, w D (?) w G (D, ?).</formula><p>The inference problem is to compute the sum-product of G:</p><formula xml:id="formula_4">Z G = D?D(G) ??? D w G (D, ?)<label>(2)</label></formula><p>To obtain Z G , the key difficulty is in the marginalization over all derivations, since ??? D w D (?) can be obtained by running standard variable elimination (VE) on factor graph D. To tackle this, <ref type="bibr">Chiang and Riley (2020, Thm. 15)</ref> propose an extended VE. For each X ? N, ? ? ? X 2 , define P X as all rules in P with left-hand side X, and then define:</p><formula xml:id="formula_5">? X (?) = (X?R)?P X ? R (?).</formula><p>(3)</p><formula xml:id="formula_6">for each rhs R = (V, E N ? E T , att, lab V , lab E , ext), where E N , E T consist of nonterminal/terminal-labeled edges only, and ? R (?) is given by: ? R (?) = ? ?? R ? (ext)=? e?E T F (e) ? (att(e)) e?E N ? lab E (e) ? (att(e))<label>(4)</label></formula><p>This defines a recursive formula for computing ? S , i.e., Z G . Next, we will show how Eq. 3-4 recover the well-known inside algorithm.</p><p>Example: the inside algorithm. Consider ? 6 in <ref type="figure" target="#fig_0">Fig.1(b)</ref>. All possible fragments R (rhs of ? 6 ) differs in the value of k, i.e., the splitting point, so we use R k to distinguish them. Then Eq. 3 becomes:</p><formula xml:id="formula_7">? X i,k (?) = i&lt;k&lt;j ? R k (?)<label>(5)</label></formula><p>Putting values into Eq. 4:</p><formula xml:id="formula_8">? R k (?) = n 2 ,n 3 p(?, n 2 , n 3 )? X i,k (n 2 )? X k,j (n 3 ) (6) where p denotes FGG rule probability p(N 1 ? N 2 N 3 ).</formula><p>It is easy to see that ? X i,k is exactly the inside score of span [i, k), and Eq. 5-6 recovers the recursive formula of the inside algorithm.</p><p>Remark. Eq. 4 can be viewed as unidirectional (from e ? E N to external nodes) belief propagation (BP) in the factor graph fragment R, where the incoming message is ? lab E (e) for e ? E N , and the outcome of Eq. 4 can be viewed as the message passed to the external nodes. The time complexity of message updates grows exponentially with the number of variables in the factors. Therefore, to decrease inference complexity, one may decompose large factors into smaller factors connected by new nodes, as shown in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tensor rank decomposition on factors</head><p>Consider a factor F (e) (Def. 2), it can be represented as an order-k tensor in</p><formula xml:id="formula_9">R N 1 ?????N k where N i |?(v i )|.</formula><p>We can use tensor rank decomposition (aka. CPD) to decompose F (e) into a weighted sum of outer products of vectors:</p><formula xml:id="formula_10">F (e) = r q=1 ? q w q e 1 ? w q e 2 ? ? ? ? ? w q e k</formula><p>where r is the rank size; w q e k ? R N k ; ? is outer product; ? q is weight, which can be absorbed into {w q e k } and we omit it throughout the paper. Dupty and Lee (2020, Sec. 4.1) show that BP can be written in the following matrix form when applying CPD on factors:</p><formula xml:id="formula_11">m ei = W T e i j?N (e)\i W e j n je<label>(7)</label></formula><formula xml:id="formula_12">n ie = c?N (i)\e m ci<label>(8)</label></formula><p>where m ei ? R N i is factor-to-node message; n ie ? R N i is node-to-factor message; N (?) indicates neighborhood ; W e j = [w 1 e j , ? ? ? , w r e j ] T ? R r?m ; is element-wise product. We remark that this amounts to replacing the large factor F (e) with smaller factors {F (e i )} connected by a new node R that represents rank, where each F (e i ) can be represented as W e i . <ref type="figure">Fig. 2</ref> illustrates this intuition. We refer to R as rank nodes and others as state <ref type="figure">Figure 2</ref>: Using CPD to decompose a factor can be seen as adding a new node. 3 Low-rank structured inference</p><formula xml:id="formula_13">nodes thereafter. v 1 v 2 v .. v k F (e) ? v 1 v 2 v .. v k R F (e1) F (e2) F (ek)</formula><formula xml:id="formula_14">(a) N 1 N 2 N 3 R U V W X 4 , (i, k) X 5 , (k, j) (b) N 1 N 2 N 3 R U V X 4 , (i, k) X 5 , (k, j)</formula><p>In this section, we recover the accelerated inside algorithms of TD-PCFG <ref type="bibr" target="#b12">(Cohen et al., 2013;</ref><ref type="bibr" target="#b50">Yang et al., 2021b)</ref> and LPCFG <ref type="bibr" target="#b9">(Chiu et al., 2021)</ref> in an intuitive and unifying manner using the FGG notations. The accelerated forward algorithm of LHMM <ref type="bibr" target="#b9">(Chiu et al., 2021)</ref> can be derived similarly.</p><p>Denote T ? R m?m?m as the tensor representation of p(N 1 ? N 2 N 3 ) , and ? i,j ? R m as the inside score of span [i, j). <ref type="bibr" target="#b12">Cohen et al. (2013)</ref> and <ref type="bibr" target="#b50">Yang et al. (2021b)</ref>  <ref type="bibr" target="#b12">Cohen et al. (2013)</ref> derived the recursive form:</p><formula xml:id="formula_15">use CPD to decom- pose T, i.e., let T = r q=1 u q ? v q ? w q where u q , v q , w q ? R m . Denote U, V, W ? R r?m as the resulting matrices of stacking all u q , v q , w q ,</formula><formula xml:id="formula_16">? i,j = j?1 k=i+1 U T ((V? i,k ) (W? k,j )) (9) = U T j?1 k=i+1 ((V? i,k ) (W? k,j )) (10)</formula><p>Eq. 9 can be derived automatically by combining Eq. 7 (or <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>) and Eq. 5-6. Cohen et al.</p><p>(2013) note that U T can be extracted to the front of the summation (Eq. 10), and V? i,k , W? k,j can be cached and reused, leading to further complexity reduction. The resulting inside algorithm time complexity is O(n 3 r + n 2 mr).</p><p>Recently, <ref type="bibr" target="#b9">Chiu et al. (2021)</ref> use low-rank matrix decomposition to accelerate PCFG inference. They first perform tensor matricization to flatten T to T ? R m?m 2 , and then let T = U T V where U ? R r?m , V ? R r?m 2 . By un-flattening V to V ? R r?m?m , their accelerated inside algorithm has the following recursive form:</p><formula xml:id="formula_17">? i,j = j?1 k=i+1 U T V ? ? k,j ? ? i,k (11) = U T j?1 k=i+1 V ? ? k,j ? ? i,k<label>(12)</label></formula><p>Eq. 11 can be derived by combining <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> and Eq. 5-6. The resulting inside time complexity is O(n 3 m 2 r + n 2 mr), which is higher than that of TD-PCFG. When learning a PCFG and a HMM, there is no need to first learn T and then perform decomposition on T. Instead, one can learn the decomposed matrices (e.g., U, V) to learn T implicitly. During inference, one can follow Eq. 10 or 12 without the need to reconstruct T.</p><p>Validity of probability. The remaining problem is to ensure that T is a valid probability tensor (i.e., being nonnegative and properly normalized) when learning it implicitly. <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> essentially transform <ref type="figure" target="#fig_2">Fig. 3(a)</ref> into a Bayesian network, adding directed arrows N 1 ? R, R ? N 2 , R ? N 3 . This is equivalent to requiring that V, W are nonnegative and column-wise normalized and U is nonnegative and row-wise normalized, as described in <ref type="bibr">Yang et al. (2021b, Thm. 1)</ref>. One can apply the Softmax re-parameterization to enforce such requirement, which is more convenient in endto-end learning. <ref type="bibr" target="#b9">Chiu et al. (2021)</ref> replace the local normalization of <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> with global normalization, and we refer readers to their paper for more details. We adopt the strategy of <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> in this work. Interestingly, when applying CPD on factors and if the rank size is smaller than the state size, we can even obtain better inference time complexities for a subset of FGGs which we refer to as B-FGGs.</p><p>We call a hyperedge a B-edge if its head contains exactly one node. B-graphs <ref type="bibr" target="#b22">(Gallo et al., 1993)</ref> are a subset of directed hypergraphs whose hyperedges are all B-edges. Many dynamic programming algorithms can be formulated through B-graphs <ref type="bibr" target="#b31">(Klein and Manning, 2001;</ref><ref type="bibr" target="#b26">Huang, 2008;</ref><ref type="bibr" target="#b1">Azuma et al., 2017;</ref><ref type="bibr" target="#b9">Chiu et al., 2021;</ref><ref type="bibr" target="#b21">Fu and Lapata, 2021)</ref>, including the inference algorithms of many structured models, e.g., HMMs, Hidden Semi-Markov Models (HSMMs), and PCFGs. We follow the concept of B-graphs to define B-FGGs.</p><p>Definition 6. A hypergraph fragment is a Bhypergraph fragment iff. there is exactly one external node and there is no nonterminal-labeled hyperedge connecting to it. An FGG is a B-FGG iff. all rhs of its rules are B-hypergraph fragments.</p><p>It is easy to see that the aforementioned models are subsumed by B-FGGs. We can design a family of accelerated inference algorithms for B-FGGs based on the following strategy. (1) If there are multiple factors within a hypergraph fragment, merge them into a single factor. Then apply CPD on the single factor, thereby introducing rank nodes. <ref type="formula" target="#formula_4">(2)</ref> Find repeated substructures that take rank nodes as external nodes. Marginalize all state nodes to derive new rules. (3) Design new inference algorithms that can be carried out in the rank space based on the general-purpose FGG inference algorithm and the derived new rules. We give two examples, the rank-space inside algorithm and the rank-space forward algorithm, in the following two subsections to help readers understand this strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The rank-space inside algorithm</head><p>Consider an B-FGG G shown in <ref type="figure" target="#fig_0">Fig.1(b)</ref> and replace the rhs of ? 6 with <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, i.e., we use CPD to decompose binary rule probability tensor. Besides U, V, W ? R r?m defined in Sec. 3, we define the start rule probability vector as s ? R m?1 , and the unary rule probability matrix as E ? R o?m where o is the vocabulary size. <ref type="figure">Fig. 4(a)</ref> is an example (partial) factor graph D generated by G. We highlight substructures of interest with dashed rectangles. Each substructure consists of a node N and two factors connecting to it. N is an external node connecting two hypergraph fragments which contain the two factors respectively. For each substructure, we can marginalize the state node N out, merging the two factors into a single one. After marginalizing all state nodes, we obtain a (partial) factor graph D shown in the right of <ref type="figure">Fig. 4(a)</ref> where H = VU T , I = WU T , J = VE T , K = WE T , L = (Us) T . We denote this transformation as M(D) = D . It is worth men- <ref type="figure">Figure 4</ref>: (a): illustration of marginalizing state nodes N. (b): rule set of the new FGG. ? 1 can be applied when k = i + 1 and k + 1 = j; ? 2 and ? 3 can be applied when i = j ? 1; ? 4 can be applied when j = i + 2.</p><formula xml:id="formula_18">N 1 R 1 N 2 N 3 R 2 N 4 N 6 N 7 N 5 O 1 O 2 R 3 R 4 R 5 R 1 R 2 R 3 R 4 O 1 O 2 R 5 ... ... L H I H K J I ... ... (a) R 1 R 2 R 3 H I X 4 , (i, k) X 5 , (k, j) R 1 R 2 O 3 H K X 4 , (i, j ? 1) O3 = wj?1 R 1 L X 2 , (0, n) R 1 X, (i, j) S ? 1 ? 2 ? 3 ? 4 ? 5 ? 6 R 1 O 2 R 3 J I O2 = wi X 5 , (i + 1, j) R 1 O 2 O 3 J K O2 = wi O3 = wi+1 O 1 O1 = w1 (b)</formula><p>tioning that H, I, J, K, L are computed only once and then reused multiple times during inference, which is the key to reduce the time complexity. Then we define a new B-FGG G with rules shown in <ref type="figure">Fig 4(b)</ref>. It is easy to verify that for each D ? D(G), we have M(D) ? D(G ), and vice versa. Moreover, we have:</p><formula xml:id="formula_19">??? D w G (D, ?) = ??? M(D) w G (M(D), ?)</formula><p>because marginalizing hidden variables does not affect the result of sum-product inference. Therefore, Z G = Z G (Eq. 2).</p><p>We can easily derive the inference (inside) algorithm of G by following Eq. 3-4 and <ref type="figure">Fig. 4(b)</ref> 3 . Let ? i,j ? R r denote the rank-space inside score for span [i, j). When j &gt; i + 2: <ref type="figure">Fig. 4</ref></p><formula xml:id="formula_20">? i,j = from ? 1 of</formula><formula xml:id="formula_21">(b) i+1&lt;k&lt;j?1 (H? i,k I? k,j ) + J :,w i I? i+1,j from ? 2 + H? i,j?1 K :,w j?1 from ? 3</formula><p>and when j = i + 2, ? i,j = J :,w i K :,w i+1 (from ? 4 ). w j is the index of the j-th word of the input sentence in the vocabulary; A :,j indicates the j-th column of A.</p><p>We note that, similar to Cohen et al. <ref type="formula" target="#formula_4">(2013)</ref>, we can cache H? i,k , I? k,j and reuse them to further accelerate inference 4 . Denote ? L i,j , ? R i,j ? R r as the inside scores of span [i, j) serving as a left/right child of a larger span. Then we have:</p><formula xml:id="formula_22">? L i,i+1 = K :,i ? R i,i+1 = J :,i ? L i,j = H? i,j ? R i,j = I? i,j ? i,j = i&lt;k&lt;j (? L i,k ? R k,j )</formula><p>and finally, Z G = L? 0,n . We minimize ? log Z G using mini-batch gradient descent for unsupervised learning. The resulting inference complexity is O(n 3 r + n 2 r 2 ) 5 , which is lower than O(n 3 r + n 2 mr) of TD-PCFG when r &lt; m, enabling the use of a large state space for PCFGs in the low-rank setting.</p><p>The key difference between the rank-space inference and the original state-space inference is that they follow different variable elimination orders. The former marginalizes all state nodes before performing inference and marginalizes rank nodes from bottom up during inference; whereas the later marginalizes both state and rank nodes alternately from bottom up during inference.</p><p>Parsing. Low-rank inference does not support the Viterbi semiring 6 , inhibiting the use of CYK decoding. Therefore, we follow <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> to use Minimum Bayes-Risk decoding <ref type="bibr" target="#b23">(Goodman, 1996)</ref>. Specifically, we estimate the span marginals using auto-differentiation <ref type="bibr" target="#b19">(Eisner, 2016;</ref><ref type="bibr" target="#b38">Rush, 2020)</ref>, which has the same complexity as the inside algorithm. Then we use the CYK algorithm to find the final parse with the maximum number of expected spans in O(n 3 ) time, similar to <ref type="bibr" target="#b42">Smith and Eisner (2006)</ref>.</p><p>Implementation. The implementation of the inside algorithm greatly influences the actual running speed. First, O(n 2 ) out of O(n 3 ) can be computed in parallel using parallel parsing techniques <ref type="bibr" target="#b51">(Yi et al., 2011;</ref><ref type="bibr" target="#b6">Canny et al., 2013;</ref><ref type="bibr" target="#b53">Zhang et al., 2020;</ref><ref type="bibr" target="#b38">Rush, 2020)</ref>. In this work, we adapt the efficient implementation of <ref type="bibr" target="#b53">Zhang et al. (2020)</ref> for fast inside computation. Second, we adopt the log-einsum-exp trick <ref type="bibr" target="#b35">(Peharz et al., 2020)</ref> to avoid expensive log-sum-exp operations on high-dimensional vectors, which reduces both GPU memory usage and total running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The rank-space forward algorithm</head><p>Consider an B-FGG G shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. We replace the rhs of ? 2 by the hypergraph fragment in the right of <ref type="figure" target="#fig_4">Fig. 5(a)</ref>, i.e., we merge the factor p(T 2 | T 1 ) and p(O 3 | T 2 ) into a single factor, which can be represented as T ? R m?m?o and can be decomposed into three matrices U, V ? R r?m , W ? R r?o via CPD, where m/o/r is the state/vocabulary/rank size. <ref type="figure" target="#fig_4">Fig. 5(b)</ref> gives an example factor graph of HMMs with sentences of length 3. Similar to previous subsection, we marginalize state nodes T to construct a new B-FGG G . The rule set of G can be obtained by replacing all variable nodes T with R and modifying all factors accordingly, as one can easily infer from <ref type="figure" target="#fig_4">Fig.  5(c)</ref>. Inference with G simply coincides with the forward algorithm, which has a O(nr 2 ) time complexity and is lower than O(nmr) of LHMM <ref type="bibr" target="#b9">(Chiu et al., 2021)</ref> when r &lt; m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Neural parameterization</head><p>We use neural networks to produce probabilities for all factors, which has been shown to benefit learning and unsupervised induction of syntactic structures <ref type="bibr" target="#b27">(Jiang et al., 2016;</ref><ref type="bibr" target="#b25">He et al., 2018;</ref><ref type="bibr" target="#b30">Kim et al., 2019;</ref><ref type="bibr" target="#b24">Han et al., 2019;</ref><ref type="bibr" target="#b28">Jin et al., 2019;</ref><ref type="bibr" target="#b55">Zhu et al., 2020;</ref><ref type="bibr" target="#b47">Yang et al., 2020</ref><ref type="bibr" target="#b50">Yang et al., , 2021b</ref><ref type="bibr" target="#b54">Zhao and Titov, 2020;</ref><ref type="bibr" target="#b52">Zhang et al., 2021;</ref><ref type="bibr" target="#b10">Chiu and Rush, 2020;</ref><ref type="bibr" target="#b9">Chiu et al., 2021;</ref><ref type="bibr" target="#b29">Kim, 2021)</ref>. We use the neural parameterization of <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> with slight modifications. We show the details in Appd. A and Appd. B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Unsupervised parsing with PCFGs</head><p>Setting. We evaluate our model on Penn Treebank (PTB) <ref type="bibr" target="#b32">(Marcus et al., 1994)</ref>. Our implementation is based on the open-sourced code of Yang et al. (2021b) 7 and we use the same setting as theirs.</p><p>For all experiments, we set the ratio of nonterminal number to the preterminal number to 1:2 8 which is the common practise. We set the rank size to 1000. We show other details in Appd. C and D.</p><p>Main result. <ref type="table">Table 1</ref> shows the result on PTB. Among previous unsupervised PCFG models, TN-PCFG <ref type="bibr" target="#b50">(Yang et al., 2021b)</ref> uses the largest number of states (500 perterminals and 250 nonterminals). Our model is able to use much more states thanks to our new inside algorithm with lower time complexity, surpassing all previous PCFG-based models by a large margin and achieving a new state-of-the-art in unsupervised constituency parsing in terms of sentence-level F1 score on PTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-F1</head><p>N-PCFG <ref type="bibr" target="#b30">(Kim et al., 2019)</ref> 50.8 C-PCFG <ref type="bibr" target="#b30">(Kim et al., 2019)</ref> 55.2 NL-PCFG <ref type="bibr" target="#b55">(Zhu et al., 2020)</ref> 55.3 TN-PCFG <ref type="bibr" target="#b50">(Yang et al., 2021b)</ref> 57.7 NBL-PCFG <ref type="bibr" target="#b49">(Yang et al., 2021a)</ref> 60.4</p><p>Ours with 9000 PTs and 4500 NTs 64.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For reference</head><p>Constituency test <ref type="bibr" target="#b7">(Cao et al., 2020)</ref> 62.8 S-DIORA <ref type="bibr" target="#b15">(Drozdov et al., 2020)</ref> 57.6 StructFormer <ref type="bibr" target="#b39">(Shen et al., 2021)</ref> 54.0 DIORA+span constraint    Ablation study. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the change of the sentence-level F1 scores and perplexity with the change of the number of preterminals. As we can see, when increasing the state, the perplexity tends to decrease while the F1 score tends to increase, validating the effectiveness of using large state spaces for neural PCFG induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">HMM language modeling</head><p>Setting. We conduct the language modeling experiment also on PTB. Our implementation is based  <ref type="bibr" target="#b5">(Buys et al., 2018)</ref> 142.3 -AWD-LSTM <ref type="bibr" target="#b33">(Merity et al., 2018)</ref> 60.0 57.3 <ref type="table">Table 2</ref>: Resulting perplexity on PTB validate set and test set. VL-HMM: <ref type="bibr" target="#b10">(Chiu and Rush, 2020)</ref>. LHMM: <ref type="bibr" target="#b9">(Chiu et al., 2021)</ref>. ? denotes results reported by ablation study of <ref type="bibr" target="#b10">Chiu and Rush (2020)</ref>.  on the open-sourced code of <ref type="bibr">Chiu et al. (2021) 9</ref> . We set the rank size to 4096. See Appd. C and D for more details.</p><p>Main result. <ref type="table">Table 2</ref> shows the perplexity on the PTB validation and test sets. As discussed earlier, VL-HMM <ref type="bibr" target="#b10">(Chiu and Rush, 2020)</ref> imposes strong sparsity constraint to decrease the time complexity of the forward algorithm and requires preclustering of terminal symbols. Specifically, VL-HMM uses Brown clustering <ref type="bibr" target="#b3">(Brown et al., 1992)</ref>, introducing external information to improve performance. Replacing Brown clustering with uniform clustering leads to a 10 point increase in perplexity on the PTB validation set. LHMM <ref type="bibr" target="#b9">(Chiu et al., 2021)</ref> and our model only impose low-rank constraint without using any external information and are thus more comparable. Our method outperforms LHMM by 4.8 point when using the same state number (i.e., 2 14 ), and it can use more states thanks to our lower inference time complexity.</p><p>Ablation study. As we can see in <ref type="table" target="#tab_4">Table 3</ref>, the perplexity tends to decrease when increasing the state number, validating the effectiveness of using more states for neural HMM language modeling.</p><p>Discussion. It is interesting to note that our HMM model is roughly equivalent to another HMM with interchanged rank and state sizes as can be seen in <ref type="figure" target="#fig_4">Fig.5(c)</ref>. To verify this equivalence, we run LHMM in the original state space with 2048 states and rank 2 15 . The resulting perplexity is 133.49 on average on the PTB test set, which is worse than that of ours (126.4). We leave further experimentation and analyses of this discrepancy for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Tensor and matrix decomposition have been used to decrease time and space complexities of probabilistic inference algorithms. Siddiqi et al. <ref type="formula" target="#formula_4">(2010)</ref> propose a reduced-rank HMM whereby the forward algorithm can be carried out in the rank space, which is similar to our model, but our method is more general. Cohen and Collins <ref type="formula" target="#formula_4">(2012)</ref>; Cohen et al. <ref type="formula" target="#formula_4">(2013)</ref> use CPD for fast (latent-variable) PCFG parsing, but they do not leverage CPD for fast learning and they need to actually perform CPD on existing probability tensors. <ref type="bibr" target="#b37">Rabusseau et al. (2016)</ref> use low-rank approximation method to learn weighted tree automata, which subsumes PCFGs and latentvariable PCFGs. Our method can subsume more models. <ref type="bibr">Yang et al. (2021b,a)</ref> propose CPD-based neural parameterizations for (lexicalized) PCFGs. <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> aim at scaling PCFG inference. We achieve better time complexity than theirs and hence can use much more hidden states. <ref type="bibr" target="#b49">Yang et al. (2021a)</ref> aims to decrease the complexity of lexicalized PCFG parsing, which can also be described within our framework. <ref type="bibr" target="#b9">Chiu et al. (2021)</ref> use low-rank matrix decomposition, which can be viewed as CPD on order-2 tensors, to accelerate inference on chain and tree structure models including HMMs and PCFGs. However, their method is only efficient when the parameter tensors are of order 2, e.g., in HMMs and HSMMs. Our method leverages full CPD, thus enabling efficient inference with higher-order factors, e.g., in PCFGs. Our method can be applied to all models considered by <ref type="bibr" target="#b9">Chiu et al. (2021)</ref>, performing inference in the rank-space with lower complexities. Besides HMMs and PCFGs, <ref type="bibr" target="#b44">Wrigley et al. (2017)</ref> propose an efficient sampling-based junction-tree algorithm using CPD to decompose high-order factors. Dupty and Lee (2020) also use CPD to decompose high-order factors for fast belief propagation. <ref type="bibr" target="#b48">Yang and Tu (2022)</ref> use CPD to decompose second-order factors in semantic dependency parsing to accelerate second-order parsing with mean-field inference. Besides CPD, <ref type="bibr" target="#b17">Ducamp et al. (2020)</ref> use tensor train decomposition for fast and scalable message passing in Bayesian networks. <ref type="bibr" target="#b2">Bonnevie and Schmidt (2021)</ref> leverage matrix product states (i.e., tensor trains) for scalable discrete probabilistic inference. <ref type="bibr" target="#b34">Miller et al. (2021)</ref> leverage tensor networks for fast sequential probabilistic inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and future work</head><p>In this work, we leveraged tensor rank decomposition (CPD) for low-rank scaling of structured inference. We showed that CPD amounts to decomposing a large factor into several smaller factors connected by a new rank node, and gave a unifying perspective towards previous low-rank structured models <ref type="bibr" target="#b50">(Yang et al., 2021b;</ref><ref type="bibr" target="#b9">Chiu et al., 2021)</ref>. We also presented a novel framework to design a family of rank-space inference algorithms for B-FGGs, a subset of FGGs which subsume most structured models of interest to the NLP community. We have shown the application of our method in scaling PCFG and HMM inference, and experiments on unsupervised parsing and language modeling validate the effectiveness of using large state spaces facilitated by our method.</p><p>We believe our framework can be applied to many other models which have high inference time complexity and are subsumed by B-FGGs, including lexicalized PCFGs, quasi-synchronous contextfree grammars (QCFGs), etc. A direct application of our method is to decrease the inference complexity of the neural QCFG <ref type="bibr" target="#b29">(Kim, 2021)</ref>.</p><formula xml:id="formula_23">W = exp(u T W h 2 (w w ) w ?? exp(u T W h 2 (w w )</formula><p>) h i (x) = g i,1 (g i,2 (W i x)) g i,j (y) = ReLU (? i,j ReLU (? i,j y)) + y where S is a finite set of states, H is the set of rank, ? is vocabulary set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Data details</head><p>Penn Treebank (PTB) <ref type="bibr" target="#b32">(Marcus et al., 1994)</ref> 10 consists of 929k training words, 73k validation words, and 82k test words, with a vocabulary of size 10k.</p><p>For PCFGs, we follow <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> and use their code to preprocess dataset. This processing discards punctuation and lowercases all tokens with 10k most frequent words as the vocabulary. The splits of the dataset are: 2-21 for training, 22 for validation and 23 for test.</p><p>For HMMs, we follow <ref type="bibr" target="#b9">Chiu et al. (2021)</ref> and use their code to preprocess dataset. We lowercase all words and substitutes OOV words with UNKs. EOS tokens have been inserted after each sentence. <ref type="bibr">10</ref> The licence of PTB dataset is LDC User Agreement for Non-Members, which can be seen on https://catalog. ldc.upenn.edu/LDC99T42</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental details</head><p>For PCFGs, we use Xavier normal initialization to initialize the weights in h i and f i . We optimize our model using Adam optimizer with ? 1 = 0.75, ? 2 = 0.999, and the learning rate 0.002, setting the dimension of all embeddings to 256.</p><p>For HMMs, we initialize all parameters by Xavier normal initialization except for w s and w w . We use AdamW optimizer with ? 1 = 0.99, ? 2 = 0.999, and the learning rate 0.001, and a max grad norm of 5. We use dropout rate of 0.1 to dropout w s and U, V in HMMs. We train for 30 epochs with a max batch size of 256 tokens, and reduce the learning by multiplying 1 2 if the validation perplexity fails to improve after 2 evaluations. Evaluations are performed one time per epoch. We follow <ref type="bibr" target="#b9">Chiu et al. (2021)</ref> to shuffle sentences and leverage bucket iterator, where batch of sentences are drawn from buckets containing sentences of similar lengths to minizing padding.</p><p>We run all experiments on NVIDIA TITAN RTX and NVIDIA RTX 2080ti and all experimental results are averaged from four runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>FGG representations of (a) HMMs and (b) PCFGs. Examples come from<ref type="bibr" target="#b8">Chiang and Riley (2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>illustrates HGG representations of HMM and PCFG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Representations of the rhs of ? 6 (Fig. 1) after decomposition. (a): TD-PCFG (Cohen et al., 2013; Yang et al., 2021b). (b): LPCFG (Chiu et al., 2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4</head><label></label><figDesc>Rank-space modeling and inference 4.1 Rank-space inference with B-FGGs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(a): merge the two factors into a single one, and apply CPD on the resulting factor. (b): factor graph of a HMM for sentences of length 3. (c): the resulting factor graph after marginalizing the state nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The change of F1 scores and perplexities with the change of number of perterminal symbols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Definition 1. A hypergraph is a tuple V, E, att, lab V , lab E where ? V and E are finite set of nodes and hyperedges. ? att : E ? V maps each hyperedge to zero or more (not necessarily distinct) endpoint nodes. ? lab V : V ? L V assigns labels to nodes. ? lab E : E ? L E assigns labels to edges.</figDesc><table><row><cell>Definition 2. A factor graph is a hypergraph with</cell></row><row><cell>mappings ? and F where</cell></row><row><cell>).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Perplexity with varying numbers of states. Following<ref type="bibr" target="#b9">Chiu et al. (2021)</ref>, we fix the rank to 2048 for faster ablation studies.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that, for the lhs of P , Chiang and Riley (2020) also draw their endpoint nodes using external node notations. We follow this practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">?X is defined as the set of assignments to the endpoints of an edge e labeled X, so ?X = ? ( 1) ? ? ? ? ? ? ( k ) where att(e) = v1 ? ? ? v k , lab V (vi) = i.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">?6 is used for generating sentences of length 1, we do not consider this in the following derivation of the inside algorithm to reduce clutter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In fact, this is a typical application of the unfold-refold transformation<ref type="bibr" target="#b20">(Eisner and Blatz, 2007;</ref><ref type="bibr" target="#b43">Vieira et al., 2021)</ref>.5  This does not take into account the one-time cost of computing H, I, J, K before inference.6  The Viterbi semiring is also known as the max-product semiring.Chiu et al. (2021, Appd. C)  andYang et al. (2021b,  Sec. 6)  have discussed this issue.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">github.com/sustcsonglin/TN-PCFG 8 Although we did not explicitly distinguish between nonterminal and preterminal symbols previously, in our implementation, we follow<ref type="bibr" target="#b30">Kim et al. (2019)</ref> to make such distinction, in which terminal words can only be generated by preterminal symbols, and binary rules can only be invoked by nonterminal symbols.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">github.com/justinchiu/low-rank-models</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Natural Science Foundation of China (61976139).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Neural parameterization of PCFGs</head><p>In this section, we give the full parameterization of PCFGs. We follow <ref type="bibr" target="#b50">Yang et al. (2021b)</ref> with slight modifications for generations of U, V, W ? R m?r in 4.2. We use the same MLPs with two residual layers as <ref type="bibr" target="#b50">Yang et al. (2021b)</ref>:</p><p>where ? is the vocabulary set, H is the set of rank, N is a finite set of nonterminals, W l = [W n ; W t ], w l , w n , w t ? W l , W n , W t . The main differences of neural parameterization between ours and previous work are that we make the projection parameter u H shared among U, V, and U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Neural parameterization of HMMs</head><p>In this section, we give the full parameterization of HMMs, which is similar to PCFGs' parameterization. Define s as start probability for HMMs. And the definitions of U, V, W are same as definitions in 4.3:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the optimization of deep networks: Implicit acceleration by overparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An algebraic formalization of forward and forward-backward algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno>abs/1702.06941</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matrix product states for inference in discrete probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Bonnevie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mikkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">187</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Della</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="480" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical study of the benefits of overparameterization in learning latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares-Darius</forename><surname>Buhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1211" to="1219" />
		</imprint>
	</monogr>
	<note type="report_type">Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<title level="m">Bridging hmms and rnns through architectural transformations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multiteraflop constituency parser using GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1898" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised parsing via constituency tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.389</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4798" to="4808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factor graph grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darcey</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-rank constraints for fast inference in structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling hidden Markov language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1341" to="1349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tensor decomposition for fast parsing with latent-variable pcfgs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12-03" />
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate PCFG parsing using tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
	<note>Georgia</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning higher-order sequential structure with cloned hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Dedieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishad</forename><surname>Gothoskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Swingle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lehrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>L?zaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dileep</forename><surname>George</surname></persName>
		</author>
		<idno>abs/1905.00507</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Handbook of Graph Grammars and Computing by Graph Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Drewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-J?rg</forename><surname>Kreowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annegret</forename><surname>Habel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations</title>
		<editor>Grzegorz Rozenberg</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="95" to="162" />
			<date type="published" when="1997" />
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
	<note>Hyperedge replacement, graph grammars</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">O</forename><surname>&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4832" to="4845" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient descent provably optimizes over-parameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An efficient lowrank tensors representation for algorithms in complex probabilistic graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaspard</forename><surname>Ducamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bonnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Nouy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Henri</forename><surname>Wuillemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Probabilistic Graphical Models</title>
		<meeting><address><addrLine>Aalborg; Sk?rping, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-09-25" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
	<note>Hotel Comwell Rebild Bakker</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neuralizing efficient higher-order belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Mohammed Haroon Dupty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun Lee</surname></persName>
		</author>
		<idno>abs/2010.09283</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inside-outside and forwardbackward algorithms are just backprop (tutorial paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-5901</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Program transformations for optimization of parsing algorithms and other weighted logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FG 2006: The 11th Conference on Formal Grammar</title>
		<meeting>FG 2006: The 11th Conference on Formal Grammar</meeting>
		<imprint>
			<publisher>CSLI Publications</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="45" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling structured inference with randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>abs/2112.03638</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Directed hypergraphs and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giustino</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pallottino</surname></persName>
		</author>
		<idno type="DOI">10.1016/0166-218X(93)90045-P</idno>
	</analytic>
	<monogr>
		<title level="j">Discret. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="201" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.3115/981863.981887</idno>
	</analytic>
	<monogr>
		<title level="m">34th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Santa Cruz, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancing unsupervised generative dependency parser with contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1526</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5315" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of syntactic structure with invertible neural projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advanced dynamic programming in semiring and hypergraph frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Advanced Dynamic Programming in Computational Linguistics: Theory, Algorithms and Applications -Tutorial notes</title>
		<meeting><address><addrLine>Manchester, UK. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="763" to="771" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of PCFGs with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning with latent neural grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compound probabilistic context-free grammars for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2369" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing and hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Parsing Technologies</title>
		<meeting>the Seventh International Workshop on Parsing Technologies<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Tsinghua University Press</publisher>
			<date type="published" when="2001-10" />
			<biblScope unit="page" from="17" to="19" />
		</imprint>
	</monogr>
	<note>IWPT-2001</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Penn Treebank: Annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Plainsboro, New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tensor networks for probabilistic sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Terilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-04-13" />
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Einsum networks: Fast and scalable learning of tractable probabilistic circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stelzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Trapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7563" to="7574" />
		</imprint>
	</monogr>
	<note>Guy Van den Broeck, Kristian Kersting, and Zoubin Ghahramani</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Introduction to tensor decompositions and their applications in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Rabanser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno>abs/1711.10781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low-rank approximation of weighted tree automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016<address><addrLine>Cadiz, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-09" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Torch-struct: Deep structured prediction library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.38</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">StructFormer: Joint unsupervised induction of dependency and constituency structure from masked language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.559</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7196" to="7209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reduced-rank hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
				<idno>AIS- TATS 2010</idno>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="741" to="748" />
		</imprint>
	</monogr>
	<note>JMLR Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Searching for more efficient dynamic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3812" to="3830" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tensor belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wrigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3771" to="3779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Benefits of over-parameterization with EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arian</forename><surname>Maleki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="10685" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved latent tree induction with distant supervision via span constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><forename type="middle">Yoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">O</forename><surname>&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Finkbeiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilpa</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4818" to="4831" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Second-order unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.347</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3911" to="3924" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Modeling label correlations for second-order semantic dependency parsing with mean-field inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno>abs/2204.03619</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural bi-lexicalized PCFG induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.209</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2688" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PCFGs can do better: Inducing probabilistic contextfree grammars with many symbols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.117</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1487" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient parallel CKY parsing on GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yue</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Parsing Technologies</title>
		<meeting>the 12th International Conference on Parsing Technologies<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video-aided unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1513" to="1524" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast and accurate neural CRF constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/560</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4046" to="4053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visually grounded compound PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.354</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4369" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The return of lexical dependencies: Neural lexicalized PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00337</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="647" to="661" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
