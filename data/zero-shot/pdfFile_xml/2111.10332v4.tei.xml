<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
							<email>zhangrenrui@pjlab.org.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Zeng</surname></persName>
							<email>zengzy@shanghaitech.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Uisee Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinben</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uisee Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexue</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
							<email>jshi@seas.upenn.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Uisee Research</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>point cloud recognition</term>
					<term>neural networks</term>
					<term>dual-scale processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud processing is a challenging task due to its sparsity and irregularity. Prior works introduce delicate designs on either local feature aggregator or global geometric architecture, but few combine both advantages. We propose Dual-Scale Point Cloud Recognition with High-frequency Fusion (DSPoint) to extract localglobal features by concurrently operating on voxels and points. We reverse the conventional design of applying convolution on voxels and attention to points. Specifically, we disentangle point features through channel dimension for dual-scale processing: one by pointwise convolution for fine-grained geometry parsing, the other by voxel-wise global attention for long-range structural exploration. We design a co-attention fusion module for feature alignment to blend local-global modalities, which conducts inter-scale crossmodality interaction by communicating high-frequency coordinates information. Experiments, ablations and error mode analysis on widely-adopted ModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of our DSPoint. Our code is also available at https://github.com/Adonis-galaxy/DSPoint. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Point-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3D vision has drawn increasing attention recently with the rapid development of 3D sensing technologies. It brings out many challenging 3D tasks, such as point cloud recognition <ref type="bibr">( [16, 24, 46]</ref>), shape <ref type="bibr" target="#b51">[52]</ref> and scene <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> segmentation, object detection based on point cloud <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b55">56]</ref> and monocular image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>, point cloud registration <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref>. Unlike 2D images that consist of pixels in uniform grids, a 3D point cloud is permutation invariant, spatially irregular, and density varying, which leads to non-trivial difficulty for algorithm designs.</p><p>Point cloud methods can be divided into two groups: projectionbased <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45]</ref> and point-wise <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref> methods. Projection-based models convert points into a regular grid representation, such as multi-view images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> or voxels <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45]</ref>, so that convolution models <ref type="bibr" target="#b17">[18]</ref> can be used directly for recognition. However, voxelizations lose local shape details and suffer from heavy memory and computation costs. In contrast, point-wise methods require no modal transformation and thus 1 * indicates equal contributions. ? indicates the corresponding author. <ref type="figure">Figure 1</ref>: Visualization of our method on ShapeNet <ref type="bibr" target="#b44">[45]</ref> Part Segmentation, compared with PAConv <ref type="bibr" target="#b47">[48]</ref>. It shows our advantage in segmenting parts into spatial-consistent regions. maintain all original information, especially the fine-grained structure. PointNet <ref type="bibr" target="#b20">[21]</ref> encodes each point with Multi-layer Perceptron (MLP) and eliminates the unordered set problem via max-pooling operation. PointNet++ <ref type="bibr" target="#b22">[23]</ref> further introduces the hierarchical architecture for point cloud's local feature aggregation. Point-wise convolution proposed by PointCNN <ref type="bibr" target="#b10">[11]</ref>, PAConv <ref type="bibr" target="#b47">[48]</ref> and others <ref type="bibr" target="#b7">[8]</ref> construct permutation-invariant convolution.</p><p>Local and global features capture different aspects of the shape. The key question is: how to integrate local and global information while maintaining separate processing to prevent over smoothing between them.</p><p>From a representation perspective, projection-based representations are better suited for global part-whole structure relationships, while point representations have advantages on local parsing of shape details. Motivated by this, PVCNN <ref type="bibr" target="#b14">[15]</ref> designs a point-voxel module to parallelly encode point clouds from dual modalities (voxels and points), by leveraging 3D convolution for the voxel branch and point-wise MLP for the point branch.</p><p>The conventional wisdom of applying local convolution on voxel and global attention on points makes sense from a computational viewpoint. Still, it achieves the opposite goal of extracting global structure from voxel representation and local shape information from the point representation. Furthermore, the simple combination (by addition) of two modalities (local and global) could blur out local details.</p><p>We propose a reverse design, where we apply a global process on voxels to extract long-range structure relationships and a local one on the points to compute detail shape features. We call our Dual-Scale network for Point cloud understanding with high-frequency encoding, as DSPoint. Specifically, we disentangle the point representation along the channel dimension: one part encoding local feature and the other part for the global feature. In each processing block, local channels are processed by point-wise dynamic convolution <ref type="bibr" target="#b47">[48]</ref>, and the global ones are firstly converted to voxelized representation and then parsed via global attention mechanism <ref type="bibr" target="#b2">[3]</ref>. From the representation perspective, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, one can compute fine-grained geometry features from 3D points locally. At the same time, the voxelization process naturally aggregates neighboring points' features and is suitable for global part-whole structural relationship reasoning. From the computation view, convolution is natural for local feature aggregation, but attention is designed for long-range dependency modeling. Consequently, processing such two modalities with convolution on the local point level and attention on the voxel level could be a good choice for point cloud understanding.</p><p>After the concurrent pathways, the voxel modality is back-projected to points by assigning each voxel's feature to every point within it. Here, we obtain two part-channel representations of each point: voxel-wise global feature and point-wise local feature. We observe that the naive addition of different modalities often results in feature misalignment and blurring. To effectively exchange local-global information, we build on a recently introduced Dual-stream Net(DS-Net) <ref type="bibr" target="#b16">[17]</ref> co-attention design. In this design, the global part configuration features serve as 'query' for the local shape feature 'keys', and vice versa. However, the direct application of DS-Net is still insufficient for removing across modality mis-alignment. This is because shape features of different points from one voxel usually are homogeneous, and directly fusing them with heterogeneous point-wise shape features would bring about ambiguities. Borrowing the high-frequency point encoding concept in NeRF <ref type="bibr" target="#b18">[19]</ref>, we encode each voxel's coordinate into high-frequency representation and integrate it with point-wise features. Aided by this inter-path coordinates communication, local-global shape features from dual modalities (voxels and points) can be highly aligned. The visual example in <ref type="figure">Figure 1</ref> illustrates the effectiveness of our 'reverse' design of using a 3D grid for global attention and local convolution for 3D raw points.</p><p>We summarize the contributions as below:</p><p>? We propose DSPoint, which concurrently processes point cloud with dual scales and modalities for robust local-global features extraction. ? A high-frequency fusion module is introduced by communicating high-dimensional coordinates information between voxel-wise and point-wise features. ? To illustrate our model's superiority, we experiment DSPoint on shape classification, shape and scene segmentation, respectively on ModelNet40, ShapeNet, and S3DIS datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep learning for Point Cloud. Projection-based models and pointwise models are two main branches of deep learning in 3D, distinguished by their data processing modality. Some of the projectionbased models project raw points onto a set of image planes with pre-defined <ref type="bibr" target="#b31">[32]</ref> or learnable <ref type="bibr" target="#b8">[9]</ref> viewpoints and then utilize 2D convolutions for robust feature extraction. One can combine images from different views so to minimize the information loss on the original point cloud. Still, the complex and time-consuming projection process makes this approach unpractical for real-time applications. Alternatively, some approaches transform points into spatial voxels, such as VoxelNet <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, which are uniform grid-based representations and thus can be applied 3D convolutions <ref type="bibr" target="#b17">[18]</ref> or attention mechanism <ref type="bibr" target="#b2">[3]</ref>. However, voxel-based networks confront information loss due to low-resolution quantization. It has an inpractical cubically growing running time. Point-wise networks directly process raw points with irregular distribution over 3D space. PointNet <ref type="bibr" target="#b20">[21]</ref> leverage Multi-layer Perceptron(MLP) to extract point-wise features and integrate them with a global pooling. PointNet++ <ref type="bibr" target="#b22">[23]</ref> proposes a hierarchical PointNet <ref type="bibr" target="#b20">[21]</ref> architecture to capture local contexts with sampling and grouping blocks. DGCNN <ref type="bibr" target="#b40">[41]</ref>, KPConv <ref type="bibr" target="#b35">[36]</ref> and PAConv <ref type="bibr" target="#b47">[48]</ref> further design convolutions on spatial points for better local geometry encoding. To aggregate both advantages, our DSPoint adopts dual-path architecture to concurrently encode point features with voxel branch and point branch, respectively, for understanding global and local features.</p><p>Dual-path Networks. Constructing multiple pathways for 2D deep learning has been explored by GoogLeNet <ref type="bibr" target="#b33">[34]</ref>, Efficient-Net <ref type="bibr" target="#b34">[35]</ref>, and MaX-DeepLab <ref type="bibr" target="#b38">[39]</ref>, in which different paths with varying feature resolutions and convolutional kernels are expected to extract distinct aspects of features. Recently, DS-Net <ref type="bibr" target="#b16">[17]</ref> and LaMa <ref type="bibr" target="#b32">[33]</ref> have designed more delicate dual-path architectures for local and global features encoding, which successively separate and fuse the two representations for sufficient cross-scale interactions. For point cloud processing, DTNet <ref type="bibr" target="#b5">[6]</ref> proposes to apply a multihead attention mechanism in transformer <ref type="bibr" target="#b36">[37]</ref> for extracting both inter-channel and inter-point features. PVCNN <ref type="bibr" target="#b14">[15]</ref> utilizes two modalities to capture point features: raw points and voxels concurrently. Therein, PVCNN encodes each raw point with MLP and each voxel with 3D convolution <ref type="bibr" target="#b14">[15]</ref> for inter-point relation modeling. Contrary to PVCNN's design, we apply point-wise convolution on neighboring 3D points for local features and global attention on all voxels for global features extraction.</p><p>High-frequency Spatial Embedding. Deep neural networks tend to focus more on low-frequency features but neglect the higher frequency counterparts according to <ref type="bibr" target="#b18">[19]</ref>, and mapping the low dimensional data into higher ones can facilitate networks for better learning abilities. NeRF <ref type="bibr" target="#b18">[19]</ref> introduces a high-frequency embedding function in a non-parametric manner. Combining this transformation with learnable MLP could improve the performance since the network can capture slight color and geometry variation in the 3D space. The positional encoding module also utilizes the function in transformer <ref type="bibr" target="#b36">[37]</ref>, which maps two or three channels' coordinates <ref type="figure">Figure 3</ref>: Input coordinates will pass through three Dual-scale blocks with residual connection, then feed through a classification head to obtain shape classification prediction. We split features along channels and pass in through the voxel-based global attention and point-based local convolution branches, respectively. Then, fuse two features with high-frequency module. into higher dimensions. In DSPoint, we adopt it for encoding 3D coordinates of dual paths and implement cross-modality coordinates interaction.</p><formula xml:id="formula_0">Dual -Scale Block ? ? ? !?# Classii/ication Head - ? ? $?% ? ? $?&amp; HF -Fusion Module ? Voxel -based Attention Point -based Convolution ? ? !?% ? ? !?% ? ? !? #% ' ? ? !? #% ' ? ? !? % ' ? ? (?(?(? % ' C C ? ? (?(?(? % ' ? ? !? #% ' ? ? !?%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we present our dual-scale network with highfrequency fusion (DSPoint) (Figure3,4). In Section 3.1, we first briefly review Dual-stream Net <ref type="bibr" target="#b16">[17]</ref> for 2D recognition. Then introduce our Dual-scale Blocks in Section 3.3. In Section 3.4, we show the high-frequency fusion module for better features alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Review of Dual-stream Net</head><p>Conventional deep neural networks for 2D recognition utilize single-stream architectures to encode the image, in which shallow layers focus on extracting fine-grained features by local convolutional kernels, and deep layers aim at capturing global representations with a large receptive field. However, local and global features describe the image from two different perspectives: one texture details and the other for long-range shape structure. Dualstream Net <ref type="bibr" target="#b16">[17]</ref> (DS-Net) proposes to maintain separate local and global visual representations, treat them equally while concurrently exchanging information between them.</p><p>Computationally, DS-Net <ref type="bibr" target="#b16">[17]</ref> splits the image feature along the channel dimension into two parts: and for dual-pathway processing. In this way, features could be disentangled and computational cost could be optimized. Local features of remain in the high resolution to preserve the visual details and are encoded by convolution layers, Global features of are downsampled to a smaller grid to filter out the low-level noise and extracted by global attention mechanism <ref type="bibr" target="#b2">[3]</ref>. We formulate the parallel propagation as</p><formula xml:id="formula_1">= Convolution( ); = Attention( ),<label>(1)</label></formula><p>where and are the specific-encoded features of and . Then, are upsampled to the original feature resolution and conduct local-global fusion with . For better blending between the two representations, DS-Net further adopts co-attention mechanism for inter-scale features alignment. During implementations of attention, supposing there are local and global features, and respectively serve as queries and extract informative features from each other by the affinity matrixes, ? ? R ? and ? ? R ? , denoted as</p><formula xml:id="formula_2">? = ? ; ? = ? ,<label>(2)</label></formula><p>where ? and ? denote the hybrid local and global features after alignment. Finally, the two representations are concatenated together and fused by a linear layer. In this dual-stream design, DS-Net obtains robust visual representation and achieves high image classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-scale Processing</head><p>Local-global Disentangling. In a point cloud, global information mainly contains overall shape properties and inter-component relationships, but local information focuses on subtle spatial geometry and density variations. Following DS-Net <ref type="bibr" target="#b16">[17]</ref>, in every processing stage, we disentangle global and local features along the channel dimension. Specifically, given a -channel point feature, we split it into with channels and with channels, where and weighing the importance between two representations and + = 1. To reserve local details, we maintain the points' spatial density for . As for downsampling , we convert the irregular points into grid-form low-resolution voxels <ref type="bibr" target="#b14">[15]</ref>. The voxelization averages all point features whose coordinates fall into the voxel grid. Compared to other downsampling methods in point clouds, such as farthest point sampling (FPS) <ref type="bibr" target="#b22">[23]</ref>, voxelization is more stable without any randomness and has the reversibility for devoxelization back to points. Also, representations from another modality could capture features from diverse aspects and thus leads to better feature extraction. Therefore, we select voxels for points' global representation. After the disentangling, we concurrently conduct the dual-scale propagation of global-local features encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual-scale Block</head><p>Demonstrated in <ref type="figure">Figure 3</ref>, our pipeline consists of three consistent Dual-scale Blocks with residuals. Input coordinates will pass through three blocks to obtain a feature representation, then a classification head will make prediction. In each block we split features channel-wise and feed one part through the point-based convolution, and the other part through voxel-based attention branches. In point-based convolution branch, we directly apply existing 3D convolution like PAConv <ref type="bibr" target="#b47">[48]</ref>. In voxel based attention branch, we voxelized points according to PVCNN <ref type="bibr" target="#b14">[15]</ref>, encode voxel coordinates using a linear layer and add to voxel features, employ a layer normalization, apply self-attention between all voxels, then devoxlied voxels to restore features. Two branches has residual connection inside shown in <ref type="figure">Figure 3</ref>(b). Details of two branches will be presented below. Last, we fuse two processed features with high-frequency module <ref type="figure">(Figure 3(c)</ref>), introduced in section 3.4. Implementation details are listed in section 4.</p><p>Point-scale Local Encoding. Convolution is natural for local feature extraction because it encodes translation-invariant properties and its limited receptive field makes it easier to compute and learn. We use the point-specific convolution operations from <ref type="bibr" target="#b7">[8]</ref>. For a point, ( , , ) with the local receptive field containing points, the convolutional kernel is dynamically generated by their relative coordinates via a Multi-layer Perceptron (MLP). We formulate the convolution operation, which transforms ( , , ) into encoded local feature ( , , ) as</p><formula xml:id="formula_3">( , , ) = ?? =1 ( , , ) ? ( , , ),<label>(3)</label></formula><p>where ( , , ) denotes the predicted kernel weight of neighboring point , and ? denotes element-wise product. After the point-scale convolution, at each point location contains local features capturing fine-grained information.</p><p>Voxel-scale Global Encoding. Attention mechanism [3] operates on the entire visual domain and conducts information interaction over long distances, which is good at summarizing overall structural shape properties. Therefore, we apply a multi-head attention mechanism over the voxel-scale branch for global features exploration. Supposing there are voxels, We denote the encoded global feature for voxel ( , , ) as</p><formula xml:id="formula_4">( , , ) = ( , , ) ( , , ),<label>(4)</label></formula><p>where ( , , ) ? R 1? denotes affinity matrix between the voxel ( , , ) with all other voxels. Because of the coarser resolution of transformed voxel grids, attention's computation and memory costs are much manageable. In addition, without low-level spatial detail distractions, can concentrate on long-range part-whole object structure relationships. Afterward, the devoxelization is conducted to project low-density voxels back to the original points, during which the voxel feature is assigned to each point within.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fusion with High-frequency Function</head><p>Shown as Figure4, we have obtained the separately encoded and for the point cloud and require effective fusion of the two representations. However, different from 2D images, the misalignment problem in 3D lies mainly in the mismatch of spatial locations, since is regional homogeneous due to devoxelization process, but is relatively point-wise heterogeneous. Besides, these modality transformations are implemented through approximation and will cause features to lose their high-frequency information. To alleviate these problems, we proposed High-frequency Fusion Module <ref type="figure">(Figure 3(c)</ref>), injecting coordinates information of another modality through high-frequency fusion to help cross-modality alignment.</p><p>We refer to high-frequency functions proposed in NeRF <ref type="bibr" target="#b18">[19]</ref>, which maps a low-frequency point coordinates into higher-dimensional vectors via a set of trigonometric functions:</p><formula xml:id="formula_5">( ) = ( , sin(2 0 ), cos(2 0 ), ? ? ? , sin(2 ?1 ), cos(2 ?1 ))<label>(5)</label></formula><p>Here is a function mapping point coordinates from R into a higher dimensional space R 2 , dimension changes from 3 to 3 +3. By combining this non-parametric transformation with the learnable MLP, the issue of neglecting low-frequency information by the deep network would be largely relieved, such as subtle variations on local geometric and density.</p><p>Specifically, we denote all the coordinates of voxels from the voxel-wise branch as Coords , and all those of points from the point-wise branch as Coords . Respectively, we encode them via high-frequency function (?) as hf = (Coords ); hf = (Coords ),</p><p>where hf and hf represent the high-frequency encoded voxels'(globle) and points'(local) coordinates, whose channels are the same as and . Then, we aggregate the voxel-related hf with point-wise features , and the point-related hf with voxel-wise features , both with simple addition. On top of that, a linear layer is applied for respectively blending and transforming dimension of the above paired voxel/point-coordinate high-frequency features with point/voxel-wise features, formulated as</p><formula xml:id="formula_7">? = + Linear(hf ); ? = + Linear(hf ),<label>(7)</label></formula><p>where ? and ? denote the hybrid features of the local and global features. This cross-scale communication of high-frequency coordinate information can mitigate the misalignment issue because hf brings about homogeneous alignment for local feature and, while hf carries heterogeneous discrimination for . Finally, we concatenate ? and ? through the channel dimension and apply a linear layer for the final fusion, which restore point features into</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HF -Fusion Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HF ? Encode Linear</head><p>Devoxelization HF ? Encode Linear <ref type="figure">Figure 4</ref>: High-frequency fusion module. We encode coordinates of one modality with a high-frequency embedding function and inject into features of another branch to help feature alignment.</p><formula xml:id="formula_8">C C Concatenate ? ? !?# ? ? !? # $ ? ? !? %# $ ? ? !? %# $ ? ? !? # $ ? ? !?(%'(%) ? ? !?(%'(%) ? ? !?% ? ? !?%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Shape Classification</head><p>We evaluate on ModelNet40 <ref type="bibr" target="#b44">[45]</ref> dataset for object classification. This dataset contains 40 categories of 12,311 meshed CAD models, 9,843 of them are used for training and the rest 2,468 for testing. We follow the same data preprocessing in PointNet <ref type="bibr" target="#b20">[21]</ref>   <ref type="bibr" target="#b35">[36]</ref> 93.2 DSPoint w. PointConv <ref type="bibr" target="#b42">[43]</ref> 92.8 DSPoint w. PAConv <ref type="bibr" target="#b47">[48]</ref> 93.5 <ref type="table">Table 3</ref>: Ablation of local operator. We use different local feature operator in our local branch and evaluate its performance. MLP stands for shared MLP from PointNet <ref type="bibr" target="#b20">[21]</ref>. SG stands for sample and grouping from PCT <ref type="bibr" target="#b4">[5]</ref>.</p><p>translation and shuffling all points. We only employ coordinates information as input, without extra use of normal vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experiment Setting.</head><p>Model Architecture. Shown in <ref type="figure">Figure 3</ref>(a), our DSPoint consists of 3 consecutive Dual-scale Blocks with skip-connection, and feeds features into a classfication head to obtain prediction. The channel dimension for each block is 64, 64, 128, respectively. Voxelization resolution for each block is 8, 6, 4, respectively. We incorporate PAConv <ref type="bibr" target="#b47">[48]</ref> as our local feature extraction operator, with 8 nearest neighbors and 8 weight matrices. The channel ratio between the local and global branches is 3:1, and of high-frequency encoding is 10. The classification head has a Linear layer to project embedding dimension from 128 to 1024, followed by a max pooling layer to aggregate all points, then two Linear layers will project features' embedding from 1024 to 512, and 512 to 40, which is the number of classes. Then we apply a softmax to obtain classification score. Linear layers are all connected with batch normalization and ReLU   activation function. To be environmental-friendly, we do not train massive amounts of models then use their ensembled score, but train only one model.</p><p>Training Setting. We use Adam optimizer, and train our model for 250 epochs and preserve the model with the best evaluation accuracy during training. During training, we set the batch size to 32, learning rate to 0.001, weight decay to 1e-4, and reduce learning rate when a metric has stopped improving at the factor of 0.5, the patience of 10, and a minimum learning rate of 0.00001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Performance. The result of classification experiments on</head><p>ModelNet40 is shown in <ref type="table">Table 1</ref>. We list previous works based on the feature representation they are using. The local feature means they only process local features around each point during inference, such as PointNet <ref type="bibr" target="#b20">[21]</ref> or KPConv <ref type="bibr" target="#b35">[36]</ref>. Global feature process points globally and builds long-range dependency using attention mechanisms, such as PCT <ref type="bibr" target="#b4">[5]</ref>. There also exist other methods that process local and global features simultaneously, such as PointASNL <ref type="bibr" target="#b48">[49]</ref>. Results show that our method outperforms or is comparable with all previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To quantify the effectiveness of DSPoint, we conduct ablation studies on ModelNet40 <ref type="bibr" target="#b44">[45]</ref>, following the same experiment setting of Shape Classification mentioned above.</p><p>Dual-scale Modality. Our DSPoint incorporates point-based representation to process local information and utilize voxel-based representation to handle long-range global dependency. We claim that local voxel-based representation pools neighborhood information together in low resolution, losing subtle features important for local information processing. Processing local features with pointbased representation requires no pooling or grouping process, and could preserve nuanced differences among all points as much as possible, benefiting local learning.</p><p>Furthermore, global point-based representation needs to process a continuous infinite coordinate space, whose position embedding is complex for a network to learn during attention mechanism. Our global processing using voxel-based representation aligns all points with mesh grids and has a discrete finite coordinate space which is easy for position embedding.</p><p>To verify our claims, we run ablation studies demonstrated in <ref type="table" target="#tab_2">Table 2</ref>. In point-based global processing, we use sample and grouping <ref type="bibr" target="#b4">[5]</ref> to sample 256 points and do self-attention, and use a single Linear layer to restore point number from 256 to 1024. In voxel-based local processing, we use the same 3D voxel convolution in PVCNN <ref type="bibr" target="#b14">[15]</ref>. Results show that our modality choice with point-based local processing and voxel-based global processing has the best performance among all four combinations.</p><p>Local Operator. While efficient global feature extraction has the only option of using an attention mechanism, local feature extraction has many comparable operators. In <ref type="table">Table 3</ref>, we substitute our local branch point-based convolution with a different local feature operator and evaluate their performance. It shows that with PAConv <ref type="bibr" target="#b47">[48]</ref> consisting the local branch operator, our method has the best performance among all evaluated local operators.</p><p>High-Frequency Fusion. We dive into the utility of High-Frequency Fusion module. We examine the influence of usage (whether use it or not) and location (before or after feature processing) of the High-Frequency Fusion module. The result are shown in <ref type="table" target="#tab_4">Table 4</ref>, and we find that putting High-Frequency Fusion module after feature processing for both local and global branch will achieve the best performance. It shows that our high-frequency fusion module incorporates coordinates and narrows the gap between two modalities after dual processing to benefit learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency</head><p>We claim that our model is light-weighted and computationefficient. It utilizes point-wise convolution as the local feature extractor, which requires less parameters compared with transformerbased methods such as PCT <ref type="bibr" target="#b4">[5]</ref>. At the same time, the dual-processing makes our latency more efficient compared with other local-global methods like PointASNL <ref type="bibr" target="#b48">[49]</ref>. The comparison of parameter amount and latency are listed in <ref type="table" target="#tab_5">Table 5</ref>, where all parameters and latency are measured on a single NVIDIA 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Down-stream Task</head><p>To demonstrate the general applicability and plug-in simplicity of our method, we incorporate it into other baselines then apply to two different downstream tasks: Shape Part Segmentation and Indoor Scene Segmentation. It could be noticed that our method achieved a great trade-off by improving its efficiency by a lot margin mentioned in Table5, with a slight cost of accuracy. Such trade-off would be more worthy in the industrial field like self-driving which requires high inference speed and light model parameter amount.  <ref type="bibr" target="#b44">[45]</ref>. It demonstrates our compared baseline PAConv <ref type="bibr" target="#b47">[48]</ref> (first row), our method DSPoint (second row), and ground truth, which indicates our excellent performance of spatial continuity on part segmentation.    operator. We use channel-wise accumulation instead of channelwise splitting for plug-in simplicity, where weight between local and global branches is 4:1.</p><formula xml:id="formula_9">- - - - - - - - - - - - - - - -</formula><p>Results are listed in <ref type="table" target="#tab_8">Table 6</ref>. Although our mIoU increase compared to PAConv <ref type="bibr" target="#b47">[48]</ref> is small, <ref type="figure" target="#fig_1">Figure 5</ref> shows clear benefits from our voxel modality, which prevents points from being fragmented into many parts. Our part segmentation is far more spatially continuous in comparison. The mIoU measurement does not reflect the fragmentation problem in PAConv <ref type="bibr" target="#b47">[48]</ref>. In many practical applications, having a spatially coherent output, as in our method, is far more important than fragmented results. It proves our strong performance by maintaining plug-in simplicity and practical utility.</p><p>Indoor Scene Segmentation. We experiment on S3DIS <ref type="bibr" target="#b1">[2]</ref> dataset, containing 272 rooms out of six areas. For a fair comparison, we use Area-5 as the test set. Each point is labelled from 13 classes, like doors or walls. For each 1m ? 1m block, we sample 4096 points. We integrate our method into all four layers of encoders of PointNet++ <ref type="bibr" target="#b22">[23]</ref>, with PAConv <ref type="bibr" target="#b47">[48]</ref> as local operator, then use channel-wise summation instead of channel-wise dividing for plugin succinctness, where weight between local and global branches is 4:1. The experiment results are shown in <ref type="table" target="#tab_9">Table 7</ref>, and visualized in <ref type="figure">Figure 6</ref>, demonstrating our excellent performance, while benefiting from long-range feature integrating in recognizing isolated parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ERROR MODE ANALYSIS</head><p>Aleatoric uncertainty measures the uncertainty that how likely one sample would be misclassified as another class. Those data near the decision boundary would have high aleatoric uncertainty. As shown in <ref type="figure" target="#fig_2">Figure 7</ref>, selected misclassified samples pair are similar to the other class hence are misclassified into each other's class. This misclassification is caused by data distribution itself which couldn't even be told by human. Thus we could not improve our performance on those data by improving our model design. By our rough estimation, it limits the upper bound of classification accuracy of this dataset to around 94%-95%. Under such circumstances, it would be more worthy to improve the model's efficiency by a  Besides, it's worth noticing that even though some test samples have significant features, they are still misclassified. As shown in <ref type="figure" target="#fig_3">Figure 8</ref>, the first sample is a cup that has a handle, like some other cups in the dataset, while all vases in the dataset have no handles. Even so, this cup is misclassified as a vase. One possible explainatino could be that its small volume of handle leads to the insignificance of the feature response, while its body resembles a vase. Similarly, the bench which has a back is mistaken as a nightstand, which has no back. It might be due to the bench having a carved back which is similar to the table-board of the nightstand. In the future, to better handle such a circumstance, we could project the point cloud into the 2D plane, which limits the z-axis variance and amplify the significance of the handle as well as the bench back. By processing 2D projection and 3D point cloud simultaneously, we might achieve better performance in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose Dual-Scale Point Cloud Recognition with High-Frequency Fusion (DSPoint) to conduct dual scales and representations 3D learning. Elaborate experiments have demonstrated the effectivity of our DSPoint, guiding a possible future direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Dual modalities (voxels and points) processing: grey voxels aggregate points to represent the plane's global structure coarsely, while green 3D points describe subtle local shape details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Visualization Results of ShapeNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Aleatoric uncertainty exhibited in test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Misclassification which might be solved by processing 2D projection and 3D point cloud simultaneously. lot margin instead of improving its accuracy slightly, which aligns with our model's superiority.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Different modalities for dual-scale processing.</figDesc><table><row><cell>Local Operator</cell><cell>Accuracy</cell></row><row><cell>DSPoint w. MLP</cell><cell>91.2</cell></row><row><cell>DSPoint w. MLP + SG</cell><cell>92.4</cell></row><row><cell>DSPoint w. KPConv</cell><cell></cell></row></table><note>(Point / Voxel: point / voxel-based representation. Local / Global: local / global feature processing.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Ablation of High-Frequency Fusion. (Local / Global:</cell></row><row><cell cols="3">local or global branch. Front / Back: put High-Frequency Fu-</cell></row><row><cell cols="3">sion module before/after the feature processing. None: don't</cell></row><row><cell cols="2">use High-Frequency Fusion module.)</cell><cell></cell></row><row><cell>Method</cell><cell>Param.</cell><cell>Latency</cell></row><row><cell>PointNet[21]</cell><cell>3.47M</cell><cell>13.6</cell></row><row><cell>PointNet++[23]</cell><cell>1.74M</cell><cell>35.3</cell></row><row><cell>DGCNN[41]</cell><cell>1.81M</cell><cell>85.8</cell></row><row><cell>KPConv[36]</cell><cell>-</cell><cell>120.5</cell></row><row><cell>FPConv[12]</cell><cell>-</cell><cell>-</cell></row><row><cell>PAConv (*PN)[48]</cell><cell>-</cell><cell>-</cell></row><row><cell>PCT[5]</cell><cell>2.88M</cell><cell>92.4</cell></row><row><cell>PT 2021[55]</cell><cell>-</cell><cell>530.2</cell></row><row><cell>PointASNL [49]</cell><cell>-</cell><cell>923.6</cell></row><row><cell>Ours</cell><cell>1.16M</cell><cell>214.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Efficiency evaluation measured on ModelNet40</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results of Shape Part Segmentation on ShapeNet Parts<ref type="bibr" target="#b43">[44]</ref>, evaluating mean class and instance IoU, and IoU within each class. We only train one model instead of using multiple models ensemble. (Result in brackets: the re-implementation result by us.)Figure 6: Visualization of Indoor Scene Segmentation on S3DIS<ref type="bibr" target="#b1">[2]</ref> Dataset. We project scenes onto a plane and visualize them in low-resolution to benefit comparison. Global attention on the 3D grid incorporates information from non-adjacent parts and helps detect spatially isolated points while maintaining local label consistency within the object parts.Shape Part Segmentation. We evaluate our model on ShapeNet Parts<ref type="bibr" target="#b43">[44]</ref> benchmark. It comprises 16,881 shapes (14,006 for training and 2,874 for testing) with 16 categories labeled in 50 parts. For each shape, we sample 2,048 points. We incorporate our methods to the last three layers of DGCNN<ref type="bibr" target="#b40">[41]</ref> with PAConv<ref type="bibr" target="#b47">[48]</ref> as local Method mAcc mIoU ceiling floor wall beam column window door chair table bookcase sofa board clutter</figDesc><table><row><cell>Local Feature</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results of Indoor Scene Segmentation on S3DIS[2] tested on Area 5. Evaluate mean accuracy, mean IoU, and IoU within each class. We only train one model instead of using multiple models ensemble.(Result in brackets: the re-implementation result by us.</figDesc><table /><note>? :CUDA implementation)</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointnetlk: Robust &amp; efficient point cloud registration using pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7163" to="7172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05304</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PCT: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Feng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Xian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Qiang</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13044</idno>
		<title level="m">Dual Transformer for Point Cloud Analysis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11873" to="11882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="820" to="830" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fpconv: Learning local flattening for point convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8778" to="8785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relationshape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03739</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud recognition via distributions of geometric distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14734</idno>
		<title level="m">Errui Ding, Baochang Zhang, and Shumin Han. 2021. Dual-stream network for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senbo</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09035</idno>
		<title level="m">Lidar point cloud guided monocular 3d object detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spherical fractal convolutional neural networks for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="452" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A network architecture for point cloud classification via automatic depth images generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Rahmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4176" to="4184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Close-range scene segmentation and reconstruction of 3D point cloud maps for mobile manipulation in domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoltan Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning 3d shapes as multi-layered height-maps using 2d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kripasindhu</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basavaraj</forename><surname>Hampiholi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
	<note>Evangelos Kalogerakis, and Erik Learned-Miller</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Mashikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Remizova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Silvestrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naejin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshith</forename><surname>Goka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07161</idno>
		<title level="m">Kiwoong Park, and Victor Lempitsky. 2021. Resolution-robust Large Mask Inpainting with Fourier Convolutions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5463" to="5474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3523" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with pseudolidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">3d shapenets for 2.5 d object recognition and next-best-view prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5670</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3173" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Teaser: Fast and certifiable point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingnan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Carlone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="314" to="333" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A new approach of point cloud processing and scene segmentation for guiding the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunmin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IET International Conference on Biomedical Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2282" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Point cloud semantic scene segmentation based on coordinate convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinglin</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">1948</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">2021. Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
