<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
							<email>piyushsharma@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
							<email>dingnan@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
							<email>rsoricut@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets are often collected with overrestrictive requirements inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pretraining data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [70] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for visionand-language pre-training. We perform an analysis of this dataset and benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks. 1 1 Our dataset is available at https : / / github . com / google-research-datasets/conceptual-12m.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transfer learning using pre-training and fine-tuning has become a prevalent paradigm in computer vision, natural language processing, and vision-and-language (V+L) research. It has been shown, for instance, that V+L pretraining leads to transferrable joint representations that benefit multiple downstream V+L tasks, including visual question answering, image and text retrieval, and referring expression comprehension <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56]</ref>.</p><p>What makes V+L pre-training successful? On one hand, this is due to advances in architectures and modeling that are mainly inspired by BERT and similar models in natural language understanding and generation <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b82">82,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b66">66]</ref>. In particular, the idea of using flexible self-attention mech- <ref type="figure">Figure 1</ref>: CC12M Even when the alt-texts do not precisely describe their corresponding Web images, they still provide rich sources for learning long-tail visual concepts such as sumo, mangosteen, and jellyfish. We scale up vision-and-language pretraining data to 12 million by relaxing overly strict filters in Conceptual Captions <ref type="bibr" target="#b70">[70]</ref>. anisms via high-capacity multi-layer Transformers <ref type="bibr" target="#b78">[78]</ref>, in combination with self-supervised learning objectives such as masked language modeling <ref type="bibr" target="#b25">[25]</ref>, has proven to be effective and widely applicable. On the other hand, the availability of large-scale labeled and weakly-labeled data in the V+L domain <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b70">70]</ref> is truly what enables such models to learn associations between the two modalities.</p><p>In either vision or language community, one notable trend is that scaling up training data is useful. In contrast, datasets in V+L research remain relatively limited in terms of scale and diversity. The capability of JFT-300M <ref type="bibr" target="#b76">[76]</ref> and Instagram <ref type="bibr" target="#b58">[58]</ref> over orders-of-magnitude smaller Im-ageNet <ref type="bibr" target="#b69">[69]</ref> has been put to test on multiple downstream image classification and object detection tasks. In NLP, the size of pre-training data sources for training deep language models rose from the 20GB BooksCorpus <ref type="bibr">[90]</ref>+English Wikipedia in BERT <ref type="bibr" target="#b25">[25]</ref>, to the 570GB dataset in GPT-3 <ref type="bibr" target="#b11">[12]</ref> and the 745GB C4 dataset in T5 <ref type="bibr" target="#b66">[66]</ref>.</p><p>In contrast, V+L datasets are limited in two ways. First, the effective sizes of popular V+L datasets are low. The number of images in these datasets range from fewer than a few hundred thousands <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b28">28]</ref> to several millions <ref type="bibr" target="#b70">[70]</ref>, with lower text quality as the scale increases. Second, many of the small-sized datasets share the same, limited visual domain; COCO-Captions <ref type="bibr" target="#b20">[20]</ref>, Visual Genome <ref type="bibr" target="#b44">[44]</ref>, and VQA2 <ref type="bibr" target="#b27">[27]</ref> are (mostly) based on several hundreds thousand of COCO images <ref type="bibr" target="#b52">[52]</ref>. The lack in scale and diversity of visual concepts (with respect to vision/language-only counterparts) makes it hard for V+L models to perform adequately in the wild.</p><p>One major reason for these gaps is the difficulty in collecting such datasets. Unlike in image classification, "text" in V+L datasets is longer and less likely to be agreed upon, making the annotation process more costly and time-consuming. One approach to remedy this is to make use of large amounts of the alt-texts accompanying images on the Web. For instance, Sharma et al. introduced Conceptual Captions (CC3M) <ref type="bibr" target="#b70">[70]</ref>, a dataset of 3.3M image, caption pairs that result from a filtering and postprocessing pipeline of those alt-texts. Despite being automatically collected, CC3M is shown to be effective in both image captioning in the wild <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b19">19]</ref> and V+L pretraining <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56]</ref>. In other words, it provides a promising start for large-scale V+L annotations.</p><p>In this paper, we explore pushing the limits of V+L data using this approach. Our key insight is that specific downstream V+L tasks (e.g., VQA, image captioning) can be overly restrictive if the goal is to collect large-scale V+L annotations. For instance, CC3M was collected to favor highprecision texts that are fit for the downstream task of image captioning. Yet, we have witnessed this dataset being increasingly adopted for V+L pre-training <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56]</ref>, arguably beyond its original purpose.</p><p>We hypothesize that the V+L field could benefit from such an insight, and therefore we introduce Conceptual 12M (CC12M), a high(er)-recall V+L dataset for the purpose of V+L pre-training. By relaxing multiple image and text filters used in CC3M, we obtain a less precise but 4x larger V+L set of image, text pairs. We perform an analysis of this dataset and show that it covers a wider range of visual concepts.</p><p>We test our hypothesis by benchmarking the effectiveness of CC12M as a pre-training data source on several V+L tasks, in comparison to CC3M. We explore two main pretraining strategies (and more in the Supplementary material): one for vision-to-language generation and the other for vision-and-language matching. Our experiments indicate that scaling up pre-training V+L has a dramatic positive effect on image captioning, novel object captioning, and (zero-shot) image retrieval.</p><p>In summary, our main contributions are:</p><p>(a) A public larger-scale V+L pre-training dataset that covers a much wider range of concepts than existing ones. (b) Evaluation on downstream vision-to-language generation and vision-and-language matching with an emphasis on long-tail recognition that consistently shows the superiority of this dataset over CC3M. (c) State-of-the-art results on the nocaps (novel object captioning) and Conceptual Captions benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Vision-and-Language Pre-Training Data</head><p>We first review the data collection pipeline for the Conceptual Captions 3M (CC3M) outlined in Sect. 3 of <ref type="bibr" target="#b70">[70]</ref>, which we followed closely. We then describe a series of relaxation and simplification to the pipeline that results in CC12M, a much larger set of image-text pairs. Finally, we perform an analysis of CC12M in comparison with CC3M and other existing V+L datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conceptual Captions 3M: Pipeline for extracting and cleaning Image Alt-Text from the Web</head><p>The Conceptual Captions dataset consists of about 3.3M Web images and their corresponding cleaned, hypernymized Alt-texts <ref type="bibr" target="#b70">[70]</ref>. This approach leverages a promising source of (weak) supervision for learning correspondance between visual and linguistic concepts: once the pipeline is established, the data collection requires no additional human intervention. It consists of the following 4 steps: (i) image-based filtering based on size, aspect ratio, encoding format and offensive content, (ii) text-based filtering based on language, captialization, token frequency, pre-defined unwanted phrases, as well as part-of-speech (POS), sentiment/polarity, and adult content detection (using Google Cloud Natural Language APIs), (iii) imagetext-based filtering based on the number of image tags (as predicted by Google Cloud Vision APIs) that overlap with the existing text, (iv) text transformations, most notably hypernymization of named entities, including proper names of persons, organizations and locations (e.g., both "Harrison Ford" and "Calista Flockhart" are replaced by "actor"), deletion of time-related spans, and digit replacement (using # as a digit abstraction).</p><p>The large scale nature and the high degree of textual and visual diversity make this dataset particularly suited to V+L pre-training <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CC12M: Relaxing filters for higher recall</head><p>Conceptual Captions has been created to work out-ofthe-box for training image captioning models, and thus it involves substantial image, text, and image-text filtering and processing to obtain clean, high-precision captions. As a result, this approach comes at the cost of low recall (many potentially useful image, Alt-text pairs are discarded). However, this trade-off may not be optimal if the dataset is to  <ref type="table">Table 1</ref>: Basic statistics of CC12M vs. CC3M be used primarily for V+L pre-training. Motivated by this, we follow a similar procedure as the one described in <ref type="bibr" target="#b70">[70]</ref> but relax some of its filters, and construct the dataset called Conceptual 12M (CC12M), as detailed below.</p><p>Filtering. As described above, the construction of CC3M used three main filtering types <ref type="bibr" target="#b70">[70]</ref>: image-based, textbased, and image-text-based. To arrive at CC12M, we keep the image-text filtering intact, and relax the unimodal filters only. First, for image-based filtering, we set the maximum ratio of larger to smaller dimension to 2.5 instead of 2. We still keep only JPEG images with size greater than 400 pixels, and still exclude images that trigger pornography detectors. Second, in text-based filtering, we allow text between 3 and 256 words in the alt-text. We still discard candidates with no noun or no determiner, but permit ones without prepositions. We discard the heuristics regarding high unique-word ratio covering various POS tags and word capitalization. We set the maximum fraction of word repetition allowed to 0.2. Given a larger pool of text due to the above relaxations, the threshold for counting a word type as rare is increased from 5 to 20.</p><p>Text transformation. The main motivation for CC3M to perform text transformation is that a majority of candidate captions contain ultrafine-grained entities such as proper names (people, venues, locations, etc.), making it extremely difficult to learn as part of the image captioning task. In contrast, we are not restricted by the end task of image caption generation. Our intuition is that relatively more difficult pre-training data would lead to better transferability. We thus do not perform hypernimization or digit substitution as in <ref type="bibr" target="#b70">[70]</ref>. The only exception to the "keep alt-texts as raw as possible" rule is performing person-name substitutions, which we identify as necessary to protect the privacy of the individuals in these images. For this step, we use the Google Cloud Natural Language APIs to detect all named entities of type Person, and substitute them by a special token PERSON . Around 25% of all the alt-texts in CC12M are transformed in this fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Characteristics of CC12M</head><p>We provide an analysis of CC12M along multiple dimensions, focusing on comparing it to the most relevant CC3M. Additional analyses are in the supplementary material.</p><p>Basic statistics. As seen in <ref type="table">Table 1</ref>, CC12M consists of 12.4M image-text pairs 2 , about 4x larger than CC3M. It has a much lower token (word count) to type (vocab size) ratio, <ref type="bibr" target="#b1">2</ref> Extracted as of May 2020. indicating a longer-tail distribution and a higher diversity degree of the concepts captured. Lastly, the average caption length of CC12M is much longer. This is overall achieved by our relaxation of the filters, especially the text one.</p><p>Quality. We compute a rough estimate of precision on 100 examples by asking two annotators to rate how well the given alt-text fits the image on a 1-5 scale: 1 (no fit), 2 (barely fit), 3 (somewhat), 4 (good fit, but disfluent language), 5 (perfect). We define precision as the fraction of captions with a score 4 or above. We see a drop in precision, 76.6% vs. 90.3% as reported for CC3M ( <ref type="table" target="#tab_5">Table 2</ref> in <ref type="bibr" target="#b70">[70]</ref>). This analysis points to the precision/recall tradeoff in transitioning from CC3M to CC12M. <ref type="figure">Fig. 1</ref> illustrates such a tradeoff: the "jellyfish" example would have been filtered out from CC3M (due to a high percentage of nouns and a lack of proprositions), but it is included in CC12M.</p><p>Visual concept distribution. We use the caption text tokens to represent the visual concepts. The long tail of visual concepts that emerge in CC12M spans many categories, and can be attributed to (1) a dramatic increase in scale, and (2) the absence of fine-grained entity hypernymization. We list some of them here to illustrate this point, in the format of " word frequency in CC3M ? ? frequency in CC12M ": ? 552, jellyfish 456 ? ? 2901. We also visualize the head of the distribution in <ref type="figure" target="#fig_0">Fig. 2</ref>. We observe that "person" becomes much more frequent due to person substitution with the token " PERSON ". Moreover, there are fewer "actor", "artist", "(football) player", as a result of removing hypernymization.</p><formula xml:id="formula_0">luffy 0 ? ? 152,</formula><p>Finally, we inspect tokens that are unseen in CC3M.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Layer Transformer Image Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Layer Transformer Image Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Captioning (IC)</head><p>[EOS]   We observe that these tokens may occur very frequently in CC12M if they are fine-grained instances such as locations ("france," "africa," "dc," "toronto") or digits ("2019", "10", "2018", "2020"). This is due to the removal of hypernymization and the dropping of time-related span deletion.</p><p>Biases. We study the context in which several sensitive terms related to gender, age, race, ethnicity appear such as "black"/"white"/"asian"/"african"/"indian", "man"/"woman", "young"/"old", etc. We observe no large biases in the distribution of these terms, either in terms of co-occurrence between sensitive term pairs or with other tokens. Furthermore, we check the distribution of web domains and, similar to visual concepts, we find this to be diverse and long-tail: &gt;100K with &gt;40K contributing &gt;10 samples. We take our preliminary study as a positive indication of no severe biases stemming from particular domains or communities. Finally, we provide a Broader Impact statement in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluating Vision-and-Language Pre-Training Data</head><p>The previous section describes CC3M and our CC12M. In this section, we evaluate both datasets on their ability to benefit V+L downstream tasks, measuring the impact from visual grounding produced under the two settings. For the sake of comparison, we do not include the images that appear in CC3M in CC12M in our experiments.</p><p>We focus on the two most fundamental V+L tasks: vision-to-language generation and vision-and-language matching. In both cases, our emphasis is on (i) the simplest setting in which the learning objectives during pre-training and downstream tasks match, and (ii) long-tail recognition and out-of-distribution generalization, as we believe this is where pre-training has the most impact. <ref type="figure" target="#fig_1">Fig. 3</ref> and <ref type="table" target="#tab_5">Table 2</ref> summarize our experimental setup, in terms of the downstream tasks and the fine-tuning and evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vision-to-Language Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pre-Training Tasks</head><p>We use image captioning (ic) as the pre-training task. The task is to predict the target caption given image features. To train the model parameters, we use the standard cross entropy loss given the groundtruth caption.</p><p>Note that there exist vision-to-language generation pretraining strategies that are different from ours. For instance, Zhou et al. <ref type="bibr" target="#b88">[88]</ref> adapt BERT <ref type="bibr" target="#b25">[25]</ref> to generate text. As masked language modeling is used for pre-training, there is no decoder and, at inference time, text is generated using the encoder network one token at a time, appending the mask token to the image and the text generated so far. Thus, this approach is inefficient as the number of passes over the input image is linear in the desired caption length. It is also unclear how to incorporate advanced decoding schemes such as beam search, top-k sampling, or nucleus sampling (see, e.g., <ref type="bibr" target="#b31">[31]</ref>) with such an approach. Finally, our experiments (see Supplementary material) suggest that the ic pre-training task is superior to its masked variants and justify using the simple ic learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Downstream Tasks</head><p>Our downstream tasks are selected to measure progress toward solving image captioning in the wild. They also stand to benefit from visual grounding, especially since pretraining, by definition, is expected to cover a wider range of (long-tail) visual concepts than fine-tuning datasets.</p><p>nocaps <ref type="bibr" target="#b1">[2]</ref> is a recent object-captioning-at-scale benchmark consisting of 4,500 validation and 10,600 test images with 10 hidden reference captions. Unlike in the standard image captioning setting, nocaps's distributions of images during training (COCO Captions) and evaluation (Open Images) are different: the Open Images dataset <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b42">42]</ref> covers one order of magnitude more objects (600 classes) than COCO [52] (80 classes). This discrepancy defines the challenge: solutions must be able to learn to describe novel concepts from sources external to the COCO training set, such as text corpora, knowledge bases, or object detection datasets. In the Supplementary material, we provide details on the nocapsleaderboard. In addition, besides CC3M and CC12M, we also explore using the Open Images Localized Narratives dataset (LocNar) <ref type="bibr" target="#b64">[64]</ref>, as an alternative "indomain" (from a visual standpoint) pre-training data source.</p><p>Localized Narratives (LocNar) <ref type="bibr" target="#b64">[64]</ref> is a collection of datasets with images that are paired with captions obtained by converting speech to text via ASR and manual postprocessing it <ref type="bibr" target="#b2">3</ref> . Inspired by the setting in nocaps, we use the COCO <ref type="bibr" target="#b52">[52]</ref> portion (train split of around 130K images) for training/fine-tuning, and Open Images <ref type="bibr" target="#b45">[45]</ref> portion of evaluation (val split of around 40K images). Note that the LocNar captions are much longer than standard captioning datasets (41.8 words/caption), setting it apart from nocaps.</p><p>Conceptual Captions 3M <ref type="bibr" target="#b70">[70]</ref> is our main reference for V+L pre-training data source. At the same time, the image captioning task on this dataset itself is a valuable benchmark for vision-to-language generation in the wild. Thus, we adopt it as a downstream task for CC12M. This means that, in the case of CC3M, from-scratch and pre-training settings collapse.</p><p>Evaluation metrics. To measure the performance on image caption generation, we consider the standard metrics BLEU-1,4 <ref type="bibr" target="#b62">[62]</ref>, ROUGE-L <ref type="bibr" target="#b51">[51]</ref>, METEOR <ref type="bibr" target="#b9">[10]</ref>, CIDEr-D <ref type="bibr" target="#b79">[79]</ref>, and SPICE <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vision-and-Language Matching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pre-training Tasks</head><p>In visual-linguistic matching (vlm), the task takes as input both image and text features and predicts whether the input image and text are matched. To train the model's parameters, we use a contrastive softmax loss, for which the origi- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Downstream Tasks</head><p>The task of caption-based image retrieval (IR) is to identify a relevant image from a pool given a caption describing its content. The Flickr30K dataset <ref type="bibr" target="#b63">[63]</ref> consists of 31,000 images from Flickr, each associated with five captions. Following existing work <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b55">55]</ref>, we use 1,000 images for validation, 1,000 images for testing, and use the rest of imagetext pairs for model training.</p><p>We further consider zero-shot caption-based image retrieval <ref type="bibr" target="#b55">[55]</ref> on the Flickr30K dataset. The term "zero-shot" refers to the setting in which we discard training data and apply pre-trained models "as-is", i.e., without fine-tuning on the target task.</p><p>Finally, we further evaluate our retrieval system on the Localized Narratives dataset <ref type="bibr" target="#b64">[64]</ref> (see Sect. 3.1.2). We use the LocNar Flickr30K portion (train split of 30,546 images, and test split of 1000 images) for training and evaluation.</p><p>Evaluation metrics. To measure the performance on image retrieval, we consider the standard metrics Recall@1 (R1), Recall@5 (R5), and Recall@10 (R10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Representing images and texts. We use Graph-RISE <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref> to featurize the entire image. We train a Faster-RCNN <ref type="bibr" target="#b68">[68]</ref> on Visual Genome <ref type="bibr" target="#b43">[43]</ref>, with a ResNet101 <ref type="bibr" target="#b29">[29]</ref> backbone trained on JFT <ref type="bibr" target="#b30">[30]</ref> and fine-tuned on Ima-geNet <ref type="bibr" target="#b69">[69]</ref>. We select top-16 box proposals and featurize each of them with Graph-RISE, similar to <ref type="bibr" target="#b19">[19]</ref>. Inspired by <ref type="bibr" target="#b50">[50]</ref>, we obtain up to 16 image tags from the Google Cloud Vision APIs, and treat them as text inputs to our model. These global, regional, and tag features end up being represented as a bag of 1+16+16 vectors, serving as bottom-up features <ref type="bibr" target="#b6">[7]</ref> for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Learning.</head><p>For ic-based pre-training and downstream tasks, we follow the state-of-the-art architecture that heavily rely on self-attention <ref type="bibr" target="#b78">[78]</ref> or similar mechanisms <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b23">23]</ref>. We implement a Transformerbased encoder-decoder model, using <ref type="bibr" target="#b19">[19]</ref> as a starting point. In addition, we encode each feature vector with a deeper embedding layer and apply layer normalization <ref type="bibr" target="#b8">[9]</ref>. Following <ref type="bibr" target="#b55">[55]</ref>, we encode the corners and the area of bounding boxes and apply layer normalization when combining geometric and regional semantic features. These modifications lead to an improved CIDEr score of 100.9 on the CC3M dev benchmark <ref type="table" target="#tab_13">(Table 7)</ref>, vs. 93.7 as reported by <ref type="bibr" target="#b19">[19]</ref>. We describe additional details in the supplementary material, including infrastructure description, runtime, model size, hyperparameter ranges and tuning methods, and the configuration of the best-performing model.</p><p>For the vlm-based pre-training and downstream tasks, we reuse the architecture above but discard the decoder. We use mean pooling to obtain a fixed-length vector for each modality, and compute the product of the transformed (lastlayer Transformer encoder representation) image and the transformed text before applying softmax. <ref type="table">Table 3</ref> shows our results on nocaps. We report in Row 1 the performance of our baseline model without pretraining. Rows 2-3 show the performance of off-the-shelf captioning systems trained on CC3M and CC12M, respectively. This indicates the "raw" power (zero-shot setting) of the pre-trained network in generating captions out of the box. We note that, without fine-tuning on COCO Captions, the model underperforms our baseline numbers on all    metrics, which is indicative of the need for the model to learn the COCO captioning style, to which the existing automatic metrics are quite sensitive. In addition, we observe a slightly better performance by CC3M except for BLUE4 and SPICE. This illustrates the benefit of data processing and bias toward high-precision captions present in CC3M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Vision-to-Language Generation</head><p>With a fine-tuned model, the benefit of transfer learning using pre-training on this task is clear (Row 1 vs. Rows 4,5,6), with CC12M outperforming CC3M by +14.2 CIDEr points and another +2.8 with CC3M+CC12M. <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates this effect; scaling up pre-training data benefits learning multimodal correspondences from a much larger pool of concepts, potentially making the model less susceptible to hallucinations (e.g., guessing "microphone" as it has not seen "bagpipes" in the training set), and also more informative (e.g. choosing "sumo wrestlers" over "men"/"people"). <ref type="table" target="#tab_9">Table 4</ref> compares our best model (ic pre-trained on CC3M+CC12M) to existing state-of-the-art results on nocaps, and show that ours achieves state-of-the-art performance on CIDEr, outperforming a concurrent work <ref type="bibr" target="#b32">[32]</ref> that uses a different pre-training approach directly on the Open Images dataset, which nocaps is based on. Importantly, we observe that the gain in the overall score can be largely attributed to the out-of-domain performance (3rd column). This result indicates that, although the annotation protocol for nocaps uses the priming of annotators to mention one or more of displayed fine-grained ground-truth object classes (e.g., "red panda") present in the image <ref type="bibr" target="#b1">[2]</ref>, the large-scale and natural fine-grainedness of CC12M succeeds in correctly learning to generate captions containing such concepts, in spite of being textually out-of-domain.</p><p>Following <ref type="bibr" target="#b1">[2]</ref>, we also report results of our best model on the COCO Captions val2017 split, see <ref type="table" target="#tab_10">Table 5</ref>, with 5K and 10K fine-tuning steps. We note that, since we do not rely on techniques such as constrained beam search (CBS) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref> that constrain the model outputs, we do not suffer from the large performance trade-offs seen with the previous solutions (degradation on in-domain performance as out-of-domain performance increases, see each model vs. "reference"). Our result on out-of-domain data, as we vary the number of fine-tuning steps (last two rows), suggests that over-fine-tuning on COCO Captions may incur a cost in terms of poor generalization.</p><p>A second set of results is reported in <ref type="table" target="#tab_11">Table 6</ref>. We observe that, even when the task requires the generation of much longer captions for LocNar, CC12M achieves superior performance (as measured by CIDEr) compared to CC3M as pretraining data. However, the gain is smaller compared to    the one observed for nocaps. We attribute this to the fact that injecting novel concepts into longer texts is harder, and also the fact that LocNar does not use priming in their annotation process, leading to more generic terms in their annotation ("musical instruments" vs. "trumpets"). Finally, we fine-tune our best pre-trained model (ic on CC12M) using CC3M in <ref type="table" target="#tab_13">Table 7</ref>, and then evaluate on the dev split. We find that we improve the CIDEr score on the dev split from 100.9 to 105.4 (+4.5 CIDER points). We note that the model trained on CC12M and evaluated directly on the CC3M dev set (without fine-tuning on the CC3M train  split) obtains a low dev CIDEr of 39.3. This again indicates that the additional processing steps done for CC3M (e.g., hypernimization) result in captions that are different enough from the ones in CC12M to require a fine-tuning step. <ref type="table" target="#tab_15">Table 8</ref> reports zero-shot and default IR performance on Flickr30K as well as default IR performance on LocNar Flickr30K. The results are consistent with those in visionto-language generation. First, both CC3M and CC12M are beneficial, improving over "from-scratch" training (Pretraining data as "None") by at least 8.6% and 6.6% in R1 on Flickr30K and LocNar Flickr30K, respectively. Additionally, CC12M significantly outperforms CC3M in all cases. Finally, combining the two datasets (CC3M+CC12M) results in even better performance. We provide qualitative results and additional discussion in the supplementary ma-  terial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Vision-and-Language Matching</head><p>Our zero-shot IR results (the three rows in <ref type="table" target="#tab_15">Table 8</ref> with fine-tuning data as "None") are also competitive to the stateof-the-art, despite the fact that our model is much smaller (6 layers of transformers of hidden layer size 512 with 8 attention heads vs. 12 layers of size 768 with 12 attention heads) and uses late fusion instead of early fusion. In particular, our zero-shot IR on CC3M outperforms the one in ViLBERT <ref type="bibr" target="#b55">[55]</ref> (35.4 vs. 31.9 in R1), while the CC12M performance goes up by +7.1% R1 to 42.5, and an additional +4.6% R1 to 47.1 when using CC3M+CC12M, surpassing the "from-scratch" setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>V+L Pre-training. V+L pre-training research makes use existing large-scale datasets with image-text pairs. A majority of these resources are image captioning datasets. CC3M <ref type="bibr" target="#b70">[70]</ref> has been the most popular for pre-training <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b50">50]</ref>. Smaller but less noisy SBU Captions <ref type="bibr" target="#b61">[61]</ref> (1M) and COCO Captions <ref type="bibr" target="#b20">[20]</ref> (106K) datasets are also of high interest. Some work <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b50">50]</ref> use V+L resources collected for dense captioning or visual question answering (VQA), such as VG <ref type="bibr" target="#b44">[44]</ref>, VQA2 <ref type="bibr" target="#b27">[27]</ref>, and GQA <ref type="bibr" target="#b34">[34]</ref>. In contrast, CC12M is not collected for specific target tasks, and thus it is order-of-magnitude larger than those datasets. <ref type="bibr" target="#b3">4</ref> Furthermore, it is much more visually diverse, especially given the fact that COCO Captions, VG, VQA2, GQA are built on top of COCO images <ref type="bibr" target="#b52">[52]</ref> or its subsets.</p><p>Objectives in V+L pre-training research are largely influenced by BERT <ref type="bibr" target="#b25">[25]</ref>. Masked language modeling has <ref type="bibr" target="#b3">4</ref> Recently appearing after we submitted our paper, ALIGN <ref type="bibr" target="#b36">[36]</ref>, CLIP <ref type="bibr" target="#b65">[65]</ref>, WIT <ref type="bibr" target="#b72">[72]</ref>, WenLan <ref type="bibr" target="#b35">[35]</ref> all explore enlarging Web-scale data for V+L pre-training with success (albeit with different focuses), further confirming our intuition that scale is a critical factor. been extended to visual region inputs, while the next sentence prediction is analogous to vlm. Based directly upon BERT, V+L pre-training research has largely been focused on V+L understanding <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56]</ref>, with classification or regression tasks that do not involve generation. One exception is UnifiedVL <ref type="bibr" target="#b88">[88]</ref>, which pre-trains a unified architecture for both image captioning (generation) and VQA (understanding). Our work focuses on simpler objectives and consider one at a time. This allows for a "clean" study of the effect of pre-training data sources. At the same time, we also pre-train vision-to-language generation and encoder-decoder jointly as opposed to an encoderonly setup. Our work also shows that ic is a strong objective for vision-to-language generation with respect to the widely-used masking-based objectives. Consistent with our results, ic is successfully adopted for learning visual representations for lower-level vision tasks <ref type="bibr" target="#b24">[24]</ref>.</p><p>Long-tail Visual Recognition in V+L. Addressing longtail distributions of visual concepts is an important component of V+L systems that generalize, as long and freeform texts exhibit a large number of compositional, finegrained categories <ref type="bibr" target="#b89">[89,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b19">19]</ref>. Our work focuses on downstream testbeds for V+L research that require this adaptation ability. For example, the train-test distribution discrepancy in nocaps exists in both visual (COCO vs. Open Images) and textual domains (80 to object classes vs. 600 classes). The same can be said for zero-shot image retrieval <ref type="bibr" target="#b55">[55]</ref>, in which the model must generalize visually and textually from the pre-training data sources of CC3M or CC12M to Flickr30K. Our work identifies pre-training with large-scale noisy data as a promising solution. In addition, for the task noval object captioning, our approach works more robustly across in-and out-of-domain scenarios and is simpler than the state-of-the-art techniques that utilize constrained beam search (CBS) <ref type="bibr" target="#b4">[5]</ref>, finite state machine construction plus CBS <ref type="bibr" target="#b5">[6]</ref>, generating slot-filling templates <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b81">81]</ref>, and copying mechanisms <ref type="bibr" target="#b83">[83]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce the new V+L pre-training resource CC12M, obtained by extending the pipeline in <ref type="bibr" target="#b70">[70]</ref>. We show that the scale and diversity of V+L pre-training matters on both generation and matching, especially on benchmarks that require long-tail recognition such as nocaps. Our results indicate leveraging noisy Web-scale image-text pairs as a promising direction for V+L research.</p><p>[90] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV, 2015. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Broader Impact</head><p>Our publicly-available V+L pre-training resource CC12M has the potential to positively impact multiple vision-and-language tasks. One main aspect that we have identified is a much higher degree of coverage of long-tail visual concepts than previous resources, including CC3M. As a result, we expect the models (pre-)trained on our data to be more robust in the wild than before.</p><p>In addition, our work could benefit the design of new setups for the downstream tasks that shift away from indomain (e.g., COCO/Visual Genome) to out-of-domain/inthe-wild (e.g., OID), similar to nocaps that our work focuses heavily on. The setups could also avoid the use of in-domain data during pre-training that in some cases resulting in transfer learning between (almost) identical sets of images, e.g., COCO, Visual Genome (VG), VQA2, VG QA, Visual7W, GQA, GuessWhat, and RefCOCO*.</p><p>At the same time, datasets curated from the Web could come with risks such as unsuitable content (adult content, profanity) and unintended privacy leakage <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>. We take the steps in Sect. 2.2 of the main text to mitigate both of these risks by applying the necessary image and text filtering steps and replacing each person name (celebrities' included) with the special &lt;PERSON&gt; token.</p><p>Less specific to the Web data are the unwanted dataset biases <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b80">80]</ref> that are prone to amplification by machine learning models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b87">87]</ref>. Our preliminary analysis in Sect. 2.3 of the main text shed light on the degree to which our data exhibits some aspects of these inherent biases, and we suspect that the better coverage of the tail in fact makes this issue less severe. Nevertheless, the users of this data and the systems trained on it shall be aware of such risks and other ones that might arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional analyses of CC12M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Out-of-domain (OOD) visual concepts on an expanded list of datasets</head><p>We use the 394 nocaps' out-of-domain classes as a proxy for OOD visual concepts and analyze popular vision-andlanguage datasets, in addition to CC3M and CC12M that we focus in the main text. These datasets span a wide range of use cases, both in terms of tasks (image-to-text generation, image-and-text matching, visual question answering (VQA), referring expression comprehension, and multimodal verification), and in terms of the stage during which they are used (pre-training, fine-tuning/evaluation, or both.)</p><p>? CC3M <ref type="bibr" target="#b70">[70]</ref> An instance of text is the caption associated with each image url of the training split. ? CC12M (ours) An instance of text is the caption associated with each image url. It has been used and is currently the most popular V+L pre-training dataset <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b50">50]</ref>. ? COCO Captions <ref type="bibr" target="#b20">[20]</ref> An instance of text comes from the caption associated with each image of the 2017 training split (five captions per image). This dataset is designed for the task of image captioning, and has been used for caption-based image retrieval as well. It has been used for V+L pre-training <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b50">50]</ref>. ? Visual Genome <ref type="bibr" target="#b43">[43]</ref> An instance of text comes from the caption of each region in images of the training split. This dataset aims to connect vision and language through scene graphs and is used for multiple tasks that include but not limited to dense image captioning, visual relationship detection and scene graph parsing, image retrieval and generation, and visual question answering. It has been used for V+L pretraining <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b21">21]</ref>. ? SBU Captions <ref type="bibr" target="#b61">[61]</ref> An instance of text is the caption associated with each image url of the "preferred" version of the dataset. This dataset is designed for the task of image captioning. It has been used for V+L pre-training <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b50">50</ref>]. ? VQA2 <ref type="bibr" target="#b27">[27]</ref> An instance of text is the question and the answers in each image-question-answers triplet of the train2014 + val2train2014 splits. This dataset is designed for the task of visual question answering (VQA) <ref type="bibr" target="#b7">[8]</ref>. It has been used for V+L pre-training <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b50">50]</ref>. ? RefCOCOg <ref type="bibr" target="#b59">[59]</ref> An instance of text is the referring expression in each region in images of the training split. This dataset is designed for the task of referring expression comprehension <ref type="bibr" target="#b39">[39]</ref>. ? NLVR2 <ref type="bibr" target="#b75">[75]</ref> An instance of text comes from the caption associated with each pair of images of the training split. This dataset is used for the task called multimodal verification in <ref type="bibr" target="#b56">[56]</ref>, but designed for the general task of visual reasoning. <ref type="table" target="#tab_17">Table 9</ref> summarizes the number of instances whose texts contain OOD visual concepts for all selected datasets. We use both the absolute frequency and the normalized one (per 1M text instances). Essentially, these numbers indicate the degree of OOD coverage. We find that CC12M has many more OOD instances than all other datasets by a large margin (6.7x median and 5.8x mean vs. the second best CC3M). Moreover, CC12M still prevails even after normalization to account for its size. In other words, CC12M covers these OOD classes better in both absolute and relative senses.     <ref type="figure" target="#fig_5">Fig. 5</ref> provides a more complete picture of the normalized frequency of OOD classes in these datasets, at different thresholds. It shows the number of OOD classes (yaxis) with at least K per 1M captions (x-axis). Evidently, other datasets experience sharper drops as K increases than CC12M (black solid curve). We also find that captioning datasets (solid curves) generally provide better coverage than non-captioning datasets: VQA2, RefCOCOg, and NLVR2 (dashed curves).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. The impact of the dataset size</head><p>We experiment with pre-training on randomly subsampled CC12M, 25% (3.1M) and 50% (6.2M) and evaluate the pre-trained models on novel object captioning on nocaps and zero-shot IR on Flickr30K. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the larger, the better trend, with 25% of CC12M gives rise to similar performance as CC3M.</p><p>C. Qualitive Results for Image Retrieval <ref type="figure" target="#fig_7">Fig. 7</ref> provides qualitative image retrieval results on the Flickr30K dataset, top-3 images retrieved by the fromscratch model trained on Flickr30K, as well as by two models pre-trained on CC3M and CC12M and then fine-tuned on Flickr30K. We report three cases in which CC12M pretraining helps correct the rankings from the other two models, which we suspect due to the model getting more familiar with the rare words, highlighted in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pre-Training: Data and Method Variants</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Vision-to-Language Pre-Training on LocNar</head><p>Open Images <ref type="table" target="#tab_19">Table 10</ref> considers pre-training on LocNar Open Images for the nocapsbenchmark. We observe inferior performance to both CC3M and CC12M. We attribute this to the long narratives in LocNar having drastically different styles from those from COCO Captions and nocaps. Furthermore, the data collection protocol in nocaps does not involve priming the annotator to mention object names present to the user, resulting in more generatic terms (instrument vs. guitar). This again highlights the natural fine-grainedness inherent in noisy Web data, especially in the case of nohypernymized data source (CC12M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Pre-Training Strategies</head><p>In the main text, we focus on the image captioning (ic) and the visual-linguistic matching (vlm) learning objectives both during pre-training and fine-tuning stages. Our motivation here is to keep the setup for evaluating pretraining data as "clean" as possible. However, other pretraining strategies exist in the literature and we describe and test the effectiveness of them in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 Masked Vision-to-Language Generation</head><p>Given the training image-text pairs, the ic objective predicts the text from the image. The following objectives predict (all or part of) the text from the image and (all or part of) the text. In order to encode both the image and the text, we concatenate the sequence of image feature vectors and the sequence of text token feature vectors, and use the Transformer encoder to encode them <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b74">74]</ref>. This   vanilla fusion is effective, shown to consistently outperform the co-attentional transformer layer <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref>, in which the "query" comes from the other modality than that of "key" and "value" (see Sect. 2 and <ref type="figure" target="#fig_0">Fig. 2</ref> in <ref type="bibr" target="#b55">[55]</ref> for details).</p><p>Masked Language Modeling (mlm). We mask a percentage of the input text tokens at random, and predict the target text sequence using the decoder. Following BERT <ref type="bibr" target="#b25">[25]</ref>, we use a mixed strategy for masking: for each selected token, we replace it with the mask token [MASK] 80% of the time, replace it with a random token 10% of the time, and leave it as is 10% of the time.</p><p>Masked Sequence to Sequence Modeling (mass). We apply the mixed masking strategy as in mlm to the input text tokens, but require that the mask is applied to consecutive tokens (i.e., a contiguous segment). The task is to sequentially predict the masked segment using the decoder. This approach is inspired by MASS <ref type="bibr" target="#b71">[71]</ref> and PEGASUS <ref type="bibr" target="#b86">[86]</ref>.</p><p>Results. <ref type="table" target="#tab_21">Table 11</ref> compares ic, mlm, and mass pretraining objectives. Our main observation is that ic clearly outperforms masked vision-to-language pre-training when the masking rate is low. Overall, ic is competitive to mlm and mass, slightly below mlm[.8] in overall CIDEr, but higher on out-of-domain CIDEr.</p><p>In addition, the trend suggests that it is critical that the text masking rate is high enough such that the models become less and less reliant on text -that is, when mlm and mass become more similar to the ic task. Note that widely-used configurations in the VLP literature on vision-and-language understanding are the ones with low text masking rates (0.2 in most cases), which consistently underperform in our generation setup.</p><p>We attribute this result to the models' (over)reliance on text during pre-training, which hurts the quality of its image representations. Supporting evidence for this phenomenon is found in the recent work of <ref type="bibr" target="#b16">[16]</ref>, which observe that image+text pre-trained models exhibit a preference for attending text rather than images during inference (in image and text understanding task). Another supporting evidence is the issue of strong language priors (well-known in the VQA community), which led to interest in adversarial test sets and other methods to overcome strong language biases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b14">14]</ref>. The same pheonmenon has been reported for multi-modal machine translation, where models trained on image+text tend to ignore the image and primarily use the text input <ref type="bibr" target="#b15">[15]</ref>. Based on these results, the design   of V+L pre-training objectives that are capable of outperforming the image-only ic objective (i.e., overcoming the language through modeling) is an interesting venue for future work. Another observation is that mass significantly works better than mlm for lower masking rates. When masking rates are high, the two objectives become more similar. This suggests the importance of bridging the gap between pretraining and fine-tuning (producing consecutive tokens).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 Image Captioning with Visual-Linguistic Matching or Masked Object Classification</head><p>vectors and up to 16 tag feature vectors, each of size 512. For the vlm objective, where text has to be encoded, we also have a sequence of text (sub)token feature vectors of size 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Model</head><p>The ic-based task uses a transformer encoder-decoder model. The vlm-based uses two transformer encoders, one for texts and the other for images.</p><p>? Transformer image encoder: number of layers L = 6.</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Evaluation</head><p>For nocaps evaluation, we submit inference results to the leaderboard https://evalai.cloudcv.org/web/ challenges/challenge-page/464/overview. Code for all evaluation metrics can be found at https : / / github.com/nocaps-org/updown-baseline/blob/ master/updown/utils/evalai.py. For in-depth discussions of these metrics see <ref type="bibr" target="#b40">[40]</ref>.</p><p>Participating in the default formulation of the nocaps challenge requires that one (i) does not use val and test Open Images's ground-truth object detection annotations, and (ii) does not use image-caption data collected via additional annotation protocols. We satisfy both requirements as we train our object detector on Visual Genome, and both CC3M and CC12M are automatically harvested from the web (alt-text) and belong to the category of noisy web data, therefore satisfying the second requirement. On the other hand, models that leverage the Open Images Localized Narratives dataset (LocNar) <ref type="bibr" target="#b64">[64]</ref> for pre-training belong to the nocaps (XD) leaderboard rather than the default one.</p><p>Some of our results on the CC3M benchmark are taken from the leaderboard, which is located at https : / / ai . google . com / research / ConceptualCaptions / leaderboard?active_tab=leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Hyperparameter search</head><p>For pre-training experiments, we do not conduct hyperparameter tuning besides an initial stage of exploration as we believe small changes would not considerably affect the downstream performance. For instance, we fix an initial learning rate to 0.000032 and observe it works consistently well (on the validation set) across scenarios.</p><p>For fine-tuning experiments, we focus on tuning one hyperparamter: the initial learning rate. In the case of nocaps, we also lightly tune the maximum number of training steps as we observe the model overfitting on COCO Captions. In all cases, we make sure to allocate similar resources to any two settings that we make a comparison between, such as pre-training data sources of CC3M and CC12M.</p><p>For generation, the ranges for the initial learning rate are {3.2e-9, 3.2e-8, 3.2e-7} and the ranges for the maximum number of training steps are {5K, 10K}. For matching, the ranges for the initial learning rate are {3.2e-8, 3.2e-7, 3.2e-6} while the maximum number of training steps is fixed to 10K.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Word clouds of top 100 tokens in CC3M (the top cloud) and in CC12M (the bottom cloud).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Main Pre-Training Tasks: image captioning (visionto-language generation) and visual-linguistic matching (visionand-language understanding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>nal image-text pairs are used as positive examples, while all other image-text pairs in the mini-batch are used as negative examples<ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b77">77]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Automatic metric scores on the nocaps val set: performance of from-scratch (Row 1), pre-trained (Rows 2-3), and fine-tuned (Rows 4-5) models. CC12M outperforms CC3M by a large margin after fine-tuning (Row 4 vs. 5). Together, they achieve a new best, surpassing 90 CIDEr points on nocaps val. Bold indicates best-to-date, underline indicates second-best. a group of people that are dressed in costumes .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on nocaps. Each example comes with a caption predicted by the model that is trained on COCO Captions without pre-training (very top, right under the image), as well as captions predicted by models pre-trained on CC3M (middle) and CC12M (bottom), where the left/right column indicates if the model is fine-tuned on COCO Captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of nocaps' out-of-domain coverage degree among captioning (solid) and 3 other tasks' (dashed) datasets (see text for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Performance with sub-sampled CC12M (25% &amp; 50%) on novel object captioning (left, CIDEr's on nocaps val) and zeroshot IR (right, recall@1 on Flickr30K test).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>holding a sword and dressed in a black robe is standing next to a stroller a man sitting on a chair with a beer in his hands roasting something to eat on a wooden stick a man and a woman hold the front arm of a large tiger that is laying on the ground among various medical devicesFigure 7 :</head><label>7</label><figDesc>Qualitative results for the image retrieval task on Flickr30K given the query text (very top) when the model is not pre-trained (top), pre-trained on CC3M (middle), and pre-trained on CC12M (bottom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.. Pooling &amp; Fully-connected Layers Match or not? Visual-Linguistic Matching (VLM) ... Tag 1 Tag 16 ...</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Multi-Layer Transformer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Text Encoder</cell><cell></cell><cell></cell></row><row><cell>Glob</cell><cell>Reg 1</cell><cell>Reg 16</cell><cell>[CLS]</cell><cell>Token 1</cell><cell>Token 2</cell><cell>.</cell><cell>Token N</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Glob Reg 1 Reg 16 Multi-Layer Transformer Text Decoder [GO] Token 1 Token N ... ... Tag 1 Tag 16 ...</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Generation (top) and matching (bottom) tasks and datasets considered in this paper. IR = Image Retrieval.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>COCO</cell><cell>nocaps</cell></row><row><cell>Method</cell><cell>val2017</cell><cell>val</cell></row><row><cell></cell><cell>CIDEr</cell><cell>CIDEr</cell></row><row><cell>UpDown (reference)</cell><cell>116.2</cell><cell>55.3</cell></row><row><cell>UpDown + CBS</cell><cell>97.7</cell><cell>73.1</cell></row><row><cell>UpDown + ELMo + CBS</cell><cell>95.4</cell><cell>74.3</cell></row><row><cell>no pretrain (reference)</cell><cell>108.5</cell><cell>54.7</cell></row><row><cell>pretrain ic on CC12M (5K)</cell><cell>108.1</cell><cell>87.4</cell></row><row><cell>pretrain ic on CC12M (10K)</cell><cell>110.9</cell><cell>87.1</cell></row></table><note>Comparison between our best model (in italics, pre-trained on CC12M with ic and fine-tuned on COCO Captions) and existing models, on the nocaps val (top) and test (bottom) splits. Bold indicates best-to-date, underline indicates second-best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance on the in-domain COCO Captions val2017 split along with the nocaps val split. Our methods are in italics with the number of fine-tuning steps in the parentheses.</figDesc><table><row><cell>Pretraining</cell><cell>Finetuning</cell><cell>LocNar</cell><cell>LocNar</cell></row><row><cell>data</cell><cell>data</cell><cell cols="2">COCO val OID val</cell></row><row><cell></cell><cell></cell><cell>CIDEr</cell><cell>CIDEr</cell></row><row><cell>None</cell><cell>LocNar COCO</cell><cell>29.6</cell><cell>33.8</cell></row><row><cell>CC3M</cell><cell>LocNar COCO</cell><cell>29.1</cell><cell>35.7</cell></row><row><cell>CC12M</cell><cell>LocNar COCO</cell><cell>30.0</cell><cell>38.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Novel object captioning on LocNar.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Performance on the Conceptual Captions (CC3M) benchmark. Our methods are in italics. "ft" stands for fine-tuning. The top two CC3M test CIDEr baseline scores are from the Con- ceptual Captions Leaderboard as of Nov 15, 2020.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Image retrieval on Flickr30K and LocNar Flickr30K</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table><row><cell>Statistics of the (normalized) frequency of nocaps'</cell></row><row><cell>out-of-domain visual concepts in the texts of popular vision-and-</cell></row><row><cell>language datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Comparison between pre-training data. LocNar Open Images's images are from the same visual domain as nocaps. All approaches use the ic pre-training objective.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>87.4 11.8 mlm[.1] 76.4 11.5 68.4 10.8 57.6 9.6 73.0 18.1 23.5 50.6 67.4 10.6 mlm[.2] 79.8 11.3 76.3 10.9 76.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">nocaps val</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre-training</cell><cell cols="2">in-domain</cell><cell cols="4">near-domain out-of-domain</cell><cell></cell><cell></cell><cell cols="2">overall</cell><cell></cell><cell></cell></row><row><cell>objective</cell><cell>CIDEr</cell><cell>SPICE</cell><cell>CIDEr</cell><cell>SPICE</cell><cell>CIDEr</cell><cell>SPICE</cell><cell>BLEU1</cell><cell>BLEU4</cell><cell>METEOR</cell><cell>ROUGE</cell><cell>CIDEr</cell><cell>SPICE</cell></row><row><cell cols="6">ic 88.3 12.3 86.0 11.8 91.3</cell><cell cols="7">11.2 54.5 10.2 78.5 23.4 25.9 76.2 20.5 24.1 52.4 76.8 10.8</cell></row><row><cell cols="6">mlm[.4] 86.5 12.3 82.7 11.5 86.3</cell><cell>11.3</cell><cell cols="2">78.0 22.7</cell><cell>25.2</cell><cell cols="3">53.7 84.0 11.6</cell></row><row><cell cols="6">mlm[.8] 89.3 12.5 87.5 11.9 91.1</cell><cell>11.3</cell><cell cols="2">78.7 23.8</cell><cell>25.9</cell><cell cols="3">54.4 88.5 11.9</cell></row><row><cell cols="6">mass[.1] 86.0 12.1 74.8 11.1 71.7</cell><cell>10.1</cell><cell cols="2">75.8 20.5</cell><cell>24.6</cell><cell cols="3">52.5 75.8 11.0</cell></row><row><cell cols="6">mass[.2] 84.9 12.0 78.1 11.2 78.6</cell><cell>10.5</cell><cell cols="2">76.0 20.8</cell><cell>24.7</cell><cell cols="3">52.7 79.2 11.2</cell></row><row><cell cols="6">mass[.4] 85.7 11.7 83.7 11.5 88.5</cell><cell>10.9</cell><cell cols="2">77.3 22.8</cell><cell>25.1</cell><cell cols="3">53.6 85.0 11.4</cell></row><row><cell cols="6">mass[.8] 88.8 12.2 85.1 11.7 87.8</cell><cell>10.6</cell><cell cols="2">78.1 23.7</cell><cell>25.5</cell><cell cols="3">54.2 86.2 11.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Comparison between the ic pre-training and masked V+L pre-training. We consider two masking schemes (mlm and mass) and four masking rates(.1, .2, .4, .8) and report their effects on the nocaps val set.</figDesc><table><row><cell>nocaps val</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Effect of visual linguistic matching (vlm) and masked object classication (moc) when combined with the ic objective on the nocaps val set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>Transformer text encoder (for vlm only): L, E, H, F, A are the same as Transformer image encoder. ? Transformer decoder: L, E, H, F, A are the same as Transformer image encoder. ? Transformer decoder: beam search width = 5. ? Transformer decoder: beam search alpha = 0.6. ? Transformer decoder: maximum output length = 36 for all datasets except for LocNar which is set to 180. Warm-up epochs: 20 for all pretraining and fine-tuning experiments. ? Learning rate -Decay rate: 0.95 for all pre-training and fine-tuning experiments. ? Learning rate -Decay epochs: 25 for all pre-training and fine-tuning experiments. ? Data augmentation: a set of input visual regions are permuted during training. ? Maximum number of steps: 2M for vision-tolanguage generation pre-training on both CC12M and CC3M (and CC3M+CC12M). For vision-andlanguage matching, 1M for CC3M instead. See Hyperparameter search below for fine-tuning experiments.</figDesc><table><row><cell>Transformer image encoder: vocab embedding size E</cell></row><row><cell>= 512.</cell></row><row><cell>? Transformer image encoder: hidden embedding size H</cell></row><row><cell>= 1024.</cell></row><row><cell>? Transformer image encoder: feedforward/filter size F</cell></row><row><cell>= H x 4 = 4096, following [25].</cell></row><row><cell>? Transformer image encoder: number of attention</cell></row><row><cell>heads A = H / 64 = 8, following [25].</cell></row><row><cell>? E.3. Training</cell></row><row><cell>? Infrastructure: Google Cloud 32-core TPUs.</cell></row><row><cell>? Batch size per core: 128 (for a total of 4096)</cell></row><row><cell>? Optimizer: Adam [41] with default hyperparameters</cell></row><row><cell>(except for the initial learning rate; see below).</cell></row><row><cell>? Learning rate -Initial: See Hyperparameter search</cell></row><row><cell>below.</cell></row><row><cell>? Learning rate -</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This dataset also contains mouse traces synchronized with the text, but we do not use the traces here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Peter Anderson for his feedback on earlier version of the draft, Bo Pang, Zhenhai Zhu for helpful discussions, Sebastian Goodman and Ashish V. Thapliyal for help with model implementation, Chris Alberti for help with the data collection pipeline, and Harsh Agrawal for detail on nocaps.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">nocaps: novel object captioning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fusion of detected objects in text for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SPICE: semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guided open vocabulary image captioning with constrained beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Partially-supervised image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Hinton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshops</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<editor>Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scottand</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Women also snowboard: Overcoming bias in captioning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">RUBi: Reducing unimodal biases in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probing the need for visual context in multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Behind the scene: Revealing the secrets of pre-trained vision-and-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The secret sharer: Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In {USENIX} Security</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dawn Song,?lfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07805</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled box proposal and featurization with ultrafine-grained semantic labels improve image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Microsoft COCO Captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UNITER: Learning UNiversal Image-TExt Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meshed-Memory Transformer for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VirTex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-stage pretraining for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10599</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meng Zhang, and Nilavra Bhattacharya. Captioning images taken by people who are blind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS workshop</title>
		<meeting>NeurIPS workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno>ICLR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">VIVO: Surpassing human performance in novel object captioning with visual vocabulary pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13682</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09506</idno>
		<title level="m">Gqa: a new dataset for compositional question answering over realworld images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06561</idno>
		<title level="m">Bridging vision and language by large-scale multi-modal pre-training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Andrew Tomkins, and Sujith Ravi. Graph-RISE: Graphregularized image semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Futang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10814</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ultra fine-grained image semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Futang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nazli Ikizler-Cinbis, and Erkut Erdem. Re-evaluating automatic metrics for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Kilickaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Open-Images: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhyanesh</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://g.co/dataset/openimages,2017.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
		<idno>abs/1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">VisualBERT: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<meeting><address><addrLine>RoBERTa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ViL-BERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Houmansadr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Im2Text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Connecting vision and language with localized narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Overcoming language priors in visual question answering with adversarial regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">WIT: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01913</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">VL-BERT: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">LXMERT: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Decoupled novel object captioner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in image captioning for learning novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<title level="m">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Multimodal transformer with multi-view visual representation for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>ICML, 2020. 14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Unified vision-language pretraining for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Capturing long-tail distributions of object subcategories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">We explore adding auxiliary losses to the main ic objective. First, we define a pre-training task that does not require text</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">We use a total of 8192 clusters, obtained via K-means over the training data. Then, we either add the vlm loss (multipled by 0.1) or the moc loss</title>
		<imprint/>
	</monogr>
	<note>We mask one of the visual regions (selected at random), and predict the cluster ID of that region. multipled by 0.1) to the main ic loss</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">We observe a slight improvement when adding moc but a slight drop when adding vlm. This again shows that ic is a good pre-training task to start with. We leave developing advanced auxiliary losses on top of it and multi-task pre-training strategies for future work. E. Implementation Details E.1. Data Preprocessing and Feature Embedding ? Text tokenizer: preprocesed with COCO tokenizer</title>
		<ptr target="https://github.com/tylin/coco-caption" />
	</analytic>
	<monogr>
		<title level="m">Table 12 reports the effect of multi-task pretraining on the nocaps val set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">We then create a vocabulary of subtokens out of these. ? Text input embedding (during pre-training only): subtoken lookup embeddings of size E = 512 are randomly initialized</title>
		<imprint/>
	</monogr>
	<note>followed by Linear(512)-ReLU-Dropout(0.3)-Linear</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">? Image&apos;s geometric features: two pairs of coordinates (top left and bottom right) and the relative area, represented by relative numbers between 0 and 1. Each of these 5 numbers is linearly projected into an embedding of size E = 512. We concatenate the result to get an embedding of size E x 5 = 2560</title>
		<imprint/>
	</monogr>
	<note>followed by Linear(512)-ReLU-Dropout(0.3)-Linear</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">? Image&apos;s semantic features: each feature vector (a global image feature vector or one of the 16 box&apos;s image feature vector</title>
		<imprint/>
	</monogr>
	<note>followed by Linear(512)-ReLU-Dropout(0.3)-Linear</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">? Image&apos;s combined geometric and semantic features: we first apply LayerNorm [9] to each of the geometric or the semantic features</title>
		<imprint/>
	</monogr>
	<note>We then add the two and apply Linear(512)-ReLU-Dropout(0.3)-Linear(512)-LayerNorm</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">? Image&apos;s tag features: same as text input embedding. For the ic objective, we have a bag of 1 + 16 visual feature</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
