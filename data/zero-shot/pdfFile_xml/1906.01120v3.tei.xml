<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Adaptive Random Path Selection Approach for Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">An Adaptive Random Path Selection Approach for Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Incremental learning</term>
					<term>continual learning</term>
					<term>deep neural networks</term>
					<term>path selection</term>
					<term>catastrophic forgetting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a conventional supervised learning setting, a machine learning model has access to examples of 'all' object classes that are desired to be recognized during the inference stage. This results in a fixed model that lacks the flexibility to adapt to new learning tasks. In practical settings, learning tasks often arrive in a sequence and the models must continually learn to increment their previously acquired knowledge. Existing incremental learning approaches fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called Adaptive RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing between tasks. We introduce a new network capacity measure that enables us to automatically switch paths if the already used resources are saturated. Since the proposed path-reuse strategy ensures forward knowledge transfer, our approach is efficient and has considerably less computation overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the Adaptive RPS-Net method surpasses the state-of-the-art performance for incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network. Our codes are available at: https://github.com/brjathu/RPSnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In several real-life applications, it is desired to continually update a model as new object classes are encountered. Conventional deep neural networks work in a static setting, where a model is trained on the training samples from all classes of interest. Such models can only recognize the set of classes initially used for training, thereby lacking the ability to incrementally learn new tasks. As such, when these networks are sequentially trained on a series of tasks, they suffer from 'catastrophic forgetting' <ref type="bibr" target="#b0">[1]</ref> thereby degrading the performance on old tasks. A typical example is the transfer learning scenario where a model pre-trained on a source task is fine-tuned to a target task, resulting in overriding the previously learned information and consequently degrading the performance on source task <ref type="bibr" target="#b1">[2]</ref>. Therefore, it is fundamentally important to develop models capable of incrementally adding new classes without the need to retrain models from scratch using all previous class-sets.</p><p>A versatile incremental learning model possess the following characteristics. (a) As a model learns new tasks, its performance on old tasks should not catastrophically deteriorate (the forgetting problem). (b) Since learning tasks are inherently related, the knowledge acquired on old tasks should help in accelerating the learning on new tasks (i.e., forward transfer). (c) Ensure that complimentary representations are learned from the current task so that the newly learned information can help improve the old task performance (i.e., backward transfer). (d) As the classincremental learning progresses, the network must share and reuse the previously tuned parameters to ensure a bounded computational complexity and memory footprint of the final model. (e) At all learning stages, the model must maintain a tight equilibrium between the existing knowledge base and newly presented information, thereby addressing stabilityplasticity dilemma.</p><p>Addressing all the above requirements is a challenging task, making incremental learning an open research problem. Among seminal previous works, <ref type="bibr" target="#b2">[3]</ref> employs a distillation loss to preserve knowledge across multiple tasks but requires prior knowledge about the task corresponding to a test sample during inference. An incremental classifier and representation learning approach <ref type="bibr" target="#b3">[4]</ref> jointly uses distillation and prototype rehearsal but retrains the complete network for new tasks, thus reducing model's stability. The progressive network <ref type="bibr" target="#b4">[5]</ref> grows paths linearly as new tasks arrive that in turn reduces scalability since the parameters rise quadratically in proportion to the number of tasks. The elastic weight consolidation scheme <ref type="bibr" target="#b5">[6]</ref> computes synaptic importance of weights that helps in overwriting less important parameters. However, the scalability is still an issue since the Fisher information metric used to estimate synaptic importance is computed offline and while the approach works quite well for permutation tasks, its performance suffers on classincremental learning <ref type="bibr" target="#b6">[7]</ref>.</p><p>This work is based on the idea that the most important characteristic of a true incremental learner is to maintain the arXiv:1906.01120v3 [cs.CV] 24 Jan 2020 right trade-off between 'stability' (leading to intransigence) and 'plasticity' (resulting in forgetting). We achieve this requisite via a dynamic path selection approach, called Adaptive RPS-Net, that automatically estimates the network capacity and selects a new trainable path to learn complimentary features, when required. Our approach proceeds with random candidate paths and discovers the optimal one for a given task. Once a task is learned, we fix the parameters associated with it, that can only be shared by future tasks. To enable parameter reuse and minimize model complexity, Adaptive RPS-Net utilizes the same path until the network saturates. To complement the previously learned representations, we propose a stacked residual design that focuses on learning the supplementary features suitable for new tasks. Besides, our learning scheme introduces an explicit controller module to maintain the equilibrium between stability and plasticity for all tasks. During training, our approach always operates with a constant parameter budget that at max equals to a conventional linear model (e.g., resent <ref type="bibr" target="#b7">[8]</ref>). Furthermore, it can be straightforwardly parallelized during both train and test stages. With these novelties, our approach performs favorably well on the class-incremental learning results, surpassing the previous best model <ref type="bibr" target="#b3">[4]</ref> by 7.38% and 10.64% on CIFAR-100 and ImageNet datasets, respectively.</p><p>Our main contributions are:</p><p>? A random path selection approach that enables forward and backward knowledge transfer via efficient path sharing and reuse. Importantly, using a new path saturation measure, it automatically switches paths when network's capacity is fully utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The residual learning framework that incrementally learns residual paths which allows network reuse and accelerate the learning process resulting in faster training as new tasks arrive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Ours is a hybrid approach that combines the respective strengths of knowledge distillation (via regularization), retrospection (via exemplar replay) and dynamic architecture selection methodologies to deliver a strong incremental learning performance.</p><p>? A novel controller that guides the plasticity of the network to maintain an equilibrium between the previously learned knowledge and the new tasks.</p><p>A preliminary version of our approach was published at the NeurIPS conference in 2019 <ref type="bibr" target="#b8">[9]</ref>. In the current work, we considerably improve <ref type="bibr" target="#b8">[9]</ref> by introducing: (a) an automatic path switching mechanism based on a novel network capacity measure (Sec. 3.2), (b) a path attention approach to appropriately modulate the response from individual network paths (Sec. 3.1.2), (c) additional experiments on the large-scale MS-Celeb and ImageNet-1K datasets, and (d) new comparative and ablation analysis e.g., with different switching rules, with a Genetic algorithm and for various attention functions (Sec. 4.3). Overall, our new Adaptive RPS-Net framework delivers better performance and is computationally more efficient compared to the conference version <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure">Fig. 1</ref>  Adaptive RPS Network <ref type="figure">Figure 1</ref>: Adaptive RPS-Net Architecture. Our incremental learning approach reuses the previous knowledge for the upcoming tasks without sacrificing the performance on old tasks. This is achieved via a novel path selection algorithm and a loss controller that avoids catastrophic forgetting. To maintain a low computational overhead, Adaptive RPS-Net selects new paths by measuring the saturation level of the network, thereby ensuring efficiency and good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The catastrophic interference problem was first noted to hinder the learning of connectionist networks by <ref type="bibr" target="#b0">[1]</ref>. This highlights the stability-plasticity dilemma in neural networks <ref type="bibr" target="#b9">[10]</ref> i.e., a rigid and stable model will not be able to learn new concepts while an easily adaptable model is susceptible to forget old concepts due to major parameter changes. The existing continual learning schemes can be divided into a broad set of three categories: (a) regularization schemes, (b) memory based retrospection and replay, and (c) dynamic sub-network training and expansion. Regularization based Methods: A major trend in continual learning research has been on proposing novel regularization schemes to avoid catastrophic forgetting by controlling the plasticity of network weights. <ref type="bibr" target="#b2">[3]</ref> proposed a knowledge distillation loss <ref type="bibr" target="#b10">[11]</ref> which forces the network to retain its predictions on the old tasks. The distillation loss was also used for class-incremental setting in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> to minimize forgetting. Kirkpatrick et al. <ref type="bibr" target="#b5">[6]</ref> proposed an elastic weight consolidation mechanism that quantifies the relevance of parameters to a particular task and correspondingly adjusts the learning rate. In a similar spirit, <ref type="bibr" target="#b12">[13]</ref> designed intelligent synapses which measure their relevance to a particular task and consequently adjust plasticity during learning to minimize interference with old tasks. Compared to these works, we also use a regularization loss but more importantly, we automatically estimate the network capacity using a new path saturation measure that helps us fix a network module upon saturation.</p><p>Memory based Approaches: Rebuffi et al. <ref type="bibr" target="#b3">[4]</ref> proposed a distillation scheme intertwined with exemplar-based retrospection to retain the previously learned concepts. <ref type="bibr" target="#b13">[14]</ref> considered a similar approach for cross-dataset continual learning <ref type="bibr" target="#b2">[3]</ref>. The combination of episodic (short-term) and semantic (long-term) memory was studied in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> to perform memory consolidation and retrieval. Particularly, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> help avoid explicitly storing exemplars in the memory, rather using a generative process to recall memories. Along similar lines, <ref type="bibr" target="#b17">[18]</ref> proposed an adversarial generative model to create pseudo-examples of the old tasks. Recently, <ref type="bibr" target="#b18">[19]</ref> proposed a dual-memory approach that not only stores the old task exemplars but also the class-specific statistics from the originally learned model. Different from these works that offer less scalability when number of tasks increase, we maintain a fixed memory budget to keep task exemplars and fix parts of the network to retain old information.</p><p>Dynamic Architectures: The third stream of works explores dynamically adapting network architectures to cope with the growing learning tasks. <ref type="bibr" target="#b4">[5]</ref> proposed a network architecture that progressively adds new branches for novel tasks that are laterally connected to the fixed existing branches. Similarly, <ref type="bibr" target="#b19">[20]</ref> proposed a network that not only grows incrementally but also expands hierarchically by grouping together the similar classes. Specific paths through the network were selected for each learning task using a genetic algorithm in PathNet <ref type="bibr" target="#b20">[21]</ref>. Afterwards, task-relevant paths were fixed and reused for new tasks to speed-up the learning efficiency.</p><p>The existing adaptive network architectures come with their respective limitations e.g., <ref type="bibr" target="#b4">[5]</ref>'s complexity grows with the tasks, <ref type="bibr" target="#b19">[20]</ref> has an expensive training procedure and a somewhat rigid architecture and <ref type="bibr" target="#b20">[21]</ref> does not allow incrementally learning new classes due to a detached output layer and the genetic learning algorithm used in <ref type="bibr" target="#b20">[21]</ref> is relatively inefficient. In comparison, we propose a random path selection methodology that provides a performance boost alongwith a faster convergence. Furthermore, our approach combines the respective strengths of the above two types of methods by introducing a distillation procedure alongside an exemplar-based memory replay to avoid catastrophic forgetting.</p><p>Differences with PathNet: The closest to our approach is PathNet <ref type="bibr" target="#b20">[21]</ref>, yet there are notable differences in terms of network architecture, learning methodology and the end problem. In terms of architecture, PathNet adds all modules in a simple feed-forward manner, while RPS-Net adds modules as residual connections to an identity path which can always be learnt even when all the modules are saturated. RPS-Net can therefore adapt to new changes more robustly. During training, PathNet uses an evolutionary algorithm while RPS-Net employs a random path selection methodology. Since PathNet usually involves only two tasks, an evolutionary algorithm with large explorations/generations is viable. However, for our case i.e., 10+ tasks, PathNet is not feasible, due to a large number of possible explorations. Furthermore, our proposed architecture is more suitable for incremental learning and offers several novel components such as a dynamic path switching, a controller to maintain network's elasticity-plasticity balance, and a goal driven path attention methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>We consider the recognition problem in an incremental setting where new tasks are sequentially added. Assume a total of K tasks, each comprising of U classes. Our goal is to sequentially learn a deep neural network, that not only performs well on the new tasks but also retains its performance on the old tasks. To address this problem, we propose a random path selection approach (Adaptive RPS-Net) that progressively builds on the previously acquired knowledge to facilitate faster convergence and better performance. In the following, we explain the novel aspects of Adaptive RPS-Net that include the proposed network architecture (Section 3.1), our path selection strategy (Section 3.2 and 3.3) and a hybrid objective function (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adaptive RPS-Net Architecture</head><p>Our network consists of L distinct layers (see <ref type="figure" target="#fig_0">Figure 2</ref>). Each layer ? [1, L] constitutes a set of basic building blocks, called modules M . For simplicity, we consider each layer to contain an equal number of M modules, stacked in parallel, i.e., M = {M m } M m=1 , along with a skip connection module M skip that carries the bypass signal. The skip connection module M skip is an identity function when the feature dimensions do not change and a learnable module when the dimensions vary between consecutive layers. A module M m is a learnable sub-network that maps the input features to the outputs. In our case, we consider a simple combination of (conv-bn-relu-conv-bn) layers for each module, similar to a single resnet block <ref type="bibr" target="#b7">[8]</ref>. In contrast to a residual block which consists of a single identity connection and a residual branch, we have one skip connection and M residual blocks stacked in parallel. The intuition behind developing such a parallel architecture is to ensure multiple tasks can be continually learned without causing catastrophic interference with other paths, while simultaneously providing parallelism to ensure efficiency.</p><p>Towards the end of each layer in our network, all the residual connections, as well as skip connections, are combined together using element-wise addition to aggregate complimentary task-specific features obtained from different paths. Notably, for the base-case when M = 1, the network is identical to a conventional resnet model. After the Global Average Pooling (GAP) layer that collapses the input feature maps to generate a final feature f ? R D , we use a fully connected layer classifier with weights W f c ? R D?C (C being the total number of classes) that is shared among all tasks. As such, we have C = K ? U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Training and Inference Paths</head><p>For a given Adaptive RPS-Net, with M modules and L layers, we can define a path P k ? R L?M for a task k, such that:</p><formula xml:id="formula_0">P k ( , m) = 1, if the module M m is added to the path, 0, otherwise.<label>(1)</label></formula><p>The path P k is basically arranged as a stack of one-hot encoded row vectors e (i) (with i th standard basis):</p><formula xml:id="formula_1">P k = P k ( ) ? {0, 1} M : P k ( ) = e (i) ? M m=1 P k ( , m) = 1 , s.t., i ? U {Z ? [1, M ]} ,</formula><p>where i is the selected module index, uniformly sampled using U(?) over the set of integers [1, M ]. The network architecture utilizes a parallel residual design where the optimal path is selected among a set of randomly sampled candidate paths for new tasks. The residual design allows forward knowledge transfer and faster convergence for later tasks. An attention mechanism (? ) is applied on the module outputs from the final layer to appropriately re-weight contributions from previously learned tasks. Our approach is trained with a hybrid objective function that ensures the right trade-off between network stability and plasticity, thus avoiding catastrophic forgetting.</p><p>We define two set of paths P tr k and P ts k that denote the train and inference paths, respectively. Both are formulated as binary matrices: P tr,ts k ? {0, 1} L?M . When training the network, any m th module in th layer with P tr k ( , m) = 1 is activated and all such modules together constitute a training path P tr k for task k. As we will elaborate in Section 3.3, the inference path is evolved during training by sequentially adding newly discovered training paths and ends up in a "common" inference path for all inputs, therefore our Adaptive RPS-Net does not require knowledge about the task an input belongs to. Some popular methods (e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>) need such information, which limits their applicability to real-world incremental class-learning settings where one does not know in advance the corresponding task for an input sample. Similar to training, only the modules with P ts k ( , m) = 1 are used in the inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Attention based on Peak Path Response</head><p>In Adaptive RPS-Net, the output tensors from each module in layer are jointly used to compute the input to the successive layer. A naive way to aggregate feature tensors T ,m is to obtain an accumulative response by assuming an equal contribution from all paths in layer :</p><formula xml:id="formula_2">T = M m=1 P k ( , m) ? T ,m ,<label>(2)</label></formula><p>where, T ,m is the output after the convolution from a module m in layer and T denotes the accumulated feature response forwarded to the next layer. This raises an important question: Do all previous paths (corresponding to old tasks) contribute equally to learn a new task? For example, for a new task with images of birds, a path previously trained with car images may contribute less than a path trained with animals. Therefore, we propose a simple Peak Path Response (PPR) based attention mechanism that is used to appropriately re-weight the contributions from different paths. In simple words, the peak response for a path signifies its relevance to the given input belonging to a new task. Specifically, PPR attention multiplies each convolution layer output tensor by its corresponding path response value ? (T ,m ) as shown below:</p><formula xml:id="formula_3">T = M m=1 ? (T ,m ) ? P k ( , m) ? T ,m .<label>(3)</label></formula><p>Here, ? is a scalar value function which maps h?w ?c tensor to a scalar weighting factor. We use ? (T ,m ) = max(T ,m ) as the PPR coefficient for each path. We also tried several other mapping functions, such as 1) path mean response, 2) computing the maximum activation along the feature maps and converting c feature maps to a single scalar with a fully connected layer, and 3) using 1 ? 1 convolution to map c features into single h ? w map and then passing it through a fully connected layer for a scalar coefficient. However, the best performance is achieved with the peak response coefficient as we illustrate via the ablation study reported in Section 4.3. Our experiments show that the proposed PPR attention delivers best performance when used in the final layer L with T L,m ? R 8?8?512 just before classification. This is intuitive since a re-weighting should be applied only when high-level representations are extracted that encapsulate class-semantics (i.e., close to final layers). Furthermore, the attention operation in the final layer helps in achieving our desired behaviour i.e., re-weighting the overall response of each path based on its relevance to a new task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Measuring Network Capacity</head><p>As we progress by adding more tasks in an incremental learning setting, it is likely that the model saturates. This prevents the deep network to adapt to new tasks. Our proposed network architecture is designed to solve this problem by switching new paths as network starts to saturate. A naive solution would be choosing new paths for a fixed set of new tasks. For example, one can use a single path to train for every two consecutive tasks. Although this solves the network saturation problem, having a fixed rule for changing paths (e.g., after every n tasks) is a sub-optimal solution. We suggest that the path switching mechanism should be based on the network capacity that will ensure a dynamic and efficient resource expansion/reuse. For example, generally earlier tasks are more divergent and require quick path switching, while later tasks can reuse the early learned features, hence several tasks can use a single path in the later training stages. Since, measuring network capacity is a non-trivial task, we propose a new measure for network saturation that helps us decide the point when paths must be switched.</p><p>Our saturation measure is based on the Fisher information matrix, that estimates the second-order derivatives of the loss function close to a minima. Given a likelihood function f (?) for parameters ? conditioned on input data distribution X , the Fisher information matrix F can be represented as:</p><formula xml:id="formula_4">F = E f (X ;?) ? log f (X ; ?)? log f (X ; ?) T .<label>(4)</label></formula><p>The matrix F is basically the expectation of Hessian but can be approximated using only first-order derivatives and is positive semi-definite in all cases. The diagonal Fisher information or the precision (inverse of variance) tells the significance of a specific parameter for the correct classification of task k. In other words, if the variance of the parameter is high or precision is low, it shows that the corresponding parameter has less contribution in the final classification and vice versa. Given the Fisher information, we define a measure for network saturation. First, we compute the maximum values of diagonal Fisher information matrix over all the exemplar set images. Notably, this is in contrast to how Fisher information has been used previously in the literature, e.g., <ref type="bibr" target="#b5">[6]</ref> used the 'average' diagonal Fisher information for all samples in an exemplar set to define the elasticity of parameters for an old task. In our case, averaging out the Fisher information is counter-intuitive because if a parameter ? a is significant for task k and has no significant contribution for any other task, then an average over all the exemplars (which contains samples from task k as well as old tasks) would undermine the parameter ? a 's significance. Therefore, I stores the element-wise maximum values of Fisher information matrices {F i } for all images in the exemplar set,</p><formula xml:id="formula_5">I ij = max F ij 1 , F ij 2 , . . . F ij |E| .<label>(5)</label></formula><p>Here, E contains exemplar images from all previous tasks and | ? | denotes set cardinality. Keeping the maximum values from the Fisher matrix helps to keep significant parameters which capture independent information irrespective of the task.</p><p>Further, as we progress with learning new tasks, the magnitude of I entries keep on decreasing. This is because the I is approximated with first-order gradients, as the calculation of second-order gradients on a deep convolutional network is computationally very expensive. Therefore, we calculate the 'relative significance' between the sets of early and final layers. If a high percentage of parameters are important for previous tasks then the network is likely to be saturated. Hence, we define relative saturation coefficient ? sat as,</p><formula xml:id="formula_6">? sat = log 1 |f l| ?f l tr(I ) 1 |el| m?el tr(I m ) .<label>(6)</label></formula><p>where tr denotes trace of the matrix, I is the Fisher information matrix for th layer and f l and el are sets of layers at the end and start of the trainable path of the network, respectively. Note that ? sat is calculated only with the images in the exemplar set (see <ref type="bibr">Equation 5</ref>). We use ten layers including kernel weights and biases in each set.</p><p>Having a set of layers for this calculation helps suppress the noise introduced in Fisher information matrix by individual weights.</p><p>Adaptive RPS-Net architecture is designed in such a way that a new path will learn the information not captured by the previously learned paths (since any path can be considered as a residual connection for the rest). Therefore, it is reasonable to assume that parameters in different paths will learn complimentary (and thus independent) information. This is why path switching is performed in our framework only when the trainable path is saturated for a given task (meaning all parameters have been fully utilized for previous task's learning) and a substitute path is required to carry on learning new tasks. This way, although all tasks are not fully independent, the features learned by each path are independent. Moreover, for a given trainable path, parameters are tuned using exemplars from previous tasks and training samples of the current task. Therefore, the newly learned parameters for the current task focus on learning features that have not been encapsulated during the old tasks learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Path Selection Approach</head><p>Adaptive RPS-Net is a progressive learning approach. The path selection scheme enables incremental and bounded resource allocation that ensures knowledge exchange between the old and new tasks resulting in positive forward and backward transfer. As discussed in the previous section, we use path saturation coefficient as a simple measure to decide whether we need to switch the current path or not. For a given threshold value 'th', after training for a task, we simply calculate the saturation coefficient in the current path configuration. If the network saturation is above the threshold th, a set of N new paths are randomly sampled. At this point, we stop the training of the old modules (i.e., fix their paths and parameters). We then train N models in parallel for the next task and chose the best one, otherwise we keep training the network with the same path configuration as before. Hence, at any point, only L layers (each with a maximum of one module) are being trained.</p><p>Path selection strategy balances the trade off between resource usage (number of neurons) and performance. If <ref type="figure">Figure 3</ref>: Path Selection Approach: Given a task k, N random paths are initialized. For each path, only the modules different from the previous inference path P ts k?1 are used to form the training path P tr k . Among N such paths, the optimal P k is selected and combined with the P ts k?1 to obtain P ts k . Notably, the path selection is only performed if ? sat,k ? th. In above scenario, ? sat,k , ? sat,k+1 , . . . , ? sat,k+l?1 are below the threshold th. Hence, the same path is used for training tasks k, k + 1, . . . , k + l, and k + l is added to the list S, such that S = {. . . , k + l} and a new path is selected for the next task k + l + 1. During training, the complexity remains bounded by a standard single path network and the resources are shared between tasks.</p><p>we set a small value for th, for a given number of total tasks, the network will choose more paths to keep the saturation low. On the other hand, with a high threshold value, the network will reuse a certain path many times until it reaches the specified th value. Therefore, with small thresholds the network takes more resources in terms of parameters, training and inference time, while for a higher th value, network will use comparatively less resources but at the cost of a slight drop in performance (see Section 4.3 for experimental results). Nevertheless, we noted that an adaptive path switching rule based on path saturation almost always perform better than a fixed rule (path switching after every n tasks) with comparable resource usage.</p><p>The path selection strategy based on network saturation is illustrated in <ref type="figure">Figure 3</ref>. Our choice of random path generation as a mechanism to select an optimal path is mainly inspired by the recent works of <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. These works show that random search for an optimal network architecture performs somewhat comparable to other computationally demanding approaches e.g., the ones based on reinforcement learning (RL). Besides, some incremental learning approaches add new resources to the network, resulting in network expansion <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>. In contrast, our path selection algorithm does not result in linear expansion of resources since a new path is created only if the current path saturates and overlapping modules are reused when the new path is intersecting with old paths. Further, even when all the modules are exhausted (saturated), the skip connections are always trained. We show via an extensive ablation study (Section 4.3) that even when all paths are saturated, our Adaptive RPS-Net can still learn useful representations as the skip connections and the final classification layer remains tunable in every case.</p><p>At any point in time, we train a single path (equivalent to a resnet) while rest of the previously trained paths are fixed. Due to this, the path we use for a task k essentially learns the residual signal relative to the fixed paths that were previously trained for old tasks. For example, if we are training P tr k , the weights of P ts S?1 P tr k are fixed, where denotes the exclusive disjunction (logical XOR operation) and S is a list containing the tasks numbers where network saturates above threshold value (resulting in path switching). S ?1 returns the last task number where saturation along the training path exceeds th and k = S ?1 + 1 will be trained with a new random path. Essentially, the complete P tr k is not used for training rather its disjoint portion that has not already been trained for any of the old tasks is learned i.e., P tr k (P tr k ? P ts S?1 ), where ? denotes logical conjunction (logical and) operator. In this way, previous knowledge is shared across the network via overlapping paths and skip connections. When the network is already trained for several tasks, a new path for the current task only needs to learn higher order residual signal in the network. This has an added advantage that convergence becomes faster as we learn more tasks since each new task will be learned taking advantage of the previous information.</p><p>The optimal path based on the performance of N path configurations is selected as P k . All such task-specific paths are progressively combined together to evolve a common inference path P ts k ,</p><formula xml:id="formula_7">P ts k = P tr 1 ? P tr 2 . . . ? P tr k ,<label>(7)</label></formula><p>where ? denotes the inclusive disjunction (logical OR) operation. At each task k, the inference path P ts k is used to evaluate all previous classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incremental Learning Objective</head><p>Loss function: We use a hybrid loss function that combines regular cross-entropy loss as well as a distillation loss to incrementally train the network.</p><p>For a task k ? [1, K] which contains U classes, we calculate the cross-entropy loss as follows,</p><formula xml:id="formula_8">L ce = ? 1 n i t i [1 : k * U ] log(softmax(q i [1 : k * U ])) (8)</formula><p>where i denotes the example index, t(x) is the one-hot encoded true label, q(x) are the logits obtained from the network's last layer and n is the mini batch size. To keep the network robust to catastrophic forgetting, we also use a distillation loss in the objective function,</p><formula xml:id="formula_9">L dist = 1 n i KL log ? q i [1 : (k ? 1) * U ] t e , ? q i [1 : (k ? 1) * U ] t e .</formula><p>Here, ? is the softmax function, t e is the temperature used in <ref type="bibr" target="#b10">[11]</ref> and q (x) are the logits obtained from the network's previous state. Controller: It is important to maintain a balance between the previously acquired learning and the knowledge available from the newly presented task. If the learning is biased towards either of the two objectives, it will result in either catastrophic forgetting (losing old task learning) or interference (obstructing learning for the new task). Since our network is trained with a combined objective function with L ce and L dist , it is necessary to adequately control the plasticity of the network. We propose the following controller that seeks to maintain an equilibrium between L ce and L dist ,</p><formula xml:id="formula_10">L = L ce + ?(k, ?) ? L dist .<label>(9)</label></formula><p>Here, ?(k, ?) is a scalar coefficient function, with ? as a scaling factor, introduced to increase the distillation contribution to the total loss. Intuitively, as we progress through training, ?(k, ?) will also increase to ensure that network remembers old information. It is defined as,</p><formula xml:id="formula_11">?(k, ?) = 1, if k ? S 0 (k ? S 0 ) * ?, otherwise.<label>(10)</label></formula><p>Here, S 0 is the task number where the network changes the path configuration for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Protocol:</head><p>For our experiments, we use evaluation protocols similar to iCARL <ref type="bibr" target="#b3">[4]</ref>. We incrementally learn 100 classes on CIFAR-100 in groups of 10, 20 and 50 at a time. For ImageNet, we use the same subset as <ref type="bibr" target="#b3">[4]</ref> comprising of 100 classes and incrementally learn them in groups of 10.</p><p>After training on a new group of classes, we evaluate the trained model on test samples of all seen classes (including current and previous tasks). Following iCARL <ref type="bibr" target="#b3">[4]</ref>, we restrict exemplar memory budget to 2k samples for CIFAR-100 and ImageNet datasets. Note that unlike iCARL, we randomly select our exemplars and do not employ any herding and exemplar selection mechanism.</p><p>We also experiment our model with MNIST and SVHN datasets. For this, we resize all images to 32?32 and keep a random exemplar set of 4.4k, as in <ref type="bibr" target="#b25">[26]</ref>. We group 2 consecutive classes into one task and incrementally learn five tasks. For evaluation, we report the average over all classes (A 5 ).</p><p>Training: For the CIFAR100 dataset, we use resnet-18 along with max pooling after 5th, 7th blocks and global generalized mean pooling (GeM) <ref type="bibr" target="#b26">[27]</ref> with pooling parameter 3 is used after 9th block. For ImageNet dataset, we use the standard resnet-18 architecture as in <ref type="bibr" target="#b3">[4]</ref>. After the GAP layer, a single fully connected layer with weights W f c ? R 512?100 is used as a classifier. For MNIST, a simple 2 layered MLP (with 400 neurons each), whereas for SVHN resnet-18 is used, similar to <ref type="bibr" target="#b25">[26]</ref>.</p><p>For each task, we train our model for 50 epochs using Adam <ref type="bibr" target="#b27">[28]</ref> with t e = 2, with learning rate starting from 10 ?3 and divided by 2 after 20, 30, 40 epochs. We set the controller's scaling factor to ? = 2.5 and ? = 10 respectively for CIFAR and ImageNet datasets. We use the ratio between the number of training samples for a task and the fixed number of exemplars as the value for ?. We fix M = 8, th = 0 through out the experiments. We do not use any weight or network regularization scheme such as dropout in our model. For augmentation, training images are randomly cropped, flipped and rotated (&lt; 10 0 ). For each task, we train N = 8 models in parallel using a Nvidia-DGX-1 machine. These models come from the randomly sampled paths in our approach and may have some parts frozen due to an overlap with previous tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Comparisons</head><p>We extensively compare the proposed technique with existing state-of-the-art methods for incremental learning. These include Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b5">[6]</ref>, Riemannian Walk (RWalk) <ref type="bibr" target="#b28">[29]</ref>, Learning without Forgetting (LwF) <ref type="bibr" target="#b2">[3]</ref>, Synaptic Intelligence (SI) <ref type="bibr" target="#b12">[13]</ref>, Memory Aware Synapses (MAS) <ref type="bibr" target="#b29">[30]</ref>, Deep Model Consolidation DMC <ref type="bibr" target="#b30">[31]</ref> and Incremental Classifier and Representation Learning (iCARL) <ref type="bibr" target="#b3">[4]</ref>. We further evaluate on three baseline approaches: Fixed Representation (FixedRep) where the convolution part of the model is frozen and only the classifier is trained for newly added classes, FineTune where the complete previously learnt model is tuned for the new data, and Oracle where the model is trained on all samples from previous and current tasks.    <ref type="figure" target="#fig_2">Fig. 4</ref> compares different methods on CIFAR-100 datasets, where we incrementally learn groups of 10, 20 and 50 classes at a time. The results indicate superior performance of the proposed method in all settings. For the case of learning 10 classes at a time, we improve upon iCARL <ref type="bibr" target="#b3">[4]</ref> by an absolute margin of 7.3%. Compared with the second best method, our approach achieves a relative gain of 5.3% and 9.7% respectively for the case of incrementally learning 20 and 50 classes on CIFAR-100 dataset. For the case of 50 classes per task, our performance is only 3.2% below the Oracle approach, where all current and previous class samples are used for training. <ref type="figure">Fig. 1</ref> compares different methods on ImageNet dataset. The results show that for experimental settings consistent with iCARL <ref type="bibr" target="#b3">[4]</ref>, our proposed method achieves a significant absolute performance gain of 10.6% compared with the existing state-of-the-art <ref type="bibr" target="#b3">[4]</ref>. Our experimental results indicate that commonly used technique of fine-tuning a model on new classes is clearly an inferior approach, and results in catastrophic forgetting. <ref type="table" target="#tab_3">Table 2</ref> compares different methods on MNIST and SVHN datasets following experimental setting of <ref type="bibr" target="#b25">[26]</ref>. The results show that Adaptive RPS-Net, surpasses all previous methods with a margin of 4.3% and 13.3% respectively for MNIST and SVHN datasets. The results further indicate that the methods which do not use a memory perform relatively lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies and Analysis</head><p>In this section, we report ablation experiments and analyze the behaviour of our approach under different configurations of hyper-parameters in our approach.</p><p>What really matters? <ref type="figure" target="#fig_3">Fig. 5a</ref> studies the impact of progressively integrating individual components of our Adaptive RPS-Net. We begin with a simple baseline model with a single path that achieves 37.97% classification accuracy on CIFAR100 dataset. When distillation loss is used alongside the baseline model, the performance increases to 44.93%. The addition of our proposed controller ?(k, ?) in the loss function further gives a significant boost of +6.83%, resulting in an overall accuracy of 51.76%. Finally, the proposed multi-path selection algorithm along with above mentioned components increases the classification <ref type="table">Table 1</ref>: Large Scale experiments on ImageNet-1K and and MS-Celeb-10K show that Adaptive RPS-Net performs favourably against all the state-of-the-art methods. Note that reported task t accuracy is an average of all 1, 2, .., t tasks.  accuracy up to 58.48%. This demonstrates that our two contributions, controller and multi-path selection, provide a combined gain of 13.6% over baseline + distillation. Note that these experimental results are obtained without our proposed dynamic path switching rule and the attention function whose effect is extensively explore later in this section. Model Size Comparison: Although our approach dynamically increases the network's capacity to allow learning new tasks, it is important to note that the parametric complexity remains bounded for a large number of tasks. <ref type="figure" target="#fig_3">Fig. 5b</ref> compares total parameters across tasks for Progressive Nets <ref type="bibr" target="#b4">[5]</ref>, iCARL <ref type="bibr" target="#b3">[4]</ref> and our Adaptive RPS-Net on CIFAR100. Our model effectively reuses previous parameters, and the model size does not increase significantly with tasks. After 10 tasks, RPS-Net has 72.26M parameters on average, compared with iCARL (21.3M) and Progressive Nets (932.84M). This shows that in RPS-Net, learnable parameters increase logarithmically, while for Progressive Nets they increase quadratically.</p><p>Varying Number of Exemplars: Ours is a memory based approach that keeps a small set of exemplars for memory replay at later stages to avoid catastrophic forgetting.</p><p>Generally, a larger set of exemplars should help improve the performance. It is therefore important to study how the RPS-Net performs for different sizes of exemplar set. <ref type="figure" target="#fig_3">Fig. 5c</ref> compares RPS-Net with the best existing method (iCARL) for various memory budgets of exemplars on CIFAR100 dataset. Overall, an improvement is observed as the memory budget is increased for both approaches, but our proposed RPS-Net consistently performs better across all budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention function:</head><p>In this section, we study several other attention mechanisms in comparison to our proposed peak response function (see <ref type="figure" target="#fig_3">Fig. 5d</ref>). To this end, we try path mean response function, in which we simply take the average of the activations after a convolution module in the last layer which results in a scalar value and can be directly used to reweight the path responses. We also use an attention function with one convolution layer and a shared fully connected layer (path response ? (A)). First a convolution layer converts w ? w ? c tensor to w ? w ? 1 matrix, and M such feature maps are concatenated to form a w ? w ? M tensor and fed into a shared fully connected layer to output a M dimensional vector and its corresponding dimensions are used to re-weight the M output tensors. In the cases where a path is not activated, a zero tensor is used. Similarly, for path response ? (B), maximum activation values of the tensor along the channels are used to get c features per path (in this case, last layer c = 512), and a shared fully connected layer is used to get M re-weight coefficients.</p><p>Scaling Factor ?: It controls the equilibrium between cross-entropy and distillation losses (or the balance between new and old tasks). In <ref type="figure" target="#fig_4">Fig. 6a</ref>, for smaller ?, the network tends to forget old information while learning the new tasks well and vice versa. For example, when ? = 1 (same as loss function used in iCaRL <ref type="bibr" target="#b3">[4]</ref>) the performance drops, showing the model is not at its equilibrium state. On the other hand, ? = 8 also shows a drop in performance towards the later tasks (51% at task 10). Empirically, we found the optimal value for ? = 2.5, to keep the equilibrium till last tasks. In this ablation experiment, we keep the network configuration same for all cases, thus we manually change the paths every two task. This is to remove the effect of dynamic path switching which change the effect of ?. For example for a small value of ?, network may not remember all the past information, thus a single path can be used many  times, while for a higher value of ?, paths will saturate faster. Varying Blocks and Paths: One of the important restriction in Adaptive RPS-Net design is the networks' capacity, upper-bounded by M ?L modules. As proposed in the learning strategy, a module is trained only once for a path. Hence, it is interesting to study whether the network saturates for a high number of tasks. To analyze this effect, we change the parameters M and J. Here, M is the number of modules in a layer, and J is the number of tasks a path is trained without switching. Here we do not use our dynamic path selection strategy since we cannot control when switching happens in that case. Our results with varying M are reported in <ref type="figure" target="#fig_4">Fig. 6c</ref>, which demonstrate that the network can perform well even when all paths are saturated. This effect is a consequence of our residual design where the last classification layer and skip connections are always trained, thus helping to continually learn new tasks even if the network is saturated. If saturation occurs, the model has already seen many tasks, therefore it learns generalizable features that can work well for future tasks with the help of residual signal (carrying complementary information) via skip connections and adaptation of the final classification layer weights.</p><p>In <ref type="figure" target="#fig_4">Fig. 6b</ref>, we illustrate results with varying paths (paths ? 1 J ) in the network. We note that learning a high number of paths degrades performance as the previously learned parameters are less likely to be effectively reused. On the other hand, we obtain comparable performance with fewer paths (e.g., 2 for CIFAR-100).</p><p>FLOPS comparison: As the number of tasks increase, the network's complexity grows. As shown in <ref type="figure" target="#fig_4">Fig. 6d</ref>, with different configurations of modules and paths, the computational complexity of our approach scales logarithmically. This proves that the complexity of Adaptive RPS-Net is bounded by O(log(#task)). This is due to the fact that the overlapping modules increase as the training progresses. Further, in our setting we chose new paths after every J &gt; 1 tasks. Hence, in practice our computational complexity is well below the worst-case logarithmic curve. For example with a setting of M =2, J=2 the computational requirements reduces by 63.7% while achieving the best performance. We also show that even when a single path is used for all the tasks (M =1), our model achieves almost the same performance as state-of-the-art with constant computational complexity.</p><p>Saturation coefficient: One of the important hyperparameter of Adaptive RPS-Net is ? sat , which controls the trade-off between the network resource allocation and performance. Compared to the case when a fixed rule is used to switch paths (e.g., after every 2 tasks resulting in 5 paths), an adaptive rule based on network saturation with th = 2 helps us achieve similar performance with only 3 paths (see <ref type="figure">Fig. 7</ref>).   <ref type="figure">Figure 7</ref>: Trend for saturation coefficient and model performance under different path switching rules. Left, we can see a sawtooth type pattern, this is mainly because once a saturation coefficient passes the threshold value, the network will chose a new path (with possibly several untrained modules). Thus the saturation coefficient will drop with a new path. Dashed line show the case when paths are changed manually, while solid lines denote the case when paths are changed automatically with a given threshold on saturation coefficient. Right, the plot shows that higher number of paths results in better performance. However, too many paths will introduce more computational overhead and memory requirements. Using an automatic path switching rule (based on a threshold for saturation coefficient), our method finds the best place to switch to a new task, and therefore balance the trade-off between performance and computational complexity. Accuracy % Convergence across different tasks task-2 task-4 task-6 task-8 task-10 This illustrates that our proposed dynamic path switching scheme that intelligently expands model capacity can help in significant reduction of the computational requirements in Adaptive RPS-Net. Specifically, the best performance is achieved by changing paths for every two tasks, manually. Further, we can surpass iCaRL <ref type="bibr" target="#b3">[4]</ref> performance with only two different paths (th = 3). This shows that, not only we able to learn the places where we need to jump the paths, this helps in maximum utilization of available resources. Forward Transfer: The convergence trends shown in <ref type="figure" target="#fig_7">Fig. 8</ref> demonstrate the forward knowledge transfer for Adaptive RPS-Net. We can see that for task-2, the model takes relatively longer to converge compared with task-10. Precisely, for the final task, the model achieves 95% of the total performance within only one epoch, while for the second task it starts with 65% and takes up-to 20 epochs to achieve 95% of the final accuracy. This trends shows the faster convergence of our model for newer tasks This effect is due to residual learning as well as overlapping module sharing in Adaptive RPS-Net design, demonstrating its forward transfer capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy vs Saturation Threshold</head><p>Backward Transfer: <ref type="figure" target="#fig_8">Fig. 9</ref> shows evolution of our model with new tasks. We can see that the performance of the current task (k) is lower than the previous tasks (&lt;k). Yet, as the model evolves, the performance of task k gradually increases. This demonstrates models' capability of backward knowledge transfer, which is also reflected in biological aspects of human brain. Specifically, hippocampus in human brain accomplishes fast learning which is later slowly consolidated with the slow learning at neocortex <ref type="bibr" target="#b34">[35]</ref>. In <ref type="figure" target="#fig_8">Fig. 9</ref>, we can see the pattern of slow learning, with the performance on new tasks gradually maturing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with a Genetic Algorithm:</head><p>We compare our random selection approach with a genetic algorithm i.e., Binary Tournament Selection (BTS) <ref type="bibr" target="#b35">[36]</ref> for 25 maximum generations, on MNIST with 5 tasks (each of 2 classes), using a simple 2 layer (100 neurons) MLP with M = 8, J = 1. On 5 runs, our proposed random selection achieves an average accuracy of 96.52% vs BTS gets 96.32%. For same time complexity as ours, BTS has an average accuracy of 71.24% for the first generation models. For BTS to gain similar performance as our random selection, it needs an average of 10.2 generations (much higher than the number of random paths), hence BTS has more compute complexity. Although BTS is a simple genetic algorithm and more sophisticated genetic algorithms may outperform random selection, but likely with a high compute cost, which is not suitable for an incremental classifier learning setting having multiple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPS-Net vs. Adaptive RPS-Net:</head><p>In terms of compu-   tational efficiency, our proposed adaptive path selection algorithm uses less number of FLOPS to gain the same performance as RPS-Net <ref type="bibr" target="#b8">[9]</ref>. For example, without the adaptive path selection, RPS-Net achieves 59.93% on CIFAR-100 with 10 tasks utilizing 5 number of paths. However, with Adaptive RPS-Net, we achieve 59.15% with th = 2 and only 3 paths. This shows a ?40% improvement in computationally efficiency at inference time. Further, performance-wise Adaptive RPS-Net surpasses <ref type="bibr" target="#b8">[9]</ref> by a considerable margin on all datasets. As an example, we consider CIFAR-100 with 10 and 20 incremental tasks, the modified RPS-Net with adaptive path selection and path attention surpasses RPS-Net [9] by 4.35% and 2.71% respectively. In <ref type="table" target="#tab_6">Table 3</ref>, we provide more comparisons on the performance gain by Adaptive RPS-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In real-life setting, learning tasks appear in a sequential order and an autonomous agent must continually increment its existing knowledge. Deep neural networks excel in the cumulative learning setting where all tasks are available all together, but their performance deteriorates for the incremental learning case. In this paper, we propose a scalable approach to class-incremental learning that aims to keep the right balance between previously acquired knowledge and the newly presented tasks. We achieve this using an optimal path selection approach that supports parallelism and knowledge exchange between old and new tasks. Our approach can automatically estimate path capacity with in the network and subsequently decide if a new path is required to continue learning new tasks. Further, a controlling mechanism is introduced to maintain an equilibrium between the stability and plasticity of the learned model. Our approach delivers strong performance gains on MNIST, SVHN, CIFAR-100, MS-Celeb and ImageNet datasets for incremental learning problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An overview of our Adaptive RPS-Net:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results on CIFAR-100. Evaluations are performed with 20, 10, 5 and 2 tasks (from left to right). We surpass state-of-the-art results on all four set of experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Detailed experimental analysis of Adaptive RPS-Net. (a) Ablation experiments to study the contribution from individual components of our approach, (b) The effect of number of tasks on the overall parametric complexity of Adaptive RPS-Net versus iCARL [4] and Progressive Networks [5]. (c) The effect of changing the number of exemplars on ours and iCARL [4]. (d) The comparison between different attention functions used to re-weight the path responses in Adaptive RPS-Net. The classification accuracy on CIFAR100 with 10 tasks is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Parameter Sensitivity Analysis. Performance trend with different settings of parameters ?, J &amp; M are reported (from left to right). The effect of changing parameters M and J on the computational complexity (in terms of FLOPS) of Adaptive RPS-Net is shown in the right-most plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Forward Transfer. Adaptive RPS-Net converges fast for the final tasks, showing forward transfer as the learning progresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Bakward Transfer. Confusion matrices over 10 incremental tasks on CIFAR-100, showing backward knowledge transfer. For better visualization, a transformation of log(x+1) is applied to the confusion matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>-100: Learning 20 Classes at a time</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DMC Oracle LwF RWalk</cell><cell>SI MAS EWC Finetuning</cell><cell></cell><cell cols="2">FixedRep iCaRL Ours</cell><cell>80 90 100</cell><cell cols="3">CIFAR-100: Learning 10 Classes at a time DMC Oracle LwF RWalk SI MAS EWC Finetuning FixedRep iCaRL Ours</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy %</cell><cell>40 50 60</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80 90 100</cell><cell cols="8">CIFAR-100: Learning 20 Classes at a time DMC Oracle LwF RWalk SI MAS EWC Finetuning FixedRep iCaRL Ours</cell><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40 Number of Classes 50 60 70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy %</cell><cell>40 50 60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>20</cell><cell>30</cell><cell>40</cell><cell cols="2">50 Number of Classes 60 70</cell><cell>80</cell><cell>90</cell><cell>100</cell><cell></cell><cell>50</cell><cell></cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison on MNIST and SVHN datasets.</figDesc><table><row><cell>Ours is a</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison between RPS-Net and Adaptive RPS-Net.Ours is a modified version of RPS-Net with adaptive path selection strategy.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Tasks RPS-Net Adaptive RPS-Net</cell></row><row><cell>CIFAR100</cell><cell>10</cell><cell>56.48%</cell><cell>60.83%</cell></row><row><cell>CIFAR100</cell><cell>20</cell><cell>50.83%</cell><cell>53.54%</cell></row><row><cell>SVHN</cell><cell>5</cell><cell>88.91%</cell><cell>90.83%</cell></row><row><cell>MS-Celeb</cell><cell>10</cell><cell>65.00%</cell><cell>69.20%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A guide to convolutional neural networks for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2001" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abitino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtysecond AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random path selection for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memory retention-the synaptic stability versus plasticity dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in neurosciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="78" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lifelong learning via progressive distillation and retrospection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fearnet: Brain-inspired model for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A bio-inspired incremental learning architecture for applied perceptual problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gepperth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Karaoguz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="924" to="934" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep generative dual memory network for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental learning using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Il2m: Class incremental learning with dual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belouadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Error-driven incremental learning in deep convolutional neural network for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01569</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reevaluating continual learning scenarios: A categorization and case for strong baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.12488" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="532" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Class-incremental learning via deep model consolidation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07864</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Progress &amp; compress: A scalable framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06370</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generative replay with feedback connections as a general strategy for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10635</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno>abs/1802.07569</idno>
		<ptr target="http://arxiv.org/abs/1802.07569" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Genetic algorithms, tournament selection, and the effects of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="212" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
