<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Matching Prior: Test-Time Optimization for Dense Correspondence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Hong</surname></persName>
							<email>sunghwan@korea.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
							<email>seungryongkim@korea.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Matching Prior: Test-Time Optimization for Dense Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional techniques to establish dense correspondences across visually or semantically similar images focused on designing a task-specific matching prior, which is difficult to model in general. To overcome this, recent learning-based methods have attempted to learn a good matching prior within a model itself on large training data. The performance improvement was apparent, but the need for sufficient training data and intensive learning hinders their applicability. Moreover, using the fixed model at test time does not account for the fact that a pair of images may require their own prior, thus providing limited performance and poor generalization to unseen images.</p><p>In this paper, we show that an image pair-specific prior can be captured by solely optimizing the untrained matching networks on an input pair of images. Tailored for such test-time optimization for dense correspondence, we present a residual matching network and a confidence-aware contrastive loss to guarantee a meaningful convergence. Experiments demonstrate that our framework, dubbed Deep Matching Prior (DMP), is competitive, or even outperforms, against the latest learning-based methods on several benchmarks for geometric matching and semantic matching, even though it requires neither large training data nor intensive learning. With the networks pre-trained, DMP attains state-of-the-art performance on all benchmarks. * Corresponding author 1 It can also be called a smoothness or a regularizer in literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Establishing dense correspondences across visually or semantically similar images facilitates a variety of computer vision applications <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>. Unlike sparse correspondence <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b75">77]</ref> that detects sparse points and finds matches across them, dense correspondence <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> aims at finding matches at each pixel and thus can benefit from prior 1 knowledge about matches among nearby pixels.</p><p>Typically, stereo matching <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6]</ref> and optical flow <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b64">66]</ref> modeled the prior term that makes the correspondence <ref type="figure" target="#fig_9">Figure 1</ref>. Visualization of DMP results: (a) source image, (b) target image, results of (c) winner-takes-all (WTA) and (d) learningbased method (e.g., GLU-Net <ref type="bibr" target="#b69">[71]</ref>). As the iteration evolves (e), (f), (g), and (h), DMP with untrained networks estimates more optimal correspondence fields by optimizing the networks on a single pair of images, while learning-based methods utilize pre-trained and fixed networks at test time, thus providing limitation.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g) (h)</formula><p>smooth while aligning discontinuities to image boundaries. Some methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref> for semantic matching also exploited this to regularize the correspondence within a local neighborhood. Although they can be formulated in various ways, these optimization-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref> formulate an objective function with explicitly defined matching data and prior terms and minimize the objective on a single image pair. These methods are capable of making corrections to the estimated correspondence during optimization, but they require a task-specific prior, which is complex to formulate. Unlike these learning-free methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>, recent methods <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b69">71]</ref> cast this task as a learning problem, seeking to learn a matching model to directly regress the correspondence. The model, often implemented based on convolutional neural networks (CNNs) <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b21">22]</ref>, is generally trained on large datasets of image pairs, based on the belief that an optimal matching prior can be learned from such observations. As proven in literature <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71]</ref>, these learning-based methods outperform traditional optimization-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>, which can be attributed to their high capacity to learn a good matching prior. They, however, often require large training data with ground-truth correspondences, which are notoriously hard to collect, or intensive learning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71]</ref>, which hinders their applicability. In addition, a pair of images may require their own prior, and thus using pre-trained and fixed parameters at test time may provide limited and poor generalization performance to unseen image pairs.</p><p>In this paper, we show that, for the first time, the matching prior must not necessarily be learned from large training data; instead, it can be captured by optimizing the untrained matching networks on a single pair of images, proving that the structure of the networks is sufficient to capture a great deal of matching statistics. Our framework, dubbed Deep Matching Prior (DMP), requires neither large training sets nor intensive training, but it is competitive against learningbased methods <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63]</ref> or even outperforms, and does not suffer from the generalization issue.</p><p>Such a test-time optimization for dense correspondence, however, is extremely hard to converge due to the limited samples, high-dimensional search space, and non-convexity of the objective. To elevate the stability and boost the convergence, we propose a residual matching network, which exploits a distilled information from matching correlation that plays as a guidance for providing a good starting match and suppressing the possibility of getting stuck in local minima. We also propose a confidence-aware contrastive loss that only takes the matches with high confidence to eliminate the ambiguous matches. <ref type="figure" target="#fig_9">Fig. 1</ref> visualizes the results of DMP in comparison with recent learning-based method <ref type="bibr" target="#b69">[71]</ref>.</p><p>The presented approach is evaluated on several benchmarks for dense correspondence and examined in an ablation study. The extensive experiments show that our model produces competitive results and even outperforms other learning-based methods <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b69">71]</ref>, and once the networks are pre-trained, state-of-the-art performance is attained on all the benchmarks in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Dense correspondence. Most early efforts for dense correspondence <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b66">68]</ref> have focused on designing a taskspecific prior term in an objective function and improving optimization while employing hand-crafted features. Kim et al. <ref type="bibr" target="#b36">[37]</ref> presented DCTM optimizer with a discontinuityaware prior term to elevate geometric invariance.</p><p>Similarly to other tasks, most recent works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b72">74,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> leverage deep learning to achieve better performance for this task, at first replacing the hand-crafted features with the deep features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>, and rapidly converging towards end-to-end networks embodying the entire pipeline <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. In this context, most recent efforts have been focusing on how to better design the matching networks and learn the networks without ground-truth correspondences. Rocco et al. <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b55">57]</ref> proposed a geometric matching network, and their success inspired many variants that use local neighborhood consensus <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b40">41]</ref>, attention mechanism <ref type="bibr" target="#b22">[23]</ref>, or attribute transfer <ref type="bibr" target="#b35">[36]</ref>. Similar to PWC-Net <ref type="bibr" target="#b64">[66]</ref> designed specifically for optical flow estimation, PARN <ref type="bibr" target="#b26">[27]</ref>, DGC-Net <ref type="bibr" target="#b48">[49]</ref>, GLU-Net <ref type="bibr" target="#b69">[71]</ref> and GOCor <ref type="bibr" target="#b68">[70]</ref> formulate their networks in a pyramidal fashion. RANSAC-Flow <ref type="bibr" target="#b61">[63]</ref> leverages RANSAC <ref type="bibr" target="#b13">[14]</ref> within the network to improve the performance. On the other hand, to overcome the lack of ground-truth correspondences for training, synthetic supervision generated by applying random transformations <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b68">70]</ref> and weak supervision in the form of image pairs based on the feature reconstruction <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b35">36]</ref> have been popularly used. However, these aforementioned works require either large labeled dataset or intensive training, which hinders their applicability.</p><p>Self-supervised representation learning. Popularized by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>, contrastive learning seeks to learn feature representation in a self-supervised manner by minimizing the distance between two views augmented from an image and maximizing the distance to other images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. Several methods <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b52">54]</ref> bring contrastive learning to the task of dense correspondence. A key benefit of self-supervised learning is that, because there is no reliance on labeled data, training needs not be limited to the training phase <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b61">63]</ref>. CRW <ref type="bibr" target="#b24">[25]</ref> and RANSAN-Flow <ref type="bibr" target="#b61">[63]</ref> attempted to apply such a selfsupervised adaptation at test time, but the performance was limited. The use of self-supervised adaptation for dense correspondence has never been thoroughly investigated and this paper is the first step in this direction.</p><p>Test-time optimization. Deep Image Prior (DIP) <ref type="bibr" target="#b70">[72]</ref> initiated the trend that the low-level statistics in a single image can be captured by the structure of randomly-initialized CNNs, of which many variants were proposed, tailored to solve an inverse problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b73">75,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b28">29]</ref>. GAN-inversion aims to invert an image back to latent code, and then reconstruct the image from the latent code to utilize the pretrained generative prior <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b76">78]</ref>. Deep Generative Prior (DGP) [52] extends this trend by, for the pretrained generator, optimizing both latent codes and parameters. These aforementioned methods were tailored to capture an image prior, but there was no attempt to capture a matching prior, which is the topic of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Matching Prior</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Let us denote a pair of images, i.e., source and target, as I s and I t , which may represent visually or semantically similar images. The objective of dense correspondence is  <ref type="figure" target="#fig_10">Figure 2</ref>. Intuition of DMP: (a) optimization-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b36">37]</ref> that formulate their objective function with data and prior terms, and minimize the energy function on a single image pair, (b) learning-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63]</ref> that learn a matching prior from large training set of image pairs, and (c) our DMP, which takes the best of both approaches to estimate an image pair-specific matching prior.</p><p>to establish a dense correspondence field F (i) between two images that is defined for each pixel i, which warps I s towards I t so as to satisfy I t (i) ? I s (i + F (i)).</p><p>To achieve this, the easiest solution may be, for a point in the source, to find the most similar one among all the points in the target with respect to a data term that accounts for matching evidence between features, defined such that</p><formula xml:id="formula_1">F * = argmin F L data (D s (F ), D t ),<label>(1)</label></formula><p>where a feature D is extracted with parameters ? f such that D = F(I; ? f ) and D s (F ) is an warped source feature with F . This solution F * , so-called winner-takes-all (WTA) solution, is highly vulnerable to local minima or outliers.</p><p>To alleviate these, optimization-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37]</ref> typically formulate an objective function involving the data term and a prior term that favors similar correspondence fields among adjacent pixels with a balancing parameter ?, as shown in <ref type="figure" target="#fig_10">Fig. 2</ref> (a), such that</p><formula xml:id="formula_2">F * = argmin F L data (D s (F ), D t ) + ?L prior (F ) . (2)</formula><p>Traditional optimization-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref> model an explicit prior term such as total variation (TV) or discontinuity-aware smoothness and optimize the energy function on a given pair of images. Since this energy function is often non-convex, they use an iterative solver, e.g., gradient descent <ref type="bibr" target="#b37">[38]</ref>, to minimize this, thus they can benefit from an error feedback to find more optimal solution by making corrections to the estimated correspondence as the iteration evolves. However, they require a hand-designed task-specific prior, which is hard to design.</p><p>Unlike these optimization-based methods, recent learning-based methods <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b69">71</ref>] learn a matching model to directly regress the correspondence such that F = F(I s , I t ; ? m ) with parameters ? m , based on an assumption that an optimal prior could be learned within the model itself from massive training samples, {(I s n , I t n )} n?{1,...,N } , as shown in <ref type="figure" target="#fig_10">Fig. 2 (b)</ref>. The model is often implemented in the form of convolutional neural networks (CNNs). During the training phase, the parameters ? ? m are first optimized, and the learned parameters are then used to regress a correspondence F * through the networks with the learned prior at test time as follows:</p><formula xml:id="formula_3">? ? m = argmin ? n L data?GT (F(I s n , I t n ; ?),F n ), F * = F(I s , I t ; ? ? m ),<label>(3)</label></formula><p>whereF n denotes a ground-truth correspondence for n-th sample in the training set. As proven in literature <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71]</ref>, these learning-based methods have shown better performance than the traditional optimization-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref> thanks to their high capacity to learn the good matching prior. To optimize the parameters by minimizing the data term, e.g., L data?GT (F,F ) = F ?F 1 , they, however, require massive training image pairs with groundtruth correspondences, which are notoriously hard to collect. Some recent methods <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b61">63]</ref> overcome this limitation by presenting an unsupervised loss defined only with source and target features, e.g., L data (D s (F ), D t ), or synthetic ground-truths, but these methods inherit the limitation of requiring an intensive training procedure and frequently fail to learn an image pair-specific prior, thus providing limited performance and generalization power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Formulation</head><p>In this session, we argue that the matching prior does not necessarily need to be learned from an intensive learning; instead, an image pair-specific matching prior can be captured by solely minimizing the data term on a single pair of images, like what is done by traditional optimization-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b36">37]</ref>, with an untrained network for matching, as shown in <ref type="figure" target="#fig_10">Fig. 2 (c)</ref>, which can be formulated as</p><formula xml:id="formula_4">? * m = argmin ? L data (D s (F(I s , I t ; ?)), D t ), F * = F(I s , I t ; ? * m ),<label>(4)</label></formula><p>where ? * m are parameters, over-fitted to a single pair of images to allow for generating F * with a pair-specific matching prior, and can be obtained using an optimizer such as <ref type="bibr">Figure 3</ref>. DMP convergence: (From left to right) source image, target image, iterative evolution of warped images. Given a good initialization, which facilitates convergence and boosts matching performance, our approach successfully estimates the correspondence fields.  <ref type="figure" target="#fig_12">Figure 4</ref>. Convergence analysis of DMP. Starting from untrained network, our DMP converges to better correspondences by optimizing the network with a well-designed loss function on a single pair of images. Compared to the learning-based methods, such as DGC-Net <ref type="bibr" target="#b48">[49]</ref> and GLU-Net <ref type="bibr" target="#b69">[71]</ref> that are pre-trained with an intensive learning on large training datasets, our DMP has shown a competitive performance after convergence, which can be expedited with pre-trained initialization (denoted by DMP ?). More details can be found in supplementary material. gradient descent <ref type="bibr" target="#b37">[38]</ref>, starting from a random initialization. Our framework, dubbed Deep Matching Prior (DMP), requires neither massive training image pairs nor groundtruth correspondences that are major bottlenecks of existing learning-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63]</ref>, but only requires a single pair of images to be matched, which is competitive when compared to learning-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63]</ref> and even outperforms, as exemplified in <ref type="figure" target="#fig_12">Fig. 4</ref> and <ref type="figure">Fig. 5</ref>. Thanks to its inherent error feedback nature, DMP does not suffer from a generalization issue for unseen image pairs, which existing learning-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63]</ref> frequently fail to avoid.</p><p>However, optimizing the existing matching network <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63</ref>] on a single image pair with simple data term <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b36">37]</ref> is extremely hard to converge due to the lack of samples, high-dimensional search space for dense correspondence and its non-convexity <ref type="bibr" target="#b36">[37]</ref>. As shown in <ref type="figure">Fig. 5</ref>, when an existing matching network, e.g., GLU-Net <ref type="bibr" target="#b74">[76]</ref>, is optimized from a random initialization with a self-supervised loss, e.g., feature matching <ref type="bibr" target="#b61">[63]</ref>, denoted by GLU-Net ?, it fails to find meaningful correspondences. To overcome these, we present a residual matching network and a confidenceaware contrastive loss tailored to boost the convergence and matching performance. In the following, we describe the network architecture and loss function in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>To guarantee a meaningful convergence during the optimization, a good initialization for correspondence F should be set, even though our network parameters ? are randomly initialized. To achieve this, we formulate our model, consisting of feature extraction networks and matching networks, in a residual manner, as illustrated in <ref type="figure">Fig. 6</ref>.</p><p>Feature extraction networks. Our model accomplishes dense correspondence using deep features, e.g., VGG <ref type="bibr" target="#b63">[65]</ref> or ResNet <ref type="bibr" target="#b21">[22]</ref>, which undergo l-2 normalization. For the backbone feature, we exploit a model pre-trained on Ima-geNet <ref type="bibr" target="#b10">[11]</ref>, as done in almost most literature <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63]</ref>. The backbone model could be directly optimized in DMP framework, but we found that using an additional adaptation layer <ref type="bibr" target="#b39">[40]</ref> to refine the backbone features and optimizing the layer only boosts the performance drastically. This is because it helps to focus on more learning matching networks at early training stages. The learnable parameters ? f in the feature extractor are thus from the adaptation layers. At the initialization, we zero-initialize the layer to make it behave like an identity.</p><p>Matching networks. Our matching networks consist of correlation map computation and inference modules. The correlation maps are first computed using the inner product between features as</p><formula xml:id="formula_5">C(i, l) = D s (i) ? D t (l),<label>(5)</label></formula><p>where l is defined within a search space in the target. Based on the correlation C, correspondence F s is estimated through an inference module with parameters ? m as</p><formula xml:id="formula_6">F s = F(C; ? m ).<label>(6)</label></formula><p>A randomly-initialized ? m , however, gives a noisy correspondence at the initial phase, making the optimization difficult. To overcome this, we enforce the inference module to estimate a residual of current best matches from given correlation volume, which is achieved by a soft-argmax operation [?] over C, such that</p><formula xml:id="formula_7">F = F(C; ? m ) + ?(C),<label>(7)</label></formula><formula xml:id="formula_8">! " "?! Confidence-aware Contrastive Loss "?! " "?! " Adaptation Backbone Cost Comp. Soft-argmax Inference Feature Extraction Network Feature Extraction Network Matching Network Feature Extraction Network Adaptation Backbone " ! "?! $ $ % &amp;'"' ( ! ( ), " )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>Warping <ref type="figure">Figure 5</ref>. Network configuration and loss function of DMP: Our networks consist of feature extraction and matching networks, which are formulated in a residual manner to guarantee a good initialization for optimization. Note that a single-level version of the networks are illustrated for brevity, while the full model is formulated in a pyramidal fashion. A confidence-aware contrastive loss enables joint learning of feature extraction and matching networks by rejecting ambiguous matches while accepting confident matches through a thresholding.</p><p>where ?(?) represents a soft-argmax operator. By setting the parameters of the last layer to generate zeros at the initialization, an optimizer can start at least the current best matches from given correlation volume. As evolving the iterations, the networks provide more regularized matching fields that encode an image pair-specific prior.</p><p>Coarse-to-fine formulation. Analogous to <ref type="bibr" target="#b65">[67,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b69">71]</ref>, we also utilize the coarse-to-fine approach through a pyramidal processing. Specifically, we exploit pyramidal features from coarse-to-fine levels to simultaneously provide robustness against deformations and improve finegrained matching details by minimizing the data term at each layer. At the coarsest level, we consider a global correlation module, while a local correlation module is used at remaining levels. In addition to the use of soft-argmax for residual flow learning, at each level, previous level's matches are up-sampled to play as a guidance. To handle any input resolution, we employ an adaptive resolution <ref type="bibr" target="#b69">[71]</ref>. The details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss function</head><p>Similarly to traditional optimization-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b36">37]</ref> and unsupervised learning-based methods <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b35">36]</ref>, the data term in DMP is defined as the similarity function S(?, ?) of features, aggregated across the points i ? {1, ..., M } as</p><formula xml:id="formula_9">L data (D s (F ), D t ) = ? 1 M i S(D t?s (i), D t (i)),<label>(8)</label></formula><p>where D t?s represents an warped source feature with F . Although source feature can be directly warped, we extract the feature directly from warped source image I t?s with the final correspondence F at the pyramid that incorporates the information at all pyramidal layers, which demonstrates more stable convergence.</p><p>Perhaps the simplest similarity function is to use the inner product between l-2 normalized features as S(D t?s (i), D t (i)) = D t?s (i) ? D t (i), accounting for the intuition that the similarities between all pairs of the warped source and target features at the same locations should be maximized. Minimizing this, however, can induce erroneous solutions, e.g., constant features at all the points. To avoid such trivial solutions, we extend this similarity function in a contrastive learning fashion, aiming to maximize the similarities at the same locations while minimizing for the others, such that given</p><formula xml:id="formula_10">S c = exp(D t?s (i) ? D t (i)/? ) j exp(D t?s (i) ? D t (j)/? ) ,<label>(9)</label></formula><p>where j ? {1, ..., M } and ? is a temperature hyperparameter, by minimizing the loss function ?log(S c ), both feature extraction and matching networks with parameters ? f and ? m , respectively, can be simultaneously optimized in a manner that the feature extractors embed the warped source and target features at same location to the same representation while the matching networks regress better correspondences that maximize the feature similarity. Even though this loss helps to successfully avoid the trivial solution and allows joint learning of feature and matching networks, the erroneous initial estimates may be propagated, without a term to eliminate such estimates. Since our DMP optimizes the networks on a single pair of images, such adverse effects may be critical. To mitigate this, we present a confidence-aware contrastive loss that enables rejecting such ambiguous matches with a thresholding while accepting the confident matches, defined such that</p><formula xml:id="formula_11">S cac = ?log (? (S c , ?)) ,<label>(10)</label></formula><p>where ? is a confidence hyperparameter; ?(x, ?) is a function designed to produce x if x ? ? and 1 otherwise.   <ref type="figure">Figure 6</ref>. Qualitative results on HPatches <ref type="bibr" target="#b2">[3]</ref> dataset. The source images were warped to the target images using correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Extension</head><p>In this section, we introduce several variants of our DMP to expedite convergence and boost the performance.</p><p>Test-time augmentation. Since DMP solely depends on a single pair of images, the samples that capture matching distribution are rather limited, even though it surprisingly well learns from such limited samples, which inspires the enrichment through a test-time augmentation. We present Augmentation-DMP (A-DMP), where original target and a randomly-augmented target are fed concurrently to our model, and used to optimize the networks simultaneously. Given a pair of images having dramatically large geometric variations, the randomly-augmented target would be more similar to a source than original target at an iteration, generating the loss signal to accelerate the convergence. In addition, thanks to our confidence-aware loss, if augmented ones are more difficult to be matched, they are rejected. We apply one of spatial transformations at each iteration such as homography <ref type="bibr" target="#b11">[12]</ref> or affine and thin plate spline (TPS) <ref type="bibr" target="#b54">[56]</ref> to obtain the augmented targets.</p><p>Pre-training. Unsurprisingly, well-initialized parameters can guarantee better performance. We also show that by pre-training our networks, similarly to others <ref type="bibr" target="#b69">[71,</ref><ref type="bibr" target="#b68">70]</ref>, denoted by DMP ?, our DMP can start with a well learned initialization, thus yielding much better results. Note that <ref type="bibr" target="#b61">[63]</ref> also attempted to pre-train and fine-tune their networks on a single pair of images, but as will be seen in our experiments, it failed to achieve the satisfactory performance, which proves that our well-designed networks and loss functions are essential for such a framework. In this work, we follow the training procedure identical to <ref type="bibr" target="#b69">[71]</ref> and use the same dataset and hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>For backbone feature extractor, we used VGG-16 <ref type="bibr" target="#b63">[65]</ref> and ResNet-101 <ref type="bibr" target="#b21">[22]</ref> pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. Specifically, for geometric matching, we used VGG-16, while for semantic matching, we additionally used ResNet-101. For the loss function, instead of using all the samples, we randomly sampled M = 256 feature vectors at each iteration, considering the trade-off between performance and complexity. We set the maximum number of iterations as 2k and 0.3k for DMP (or A-DMP) and DMP ?, respectively. In addition, we set the temperature ? and confidence ? as 0.1 and 0.01, respectively, following ablation study in Sec. 4.5. We used Adam optimizer <ref type="bibr" target="#b37">[38]</ref> fixed for all experiments, but the advanced optimizers may improve performance <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b45">46]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>In this section, we conduct comprehensive experiments for geometric matching and semantic matching, by evaluating our approach through comparisons to state-of-theart methods including, CNNGeo <ref type="bibr" target="#b54">[56]</ref>, PARN <ref type="bibr" target="#b26">[27]</ref>, NC-Net <ref type="bibr" target="#b57">[59]</ref>, DGC-Net <ref type="bibr" target="#b48">[49]</ref>, SAM-Net <ref type="bibr" target="#b35">[36]</ref>, GLU-Net <ref type="bibr" target="#b69">[71]</ref>, GOCor <ref type="bibr" target="#b68">[70]</ref>, RANSAC-Flow <ref type="bibr" target="#b61">[63]</ref>, and SCOT <ref type="bibr" target="#b46">[47]</ref>. For the geometric matching task, we evaluate our method on Hpatches <ref type="bibr" target="#b2">[3]</ref> and ETH3D <ref type="bibr" target="#b59">[61]</ref> datasets, while for the semantic matching task, we evaluate our method on TSS <ref type="bibr" target="#b66">[68]</ref> and PF-PASCAL <ref type="bibr" target="#b18">[19]</ref> datasets. We also include the results of our variants for all the experiments and analyze the influence of different components of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Geometric Matching Results</head><p>HPatches. We first evaluate our method on Hpatches <ref type="bibr" target="#b2">[3]</ref> which consists of images with different views of the same scenes. Each sequence contains a source and five target images with different viewpoints with corresponding groundtruth flows. We use images of high resolutions ranging from 450 ? 600 to 1,613 ? 1,210, whereas the down-scaled images (240 ? 240) are also used for the evaluation as in <ref type="bibr" target="#b48">[49]</ref>. For the evaluation metric, we use the Average Endpoint Error (AEE), computed by averaging the euclidean distance between the ground-truth and estimated flow, and Percentage of Correct Keypoints (PCK), computed as the ratio of estimated keypoints within the threshold from groundtruths to the total number of keypoints. <ref type="table" target="#tab_0">Table 1</ref> summarizes the quantitative results, and <ref type="figure" target="#fig_3">Fig. 7</ref> visualizes the qualitative results. We also provide an ablation study for RANSAC-Flow <ref type="bibr" target="#b61">[63]</ref> to validate the effect of test-time optimization, which deteriorates performance as studied in <ref type="bibr" target="#b61">[63]</ref>. However, DMP is competitive against the state-of-the-art learning-based methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b61">63]</ref> thanks to the feedback signals during the test-time optimization. Leveraging RANSAC <ref type="bibr" target="#b13">[14]</ref> within our network allows outstanding performance, and with the networks pre-trained, DMP ? even outperforms GOCor <ref type="bibr" target="#b68">[70]</ref> by a large margin.</p><p>ETH3D. We further evaluate our method on ETH3D <ref type="bibr" target="#b59">[61]</ref> dataset. Unlike Hpatches <ref type="bibr" target="#b2">[3]</ref>, ETH3D consists of real 3D scenes, where the image transformations are not constrained to homography transformation, thus more challenging. We follow the protocol of <ref type="bibr" target="#b69">[71]</ref>, where we sample the image   pairs at different intervals to evaluate on varying magnitude of geometric transformations. We evaluate on 7 intervals in total, each interval containing approximately 500 image pairs, and employ the standard evaluation metrics, AEE and PCK. <ref type="figure" target="#fig_4">Fig. 8</ref> and <ref type="figure" target="#fig_4">Fig. 8</ref> show quantitative and quantitative results. Our methods yield highly competitive results, and DMP ?, significantly outperforming other methods, attains state-of-the-art performance .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semantic Matching Results</head><p>TSS. For semantic matching, we evaluate our method on TSS <ref type="bibr" target="#b66">[68]</ref> benchmark, which contains 3 groups (FG3DCar, JODS, and PASCAL) of 400 image pairs of 7 object categories, foreground masks and dense ground-truth flow. We employ the PCK to measure the precision. For TSS, we set the threshold as ? = 0.05. Following <ref type="bibr" target="#b69">[71]</ref>, we address the reflections by inferring the flow on both the source and flipped target and the source and original target. We then output the flow field with smaller mean horizontal magnitude. <ref type="table" target="#tab_2">Table 2</ref> summarizes the PCK values. <ref type="figure" target="#fig_5">Fig. 9</ref> visualizes the qualitative comparisons. As shown in the results, our method produces highly competitive results compared to other learning-based methods. Interestingly, DMP ? already attains state-of-the-art performance with VGG-16 <ref type="bibr" target="#b63">[65]</ref> as   feature backbone, but using the ResNet-101 <ref type="bibr" target="#b21">[22]</ref> further boosts the performance.</p><p>PF-PASCAL. We further evaluate on PF-PASCAL <ref type="bibr" target="#b18">[19]</ref> dataset, containing 1,351 image pairs over 20 object categories with keypoint annotations. Following the common practice <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b19">20]</ref>, we use PCK for the evaluation metric, and the results are reported with the PCK threshold ? = 0.1. <ref type="table" target="#tab_2">Table 2</ref> shows the quantitative results and <ref type="figure" target="#fig_7">Fig. 10</ref> visualizes the qualitative comparisons. Similar to the experiments on TSS <ref type="bibr" target="#b66">[68]</ref>, our DMP provides the highest matching accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>We show an ablation analysis on both architecture and loss function in our model. We measure the AEE over all the scenes of the HPatches <ref type="bibr" target="#b2">[3]</ref>, and each ablation experiment is conducted under the same experimental setting. We further report an analysis on computational complexity.</p><p>Network architecture. <ref type="table">Table 3</ref> shows the comparisons with different architectural components in our model. The baseline only consists backbone feature and inference module, optimized using contrastive loss without confidence threshold. Since optimization-based model is highly sensitive to initialization, exploiting residual correspondence module in matching networks dramatically helps the convergence and boosts performance in comparison to the baseline. We found that jointly exploiting both adaptation feature and residual correspondence showed apparent improvements. In addition, our model was not sensitive to the number of samples chosen for the loss computation.  <ref type="table">Table 3</ref>. Ablation study of DMP.  <ref type="figure" target="#fig_9">Figure 11</ref>. Ablation study on temperature and confidence.</p><p>Loss function. As shown in <ref type="table">Table 3</ref>, the performance improvement by confidence threshold was apparent. In addition, <ref type="figure" target="#fig_9">Fig. 11</ref> plots the accuracy as varying both the temperature (? ) and confidence (?). We found that the temperature controlling the sharpness of the softmax function is highly influential to the convergence and performance of our model. We thus found from extensive experiments that setting the temperature and confidence as 0.1 and 0.01, respectively, reports the best results. If the confidence is set too high, the loss signal does not occur and learning stops.</p><p>Computational complexity. In experiments, we found that given good initialization, DMP, including all the variants, converges within 100 iterations. However, given difficult image pairs to be matched, DMP struggles to find the good correspondences, and we thus let it iterate 2k times. The average runtime of 100 iterations of DMP and DMP ? is 3-5 seconds, while A-DMP is 6-10 seconds on a single GPU Geforce GTX 2080 Ti. More details can be found in supplementary material. A natural next step, which we leave for future work, is to reduce the runtime for the optimization by advanced techniques, e.g., early-stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proven that, for the first time, a matching prior must not necessarily be learned from large training data and have introduced DMP that captures an image pair-specific matching prior by optimizing the untrained matching networks on a single pair of images. Tailored for such test-time optimization for matching, we have developed a residual matching network and a confidenceaware contrastive loss. Our experiments have shown that although our framework requires neither a large training data nor an intensive learning, it is competitive against the latest learning-based methods and even outperforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>As shown in <ref type="figure" target="#fig_9">Fig. 1</ref>, our network consists of two parts: feature extraction networks to extract deep features and matching networks. Note that in the paper, a single-level version of the networks are illustrated for brevity, while the full model is formulated in a pyramidal fashion. In this section, we will explain our full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction network.</head><p>Here, we explain feature extraction networks in detail. We employ an adaptive resolution startegy introduced by [71] to let the network take any resolution, which we down-sample the original input images to 256?256. We then extract features from both the original and down-sampled resolutions using pre-trained backbone networks and freeze them during the optimization and training. After the feature extraction, we additionally use an adaptation layer to refine the features. Adaptation layers are random initialized and separated for each pyramidal feature map in a residual fashion <ref type="bibr" target="#b21">[22]</ref>. As described in the paper, the backbone model could be directly optimized in DMP framework, but we found that using an additional adaptation layer to refine the backbone features and optimizing the layer only boosts the performance drastically.</p><p>Specifically, we compute the residuals by adding 3 ? 3, 5 ? 5, 7 ? 7 and 9 ? 9 convolutional layers with padding of 1, 2, 3 and 4, respectively, on top of each pyramidal level. We set stride to 1 to ensure that the spatial resolution is preserved. Given VGG-16 as backbone network, identical to <ref type="bibr" target="#b69">[71]</ref>, we employ the activation after Conv5-3 and Conv4-3 for the resized (256 ? 256) input images, and Conv4-3 and Conv3-3 for the original resolution image, which outputs spatial resolution of 16?16, 32?32, H 8 ? W 8 and H 4 ? W 4 , respectively. The number of feature channels of each adaptaion layer are thus 512, 256, 256, and 128, respectively. On the contrary, if ResNet-101 is used as backbone network, we employ the activation after Conv3 and Conv4 for the resized input, whereas for the original resolution we employ Conv2 and Conv3. The number of feature channels of each adaptaion layer are thus 1024, 512, 512, and 256, respectively. It should be noted that for geometric matching task, we use VGG features, while for semantic matching task, we use both ResNet and VGG, which by default, unless mentioned, all our models use VGG-16 features.</p><p>Matching network. We then provide additional details of matching networks, which consists of two parts: cost computation and inference modules. For the global correlation, we compute the pairwise inner product between features from coarsest level. For the local correlation, we employ l = 4 for the search space in the target. As in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b69">71]</ref> we feed global correlation into a inference module, which consists of 5 feed-forward convolutional blocks with a 3 ? 3 filter. The number of output channels of each layers are 128, 128, 96, 64, and 32, respectively. For the remaining levels, we use an inference module designed for the local correlation volume which infers the flow field similar to the one in PWC-Net <ref type="bibr" target="#b64">[66]</ref>. The numbers of output channels at each layer are 128, 128, 96, 64, and 32, respectively and the size spatial kernel of is also 3 ? 3. The final output of the inference module is computed by feeding into a linear 2D convolution. The soft-argmax <ref type="bibr" target="#b31">[32]</ref> computes an output by averaging all the spatial positions with weighted corresponding probabilities. The temperature for the soft-argmax is set to 0.02.</p><p>The flow field inferred at each level is up-sampled using bilinear interpolation. From experiments, we observed that using transposed convolution degraded the performance. We thus employed bilinear interpolation at every pyramidal layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence Analysis Details</head><p>In the paper, we showed the comparison of AEE over iteration between models as shown in <ref type="figure" target="#fig_10">Fig. 2 (Fig. 4</ref> in the paper). Here, we describe the details for this experiment. For a fair comparison, we iterated 2k times for all the test-time optimization methods, which include GLU-Net ?, DMP, A-DMP and DMP ?. We evaluated each method on Hpatches <ref type="bibr" target="#b2">[3]</ref> benchmark, which consists of 295 target images, and averaged the AEE at every 10-th iteration. Note that in <ref type="figure" target="#fig_10">Fig. 2</ref>, we only show the range of 0-400 for x-axis as the AEE for all the methods except GLU-Net ? converge. To conduct experiment on GLU-Net ?, we simply replaced the model to GLU-Net within our optimization implementation under the identical experimental setting to DMP testtime optimization. We did not find noticeable differences when we attempted optimizing with different hyperparameter settings e.g., learning rate. We conducted experiment on GLU-Net ? to show that the several choices we made, including architecture and loss, were critical for the untrained network to guarantee a meaningful convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limitations</head><p>In this session, we would like to discuss the limitations of DMP and its variants. One limitation that all the methods, including DMP and its variants, is the time they take to converge. Although with good initialization, the optimization time required to obtain correspondences significantly reduces, our approach fundamentally isn't applicable for realtime applications. Furthermore, even though DMP attained  competitive results for standard benchmarks by optimizing from untrained networks, it fails to find accurate correspondences given difficult images, i.e., ETH3D interval 15. To overcome, we pre-trained DMP to provide strong initializa-tion, but this may result in weakening of DMP's advantage, an ability to avoid generalization issues. Although designed to address difficult cases, A-DMP suffers from doubled optimization time. RANSAC-DMP successfully avoids this challenge, but the use of RANSAC often yields unstable results that may lead to failure to find correspondences.</p><p>We proposed, for the first time, to find correspondences between a pair of images by test-time optimization, and we believe that further improvements could be made in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation and Experimental Details</head><p>We first pre-process the input images by centering the mean and normalizing the values using the mean and standard deviation of ImageNet <ref type="bibr" target="#b10">[11]</ref>. For DMP and A-DMP, we initially set the learning rate to 3e ?3 and divide it by 2 at every 300 iterations. For DMP ? and variants that exploit RANSAC <ref type="bibr" target="#b61">[63]</ref>, we use learning rate of 1e ?5 . We use Adam optimizer <ref type="bibr" target="#b37">[38]</ref> with ? 1 = 0.9 and ? 2 = 0.999. We implemented our model using PyTorch <ref type="bibr" target="#b51">[53]</ref>.</p><p>To obtain the randomly-augmented target for A-DMP, we use the same kind of geometric transformation to GLU-Net and DGC-Net. Specifically, Rocco et al. <ref type="bibr" target="#b54">[56]</ref> generates synthetic data using affine and thin-plate spline transformation which we additionally use homography transformation as in DGC-Net as shown in <ref type="figure" target="#fig_11">Fig. 3</ref>. To conduct experiments on variants of ours that utilize RANSAC to obtain coarsely aligned pair of images, we followed the protocol of <ref type="bibr" target="#b61">[63]</ref> to obtain a pair of coarsely aligned images first and then fed the aligned images into our network.</p><p>We additionally showed an ablation study on RANSAC-Flow <ref type="bibr" target="#b61">[63]</ref> in the paper, to validate the effect of test-time optimization. We first obtained coarsely aligned input images and then implemented using the full loss function provided in <ref type="bibr" target="#b61">[63]</ref> for the test-time optimization and iterated 2000 times with identical hyperparameter setting to RANSAC-Flow trained on Mega-Depth <ref type="bibr" target="#b42">[43]</ref>. We did not find drastic diffference when the matchability loss was not included within the total loss. For evaluating test-time optimization of RANSAC-Flow on original resolution of Hpatches, we up-sampled the estimated flow using bilinear interpolation and calculated the AEE and PCK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Results</head><p>In this section, we provide additional qualitative examples on the Hpatches <ref type="bibr" target="#b2">[3]</ref>, ETH3D <ref type="bibr" target="#b59">[61]</ref>, TSS <ref type="bibr" target="#b66">[68]</ref>, and PF-PASCAL <ref type="bibr" target="#b18">[19]</ref>.</p><p>We first show more qualitative results of convergence process of DMP. Given good initialization, DMP guarantees a meaningful convergence, which also indicates that once the warped image is similar enough to the target image, the convergence process is boosted and DMP can successfully correct the errors in the flow fields during the optimization to find the optimal flow field. As shown in <ref type="figure" target="#fig_12">Fig. 4</ref>, the convergence is boosted when the warped image is similar to the target image.</p><p>For geometric matching task, DMP shows highly competitive results , nearly approximating the ground-truth flow as shown in <ref type="figure">Fig. 5</ref>. Note that our variants estimate extremely accurate flow fields, demonstrating the superiority of our approach. <ref type="figure">Fig. 6</ref> shows the qualitative comparison on ETH3D <ref type="bibr" target="#b59">[61]</ref> dataset. All the results are from the highest intervals, which demand addressing extreme viewpoint changes. Note that our approaches, compared to GLU-Net <ref type="bibr" target="#b69">[71]</ref> which obtains satisfactory results, successfully es-timate the correspondence field between images with extreme appearance variations.</p><p>Semantic matching task requires estimating correspondence fields between images with intra-class variations. Our works, compared to GLU-Net <ref type="bibr" target="#b69">[71]</ref>, consistently obtain sharp and extremely accurate warped images as shown in <ref type="figure" target="#fig_3">Fig. 7</ref> and <ref type="figure" target="#fig_4">Fig. 8</ref>. We obtain results with fine details preserved and accurately aligned, which demonstrate the superiority of our approaches on semantic matching task.  <ref type="figure">Figure 5</ref>. Qualitative results on the Hpatches benchmark <ref type="bibr" target="#b2">[3]</ref>. (a) source and (b) target images, warped source images using correspondences of (c) GLU-Net <ref type="bibr" target="#b69">[71]</ref>, (d) DMP, (e) DMP ?, (f) RANSAC-DMP, and (g) Ground-truth. Here, we provide only the samples with extremely large geometric variations to compare the outputs produced by each variants and GLU-Net. Note that DMP, starting from untrained network, achieves competitive results against GLU-Net trained on a large-scale dataset. Thanks to RANSAC <ref type="bibr" target="#b13">[14]</ref>, DMP starts the optimization with good initialization, which results RANSAC-DMP producing highly accurate flow fields. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on ETH3D<ref type="bibr" target="#b59">[61]</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Quantitative results on ETH3D [61] dataset. AEE and PCK are computed on image pairs sampled at different intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative results on TSS<ref type="bibr" target="#b66">[68]</ref> benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>NC-Net [59] (d) GLU-Net [71] (e) DMP (f) A-DMP (g) DMP ?-VGG (h) DMP ?-ResN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Qualitative results on PF-PASCAL<ref type="bibr" target="#b18">[19]</ref> benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 1 .</head><label>1</label><figDesc>Overview of DMP architecture. Overview of our proposed iterative architecture, which consists of feature extraction network and matching network. Source and target images are first fed into feature backbone network to obtain deep features. Each pyramidal features are then fed into adaptation layers and the refined features are obtained. Subsequently, the refined features are fed into a matching network and the estimated flow is up-sampled to warp the next level feature. The final output consists of refined features from target image and the flow field of size H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 .</head><label>2</label><figDesc>Convergence analysis of DMP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 .</head><label>3</label><figDesc>Example of the synthetic images<ref type="bibr" target="#b54">[56]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 .</head><label>4</label><figDesc>Convergence of DMP. (a) source image, (b) target image, (c), (d), (e), (f), (g), and (h) iterative evolution of warped images by DMP. The error signal received at each iteration helps to correct the flow field, which the predicted transformation fields become progressively more accurate through iterative estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>Qualitative results on the ETH3D benchmark [61]: (a) source and (b) target images, warped source images using correspondences of (c) GLU-Net [71], (d) DMP, (e) A-DMP, and (f) DMP ? . Note that our loss function allows error correction, allowing more optimal estimation of flow fields. Qualitative results on the TSS [68] benchmarks. (a) source image, (b) target image, (c) ground-truth, (d) GLU-Net [71], (e) DMP, and (f) DMP ?-ResN. It is clearly visible that warped source images produced by our models resemble the target images. Note that more accurate flow fields are estimated when ResNet is used for the feature backbone network. Qualitative results on the PF-PASCAL [19] benchmarks. (a) source image, (b) target image, (c) DMP, (d) A-DMP, (e) DMP ?, and (f) DMP ?-ResN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>12.14 16.50 9.07 50.01 5.71 20.48 34.15 43.94 62.01 33.26 58.06 GLU-Net [71] 0.59 4.05 7.64 9.82 14.89 7.40 83.47 1.55 12.66 27.54 32.04 52.47 25.05 78.54 10.07 23.86 27.17 38.41 20.16 81.43 13.24 7.40 81.28 12.41 18.64 20.09 35.81 35.00 24.39 22.06 1.29 3.52 4.01 6.88 10.27 5.19 88.23 10.25 15.22 15.19 30.43 48.09 23.84 26.21 DMP 1.21 5.12 12.31 13.68 16.12 9.69 79.21 3.21 15.54 32.54 38.62 63.43 30.64 63.21 A-DMP 1.31 4.81 10.21 10.69 13.88 8.18 82.35 3.42 14.21 29.90 32.82 55.8 27.22 71.64 96.28 4.32 11.21 22.80 31.34 33.64 20.65 75.35</figDesc><table><row><cell>Methods</cell><cell>Pre-Test-train opt.</cell><cell>I</cell><cell>II</cell><cell cols="3">Hpatches (240 ? 240) III IV V</cell><cell>Avg.</cell><cell>PCK</cell><cell>I</cell><cell>II</cell><cell>III</cell><cell>Hpatches IV</cell><cell>V</cell><cell>Avg.</cell><cell>PCK</cell></row><row><cell>CNNGeo [56]</cell><cell></cell><cell cols="6">9.59 18.55 21.15 27.83 35.19 22.46</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="15">DGC-Net [49] 9.07 GLU-GOCor [70] 1.74 5.88 ---0.51 2.36 2.91 1.29 RANSAC-Flow [63] ----4.41 5.12 3.06 --3.81 5.76 5.92 8.31 RANSAC-DMP 0.53 2.21 2.76 4.62 5.14 1.48 4.67 7.82 9.96 13.68 7.53 79.94 4.24 15.92 27.42 36.77 46.51 26.15 54.84 ------3.05 DMP ? 0.77 3.36 5.22 7.32 9.38 5.21 90.89 2.41 9.88 20.64 28.21 30.15 18.23 81.74</cell></row><row><cell>RANSAC-DMP ?</cell><cell></cell><cell>0.48</cell><cell>2.24</cell><cell>2.41</cell><cell>4.32</cell><cell>5.16</cell><cell cols="3">2.92 97.52 3.57</cell><cell cols="5">8.59 10.18 21.21 23.81 13.47 87.62</cell></row></table><note>Quantitative evaluation on HPatches [3] dataset in terms of AEE and PCK. Lower AEE and higher PCK (5-pixel (%)) are better. Pre-train: Pre-training, Test-opt.: Test-time optimization.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Quantitative evaluation on TSS [68] and PF-PASCAL (PF-PA.) [19] benchmark. Higher PCK is better. FG: FG3DCar, JO: JODS, PA: PASCAL datasets.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research was supported by the MSIT, Korea, under the ICT Creative Consilience program (IITP-2021-2020-0-01819) and Regional Strategic Industry Convergence Security Core Talent Training Business (IITP-2019-0-01343) supervised by the IITP and National Research Foundation of Korea (NRF-2021R1C1C1006897).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this section, we provide more details of DMP and more results on the Hpatches <ref type="bibr" target="#b2">[3]</ref>, ETH3D <ref type="bibr" target="#b59">[61]</ref>, TSS <ref type="bibr" target="#b66">[68]</ref>, and PF-PASCAL <ref type="bibr" target="#b18">[19]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
		<idno>ACM SIGGRAPH. 2005. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Patchmatch stereo-stereo matching with slanted support windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brief: Binary robust independent elementary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2414" to="2422" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contrastive learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep image homography estimation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image processing using multi-code gan prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Contrastive learning for weakly supervised phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09920</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Rafael S Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Space-time correspondence as a contrastive random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the &quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pyramidal affine regression networks for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint learning of semantic alignment and object landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sang Ryul Jeon, Dongbo Min, and Kwanghoon Sohn. Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic attribute matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somi</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dctm: Discrete-continuous transformation matching for semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mast: A memoryaugmented self-supervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">Adrian</forename><surname>Prisacariu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08844</idno>
		<title level="m">Dual-resolution correspondence networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01088</idno>
		<title level="m">Visual attribute transfer through deep image analogy</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="978" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dgc-net: Dense geometric correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iaroslav</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploiting deep generative prior for versatile image restoration and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals ; Xingang Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Representation learning with contrastive predictive coding</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Pedro O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Benmaleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.05499</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<title level="m">On the convergence of adam and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-toend weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient neighbourhood consensus networks via submanifold sparse convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Selfsupervised visual descriptor learning for dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="427" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with highresolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-supervised learning of depth and motion under photometric inconsistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Ransac-flow: generic two-stage image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Darmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01526</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">zero-shot&quot; super-resolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Gocor: Bringing globally optimized correspondence volumes into your neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Glunet: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09157</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">D2d: Learning to find good correspondences for image matching and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08480</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Indomain gan inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
