<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">More layers! End-to-end regression and uncertainty on tabular data with deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bondarenko</surname></persName>
							<email>i.bondarenko@g.nsu.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Novosibirsk State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Novosibirsk Research Center Novosibirsk</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">More layers! End-to-end regression and uncertainty on tabular data with deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper attempts to analyze the effectiveness of deep learning for tabular data processing. It is believed that decision trees and their ensembles is the leading method in this domain, and deep neural networks must be content with computer vision and so on. But the deep neural network is a framework for building gradientbased hierarchical representations, and this key feature should be able to provide the best processing of generic structured (tabular) data, not just image matrices and audio spectrograms. This problem is considered through the prism of the Weather Prediction track in the Yandex Shifts challenge (in other words, the Yandex Shifts Weather task). This task is a variant of the classical tabular data regression problem. It is also connected with another important problem: generalization and uncertainty in machine learning. This paper proposes an end-to-end algorithm for solving the problem of regression with uncertainty on tabular data, which is based on the combination of four ideas: 1) deep ensemble of self-normalizing neural networks, 2) regression as parameter estimation of the Gaussian target error distribution, 3) hierarchical multitask learning, and 4) simple data preprocessing. Three modifications of the proposed algorithm form the top-3 leaderboard of the Yandex Shifts Weather challenge respectively. This paper considers that this success has occurred due to the fundamental properties of the deep learning algorithm, and tries to prove this.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning is a very effective technique for such tasks, as computer vision, automatic speech recognition and natural language understanding. The "classic" ML solutions in these areas were based on pipelines of independent feature extractors, data preprocessors and dimensionality reductors, each of them usually based either on domain-specific heuristics or on locally optimal choice. A linear or "flat" nonlinear estimator, such as SVM, was used at the end of this pipeline. For example, the ILSVRC 2010 winner had the architecture of this type <ref type="bibr" target="#b0">[1]</ref>. Contrary to this approach, any deep learning solution uses a very "pure" feature preprocessing. The key nuance of deep learning is a pipeline of hierarchical feature transformations (as hidden layers) with final classifier/regressor (as output layer), and all of them are trained together to minimize the joint loss function using the gradient descent. Since 2012, all ILSVRC winners are inspired by deep learning <ref type="bibr" target="#b1">[2]</ref>. We can see a similar trend in other ML domains, such as automatic speech recognition or natural language understanding. But not in a tabular data processing. Why? In the recent years some interesting papers about neural networks for tabular data processing were published <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, but gradient boosted decision trees and other techniques based on decision tree ensembles are still very widespread. Some researchers think that speech and vision data has a local ordering (neighbour pixels of a raster matrix or neighbour spectrums of a spectrogram are correlated), but tabular data does not have this property (the column order in a table does not matter). Hence, they postulate that neural networks which can detect such a local order using shared weights (by receptive fields in convolutional networks or by time in recurrent networks) have less effectiveness on any data without a local order compared to decision trees. However, there is reason to believe that it is not true, and the key advantage of deep learning is based on another thing, namely the above-mentioned building of "natural" hierarchy of feature space transformations with end-to-end gradient learning.</p><p>Yet another issue to analyse is uncertainty. The truth is, machine learning is an inductive approach to building an artificial intelligence system. Consequently, training data defines the behaviour of such system on any new data. But the data source can be changed, and the next portion of data will be sampled from a different statistical population relative to training set. So, any AI algorithm has to estimate its own competence. In critical domains, such as medicine or finance, it must give predictions only if it is sure. Also, the algorithm should alert the operator if some of the input data is not typical for the task being solved. Aside from that, solving the uncertainty and generalization problem in machine learning also improves the robustness of the algorithm to adversarial attacks. Therefore, ability to model uncertainty and reject "unknown" entities is a significant feature of any machine learning system in practice. In this context, the Yandex Shifts challenge, devoted to robustness and uncertainty under real-world distributional shift, generates interesting and indicative tasks, including weather prediction that accounts for different climatic zones and time periods. <ref type="bibr" target="#b0">1</ref> So, developing a deep learning algorithm that can either solve a regression task or estimate data uncertainty and self-competence seems to be an actual and non-trivial problem. And depth can help us approximate unknown dependencies between inputs and targets in a more efficient way than ensembles of decision trees and other ML approaches, some nuances notwithstanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task, quality measure and baseline</head><p>Data for the Weather Prediction task is described in details in <ref type="bibr" target="#b5">[6]</ref>. Nevertheless, some important peculiarities should be mentioned.</p><p>Data consist of 123 inputs, one target and some additional meta information (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Inputs include source measures by weather stations and multi-criterion forecasts by three systems: Global Forecast System (GFS), the Canadian Meteorological Center (CMC), and the Weather Research and Forecasting (WRF). Target column contains the temperature that was actually registered, accurate within one degree Celsius. The meta information consists of several columns, including some time data, but time factor is not presented explicitly in the inputs. In that way, the Weather Prediction task is a regression on "typical" tabular data, without any time series. All data samples are grouped by inner and outer partitions. The inner partition includes meteorological data from tropical, dry and mild temperature climate zones, and the outer partition consists of meteorological data collected at more inclement conditions: snow and polar. Also, the outer partition data differs in the time period of collection (the outer partition was collected later). Split into training, development and evaluation sub-sets is implemented with this grouping taken into account.</p><p>? The training set includes the inner partition only, and is named as train.</p><p>? The development set consists of data from both partitions (dev_in and dev_out), and this set can be used for preliminary evaluation of proposed algorithm quality by challenge participants (but not for training directly or validation-in-training).</p><p>? The evaluation set includes the same data packages as the development set (names of the packages are eval_in and eval_out), but it is larger and does not include targets; so, it is used for the final evaluation of the algorithm quality by the challenge organizers.</p><p>A joint assessment of uncertainty and robustness is based on R-AUC MSE score instead of the usual mean square error (MSE). This score is the main performance metric, and it is calculated as the area under error-retention curve (you can see the example on <ref type="figure" target="#fig_1">Fig. 2</ref>). Error-retention curve is built as the MSE of all predictions after their retention by uncertainty value with different uncertainty thresholds (i.e., only predictions with uncertainties not exceeding the specified threshold are retained and are taken into consideration when calculating the MSE). The lower the value of the area under this curve, the better. R-AUC MSE was proposed as a metric for regression with uncertainty in <ref type="bibr" target="#b5">[6]</ref>, but evidently this assessment scenario was inspired by assessing misclassification detection via rejection curves in <ref type="bibr" target="#b6">[7]</ref>. The baseline proposed by the organizers represents an ensemble of 10 gradient boosted decision trees, which are built on the basis of CatBoost algorithmic techniques <ref type="bibr" target="#b7">[8]</ref>. It is known that default hyper-parameters of CatBoost are determined very well, but the authors of the baseline proposed a set of special non-default values for increased effectiveness:</p><p>? tree depth is 8 (default value is 6);</p><p>? 20000 trees in each CatBoost (an extremely large value, compared to the default value of 1000);</p><p>? a special loss function RMSEWithUncertainty, proposed in <ref type="bibr" target="#b8">[9]</ref> instead of the usual MSE or RMSE to consider the uncertainty in training and inference.</p><p>The above-mentioned loss function allows a trainable algorithm to estimate the best normal distribution described by our regression targets in the most likely way (instead of the "classic" average squared difference between targets and point-estimated outputs). It is formulated in the following way:</p><formula xml:id="formula_0">L(?|D) = E D [? log (p(y|x, ?))] = ? 1 N N i=1 log p(y (i) |x (i) , ?) ,<label>(1)</label></formula><p>where:</p><formula xml:id="formula_1">p(y|x, ?) = N (y|?, ?), {?, log(?)} = F (x).<label>(2)</label></formula><p>So, the CatBoost regressor F (x) predicts two values: the mean ? and the logarithm of the standard deviation ? (prediction of the logarithm instead of the immediate value helps to provide only positive standard deviations).</p><p>3 Proposed method and its quality</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>A special deep ensemble (i.e. ensemble of deep neural networks) with uncertainty, hierarchical multitask learning and some other features is proposed. It is built on the basis of the following key techniques:</p><p>1. A simple preprocessing is applied to the input data:</p><p>? imputing: the missing values are replaced in all input columns following a simple constant strategy (fill value is ?1); ? quantization: each input column is discretized into quantile bins, and the number of these bins is detected automatically; after that the bin identifier can be considered as a quantized numerical value of the original feature; ? standardization: each quantized column is standardized by removing the mean and scaling to unit variance; ? decorrelation: all possible linear correlations are removed from the feature vectors discretized by above-mentioned way; the decorrelation is implemented using PCA <ref type="bibr" target="#b9">[10]</ref>.</p><p>2. An even simpler preprocessing is applied to targets: it is based only on removing the mean and scaling to unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A deep neural network is built for regression with uncertainty:</head><p>? self-normalization: this is a self-normalized neural network, also known as SNN <ref type="bibr" target="#b2">[3]</ref>;</p><p>? inductive bias: neural network weights are tuned using a hierarchical multitask learning with the temperature prediction as a high-level regression task and the coarsened temperature class recognition as a low-level classification task; ? uncertainty: a special loss function similar to RMSEWithUncertainty [9] is applied to training according to formulas <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>, but ? = softplus(?) is estimated instead of log(?); ? robustness: a supervised contrastive learning based on N-pairs loss <ref type="bibr" target="#b10">[11]</ref> is applied instead of the crossentropy-based classification as a low-level task in the hierarchical multitask learning; it provides more robustness of the trainable neural network <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>A special technique of deep ensemble creation is implemented: it uses a model average approach like bagging <ref type="bibr" target="#b12">[13]</ref>, but new training and validation sub-sets for corresponding ensemble items are generated using stratification based on coarsened temperature classes.</p><p>The deep ensemble size is 20. Hyper-parameters of each neural network in the ensemble (hidden layer size, number of hidden layers and alpha-dropout as special version of dropout in SNN <ref type="bibr" target="#b2">[3]</ref>) are the same. They are selected using a hybrid approach: first automatically, and then manually. The automatic approach is based on a Bayesian optimization with Gaussian Process regression <ref type="bibr" target="#b13">[14]</ref>, and it discovered the following hyper-parameter values for training the neural network in a single-task mode:</p><p>? hidden layer size is 512;</p><p>? number of hidden layers is 12;</p><p>? alpha-dropout is 0.0003.</p><p>After the implementation of the hierarchical multitask learning, the depth is manually increased up to 18 hidden layers: the low-level task (classification or supervised constrastive learning) is added after the 12th layer, and the high-level task (regression with uncertainty) is added after the 18th layer. General architecture of single neural network is shown on <ref type="figure" target="#fig_2">Fig. 3</ref>. All "dense" components are feed-forward layers with self-normalization. Alpha-dropout is not shown to oversimplify the figure. The weather_snn_1_output layer estimates the mean and the standard deviation of normal distribution, which is implemented using the weather_snn_1_distribution layer. Another output layer named as weather_snn_1_projection calculates low-dimensional projections for the supervised contrastive learning. Rectified ADAM <ref type="bibr" target="#b14">[15]</ref> with Lookahead <ref type="bibr" target="#b15">[16]</ref> is used for training with the following parameters: learning rate is 0.0003, synchronization period is 6, and slow weights step size is 0.5. You can see the more detailed description of other training hyper-parameters in the Jupyter notebook deep_ensemble_with_uncertainty_and_spec_fe.ipynb. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>Experiments were conducted according to data partitioning in section 2. Development and evaluation sets were not used for training and for hyper-parameter search. The quality of each modification of the method was first estimated on the development set, because all participants had access to the development set targets. After the preliminary estimation, the selected modification of the method was submitted to estimate R-AUC MSE on the evaluation set with targets concealed from participants.</p><p>In comparison with the baseline, the quality of the deep learning method is better (i.e. R-AUC MSE is less) on both datasets for testing: Also, the results of the deep learning method are better with all possible values of the uncertainty threshold for retention (see <ref type="figure" target="#fig_4">Fig. 4</ref>). The total number of all submitted methods at the evaluation phase is 73. Six selected results (top-5 results of all participants and the baseline result) are presented in <ref type="table">Table.</ref> 1. The first three places are occupied by the following modifications of the proposed deep learning method:</p><p>? SNN Ens U MT Np SpecFE is the final solution with "all-inclusive";</p><p>? SNN Ens U MT Np v2 excludes the feature quantization;</p><p>? SNN Ens U MT excludes the feature quantization too, and classification is used instead of supervised contrastive learning as the low-level task in the hierarchical multitask learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Why is the neural network method better? Is its advantage randomness (particularly, task specifics), or does it have some fundamental basis? And lastly, what is the rationale for certain techniques in this method? These are very interesting questions, and they need to be discussed, of course.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Why deep learning? Problems of decision trees and their ensembles</head><p>Decision tree, including gradient boosted decision tree, is a very popular method to build classifiers and regressors, if the data is structured, but does not have local ordering. But any gradient boosted decision tree, like any other algorithm with decision trees, has two significant problems:</p><p>1. The decision tree is built by a greedy algorithm.</p><p>2. The decision tree cannot extrapolate effectively.</p><p>The first problem is as follows. Base decision trees <ref type="bibr" target="#b16">[17]</ref> and their boosted ensembles <ref type="bibr" target="#b17">[18]</ref> use a greedy algorithm for recursive partitioning of the source dataset and all sub-sets, generated step-by-step. Any greedy algorithm is less effective in finding the optimal solution than gradient optimization, including gradient-based methods of deep learning.</p><p>At the heart of the second problem is common inefficiency of decision trees and several other "classic" ML algorithms (for example, nearest neighbors) in data extrapolation. In comparison with linear regression, decision tree can model nonlinear dependencies, but decision tree cannot do it for "out-of-distribution" data samples (illustration of this fact is shown on <ref type="figure" target="#fig_5">Fig. 5a and 5b</ref>). As you can see, decision tree has better prediction quality in training and initial testing (blue points) than linear regression. But this powerful nonlinear algorithm loses catastrophically compared to linear regression, if the input data is new, i.e. out-of-distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The necessity of neural networks and more layers</head><p>Applying deep neural network to the tabular data processing is conditional on following reasons.</p><p>Firstly, a neural network is an end-to-end pipeline of hierarchical feature extractors and a final estimator, and all pipeline components (i.e. layers) are trained together to minimize a task-conditioned loss function by gradient-based methods.</p><p>Secondly, a neural network is an effective form of non-linear regression, which has less problems with extrapolation and "out-of-distribution" data samples. Comparison of linear regression, decision tree and neural network with 3 layers is illustrated on the basis of toy regression example (see <ref type="figure" target="#fig_5">Fig. 5</ref>).</p><p>Both decision tree (5b) and neural network (5c) have better prediction quality in training and initial testing than linear regression (5a). But neural network retains this quality on "out-of-distribution" data too.</p><p>However, until recently, fully-connected feed-forward neural networks were less used for tabular data processing in comparison with decision trees, than convolutional neural networks for computer vision and recurrent neural network for sequental data. And neural networks have some problems, first of all:</p><p>? vanising gradient problem;</p><p>? bias-variance tradeoff;</p><p>? uncertainty and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Vanishing gradient problem</head><p>The chain rule used in gradient calculation by the backprop can lead to very small values of gradient and thereby hinder weight updating. This problem is known as vanishing gradient. Normalization of all signals in the neural networks helps to solve this problem. This technique was analyzed in <ref type="bibr" target="#b18">[19]</ref>, and as a result the following techniques were proposed:</p><p>1. All feature vectors should be standardized and decorrelated (removal of linear correlations can be implemented with data projecting to principal components).</p><p>2. If solved task is regression, then all targets should be standardized too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Activation functions of hidden neurons should have a bipolar nonlinearity (for example, hyperbolic tangent or another similar function) instead of unipolar one (logistic sigmoid).</head><p>But the third technique is not effective in a very deep neural network, because sigmoid-like activation function has a small derivative for non-small absolute value of argument, and it causes the vanishing gradient problem again. Applying piecewise linear function, like rectified linear unit <ref type="bibr" target="#b19">[20]</ref>, partially solves the small derivative problem (at least, for positive arguments), however, it generates yet another problem, connected with the systematic shift of hidden signals. There are several techniques of the "forced" normalization that were proposed to solve this problem, such as BatchNorm <ref type="bibr" target="#b20">[21]</ref>, LayerNorm <ref type="bibr" target="#b21">[22]</ref> and WeightNorm <ref type="bibr" target="#b22">[23]</ref>. BatchNorm (batch normalization) is popular in convolutional neural networks (CNNs), and LayerNorm (layer normalization) and WeightNorm (weight normalization) are suitable techniques for recurrent neural networks (RNNs) too. But training with abovementioned techniques of "forced" normalization is perturbed by all kinds of training stochasticity: stochastic gradient descent (SGD), stochastic regularization (like dropout), and the estimation of the normalization parameters. This problem is not critical for CNNs and RNNs, because their weights are shared (by receptive fields or by time). In contrast, fully-connected feed-forward neural networks (FFNNs) trained with "forced" normalization techniques suffer from these perturbations and have high variance in the training error. So, the special construction of self-normalizing neural networks, proposed in <ref type="bibr" target="#b2">[3]</ref>, is more efficient for FFNNs, including the case of tabular data processing.</p><p>Self-normalization is based on three principles:</p><p>1. Scaled exponentional linear unit (SELU) as activation function, which implements the following nonlinearity after summator output s:</p><formula xml:id="formula_2">f (s) = 1.0507 ? s if s &gt; 0 1.6733 ? e s ? 1.6733 if s ? 0</formula><p>2. Special initialization known as Lecun initialization: initial weights of neurons are sampled from the normal distribution N (0, 1 nin ), where n in is the number of the neuron inputs.</p><p>3. New dropout technique named as "alpha dropout", which is based on randomly setting activations to the negative saturation value using the multiplicative noise.</p><p>A detailed mathematical proof of the self-normalization properties in such neural network is presented in the appendix to the paper <ref type="bibr" target="#b2">[3]</ref>. Also, high effectiveness of self-normalizing networks in comparison with residual connections and different forms of the "forced" normalization is achieved on the experimental data from UCI <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Bias-variance tradeoff in deep learning</head><p>The classic bias-variance problem is that increasing the ML algorithm complexity decreases the bias and increases the variance (this ML algorithm starts to approach overfitting) <ref type="bibr" target="#b24">[25]</ref>. Therefore, any researcher who creates a deep learning method has to find a compromise solution, i.e. the happy medium between underfitting and overfitting, in some way. And the depth was usually reached at the expense of fully connected layers disavowal to limit the neural network's complexity. For example, it was implemented on the basis of restriction of inter-layer connections using receptive fields and shared weights in the convolutional neural networks.</p><p>In recent years, research of the bias-variance tradeoff concerning deep learning has received a new development. According to <ref type="bibr" target="#b25">[26]</ref>, increasing the deep learning algorithm complexity (in terms of the hidden layer size) decreases both the bias and the variance. At that, there are two kinds of variance:</p><p>? variance due to optimization: different initial weights lead to different training results;</p><p>? variance due to sampling: different data subsets lead to different training results.</p><p>Joint variance is the combination of these two kinds of variance. And this joint variance has a unimodal dependence on the neural network complexity formulated as the hidden layer size. And interestingly, the variance due to sampling demonstrates a "traditional" behaviour, i.e. it slowly increases and eventually plateaus with complexity increasing, but the variance due to optimization at first increases and then decreases significantly. As a result, this determines the unimodality of the joint variance. Consequently, two conclusions follow on basis of the bias-variance tradeoff in deep learning:</p><p>1. Deep learning is a good thing for any task, and increasing depth helps to partially solve the overfitting problem.</p><p>2. Depth is not a silver bullet. It determines a lot, but not everything for overcoming the bias-variance tradeoff. Another part of solution can consist of more traditional techniques, such as regularization and ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hierarchical multitask learning: why multitask and why hierarchical?</head><p>Regularization is a well-known technique to increase the robustness of the trainable algorithm. Penalty is the simplest kind of regularization, and it is very efficient for linear models (for example, see <ref type="bibr" target="#b26">[27]</ref>). But deep neural network builds a hierarchical feature representation due its architecture and training, and regularization in deep learning should help to find a "good" hierarchy. Penalty is ineffective for solving such a problem.</p><p>There is yet another approach that can help to regularize the deep neural network. It is multi-task learning (MTL), and it is one of the most efficient techniques of "forcing" the network to search for an appropriate hierarchical system of the feature transformation. Multi-task learning introduces an inductive bias, based on a-priori relations between tasks: the trainable model is compelled to model more general dependencies by using the abovementioned relation as an important data feature <ref type="bibr" target="#b27">[28]</ref>.</p><p>Hierarchical MTL, in which different tasks use different levels of the deep neural network, provides more effective inductive bias compared to "flat" MTL. Also, hierarchical MTL helps to solve vanishing gradient problem in addition to the self-normalization, considered in subsection 4.3. The idea of hierarchical MTL was first presented in <ref type="bibr" target="#b28">[29]</ref> as applied to natural language processing. Indeed, syntax and morphology are different levels of the natural language system, and syntax is based on morphology. Thus, such "natural" hierarchy can be projected onto a deep neural network trainable for natural language understanding.</p><p>Similarly to computational linguistics, the Weather Prediction task of the Yandex Shifts challenge can be decomposed into a hierarchical system of subtasks. Temperature prediction in the form of regression is a high-level fine-grained subtask with respect to the recognition of coarse temperature classes. Therefore, these two subtasks also form a "natural" hierarchy, which can be modeled using hierarchical MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Neural network: uncertainty and robustness</head><p>Uncertainty modeling is one of the problems of neural networks. In the classification scenario, the neural network-based classifier is too "optimistic", and it always returns a more "contrastive" probability distribution of classes. In the regression scenario, the regression model yields point predictions, and its training criteria (such as MSE or MAE) do not capture predictive uncertainty. Regression is a more interesting task in the context of the Weather Prediction on the Yandex Shifts challenge, and the uncertainty in the classification will not be analyzed further.</p><p>There are two key approaches to account and predict uncertainty with neural networks:</p><p>1. Bayesian approach, which models epistemic uncertainty using a deep ensemble or its approximation, such as a Bayesian neural network. As is well known, this network is an approximation of an infinite ensemble <ref type="bibr" target="#b29">[30]</ref>. The epistemic uncertainty is "model-driven", and it is related to the model behaviour (particularly, with its generalization ability).</p><p>2. non-Bayesian approach, which models aleatoric uncertainty by means of direct estimation of normal distribution parameters. The aleatoric uncertainty is "data-driven", i.e. it is caused by incorrigible errors in measuring the target value. But these errors are distributed normally, and it is possible to reconstruct parameters of this distribution with maximum likelihood <ref type="bibr" target="#b30">[31]</ref>.</p><p>In practice, joint uncertainty includes both types of uncertainty (epistemic and aleatoric). So, accounting for their combination using the ensemble of deep regressors with the special loss function similar to described in the formula (1) makes sense. The deep ensemble is a form of the Bayesian approach, and the loss function as a negative log-likelihood of parameters estimation of the Gaussian target error distribution embodies the non-Bayesian approach. An analogous technique is proposed in <ref type="bibr" target="#b31">[32]</ref>, but the authors contrapose the non-Bayesian approach to the Bayesian one, interpreting the Bayesian approach as a Bayesian neural network only. However, the ensemble of usual (non-Bayesian) neural networks is related to the Bayesian neural network: this is a technique of deep ensemble approximation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>There is every reason to believe that deep learning is the most efficient framework for the tabular data processing, and "classical" ML will become inferior to DL-based approaches in this domain, as it happened earlier in computer vision, for example. More layers provide more quality. Some problems connected with depth are solved using self-normalization and hierarchical multitask learning. Uncertainty can be modeled with combination of Bayesian and non-Bayesian techniques. Several variants of the algorithm built on the basis of these affirmations became winners of the Weather Prediction track of the Yandex Shifts challenge. This result could be considered as the confirmation of the effectiveness of deep learning, but so far this conclusion seems too optimistic. Really, this result inspires confidence in the correctness of the "deep way" and "more layers". But for the final confirmation, it is necessary to conduct a number of comparative experiments with various datasets devoted to classification and regression, including datasets with distributional shift for more robustness and more effective generalization evaluation. Besides, this paper does not consider the explaination problem of the deep learning algorithm, and an algorithm explainability can be very important in the real world cases. Therefore, these aspects should be used as directions for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Fragment of tabular data for the Weather Prediction task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example MSE retention curves on evaluation dataset, reached by the baseline solution, proposed by organizers (for single CatBoost and ensemble of CatBoosts).<ref type="bibr" target="#b5">[6]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the deep neural network with hierarchical multitask learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>the development set (for preliminary testing): 1. proposed deep ensemble = 1.015; 2. baseline (CatBoost ensemble) = 1.227; ? the evaluation set (for final testing): 1. proposed deep ensemble = 1.141; 2. baseline (CatBoost ensemble) = 1.335.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Error retention curves on the development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>An example on synthetic data. Blue points are data for training and initial testing. Orange points are new data to predict. Shown: (a) predictions of linear regression; (b) predictions of decision tree; (c) predictions of neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Selected final results on the evaluation set. The author's team is bond005, and the baseline is developed by the Shifts Team</figDesc><table><row><cell cols="2">Rank Team</cell><cell>Method</cell><cell>R-AUC MSE</cell></row><row><cell>1</cell><cell>bond005</cell><cell cols="2">SNN Ens U MT Np SpecFE 1.1406288012</cell></row><row><cell>2</cell><cell>bond005</cell><cell>SNN Ens U MT Np v2</cell><cell>1.1415403291</cell></row><row><cell>3</cell><cell>bond005</cell><cell>SNN Ens U MT</cell><cell>1.1533415892</cell></row><row><cell>4</cell><cell cols="2">CabbeanWeather Steel box v2</cell><cell>1.1575201873</cell></row><row><cell>5</cell><cell>KDDI Research</cell><cell>more seed ens</cell><cell>1.1593224114</cell></row><row><cell>55</cell><cell>Shifts Team</cell><cell>Shifts Challenge</cell><cell>1.3353865316</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://research.yandex.com/shifts/weather</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This notebook is available at https://github.com/bond005/yandex-shifts-weather</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>The author thanks Dr. Evgenii Vityaev for inspirational discussions about relations between deep learning, probability theory and formal logic.</p><p>Especially the author would like to thank his wife Viktoria Kondrashuk for her love, patience and constant support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Videos semantic indexing using image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nec-uiuc.pdf" />
	</analytic>
	<monogr>
		<title level="m">National Institute of Standards and Technology (NIST)</title>
		<editor>P. Over, G. Awad, J. G. Fiscus, B. Antonishek, M. Michel, W. Kraaij, A. F. Smeaton, and G. Qu?not</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>TRECVID 2010 workshop participants notebook papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A novel method for classification of tabular data using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buturovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miljkovi?</surname></persName>
		</author>
		<ptr target="https://www.biorxiv.org/content/early/2020/05/03/2020.05.02.074203" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tabnet: Attentive interpretable tabular learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6679" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shifts: A dataset of real distributional shift across multiple large-scale tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Band</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chesnokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Noskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ploskonosov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shmatova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yangel</surname></persName>
		</author>
		<idno>abs/2107.07455</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Uncertainty estimation in deep learning with application to spoken language assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<ptr target="http://mi.eng.cam.ac.uk/~mjfg/thesis_am969.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Catboost: unbiased boosting with categorical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32st International Conference on Neural Information Processing Systems NIPS&apos;18</title>
		<meeting>the 32st International Conference on Neural Information Processing Systems NIPS&apos;18</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6638" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertainty in gradient boosting via ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ustimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Learning Representations ICLR</title>
		<meeting>the 9th International Conference on Learning Representations ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Westad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Houmoller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multivariate Data Analysis: In Practice</title>
		<meeting><address><addrLine>Norway</addrLine></address></meeting>
		<imprint>
			<publisher>CAMO</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="19" to="74" />
		</imprint>
	</monogr>
	<note>5th ed. Oslo</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1849" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems NeurIPS&apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems NeurIPS&apos;20</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="661" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tutorial on bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<idno>abs/1807.02811</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1908.03265</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems NeurIPS&apos;19</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems NeurIPS&apos;19</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9593" to="9604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, Tricks of the Trade, ser. Lecture Notes in Computer Science LNCS 1524</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning ICML&apos;15</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning ICML&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems NIPS&apos;16</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural networks and the bias/variance dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A modern take on the bias-variance tradeoff in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tantia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scicluna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2019 Workshop on Identifying and Understanding Deep Learning Phenomena</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Example: Polynomial curve fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?gaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning ICML&apos;15</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning ICML&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimating the mean and variance of the target probability distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN&apos;94)</title>
		<meeting>1994 IEEE International Conference on Neural Networks (ICNN&apos;94)<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6405" to="6416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian deep learning and a probabilistic perspective of generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems NeurIPS&apos;20</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<meeting>the 34th International Conference on Neural Information Processing Systems NeurIPS&apos;20</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4697" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
