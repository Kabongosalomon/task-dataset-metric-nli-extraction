<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingtong</forename><surname>Kong</surname></persName>
							<email>ltkong@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
							<email>byronjiang@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chu</surname></persName>
							<email>wenqingchu@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Huang</surname></persName>
							<email>skyhuang@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<email>yingtai@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<email>jieyang@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>China</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Youtu</forename><surname>Lab</surname></persName>
						</author>
						<title level="a" type="main">IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prevailing video frame interpolation algorithms, that generate the intermediate frames from consecutive inputs, typically rely on complex model architectures with heavy parameters or large delay, hindering them from diverse real-time applications. In this work, we devise an efficient encoder-decoder based network, termed IFRNet, for fast intermediate frame synthesizing. It first extracts pyramid features from given inputs, and then refines the bilateral intermediate flow fields together with a powerful intermediate feature until generating the desired output. The gradually refined intermediate feature can not only facilitate intermediate flow estimation, but also compensate for contextual details, making IFRNet do not need additional synthesis or refinement module. To fully release its potential, we further propose a novel task-oriented optical flow distillation loss to focus on learning the useful teacher knowledge towards frame synthesizing. Meanwhile, a new geometry consistency regularization term is imposed on the gradually refined intermediate features to keep better structure layout. Experiments on various benchmarks demonstrate the excellent performance and fast inference speed of proposed approaches. Code is available at https: //github.com/ltkong218/IFRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warp</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video frame interpolation (VFI), that converts low frame rate (LFR) image sequences to high frame rate (HFR) videos is an important low-level computer vision task. Related techniques are widely applied to various practical applications, such as slow-motion generation <ref type="bibr" target="#b32">[22]</ref>, novel view synthesis <ref type="bibr" target="#b68">[58]</ref> and cartoon creation <ref type="bibr" target="#b54">[44]</ref>. Although it has been studied by a large number of researches, there are still * Equal contribution. This work was done when Lingtong Kong was an intern at Tencent Youtu Lab.</p><p>? Corresponding author: Jie Yang (jieyang@sjtu.edu.cn). This research is partly supported by NSFC, China (No: 61876107, U1803261). great challenges when dealing with complicated dynamic scenes, which include large displacement, severe occlusion, motion blur and abrupt brightness change. Recently, with the development of optical flow networks <ref type="bibr" target="#b23">[13,</ref><ref type="bibr" target="#b35">25,</ref><ref type="bibr" target="#b58">48,</ref><ref type="bibr" target="#b59">49]</ref>, significant progress has been made by flow-based VFI approaches <ref type="bibr" target="#b32">[22,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b49">39,</ref><ref type="bibr" target="#b62">52]</ref>, since optical flow can provide an explicit correspondence to register frames in a video sequence. Successful flow-based approaches usually follow a three-step pipeline: 1) Estimate optical flow between target frame and input frames.</p><p>2) Warp input frames or context features by predicted flow fields for spatial alignment. 3) Refine warped frames or features and generate the target frame by a synthesis network. Denoting input frames and target frame to be I 0 , I 1 and I t (0 &lt; t &lt; 1), existing methods either first estimate optical flow F 0?1 , F 1?0 <ref type="bibr">[3,</ref><ref type="bibr" target="#b32">22,</ref><ref type="bibr" target="#b44">34,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b48">38]</ref>, and then approximate or refine bilateral intermediate flow F t?0 , F t?1 <ref type="bibr">[9,</ref><ref type="bibr" target="#b32">22,</ref><ref type="bibr" target="#b52">42,</ref><ref type="bibr" target="#b62">52]</ref> as shown in <ref type="figure">Figure 2</ref> (a), or throw the intractable intermediate flow estimation sub-task to a learnable flow network for end-to-end training <ref type="bibr" target="#b30">[20,</ref><ref type="bibr" target="#b63">53,</ref><ref type="bibr" target="#b67">57]</ref> as depicted in <ref type="figure">Figure 2</ref> (b). Their common step is to further employ an image synthesis network to encode spatial aligned context feature <ref type="bibr" target="#b44">[34]</ref> for target frame generation or refinement.  <ref type="figure">Figure 2</ref>. Different flow-based VFI paradigms. We roughly classify existing flow-based VFI methods based on encoder-decoders with specific function. In (a) <ref type="bibr">[3,</ref><ref type="bibr" target="#b32">22,</ref><ref type="bibr" target="#b44">34,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b48">38,</ref><ref type="bibr" target="#b49">39,</ref><ref type="bibr" target="#b52">42,</ref><ref type="bibr" target="#b62">52]</ref>, FlowNet estimates conventional optical flow F0?1, F1?0, the middle part approximates or further refines flow fields Ft?0, Ft?1. In (b) <ref type="bibr" target="#b30">[20,</ref><ref type="bibr" target="#b63">53,</ref><ref type="bibr" target="#b67">57]</ref>, the Intermediate FlowNet directly predicts intermediate flow of Ft?0, Ft?1. Both (a) and (b) contain a separate synthesis network for target frame generation. In (c), proposed IFRNet jointly refines the intermediate flow Ft?0, Ft?1 together with a powerful intermediate feature?t to generate the target frame in a single encoder-decoder.</p><p>Although above pipeline that first estimates intermediate flow and then context feature has become the most popular paradigm for flow-based VFI approaches <ref type="bibr">[9,</ref><ref type="bibr" target="#b44">34,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b49">39,</ref><ref type="bibr" target="#b52">42]</ref>, it suffers from several defects. First, they divide intermediate flow and context feature refinement into separate encoderdecoders, which ignores the mutual promotion of these two crucial elements for frame interpolation. Second, their cascaded architecture based on above design concept can substantially increase the inference delay and model parameters, blocking them from mobile and real-time applications.</p><p>In this paper, we propose a novel Intermediate Feature Refine Network (IFRNet) for VFI to overcome the above limitations. For the first time, we merge above separated flow estimation and feature refinement into a single encoder-decoder based model for compactness and fast inference, abstracted in <ref type="figure">Figure 2</ref> (c). It first extracts pyramid features from given inputs by the encoder, and then jointly refines the bilateral intermediate flow fields together with a powerful intermediate feature through coarse-to-fine decoders. The improved architecture can benefit intermediate flow and intermediate feature with each other, endowing our model with the ability to not only generate sharper moving objects but also capture better texture details.</p><p>For better supervision, we propose task-oriented flow distillation loss and feature space geometry consistency loss to effectively guide the multi-scale motion estimation and intermediate feature refinement. Specifically, our flow distillation approach adjusts the robustness of distillation loss adaptively in space and focuses on learning the useful teacher knowledge for frame synthesizing. Besides, proposed geometry consistency loss can employ the extracted intermediate features from ground truth to constrain the reconstructed intermediate features for keeping better structure layout. <ref type="figure" target="#fig_0">Figure 1</ref> gives a speed, accuracy and parameters comparison among advanced VFI methods, demonstrating the state-of-the-art performance of our approaches. In summary, our main contributions are listed as follows:</p><p>? We devise a novel IFRNet to jointly perform intermediate flow estimation and intermediate feature refinement for efficient video frame interpolation.</p><p>? Task-oriented flow distillation loss and feature space geometry consistency loss are newly proposed to promote intermediate motion estimation and intermediate feature reconstruction of IFRNet, respectively.</p><p>? Benchmark results demonstrate that our IFRNet not only achieves state-of-the-art VFI accuracy, but also enjoys fast inference speed and lightweight model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Frame Interpolation. The mainstream VFI methods can be classified into flow-based <ref type="bibr">[3,</ref><ref type="bibr" target="#b32">22,</ref><ref type="bibr" target="#b40">30,</ref><ref type="bibr" target="#b44">34,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b48">38,</ref><ref type="bibr" target="#b49">39,</ref><ref type="bibr" target="#b52">42,</ref><ref type="bibr" target="#b62">[52]</ref><ref type="bibr" target="#b63">[53]</ref><ref type="bibr" target="#b64">[54]</ref><ref type="bibr" target="#b67">57]</ref>, kernel-based <ref type="bibr">[7,</ref><ref type="bibr">8,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b36">26,</ref><ref type="bibr" target="#b46">36,</ref><ref type="bibr" target="#b47">37,</ref><ref type="bibr" target="#b50">40]</ref> and hallucination-based approaches <ref type="bibr">[10,</ref><ref type="bibr" target="#b26">16,</ref><ref type="bibr" target="#b34">24]</ref>. Different VFI paradigms have their own merits and flaws due to the substantial frame synthesizing manner. For example, kernel-based methods are good at handling motion blur by convolving over local patches <ref type="bibr" target="#b46">[36,</ref><ref type="bibr" target="#b47">37]</ref>, successive works mainly extend it to deal with high resolution videos <ref type="bibr" target="#b50">[40]</ref>, increase the degrees of freedom for convolution kernel <ref type="bibr">[7,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b36">26]</ref>, or combine them with other paradigms for compensation <ref type="bibr">[4,</ref><ref type="bibr">12]</ref>. However, they are typically computationally expensive and short of dealing with occlusion. In another way, hallucination-based methods directly synthesize frames from the feature domain by blending fieldof-view features generated by deformable convolution <ref type="bibr">[11]</ref> or PixelShuffle operations <ref type="bibr">[10]</ref>. They can naturally generate complex contextual details, while the predicted frames tend to be blurry when fast-moving objects exist. Recently, significant progress has been made by flowbased VFI approaches, since optical flow can provide an explicit correspondence for frame registration. These solutions either employ an off-the-shelf flow model <ref type="bibr" target="#b44">[34,</ref><ref type="bibr" target="#b62">52]</ref> or estimate task-specific flow <ref type="bibr" target="#b32">[22,</ref><ref type="bibr" target="#b40">30,</ref><ref type="bibr" target="#b49">39,</ref><ref type="bibr" target="#b52">42,</ref><ref type="bibr" target="#b63">53]</ref> as a guidance for pixel-level motion. Common subsequent step is to forward <ref type="bibr" target="#b24">[14]</ref> or backward <ref type="bibr" target="#b61">[51]</ref> warp input images to target frame, and finally refine warped frames by an image synthesis network <ref type="bibr">[12,</ref><ref type="bibr" target="#b44">34,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b49">39]</ref>, often instantiated as a Grid-Net <ref type="bibr" target="#b25">[15]</ref>. For achieving better image interpolation quality, more complicated deep models are devised to estimate intermediate flow fields <ref type="bibr">[9,</ref><ref type="bibr" target="#b62">52]</ref> and refine the generated target frame <ref type="bibr" target="#b32">[22,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b48">38,</ref><ref type="bibr" target="#b49">39]</ref>. However, the heavy computation cost and large inference delay make them unsuitable for resource limited devices. To take a breath from above module cascading competition, and reconsider the improvement of prior efficient flow-based VFI paradigm, e.g. DVF <ref type="bibr" target="#b40">[30]</ref>, we propose a novel single encoder-decoder based IFRNet, that can perform real-time inference with excellent accuracy. Optical Flow Estimation. Finding dense correspondence between adjacent frames, namely optical flow estimation <ref type="bibr" target="#b29">[19]</ref>, has been studied for decades for its fundamental role in many downstream video processing tasks <ref type="bibr">[5,</ref><ref type="bibr" target="#b65">55]</ref>. FlowNet <ref type="bibr" target="#b23">[13]</ref> is the first attempt to apply deep learning for optical flow estimation based on the encoderdecoder U-shape network. Inspired by traditional coarseto-fine paradigm, SPyNet <ref type="bibr" target="#b51">[41]</ref>, PWC-Net <ref type="bibr" target="#b58">[48]</ref> and Fast-FlowNet <ref type="bibr" target="#b35">[25]</ref> integrate pyramid feature, backward warping and achieve impressive real-time performance. Knowledge distillation <ref type="bibr" target="#b28">[18]</ref> also plays an important role in optical flow prediction, usually embodied as generating pseudo label in unsupervised optical flow learning <ref type="bibr" target="#b37">[27,</ref><ref type="bibr" target="#b38">28]</ref> or related tasks <ref type="bibr" target="#b11">[1,</ref><ref type="bibr" target="#b53">43]</ref>. A recent VFI method <ref type="bibr" target="#b30">[20]</ref> also uses a distillation strategy to promote motion prediction. Beyond the difference of architecture design, our distillation approach can focus on the useful knowledge for intermediate frame synthesizing in a task adaptative manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we first introduce the IFRNet architecture built on the principle of joint refinement of intermediate flow and intermediate feature, to obtain an efficient encoder-decoder based framework for VFI. Then two novel objective functions, i.e., task-oriented flow distillation loss and feature space geometry consistency loss are introduced to help our model achieve excellent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">IFRNet</head><p>Given two input frames I 0 and I 1 at adjacent time instances, video frame interpolation aims to synthesize an intermediate frame I t , where 0 &lt; t &lt; 1. To achieve this goal, proposed model performs a first extraction phase so as to retrieve a pyramid of features from each frame, then in a coarse-to-fine manner it progressively refines bilateral intermediate flow fields together with reconstructed intermediate feature until reaching the highest level of the pyramid to obtain the final output. <ref type="figure" target="#fig_2">Figure 3</ref> sketches the overall architecture of proposed IFRNet. Pyramid Encoder. To obtain contextual representation from each input frame, we design a compact encoder E to extract a pyramid of features. Purposely, the parameter shared encoder is built of a block of two 3?3 convolutions in each pyramid level, respectively with strides 2 and 1. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, IFRNet extracts 4 levels of pyramid features, counting 8 convolution layers, each followed by a PReLU activation <ref type="bibr" target="#b27">[17]</ref>. By gradually decimating the spatial size, it increases the feature channels to 32, 48, 72 and 96, generating pyramid features ? k 0 , ? k 1 in level k (k ? {1, 2, 3, 4}) for frames I 0 and I 1 , respectively. Coarse-to-Fine Decoders. After extracting meaningful hierarchical representations, we then gradually refine intermediate flow fields through multiple decoders by backward warping pyramid features ? k 0 , ? k 1 to generate? k 0 ,? k 1 according to F k t?0 and F k t?1 , respectively. The main advantage of coarse-to-fine warping strategy consists of computing easier residual flow at each scale. Different from previous VFI approaches containing post-refinement <ref type="bibr">[12,</ref><ref type="bibr" target="#b30">20,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b49">39]</ref>, we explore to improve the bilateral flow prediction during its coarse-to-fine procedure for higher efficiency. Specifically, we make each decoder D k+1 output a higher level reconstructed intermediate feature? k t besides bilateral flow fields F k t?0 , F k t?1 , which can fill up the missing reference information to facilitate motion estimation. On the other hand, better predicted flow fields F k t?0 , F k t?1 will align source pyramid features to the target position more precisely, thus, generating better? k 0 ,? k 1 , which can in turn improve higher level intermediate feature reconstruction. Therefore, decoders in proposed IFRNet can jointly refine bilateral intermediate flow fields together with reconstructed intermediate feature, benefitting each other until reaching desired output. Moreover, the gradually refined intermediate feature, containing bilateral occlusion and global context information, can finally generate fusion mask and compensate for motion details, that are often missing by flow-based methods, enabling IFRNet a powerful encoder-decoder VFI architecture without additional refinement <ref type="bibr" target="#b45">[35,</ref><ref type="bibr" target="#b49">39]</ref>.</p><p>Concretely, in each pyramid level, we stack corresponding input features into a holistic volume that is forwarded by a compact decoder network D k , consisting of a block of six 3?3 convolutions and one 4?4 deconvolution, with strides 1 and 1/2, respectively. A PReLU <ref type="bibr" target="#b27">[17]</ref> follows each convolution layer. Details of each decoder is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In order to keep relative large receptive field and channel numbers for motion estimation and feature encoding while maintaining efficiency, we modify the third and the fifth convolution to update only partial channels of previous output tensor. Furthermore, residual connection and interlaced placement can promote information propagation and joint refinement. More details are shown in supplementary. Note that inputs of D 4 and outputs of D 1 are different from other decoders due to the task-related characteristics. In summary, features among decoders can be computed by</p><formula xml:id="formula_0">[F 3 t?0 , F 3 t?1 ,? 3 t ] = D 4 ([? 4 0 , ? 4 1 , T ]),<label>(1)</label></formula><formula xml:id="formula_1">[F k?1 t?0 , F k?1 t?1 ,? k?1 t ] = D k ([F k t?0 , F k t?1 ,? k t ,? k 0 ,? k 1 ]),<label>(2)</label></formula><formula xml:id="formula_2">[F t?0 , F t?1 , M, R] = D 1 ([F 1 t?0 , F 1 t?1 ,? 1 t ,? 1 0 ,? 1 1 ]), (3)</formula><p>where D k (k = 2, 3) stand for decoders at middle pyramid levels, [?] denotes concatenation operation. T is a onechannel conditional input for arbitrary time interpolation, whose values are all the same and set to t. M is a onechannel merge mask exported by a sigmoid layer whose elements range from 0 to 1, and R is a three-channel image residual that can compensate for details. Finally, we can synthesize the desired frame? t by following formulation</p><formula xml:id="formula_3">I t = M ? 0 + (1 ? M ) ? 1 + R,<label>(4)</label></formula><formula xml:id="formula_4">I 0 = w(I 0 , F t?0 ),? 1 = w(I 1 , F t?1 ),<label>(5)</label></formula><p>where w means backward warping, is element-wise multiplication. M adjusts the mixing ratio according to bidirectional occlusion information, while R compensates for some details when flow-based generation is unreliable, such as regions of target frame are occluded in both views. Discussion with Optical Flow Networks. Different from the coarse-to-fine pipeline in real-time optical flow <ref type="bibr" target="#b35">[25,</ref><ref type="bibr" target="#b58">48]</ref> which mainly deals with large displacement matching challenge, in video interpolation, since the target frame is missing, its motion estimation becomes a "chicken-and-egg" problem. Therefore, decoders of IFRNet reconstruct intermediate feature besides intermediate flow fields, performing spatio-temporal feature aggregation and intermediate motion refinement jointly to benefit from each other. Image Reconstruction Loss. According to above analysis, an efficient IFRNet has been designed for VFI, which is end-to-end trainable. For the purpose of generating intermediate frame, we employ the same image reconstruction loss L r as <ref type="bibr" target="#b49">[39]</ref> between network output? t and ground truth frame I gt t , which is the sum of two terms and denoted by</p><formula xml:id="formula_5">L r = ?(? t ? I gt t ) + L cen (? t , I gt t ),<label>(6)</label></formula><p>where ?(x) = (x 2 + 2 ) ? with ? = 0.5, = 10 ?3 is the Charbonnier loss <ref type="bibr">[6]</ref> severing as a surrogate for the L 1 loss. While L cen is the census loss, which calculates the soft Hamming distance between census-transformed <ref type="bibr" target="#b42">[32]</ref> image patches of size 7?7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task-Oriented Flow Distillation Loss</head><p>Training IFRNet with above reconstruction loss L r can already perform intermediate frame synthesizing. However, the simple optimization target usually drops into local minimum, since illuminance cases are often challenging, i.e., extreme brightness and repetitive texture regions. To deal with this problem, we try to adopt the knowledge distillation <ref type="bibr" target="#b28">[18]</ref> strategy to guide multi-scale intermediate flow estimation of IFRNet by an off-the-shelf teacher flow network, that helps to align multi-scale pyramid features explicitly. In practice, the pre-trained teacher is only used during training, and we calculate its flow prediction as pseudo label F p t?0 , F p t?1 in advance for efficiency. Note that RIFE <ref type="bibr" target="#b30">[20]</ref> also uses flow distillation. However, their indiscriminate distillation manner usually learns undesired noise existed in pseudo label. Even if ground truth is available, optical flow itself is often a sub-optimal representation for specific video task <ref type="bibr" target="#b63">[53]</ref>. To overcome above limitations, we propose task-oriented flow distillation loss that can decrease the adverse impacts while focusing on the useful knowledge for better VFI.</p><p>Observing that F t?0 , F t?1 which directly control frame synthesis are sensitive to harmful information in pseudo label. Therefore, we impose multi-scale flow distillation except for the decoder D 1 , and leave its flow prediction totally constrained by the reconstruction loss L r in a task-oriented manner <ref type="bibr" target="#b63">[53]</ref>. Furthermore, we can compare above relaxed flow prediction F t?0 , F t?1 with pseudo label F p t?0 , F p t?1 to calculate robustness masks P 0 , P 1 , and use them to adjust the robustness of distillation loss spatially in lower multiple scales for better task-oriented flow distillation, whose procedure is depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. Specifically, we can obtain P l (l ? {0, 1}) by the following formulation</p><formula xml:id="formula_6">P l = exp(??|F t?l ? F p t?l | epe ),<label>(7)</label></formula><p>where | ? | epe calculates per-pixel end-point-error, the coefficient ? controlling sensibility for robustness is set to 0.3 according to grid search. Foundation of above operations is based on the assumption that task-oriented flow generally agrees with true optical flow but differs in some details. Following previous experience <ref type="bibr" target="#b31">[21,</ref><ref type="bibr" target="#b57">47]</ref>, our taskoriented flow distillation employs the generalized Charbonnier loss ?(x) = (x 2 + 2 ) ? for better robust learning of intermediate flow, where parameters and ? control the robustness of this loss. Formally, it can be written as</p><formula xml:id="formula_7">L d = 3 k=1 1 l=0 ?(U 2 k (F k t?l ) ? F p t?l ),<label>(8)</label></formula><p>where U s is the bilinear upsampling operation with scale factor s. However, different from the fixed format like previous methods <ref type="bibr" target="#b31">[21,</ref><ref type="bibr" target="#b57">47]</ref>, we make it adjustable about VFI task by letting and ? be functions of the robustness parameter p, where p ? (0, 1] means the robustness value of any position in aforementioned robustness masks P 0 , P 1 . In general, we employ the linear and exponential linear functions to generate ? and separately as follows</p><formula xml:id="formula_8">? = p/2, = 10 ?(10p?1)/3 .<label>(9)</label></formula><p>The coefficients are selected based on two typical cases. For example, when p = 1.0, ?(x) becomes the surrogate L 1 loss in Eq. 6. And when p = 0.4, it turns to be the robust loss used in LiteFlowNet <ref type="bibr" target="#b31">[21]</ref>. <ref type="figure">Figure 5</ref> gives some intuitive examples of this adaptive robust loss. Comprehensively speaking, in each spatial location, if the task-oriented flow prediction of decoder D 1 is consistent with that in pseudo label, the gradient of the adaptive distillation loss is relatively steep, which tends to distill this helpful information to the bottom three decoders by common gradient descent optimizer. On the other hand, the loss will become more robust to downgrade this relatively harmful flow knowledge. (x) = (x 2 + 2 ) , = p/2, = 10 (10p 1)/3 p = 1.0 p = 0.9 p = 0.8 p = 0.7 p = 0.6 p = 0.5 p = 0.4 p = 0.3 p = 0.2 <ref type="figure">Figure 5</ref>. Task-oriented flow distillation loss. It takes the format of generalized Charbonnier loss, while the concrete form in each location is controlled by the corresponding robustness parameter p, which is determined by Eq. 7 to acquire task adaptive ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Space Geometry Consistency Loss</head><p>Besides above task-oriented flow distillation loss for facilitating multi-scale intermediate flow estimation, better supervision of intermediate feature is preferred for further improvement. Observing that extracted pyramid features ? k 0 , ? k 1 by the encoder E, in a sense, play an equivalent role as the reconstructed intermediate feature? k t from the decoder D k+1 , we try to employ the same parameter shared encoder E to extract a pyramid of features ? k t from ground truth frame I gt t , and use ? k t to regularize the reconstructed intermediate feature? k t in multi-scale feature domain. Intuitively, we can adopt the commonly used L 1 loss to restrict? k t to be close to ? k t . However, the overtighten constraint will harm the global context and occlusion information contained in reconstructed intermediate feature? k t . To relax it and inspired by the local geometry alignment property of census transform <ref type="bibr" target="#b66">[56]</ref>, we extend the census loss L cen <ref type="bibr" target="#b42">[32]</ref> into multi-scale feature space for progressive supervision, where the soft Hamming distance is calculated between census-transformed corresponding feature maps with 3?3 patches in a channel-by-channel manner. Formally, this loss can be written as</p><formula xml:id="formula_9">L g = 3 k=1 L cen (? k t , ? k t ).<label>(10)</label></formula><p>Our motivation is that the extracted pyramid feature, containing useful low-level structure information for frame synthesizing, can regularize the reconstructed intermediate feature to keep better geometry layout. For each spatial location, L g only constrain the geometry of its neighbor local patch in every feature map. Consequently, there is no restriction on the channel-wise representation for? k t to encode bilateral occlusion and residual information.</p><p>Based on above analysis, our final loss function, containing three parts for joint optimization, is formulated as</p><formula xml:id="formula_10">L = L r + ?L d + ?L g ,<label>(11)</label></formula><p>where weighting parameters are set to ? = 0.01, ? = 0.01. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce implementation details and datasets used in this paper. Then, we quantitatively and qualitatively compare IFRNet with recent state-of-the-arts on various benchmarks. Finally, ablation studies are carried out to analyze the contribution of proposed approaches. Experiments in the main paper follow a common practice of t = 0.5, that is synthesizing the single middle frame. IFR-Net also supports multi-frame interpolation with temporal encoding T , whose results are presented in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement proposed algorithm in PyTorch, and use Vimeo90K <ref type="bibr" target="#b63">[53]</ref> training set to train IFRNet from scratch. Our model is optimized by AdamW <ref type="bibr" target="#b41">[31]</ref> algorithm for 300 epochs with total batch size 24 on four NVIDIA Tesla V100 GPUs. The learning rate is initially set to 1 ? 10 ?4 , and gradually decays to 1 ? 10 ?5 following a cosine attenuation schedule. During training, we augment the samples by random flipping, rotating, reversing sequence order and random cropping patches with size 224 ? 224. For optical flow distillation, we extract pseudo label of bilateral intermediate flow fields with the pre-trained LiteFlowNet <ref type="bibr" target="#b31">[21]</ref> in advance, and perform consistent augmentation operations with frame triplets during the whole training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics and Datasets</head><p>We evaluate our method on various datasets covering diverse motion scenes for comprehensive comparison. Common metrics, such as PSNR and SSIM <ref type="bibr" target="#b60">[50]</ref> are adopted for quantitative evaluation. For Middlebury, we use the official IE and NIE indices. Now, we briefly introduce the used test datasets to assess our approaches. Vimeo90K <ref type="bibr" target="#b63">[53]</ref>: It contains frame triplets of 448?256 resolution. There are 3,782 triplets consisted in the test part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF101 [45]:</head><p>We adopt the test set selected in DVF <ref type="bibr" target="#b40">[30]</ref>, which includes 379 triplets of 256?256 frame size. SNU-FILM <ref type="bibr">[10]</ref>: SNU-FILM contains 1,240 frame triplets of approximate 1280?720 resolution. According to motion magnitude, it is divided into four different parts, namely, Easy, Medium, Hard, and Extreme for detailed comparison. Middlebury <ref type="bibr">[2]</ref>: The Middlebury benchmark is a widely used dataset to evaluate optical flow and VFI methods. Image resolution in this dataset is around 640?480. In this paper, we test on the Evaluation set without using Other set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-Arts</head><p>We compare IFRNet with state-of-the-art VFI methods, including kernel-based SepConv <ref type="bibr" target="#b47">[37]</ref>, AdaCoF <ref type="bibr" target="#b36">[26]</ref> and CDFI <ref type="bibr">[12]</ref>, flow-based ToFlow <ref type="bibr" target="#b63">[53]</ref>, DAIN <ref type="bibr">[3]</ref>, Soft-Splat <ref type="bibr" target="#b45">[35]</ref>, BMBC <ref type="bibr" target="#b48">[38]</ref>, RIFE <ref type="bibr" target="#b30">[20]</ref> and ABME <ref type="bibr" target="#b49">[39]</ref>, and hallucination-based CAIN <ref type="bibr">[10]</ref> and FeFlow <ref type="bibr" target="#b26">[16]</ref>. For results on SNU-FILM, we execute the released codes of CDFI and RIFE and refer to the other results tested in ABME. For Middlebury, we directly test on the Evaluation part and submit interpolation results to the online benchmark. To measure the inference speed and computation complexity, we run all methods on one Tesla V100 GPU under 1280?720 resolution and average the running time with 100 iterations. For fair comparison, we further build a large and a small version of IFRNet by scaling feature channels with 2.0 and 0.75, respectively, and separate above methods into two classes, i.e., fast and slow, according to their inference time. Quantitative Evaluation. <ref type="table" target="#tab_0">Table 1 and Table 2</ref> summarize quantitative results on diverse benchmarks. On Vimeo90K and UCF101 test datasets, IFRNet large achieves the best results on both PSNR and SSIM metrics. A recent method ABME <ref type="bibr" target="#b49">[39]</ref> also gets similar accuracy. However, our model runs 11.5 ? faster with similar amount of parameters due to the efficiency of single encoder-decoder based architecture. Our large model also obtains leading results on the Easy, (a) Ground Truth (b) <ref type="bibr">Overlaid</ref> (c) SepConv <ref type="bibr" target="#b47">[37]</ref> (d) DAIN <ref type="bibr">[3]</ref> (e) CAIN <ref type="bibr">[10]</ref> (f) AdaCoF <ref type="bibr" target="#b36">[26]</ref> (g) CDFI <ref type="bibr">[12]</ref> (h) ABME <ref type="bibr" target="#b49">[39]</ref> (i) Ours <ref type="figure">Figure 6</ref>. Medium and Hard parts of SNU-FILM datasets, while only falls behind ABME on the Extreme part. We attribute the reason to be that the bilateral cost volume constructed by ABME is good at estimating large displacement motion. In <ref type="table" target="#tab_1">Table 2</ref>, IFRNet large achieves top-performing VFI accuracy in most of the eight Middlebury test sequences, and outperforms the previous state-of-the-art SoftSplat <ref type="bibr" target="#b45">[35]</ref> on both average IE and NIE metrics. Although the improvement is limited, our approach runs 2.5 ? faster than Soft-Splat which takes cascaded VFI architecture. For FLOPs in convolution layers, IFRNet large also consumes significantly less computation than other VFI architectures.</p><p>In regard to real-time and lightweight VFI approaches, IFRNet yields about 0.2 dB better result than RIFE <ref type="bibr" target="#b30">[20]</ref> on Vimeo90K, and the margin is more distinct on large motion cases in SNU-FILM dataset. It is worth noting that IFRNet only contains half parameters to achieve better results than RIFE thanks to the superiority of joint refinement of intermediate flow and context feature. Compared with CDFI full <ref type="bibr">[12]</ref>, IFRNet has the same 5M parameters, while achieving 0.63 dB higher PSNR on Vimeo90K with 15.2 ? faster inference speed. Moreover, IFRNet small can further improve speed by 31% and reduce parameters and computation complexity by 44% than IFRNet while with only slight frame interpolation accuracy decrease. Qualitative Evaluation. <ref type="figure">Figure 6</ref> visually compares wellbehaved VFI methods on SNU-FILM (Hard) dataset which contains large and complex motion scenes. It can be seen that kernel-based <ref type="bibr">[12,</ref><ref type="bibr" target="#b36">26,</ref><ref type="bibr" target="#b47">37]</ref> and hallucination-based <ref type="bibr">[10]</ref> methods fail to synthesize sharp motion boundary, containing ghost and blur artifacts. Compared with flow-based algorithms <ref type="bibr">[3,</ref><ref type="bibr" target="#b49">39]</ref>, our approach can generate texture details faithfully thanks to the powerfulness of gradually refined intermediate feature. In short, IFRNet can synthesize pleasing target frame with more comfortable visual experience. More qualitative results can be found in our supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To verify the effectiveness of proposed approaches, we carry out ablation study in terms of network architecture and loss function on Vimeo90K and SNU-FILM Hard datasets. Intermediate Feature. To ablate the effectiveness of intermediate feature? k t in IFRNet, we build a model by removing? k t from the input and output of multiple decoders, while      keeping feature channels of middle parts of decoders unchanged. Also, we selectively remove residual R in Eq. 4 to isolate the improvement from intermediate flow and residual. We train them with only the reconstruction loss L r under the same learning schedule as before. As listed in <ref type="table" target="#tab_2">Table 3</ref>, from the first two rows, we can observe that intermediate feature can provide reference anchor information to promote intermediate flow estimation. <ref type="figure" target="#fig_5">Figure 7</ref> also presents some visual examples to confirm the conclusion. Compared with the last and the second rows in <ref type="table" target="#tab_2">Table 3</ref> Task-Oriented Flow Distillation. <ref type="table" target="#tab_3">Table 4</ref> compares VFI accuracy under different combinations of proposed loss functions quantitatively. It can be seen that adding taskoriented flow distillation loss L d consistently improves PSNR of 0.2 dB on Vimeo90K. To verify the superiority of its task adaptive ability, we also perform flow distillation with generalized Charbonnier loss under different robustness shown in <ref type="figure">Figure 5</ref>, whose results are summarized in <ref type="figure" target="#fig_6">Figure 8</ref>. It turns out that robustness parameter p = 0.3 achieves best VFI accuracy in the fixed robustness setting.</p><p>On the other hand, flow distillation can damage frame quality when p approaches to 1.0 due to the harmful knowledge in pseudo label. In a word, proposed task-oriented approach achieves the best accuracy thanks to its spatial adaptive abil-            ity for adjusting robustness loss during flow distillation. Feature Space Geometry Consistency. As shown in <ref type="table" target="#tab_3">Table 4</ref>, adding proposed feature space geometry consistency loss L g based on above contributions, we can obtain a further improvement, that confirms the complementary effect of L g in regard to L d . <ref type="figure" target="#fig_15">Figure 9</ref> visually compares mean feature maps of intermediate feature? 1 t w/o and w/ L g . It shows that L g can regularize the reconstructed intermediate feature to keep better geometry layout in multi-scale feature space, resulting in better VFI performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have devised an efficient deep architecture, termed IFRNet, for video frame interpolation, without any cascaded synthesis or refinement module. It gradually refines intermediate flow together with a powerful intermediate feature, that can not only boost intermediate flow estimation to synthesize sharp motion boundary but also provide global context representation to generate vivid motion details. Moreover, we have presented task-oriented flow distillation loss and feature space geometry consistency loss to fully release its potential. Experiments on various benchmarks demonstrate the state-of-the-art performance and fast inference speed of proposed approaches. We expect proposed single encoder-decoder joint refinement based IFR-Net to be a useful component for many frame rate upconversion and intermediate view synthesis systems. <ref type="figure" target="#fig_0">Figure 10</ref>. Qualitative results of IFRNet for 8? interpolation on GoPro <ref type="bibr" target="#b43">[33]</ref> and Adobe240 <ref type="bibr" target="#b56">[46]</ref> test datasets. Please watch the video with Adobe Reader. Each video has 9 frames where the first and the last frames are input, and the middle 7 frames are predicted by IFRNet.</p><p>In the supplementary, we first present multi-frame interpolation experiments of IFRNet. Second, qualitative video comparisions with other advanced VFI approaches are displayed. Third, we depict structure details of IFRNet and its variants. Fourth, we provide more visual examples and analysis of middle components for better understanding the workflow of IFRNet. Finally, we show the screenshot of VFI results on the Middlebury benchmark. Please note that the numbering within this supplementary has manually been adjusted to continue the ones in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Multi-Frame Interpolation</head><p>Different from other multi-frame interpolation methods which scales optical flow <ref type="bibr">[3,</ref><ref type="bibr" target="#b32">22]</ref> or interpolates middle frames recursively <ref type="bibr">[10,</ref><ref type="bibr" target="#b36">26]</ref>, IFRNet can predict multiple intermediate frames by proposed one-channel temporal encoding mask T , which is one of the input of the coars- * Equal contribution. This work was done when Lingtong Kong was an intern at Tencent Youtu Lab. Code is available at https://github. com/ltkong218/IFRNet. ? Corresponding author: Jie Yang (jieyang@sjtu.edu.cn). This research is partly supported by NSFC, China (No: 61876107, U1803261).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>GoPro <ref type="bibr" target="#b43">[33]</ref> Adobe240 <ref type="bibr" target="#b56">[46]</ref>  Thanks to the modularity character of IFRNet, the encoder only needs a single forward pass, while the decoders infer 7 times with different temporal embedding to convert videos from 30 fps into 240 fps. Therefore, the speed advantage of IFRNet is still or even more obvious than other approaches. <ref type="figure" target="#fig_0">Figure 10</ref> gives some qualitative results of IFRNet for 8? interpolation, demonstrating its superior ability for frame rate up-conversion and slow motion generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Video Comparison</head><p>In this part, we qualitatively compare interpolated videos by proposed IFRNet against other open source VFI methods on SNU-FILM <ref type="bibr">[10]</ref> dataset, whose results are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. As can be seen, our approach can generate motion boundary and texture details faithfully thanks to the powerfulness of gradually refined intermediate feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Network Architecture</head><p>In this section, we present the structure details of five sub-networks of IFRNet, i.e., pyramid encoder E and coarse-to-fine decoders D 4 , D 3 , D 2 , D 1 . In each following figure, arguments of 'Conv' and 'Deconv' from left to right are input channels, output channels, kernel size, stride and padding, respectively. Dimensions of input and output tensors from left to right stand for feature channels, height and width, separately. A PReLU <ref type="bibr" target="#b27">[17]</ref> follows each 'Conv' layer, while there is no activation after each 'Deconv' layer. In practice, the intermediate flow fields are estimated in a residual manner, which is not reflected in the figures to emphasize the primary network structure. We take input frames with spatial size of 640?480 as example.</p><p>Conv <ref type="figure" target="#fig_0">(3, 32, 3, 2, 1</ref>  <ref type="figure" target="#fig_0">193, 192, 3, 1, 1)   Conv(192, 192, 3, 1, 1)</ref> Split Conv <ref type="figure" target="#fig_0">(32, 32, 3, 1, 1</ref>  ers in coarse-to-fine decoders of IFRNet large and IFRNet small are set to 64 and 24, separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat</head><p>Conv <ref type="figure" target="#fig_0">(148, 144, 3, 1, 1)</ref> Conv <ref type="figure" target="#fig_0">(144, 144, 3, 1, 1</ref>  <ref type="figure" target="#fig_0">Figure 16</ref>. Details of the top decoder D 1 . loss, which can decrease the adverse impacts while focusing on the useful knowledge for better frame interpolation. It seems that intermediate flow prediction of IFRNet behaves smoother and contains less artifacts than flow prediction of pseudo label, that helps to achieve better VFI accuracy.   <ref type="figure" target="#fig_0">Figure 19</ref> gives visual understanding of frame interpola- For better visualization of residual R, we multiply it by 10 and add a bias of 0.5. Each column represents a separate example on Vimeo90K <ref type="bibr" target="#b63">[53]</ref> dataset. Zoom in for best view. tion process of IFRNet. Thanks to the reference anchor information offered by intermediate feature together with effective supervision provided by geometry consistency loss and task-oriented flow distillation loss, IFRNet can estimate relatively good intermediate flow with clear motion boundary. Further, we can see that merge mask M can identify occluded regions of warped frames by adjusting the mixing weight, where it tends to average the candidate regions when both views are visible. Finally, residual R can compensate for some contextual details, which usually response at motion boundary and image edges. Different from other flow-based VFI methods that take cascaded structure design, merge mask M and residual R in IFRNet share the same encoder-decoder with intermediate optical flow, making proposed architecture achieve better VFI accuracy while being more lightweight and fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Visualization and Discussion</head><p>Readers may think our IFRNet is similar with PWC-Net <ref type="bibr" target="#b58">[48]</ref> which is designed for optical flow. However, It is non-trivial to adapt PWC-Net for frame interpolation, since previous related works employ it as one of many components. We summarize their difference in several aspects: 1) Anchor feature in PWC-Net is extracted by the encoder, while in IFRNet, it is reconstructed by the decoder. 2) Besides motion information in intermediate feature, there are occlusion, texture and temporal information in it. 3) PWC-Net designed for motion estimation, is optimized only by flow regression loss with strong augmentation. However, IFRNet designed for frame synthesizing, is optimized in a multi-target manner with weak data augmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Screenshots of the Middlebury Benchmark</head><p>We take screenshots of the online Middlebury benchmark for VFI on the November 16th, 2021, whose results are shown in <ref type="figure" target="#fig_21">Figure 20</ref> and <ref type="figure" target="#fig_0">Figure 21</ref>. Since the average rank is a relative indicator, previous methods <ref type="bibr">[3,</ref><ref type="bibr" target="#b26">16,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b48">38]</ref> usually report average IE (interpolation error) and average NIE (normalized interpolation error) for comparison. As summarized in <ref type="table" target="#tab_1">Table 2</ref> in our main paper, proposed IFRNet large model achieves best results on both IE and NIE metrics among all published VFI methods that are trained on Vimeo90K <ref type="bibr" target="#b63">[53]</ref> dataset. Moreover, IFRNet large runs several times faster than previous state-of-the-art algorithms <ref type="bibr" target="#b45">[35,</ref><ref type="bibr" target="#b49">39]</ref>, demonstrating the superior VFI accuracy and fast inference speed of proposed approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Speed, accuracy and parameters comparison. Proposed IFRNet achieves state-of-the-art frame interpolation accuracy with fast inference speed and lightweight model size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>arXiv:2205.14620v1 [cs.CV] 29 May 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture overview and loss functions of IFRNet. Our model is an efficient encoder-decoder based network, which first extracts pyramid context features from input frames with a shared encoder, and then gradually refines bilateral intermediate flow fields Ft?0, Ft?1 together with reconstructed intermediate feature?t through coarse-to-fine decoders, until yielding the final output. Besides the common image reconstruction loss Lr, task-oriented flow distillation loss L d and feature space geometry consistency loss Lg are newly devised to guide the feature alignment procedure more effectively towards intermediate frame synthesizing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Details of the decoder in each pyramid level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Ablation study on different flow distillation losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Visual comparison of intermediate flow and predicted frame of IFRNet w/ and w/o intermediate feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Visual comparison of intermediate flow and predicted frame of IFRNet w/o and w/ intermediate feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Ablation study on different flow distillation losses. #3885. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Ablation study on different flow distillation losses. #3885. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Ablation study on different flow distillation losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Visual comparison of intermediate flow and predicted frame of IFRNet w/ and w/o intermediate feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 .</head><label>8</label><figDesc>Visual comparison of intermediate flow and predicted frame of IFRNet w/o and w/ intermediate feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>[ 2 ]Figure 9 .</head><label>29</label><figDesc>Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth, Michael J. Black, and Richard Szeliski. A database and eval-Visual comparison of mean feature map of intermediate feature? 1 t w/o and w/ Lg. Leftmost is the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 .</head><label>9</label><figDesc>Visual comparison of mean feature map of intermediate feature? 1 t w/o and w/ Lg. Leftmost is the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 .</head><label>14</label><figDesc>Details of the middle decoder D 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 Figure 17 .</head><label>1717</label><figDesc>presents some visual examples to show the robustness masks in proposed task-oriented flow distillation Illustration of task-oriented flow distillation. From top to bottom rows are ground truth frame I gt t , pseudo label of intermediate flow fields F p t?0 , F p t?1 , predicted intermediate flow fields Ft?0, Ft?1, task-oriented robustness masks P0, P1. Darker color in P0, P1 approaches to 1, while brighter color tends to 0.Each column represents a separate example on Vimeo90K<ref type="bibr" target="#b63">[53]</ref> dataset. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 .</head><label>18</label><figDesc>Illustration of mean feature map of intermediate feature? 1 t w/o and w/ Lg. From top to bottom rows are ground truth frame I gt t , mean feature map of? 1 t w/o Lg, mean feature map of? 1 t w/ Lg. Each column represents a separate example on Vimeo90K [53] dataset. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18</head><label>18</label><figDesc>depicts more visual results of mean feature maps of intermediate feature w/o and w/ proposed geometry consistency loss, demonstrating its effect on regularizing refined intermediate feature to keep better structure layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 .</head><label>19</label><figDesc>Illustration of intermediate components of IFRNet. From top to bottom rows are input frames I0, I1, predicted intermediate flow fields Ft?0, Ft?1, warped input frames?0,?1, merge mask M , merged frame? t , residual R, final prediction?t and ground truth I gt t , where merged frame is calculated by? t = M ? 0 + (1 ? M ) ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 20 .</head><label>20</label><figDesc>Screenshot of our IE-ranking on the Middlebury benchmark (taken on the November 16th, 2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 21 .</head><label>21</label><figDesc>Screenshot of our NIE-ranking on the Middlebury benchmark (taken on the November 16th, 2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>.9702 34.78/0.9669 39.41/0.9900 34.97/0.9762 29.36/0.9253 24.31/0.8448 0.065 21.7 0.36 CAIN [10] 34.65/0.9730 34.91/0.9690 39.89/0.9900 35.61/0.9776 29.90/0.9292 24.78/0.8507 0.069 42.8 1.29 AdaCoF [26] 34.47/0.9730 34.90/0.9680 39.80/0.9900 35.05/0.9754 29.46/0.9244 24.31/0.8439 0.054 21.8 0.36 RIFE [20] 35.62/0.9780 35.28/0.9690 40.06/0.9907 35.75/0.9789 30.10/0.9330 24.84/0.8534 0.026 ] 32.09/0.9490 35.11/0.9684 37.72/0.9840 32.47/0.9554 26.95/0.8871 22.70/0.8083 0.161 19.8 Quantitative comparison (PSNR/SSIM) of VFI results on the Vimeo90K, UCF101 and SNU-FILM datasets. For each item, the best result is boldfaced, and the second best is underlined. Top and bottom parts are divided by running time.</figDesc><table><row><cell>Method</cell><cell>Vimeo90K</cell><cell>UCF101</cell><cell>Easy</cell><cell>SNU-FILM Medium Hard</cell><cell>Extreme</cell><cell>Time (s)</cell><cell>Params (M)</cell><cell>FLOPs (T)</cell></row><row><cell>SepConv [37] IFRNet IFRNet small</cell><cell cols="7">33.79/09.8 35.80/0.9794 35.29/0.9693 40.03/0.9905 35.94/0.9793 30.41/0.9358 25.05/0.8587 0.025 5.0 35.59/0.9786 35.28/0.9691 39.96/0.9905 35.92/0.9792 30.36/0.9357 25.05/0.8582 0.019 2.8</cell><cell>0.20 0.21 0.12</cell></row><row><cell cols="9">ToFlow [53] CyclicGen [291.77 33.73/0.9682 34.58/0.9667 39.08/0.9890 34.39/0.9740 28.44/0.9180 23.39/0.8310 0.152 1.4 0.62 DAIN [3] 34.71/0.9756 34.99/0.9683 39.73/0.9902 35.46/0.9780 30.17/0.9335 25.09/0.8584 1.033 24.0 5.51 SoftSplat [35] 36.10/0.9700 35.39/0.9520 ----0.195 12.2 0.90 BMBC [38] 35.01/0.9764 35.15/0.9689 39.90/0.9902 35.31/0.9774 29.33/0.9270 23.92/0.8432 3.845 11.0 2.50 CDFI full [12] 35.17/0.9640 35.21/0.9500 40.12/0.9906 35.51/0.9778 29.73/0.9277 24.53/0.8476 0.380 5.0 0.82 ABME [39] 36.18/0.9805 35.38/0.9698 39.59/0.9901 35.77/0.9789 30.58/0.9364 25.42/0.8639 0.905 18.1 1.30 IFRNet large 36.20/0.9808 35.42/0.9698 40.10/0.9906 36.12/0.9797 30.63/0.9368 25.27/0.8609 0.079 19.7 0.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results on the Middlebury benchmark. For each item, the best result is boldfaced, and the second best is underlined.</figDesc><table><row><cell>Method</cell><cell>Average</cell><cell>Mequon Schefflera</cell><cell>Urban</cell><cell>Teddy</cell><cell>Backyard Basketball Dumptruck Evergreen</cell></row><row><cell>AdaCoF [26] BMBC [38] SoftSplat [35] IFRNet large</cell><cell cols="5">78 0.67 4.751 0.730 2.41 0.60 3.10 0.59 3.48 0.84 4.84 0.92 8.68 0.90 4.13 0.84 5.77 0.58 5.60 0.57 4.479 0.696 2.30 0.57 3.07 0.58 3.17 0.77 4.24 0.84 7.79 0.85 4.08 0.82 5.63 0.58 5.55 0.56 4.223 0.645 2.06 0.53 2.80 0.52 1.99 0.52 3.84 0.80 8.10 0.85 4.10 0.81 5.49 0.56 5.40 0.57 4.216 0.644 2.08 0.53 2.78 0.51 1.74 0.43 3.96 0.83 7.55 0.87 4.42 0.84 5.56 0.56 5.64 0.58</cell></row></table><note>Qualitative comparison of different VFI methods on SNU-FILM (Hard) dataset. Proposed IFRNet algorithm can synthesize fast moving objects with sharp boundary while maintaining distinct contextual details. Zoom in for best view.IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE SuperSlomo [22] 5.310 0.778 2.51 0.59 3.66 0.72 2.91 0.74 5.05 0.98 9.56 0.94 5.37 0.96 6.69 0.60 6.73 0.69 ToFlow [53] 5.490 0.840 2.54 0.55 3.70 0.72 3.43 0.92 5.05 0.96 9.84 0.97 5.34 0.98 6.88 0.72 7.14 0.90 DAIN [3] 4.856 0.713 2.38 0.58 3.28 0.60 3.32 0.69 4.65 0.86 7.88 0.87 4.73 0.85 6.36 0.59 6.25 0.66 FeFlow [16] 4.820 0.719 2.28 0.51 3.50 0.66 2.82 0.70 4.75 0.87 7.62 0.84 4.74 0.86 6.07 0.64 6.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on different architecture variants. 'IF' means intermediate feature? k t and 'R' stands for residual R.</figDesc><table><row><cell>779 780</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>781</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>782</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>783</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>784</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>785</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>786</cell><cell>Architecture</cell><cell></cell><cell>Vimeo90K</cell><cell>Hard</cell></row><row><cell>787 788 789 790 791</cell><cell>IF</cell><cell>R</cell><cell>PSNR 34.83 35.22 35.11 35.51</cell><cell>PSNR 29.96 30.22 30.06 30.27</cell></row><row><cell>792</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>793</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>794</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>795</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>796</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>797</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>798</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>799</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>800</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>801</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>802</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>803</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>804</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>805</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>806</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>807</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>808</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>809</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on different loss functions.</figDesc><table><row><cell></cell><cell>35.80</cell><cell cols="4">task-oriented flow distillation</cell><cell></cell></row><row><cell>PSNR on Vimeo90K Test</cell><cell>35.60 35.65 35.70 35.75</cell><cell cols="4">fixed robustness flow distillation wo flow distillation</cell><cell></cell></row><row><cell></cell><cell>35.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4 Robustness Parameter p 0.5 0.6 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Barlaud. Two deterministic half-quadratic regularization algorithms for computed imaging. In Proceedings of 1st International Conference on Image Processing, volume 2, pages 168-172 vol.2, 1994. 4 [7] Xianhang Cheng and Zhenzhong Chen. Video frame interpolation via deformable separable convolution. volume 34,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Chen Change Loy. Basicvsr: The search for essential com-[5] Kelvin C.K. Chan, Xintao Wang, Ke Yu, Chao Dong, and ponents in video super-resolution and beyond. In Proceed-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ings of the IEEE/CVF Conference on Computer Vision and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pattern Recognition (CVPR), pages 4947-4956, June 2021.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[6] P. pages 10607-10614, 04 2020. 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[8] Xianhang Cheng and Zhenzhong Chen. Multiple video</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>frame interpolation via enhanced deformable separable con-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>volution. pages 1-1, 2021. 2</cell></row><row><cell>w/o IF</cell><cell>w/o IF</cell><cell>w/ IF</cell><cell>w/ IF</cell><cell>[9] Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lu, Jin Tang, and Konstantinos N Plataniotis. All at</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>once: Temporally adaptive multi-frame interpolation with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>advanced motion modeling. In European Conference on</cell></row><row><cell>w/o IF</cell><cell>w/o IF</cell><cell>w/ IF</cell><cell>w/ IF</cell><cell>Computer Vision, 2020. 1, 3 [10] Jinsoo Choi, Jaesik Park, and In So Kweon. High-quality</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>frame interpolation via tridirectional inference. In Proceed-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ings of the IEEE/CVF Winter Conference on Applications of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Computer Vision (WACV), pages 596-604, January 2021. 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[11] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kyoung Mu Lee. Channel attention is all you need for video</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>frame interpolation. In AAAI, 2020. 2, 6, 7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[12] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Zhang, Han Hu, and Yichen Wei. Deformable convolutional</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>networks. In 2017 IEEE International Conference on Com-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>eval-</cell><cell>puter Vision (ICCV), pages 764-773, 2017. 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell></row></table><note>[2] Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth, Michael J. Black, and Richard Szeliski. A database and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on different loss functions.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Loss Function</cell><cell></cell><cell>Vimeo90K</cell><cell>Hard</cell></row><row><cell></cell><cell>Lr</cell><cell></cell><cell>L d</cell><cell></cell><cell>Lg</cell><cell>PSNR 35.51 35.72 35.61 35.80</cell><cell>PSNR 30.27 30.38 30.30 30.41</cell></row><row><cell></cell><cell>35.80</cell><cell cols="4">task-oriented flow distillation</cell><cell></cell></row><row><cell>PSNR on Vimeo90K Test</cell><cell>35.60 35.65 35.70 35.75</cell><cell cols="4">fixed robustness flow distillation w/o flow distillation</cell><cell></cell></row><row><cell></cell><cell>35.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="2">0.4 Robustness Parameter p 0.5 0.6 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .Table 3 .Table 4 .</head><label>234</label><figDesc>Evaluation results on the Middlebury benchmark. For each item, the best result is boldfaced, and the second best is underlined. Ablation study on different architecture variants. 'IF' means intermediate feature? k t , while 'R' stands for residual R. Ablation study on different loss functions.</figDesc><table><row><cell cols="3">Method</cell><cell></cell><cell></cell><cell cols="3">Average IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE Mequon Schefflera Urban Teddy Backyard Basketball Dumptruck Evergreen</cell></row><row><cell cols="8">SuperSlomo [23] 5.310 0.778 2.51 0.59 3.66 0.72 2.91 0.74 5.05 0.98 9.56 0.94 5.37 0.96 6.69 0.60 6.73 0.69 ToFlow [50] 5.490 0.840 2.54 0.55 3.70 0.72 3.43 0.92 5.05 0.96 9.84 0.97 5.34 0.98 6.88 0.72 7.14 0.90 DAIN [3] 4.856 0.713 2.38 0.58 3.28 0.60 3.32 0.69 4.65 0.86 7.88 0.87 4.73 0.85 6.36 0.59 6.25 0.66 FeFlow [17] 4.820 0.719 2.28 0.51 3.50 0.66 2.82 0.70 4.75 0.87 7.62 0.84 4.74 0.86 6.07 0.64 6.78 0.67 AdaCoF [25] 4.751 0.730 2.41 0.60 3.10 0.59 3.48 0.84 4.84 0.92 8.68 0.90 4.13 0.84 5.77 0.58 5.60 0.57 BMBC [36] 4.479 0.696 2.30 0.57 3.07 0.58 3.17 0.77 4.24 0.84 7.79 0.85 4.08 0.82 5.63 0.58 5.55 0.56 SoftSplat [33] 4.223 0.645 2.06 0.53 2.80 0.52 1.99 0.52 3.84 0.80 8.10 0.85 4.10 0.81 5.49 0.56 5.40 0.57 IFRNet large 4.216 0.644 2.08 0.53 2.78 0.51 1.74 0.43 3.96 0.83 7.55 0.87 4.42 0.84 5.56 0.56 5.64 0.58</cell></row><row><cell></cell><cell></cell><cell cols="2">Architecture IF R</cell><cell></cell><cell>Vimeo90K PSNR SSIM</cell><cell cols="2">Hard PSNR SSIM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.83 0.9749 35.22 0.9766 35.11 0.9762 35.51 0.9779</cell><cell cols="2">29.96 0.9339 30.22 0.9351 30.06 0.9341 30.27 0.9352</cell></row><row><cell></cell><cell cols="7">Loss Function Lr L d Lg PSNR SSIM PSNR SSIM Vimeo90K Hard 35.51 0.9779 30.27 0.9352 35.72 0.9792 30.38 0.9357 35.61 0.9783 30.30 0.9354 35.80 0.9794 30.41 0.9358</cell></row><row><cell></cell><cell>35.80</cell><cell cols="4">task-oriented flow distillation</cell><cell></cell></row><row><cell>PSNR on Vimeo90K Test</cell><cell>35.60 35.65 35.70 35.75</cell><cell cols="4">fixed robustness flow distillation wo flow distillation</cell><cell></cell></row><row><cell></cell><cell>35.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="2">0.4 Robustness Parameter p 0.5 0.6 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .Table 3 .Table 4 .</head><label>234</label><figDesc>Evaluation results on the Middlebury benchmark. For each item, the best result is boldfaced, and the second best is underlined. Ablation study on different architecture variants. 'IF' means intermediate feature? k t , while 'R' stands for residual R. Ablation study on different loss functions.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell cols="4">Average IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE IE NIE Mequon Schefflera Urban Teddy Backyard Basketball Dumptruck Evergreen</cell></row><row><cell cols="8">SuperSlomo [23] 5.310 0.778 2.51 0.59 3.66 0.72 2.91 0.74 5.05 0.98 9.56 0.94 5.37 0.96 6.69 0.60 6.73 0.69 ToFlow [50] 5.490 0.840 2.54 0.55 3.70 0.72 3.43 0.92 5.05 0.96 9.84 0.97 5.34 0.98 6.88 0.72 7.14 0.90 DAIN [3] 4.856 0.713 2.38 0.58 3.28 0.60 3.32 0.69 4.65 0.86 7.88 0.87 4.73 0.85 6.36 0.59 6.25 0.66 FeFlow [17] 4.820 0.719 2.28 0.51 3.50 0.66 2.82 0.70 4.75 0.87 7.62 0.84 4.74 0.86 6.07 0.64 6.78 0.67 AdaCoF [25] 4.751 0.730 2.41 0.60 3.10 0.59 3.48 0.84 4.84 0.92 8.68 0.90 4.13 0.84 5.77 0.58 5.60 0.57 BMBC [36] 4.479 0.696 2.30 0.57 3.07 0.58 3.17 0.77 4.24 0.84 7.79 0.85 4.08 0.82 5.63 0.58 5.55 0.56 SoftSplat [33] 4.223 0.645 2.06 0.53 2.80 0.52 1.99 0.52 3.84 0.80 8.10 0.85 4.10 0.81 5.49 0.56 5.40 0.57 IFRNet large 4.216 0.644 2.08 0.53 2.78 0.51 1.74 0.43 3.96 0.83 7.55 0.87 4.42 0.84 5.56 0.56 5.64 0.58</cell></row><row><cell></cell><cell cols="2">Architecture IF R</cell><cell></cell><cell>Vimeo90K PSNR SSIM</cell><cell cols="3">Hard PSNR SSIM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.83 0.9749 35.22 0.9766 35.11 0.9762 35.51 0.9779</cell><cell cols="3">29.96 0.9339 30.22 0.9351 30.06 0.9341 30.27 0.9352</cell></row><row><cell></cell><cell cols="7">Loss Function Lr L d Lg PSNR SSIM PSNR SSIM Vimeo90K Hard 35.51 0.9779 30.27 0.9352 35.72 0.9792 30.38 0.9357 35.61 0.9783 30.30 0.9354 35.80 0.9794 30.41 0.9358</cell></row><row><cell></cell><cell>35.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR on Vimeo90K Test</cell><cell>35.60 35.65 35.70 35.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>35.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="2">0.4 Robustness Parameter p 0.5 0.6 0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparison for 8? interpolation.est decoder D 4 . The temporal encoding is a conditional input signal whose values are all the same and set to t, where t ? {1/8, 2/8, . . . , 7/8} in 8? interpolation setting. Also, proposed task-oriented flow distillation loss and feature space geometry consistency loss still work for any intermediate time instance t. To evaluate IFRNet for 8? interpolation, we use the train/test split of FLAVR<ref type="bibr" target="#b33">[23]</ref>, where we train IFRNet on GoPro<ref type="bibr" target="#b43">[33]</ref> training set with the same learning schedule and loss functions as our main paper. Then we test the pre-trained model on GoPro testing and Adobe240<ref type="bibr" target="#b56">[46]</ref> datasets whose results are listed inTable 5.IFRNet outperforms all of the other SOTA methods with 2 input frames on both GoPro and Adobe240 datasets Video comparison on SNU-FILM[10]  dataset. Please watch the video with Adobe Reader and zoom in for best view.</figDesc><table><row><cell>Time</cell></row></table><note>in both PSNR and SSIM metrics. For example, IFRNet achieves 0.84 dB better results than DAIN [3] on GoPro and exceeds SuperSloMo [22] by 1.27 dB on Adobe240.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Details of the pyramid encoder E. The two input frames I l , l ? {0, 1} are encoded by the same Siamese network.As for IFRNet large and IFRNet small, feature channels from the first to the fourth pyramid levels are set to 64, 96, 144, 192 and 24, 36, 54, 72, respectively. Correspondingly, channel numbers in multiple decoders are adjusted. Also, feature channels of the third and the fifth convolution lay-</figDesc><table><row><cell>) Conv(32, 32, 3, 1, 1) , ? ? Conv(32, 48, 3, 2, 1) Conv(48, 48, 3, 1, 1) Conv(48, 72, 3, 2, 1) Conv(72, 72, 3, 1, 1) Conv(72, 96, 3, 2, 1) Conv(96, 96, 3, 1, 1) , ? ? , ? ? , ? ? , ? ? Figure 12. Concat Conv(</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Details of the bottom decoder D 4 .</figDesc><table><row><cell></cell><cell></cell><cell cols="3">, ? ? ; , ? ? ;</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">, ? ?</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(192, 192, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(32, 32, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(192, 192, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Deconv(192, 76, 4, 2, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">+ , ? ? ; Concat ! , ? ? ; * , ? ? ; * , ? ? ;</cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(220, 216, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(216, 216, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(32, 32, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(216, 216, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(32, 32, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Conv(216, 216, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Deconv(216, 52, 4, 2, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell>! , ?</cell><cell>?</cell><cell>; ? , ?</cell><cell>?</cell><cell>; ? , ?</cell><cell>?</cell></row></table><note>? , ? ? ; ? , ? ? Figure 13.? , ? ? ; ? , ? ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Figure 15. Details of the middle decoder D 2 .</figDesc><table><row><cell>! , ?</cell><cell>?</cell><cell cols="2">; + , ?</cell><cell>?</cell><cell cols="2">; + , ?</cell><cell>?</cell><cell>;</cell></row><row><cell></cell><cell cols="2">? , ?</cell><cell>?</cell><cell cols="2">; ? , ?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Split</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(32, 32, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Conv(144, 144, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Split</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(32, 32, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Conv(144, 144, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Deconv(144, 36, 4, 2, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Split</cell><cell></cell><cell></cell></row><row><cell>! , ?</cell><cell>?</cell><cell cols="2">; ? , ?</cell><cell>?</cell><cell cols="3">; ? , ?</cell><cell>?</cell></row><row><cell>! , ?</cell><cell>?</cell><cell cols="2">; ) , ?</cell><cell>?</cell><cell cols="2">; ) , ?</cell><cell>?</cell><cell>;</cell></row><row><cell></cell><cell cols="2">? , ?</cell><cell>?</cell><cell cols="2">; ? , ?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(100, 96, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(96, 96, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Split</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(32, 32, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(96, 96, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Split</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(32, 32, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Concat</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv(96, 96, 3, 1, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Deconv(96, 8, 4, 2, 1)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Split</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">? , ?</cell><cell>?</cell><cell cols="2">; ? , ?</cell><cell>?</cell><cell>;</cell></row><row><cell></cell><cell></cell><cell>, ?</cell><cell>?</cell><cell>; , ?</cell><cell>?</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Lingtong Kong 1 * , Boyuan Jiang 2 * , Donghao Luo 2 , Wenqing Chu 2 , Xiaoming Huang 2 , Ying Tai 2 , Chengjie Wang 2 , Jie Yang 1 ? 1 Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, 2 Youtu Lab, Tencent {ltkong, jieyang}@sjtu.edu.cn {byronjiang, michaelluo, wenqingchu, skyhuang, yingtai, jasoncjwang}@tencent.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3698" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="933" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="4947" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Image Processing</title>
		<meeting>1st International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Video frame interpolation via deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10607" to="10614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multiple video frame interpolation via enhanced deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">All at once: Temporally adaptive multi-frame interpolation with advanced motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadi</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-quality frame interpolation via tridirectional inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="596" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning end-to-end scene flow by distilling single tasks knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Image Processing</title>
		<meeting>1st International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video frame interpolation via deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple video frame interpolation via enhanced deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">All at once: Temporally adaptive multi-frame interpolation with advanced motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadi</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cdfi: Compression-driven network design for frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zharkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A nonaliasing, real-time spatial transform technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">M</forename><surname>Fant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Residual convdeconv grid network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Tr?meau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Featureflow: Robust video interpolation via structure-totexture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rife: Real-time intermediate flow estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varan</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flavr: Flow-agnostic video representations for fast frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arxiv</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fisr: Deep joint frame interpolation and super-resolution with a multi-scale temporal loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fastflownet: A lightweight network for fast optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingtong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning optical flow with unlabeled data distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ddflow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep video frame interpolation using cyclic frame generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Tung</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Asymmetric bilateral motion estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Im-net for high resolution video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Sabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omry</forename><surname>Sendik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Xvfi: extreme video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep animation video interpolation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<idno>2017. 9</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">One-dimensional resampling with inverse and forward mapping functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Wolberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Sueyllam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graphics Tools</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Zoom-in-to-check: Boosting video interpolation via instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV &apos;94</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A flexible recurrent residual pyramid network for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
