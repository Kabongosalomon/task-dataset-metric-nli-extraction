<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INCORPORATING LUMINANCE, DEPTH AND COLOR INFORMATION BY A FUSION-BASED NETWORK FOR SEMANTIC SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Wei</forename><surname>Hung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yuan</forename><surname>Lo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ming</forename><surname>Hang</surname></persName>
							<email>hmhang@nctu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">INCORPORATING LUMINANCE, DEPTH AND COLOR INFORMATION BY A FUSION-BASED NETWORK FOR SEMANTIC SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-RGB-D semantic segmentation</term>
					<term>depth map</term>
					<term>illuminance</term>
					<term>fusion-based network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation has made encouraging progress due to the success of deep convolutional networks in recent years. Meanwhile, depth sensors become prevalent nowadays; thus, depth maps can be acquired more easily. However, there are few studies that focus on the RGB-D semantic segmentation task. Exploiting the depth information effectiveness to improve performance is a challenge. In this paper, we propose a novel solution named LDFNet, which incorporates Luminance, Depth and Color information by a fusion-based network. It includes a sub-network to process depth maps and employs luminance images to assist the depth information in processes. LDFNet outperforms the other state-of-art systems on the Cityscapes dataset, and its inference speed is faster than most of the existing networks. The experimental results show the effectiveness of the proposed multi-modal fusion network and its potential for practical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Because of the success of deep convolutional neural networks (CNNs) in recent years, researchers have made a breakthrough in semantic segmentation. FCN <ref type="bibr" target="#b11">[12]</ref> is a pioneer, then SegNet <ref type="bibr" target="#b0">[1]</ref>, DeepLab <ref type="bibr" target="#b1">[2]</ref> and PSPNet <ref type="bibr" target="#b19">[20]</ref> are proposed successively. Although these networks show outstanding performance, their computational cost is generally considered too high to be widely deployed. On the other hand, ENet <ref type="bibr" target="#b12">[13]</ref> is proposed for low complexity, but its accuracy is much sacrificed. Afterward, ERFNet <ref type="bibr" target="#b13">[14]</ref> combines the efficiency of the factorized convolution and the capability of the Non-Bottleneck <ref type="bibr" target="#b6">[7]</ref> for better trade-off between accuracy and computational efficiency, but there is still room for further improvement.</p><p>More recently, DenseNet <ref type="bibr" target="#b3">[4]</ref> introduces a dense connection concept that connects each layer to all the other layers in a feed-forward manner. This strategy reinforces the information propagation and decreases the model complexity. This design is also applicable to the segmentation systems. <ref type="bibr" target="#b0">1</ref> Project page: https://github.com/shangweihung/LDFNet Nowadays depth sensors such as Kinect are quite affordable, so RGB-D semantic segmentation is an emerging topic. Typically, because the depth map edges are aligned with RGB image contours, the depth values of objects tend to be uniform or varying gradually along a spatial axis. Therefore, the depth information can be used as a good indicator of objects <ref type="bibr" target="#b16">[17]</ref>. The depth maps can thus be treated as complementary data to RGB images, but it is a challenge to extract the complementary information from the depth maps effectively. One simple way is stacking a depth map with a RGB image to form 4 input channels to a CNN, but the attempts so far are not yet successful to exploit the desirable information from depth data complementary to that of RGB data. Gupta et al. <ref type="bibr" target="#b4">[5]</ref> introduce the HHA encoding to represent the depth information, yet this transformation does not provide extra useful information than the original depth data itself. FuseNet <ref type="bibr" target="#b5">[6]</ref> processes the depth maps by a fusionbased network that feeds the RGB images and the depth maps into two separate sub-networks respectively, then fuses their features together. Even though making some improvements, it increases considerably the number of parameters and the amount of computation.</p><p>In this paper, we propose a new solution for RGB-D semantic segmentation, which incorporates both the Luminance and Depth information by a Fusion-based network, named LDFNet. It exploits the information embedded in the depth map by a two-branch architecture similar to that of FuseNet, but we adopt the ERFNet structure as a backbone for the RGB branch, so-called RGB Encoder and Decoder due to its high efficiency, then we design a new structure for the depth branch (see <ref type="figure" target="#fig_0">Figure 1</ref>). Our depth branch accepts the notion of the dense connectivity to process the depth maps more efficiently, so that the entire network complexity would not increase too high with the extra depth inputs. Furthermore, we add a dense block at the early stage of the depth branch to purposely extract the boundary and contour features from the depth map.</p><p>Because capturing the depth information accurately is a difficult task, the current popular depth sensors cannot provide high quality and high definition depth maps. The captured depth maps are typically at low resolution and have defects such as strong noises and wide occlusion regions. These defects may lead to the poor performance of the depth branch if it works alone by using the captured depth maps only. As a result, inspired by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> that uses the luminance information (Y) for depth map enhancement, we include the luminance images as an input to the depth branch. That is, the luminance images derived from the RGB inputs are stacked with the depth channel in the depth branch to enhance its capability, and thus our depth branch is called D&amp;Y Encoder. To the best of our knowledge, we are the first that proposes the D&amp;Y method for RGB-D semantic segmentation. The proposed LDFNet achieves very competitive results in terms of both accuracy and complexity efficiency compared to the other state-of-the-art methods on the Cityscapes dataset <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>The entire architecture of the proposed LDFNet is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. It consists of RGB Encoder, Decoder, and D&amp;Y Encoder. In the following paragraphs, we will discuss the details and the reasons behind our network design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">RGB Encoder and Decoder</head><p>We adopt the network architecture proposed in ERFNet <ref type="bibr" target="#b13">[14]</ref> as our network backbone in RGB Encoder and Decoder because of its good performance in considering both reliability (accuracy) and efficiency (complexity). ERFNet is composed of Non-bottleneck-1D by the Non-bottleneck suggested in ResNet <ref type="bibr" target="#b6">[7]</ref>. The difference between these two is that each convolutional kernel of the Non-bottleneck is factorized into two one-dimensional convolutional kernels. To be more specific, each 3?3 kernel is replaced by a 3?1 and a 1?3 kernels, and thus the number of parameters can be decreased.</p><p>Feature map downsampling makes the receptive fields wider and thus can extract a larger size of contextual representations, but it may also lose detailed spatial information that is especially crucial for semantic segmentation. Rather than overly downsampling the feature maps, compared to SegNet <ref type="bibr" target="#b0">[1]</ref> (five downsampling operations in total), ERFNet achieves a better balance by using three Downsampler Blocks. In order to enlarge the receptive fields without additional parameters and computation, the dilated convolutions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref> with different rates are interweaved in certain layers.</p><p>For the decoder, instead of using the max-unpooling layers introduced in SegNet [1], ERFNet chooses the deconvolution filter for restoring the feature maps to the original resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">D&amp;Y Encoder</head><p>FuseNet <ref type="bibr" target="#b5">[6]</ref> uses two identical architectures for its two encoders. By contrast, our second branch, D&amp;Y Encoder, has a different structure. Because DenseNet <ref type="bibr" target="#b3">[4]</ref> is believed to have a much higher efficiency without sacrificing the accuracy, our D&amp;Y Encoder adopts the notion of dense connectivity to enhance the information flow from the earlier layers to the latter layers.</p><p>Compared to RGB Encoder, each Non-Bottleneck in the D&amp;Y Encoder is replaced by a dense module. The dense module begins with a 1?1 convolution layer for channel reduction to improve efficiency then a 3?3 convolution layer follows to extract new features. Next, the second and the third Downsampler Blocks are replaced by the transition layers proposed in DenseNet, which are made up of a 1?1 convolution layer followed by a 2?2 average pooling layer. Since extracting depth features cannot benefit by simply using a deeper network, we only place 3 and 4 dense modules in the second and the third dense block, respectively to save computational cost. Instead, we employ a larger growth rate for each dense module to make D&amp;Y Encoder wider. This shallow but wide design is able to improve efficiency with little performance degradation in our case.</p><p>On top of that, to fully make use of the depth information, we add a dense block in a shallow position called Shallow Block right after the first Downsampler Block to extract more boundary information for efficaciously addressing the object localization issue in semantic segmentation. The benefits of Shallow Block will be shown in Section 3.3.</p><p>In order to reduce the defects of the captured depth maps used by D&amp;Y Encoder, we stack the luminance images with the depth maps as two-channel inputs. The luminance information can guide D&amp;Y Encoder to suppress the noise effects contained in the depth maps and extract valid information for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fusion Mechanism</head><p>We take the essence of the fusion idea introduced in FuseNet <ref type="bibr" target="#b5">[6]</ref> and further develop a more effective approach in our fusion-based LDFNet. According to FuseNet and our experimental results, a simple four channels stack cannot effectively extract information from the depth map. Hence, instead of simply appending the depth channel to the RGB channels, we adopt the design of two parallel sub-networks. However, different from FuseNet that simply uses an identical structure for both the main RGB sub-network and the D&amp;Y sub-network, our network adopts different architectures for them. The output features of each dense block in D&amp;Y Encoder is fused to RGB Encoder at the same resolution by the element-wise summation (see <ref type="figure" target="#fig_1">Figure 2</ref>). We also fuse the features after each transition layer, so there are five fusion operations in total. To allow this fusion process, the difference in the numbers of channels in the two encoders is eliminated by using properly the 1?1 convolution layers. Our fusion mechanism enables our network to integrate the multi-modal information in an efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIEMENTS</head><p>In this section, we conduct a series of experiments to evaluate the effectiveness of our network design choices and compare its performance with other schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>Our networks are trained by using Adam optimization <ref type="bibr" target="#b8">[9]</ref>. The L2 weight decay of the optimizer is set to 0.0001, and the batch size is set to 4. Also, due to the imbalance of pixels of each class presented in the dataset, a classical class weighting scheme defined in <ref type="bibr" target="#b12">[13]</ref> is employed: , where we set c to 1.1 in our case. The initial learning rate is 0.0005, and the poly learning rate policy <ref type="bibr" target="#b1">[2]</ref> is used. We also include the dropout layers <ref type="bibr" target="#b14">[15]</ref> at the end of each Non-Bottleneck and dense module in training with a rate of 0.05 as regularization. For our dense blocks, we set the growth rate to 42. Every convolutional layers are followed by a batch normalization layer <ref type="bibr" target="#b7">[8]</ref> and a ReLU. We also adopt data augmentation in training by using random horizontal flip and a translation of 0~2 pixels on both axes. The mean of intersection-over-union (mIoU) is the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset</head><p>We use Cityscapes dataset <ref type="bibr" target="#b2">[3]</ref>, which consists of 5,000 pixellevel finely annotated street scene images. The overall dataset is divided into three subsets: training, validation, and testing with 2,975, 500 and 1525 images, respectively. Totally, 19 classes such as building, road, and pedestrian are defined in the Cityscapes dataset. The testing data labels are unavailable, but we can evaluate our network on the online test server. The original dataset resolution is 1024?2048 and they are resized to 512?1024 for our training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation Study</head><p>We vary the network structure to see the performance of different network design choices. The experimental results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. First, ERFNet-Depth uses only the depth maps for prediction. The result indicates that the depth maps can provide a certain amount of information for this purpose, but its accuracy is low, compared to the RGB images. Then, we try two structures to process the depth information: 1) stacking the depth maps as the 4 th input channel, and 2) using a two-branch architecture. ERFNet-RGB uses the RGB input images only. ERFNet-Stack that simply stacks RGB and D channels produces similar results as ERFNet-RGB. In other words, the stack method cannot benefit from the additional depth information. By contrast, the proposed LDFNet achieves a significant improvement, a mIoU of 68.33%. The difference of the mIoU scores between our method and ERFNet-Stack shows that our fusion mechanism is a more effective design for depth information extraction. Proper use of the depth map can boost accuracy.</p><p>Next, we examine the capability of different structures in constructing D&amp;Y Encoder. Compared to LDFNet, LDFnon-Dense uses the ERFNet-based <ref type="bibr" target="#b13">[14]</ref> structure; that is, its D&amp;Y Encoder is identical to RGB Encoder. The results show that LDFNet can obtain a higher mIoU score with fewer parameters. Therefore, adopting the dense connectivity <ref type="bibr" target="#b3">[4]</ref> is a preferred solution.</p><p>We next confirm the advantages of using Shallow Block, which is located after the first Downsampler Block in the D&amp;Y Encoder. Both LDF-w/o-Shallow and LDF-58-w/o-Shallow discard Shallow Block, but LDF-58-w/o-Shallow increases the numbers of dense modules in its second and third dense blocks to 5 and 8 respectively. Compared to these two, LDFNet can achieve higher accuracy, even though LDF-58-w/o-Shallow has more modules in its deeper layers. Our reasoning is that the depth information has a strong correlation to the object edge, contour, and boundary information, so placing Shallow Block at the early stage is beneficial to extract these desired low-level features.</p><p>Furthermore, we would like to show the usefulness of using the luminance information in D&amp;Y Encoder. Because the depth maps produced by depth sensors like Kinect contain defects and the resolution of depth sensors is relatively small ) ln( / 1 class class p c ? ? ?    <ref type="bibr" target="#b1">[2]</ref> 44.0M no n/a PSPNet <ref type="bibr" target="#b19">[20]</ref> 65.7M no n/a Dilation10 <ref type="bibr" target="#b18">[19]</ref> 140.8M no 0.25 FCN-8s <ref type="bibr" target="#b11">[12]</ref> 134.5M no 2.0 SegNet <ref type="bibr" target="#b0">[1]</ref> 29.5M 4 16.7 LDFNet (ours) 2.31M 2 18.4 compared to its RGB counterpart, these sensor errors would lead to incorrect information be fused into RGB Encoder. After inserting the luminance information into the depth processing branch, the noise effects could be suppressed. The experimental results verify this conjecture; that is, comparing LDF-w/o-Y that does not use luminance information to LDFNet, there is a great improvement in the mIoU score. Finally, although LDFNet has only slightly more parameters than its backbone model, ERFNet, we would like to testify whether the improvement is coming from the proposed fusion mechanism or simply due to the increased parameters. Thus, we build LDF-RGB-RGB, which is identical to the LDFNet structure except that its inputs are two duplicate RGB images fed into the two branches respectively. Its accuracy is between ERF-RGB and LDFNet, demonstrating the increased parameters indeed provide some improvements, but our fusion mechanism of incorporation multi-modal information contributes significantly more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Results</head><p>We train LDFNet in two stages (both the training and validation data are included in training) for the final evaluation. First, we train only the two encoders by downsized labels. Second, we add the decoder together with the encoders in training. We do not use any testing tricks such as multi-crop and multi-scale testing in evaluations. In <ref type="table" target="#tab_1">Table  2</ref>, we report the results evaluated on the Cityscapes test set and the comparisons with the other state-of-art systems. LDFNet achieves a 71.3% mIoU score without any pretrained model and surpasses all the other methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref> designed for RGB-D semantic segmentation on this benchmark. Moreover, in <ref type="table" target="#tab_2">Table 3</ref>, LDFNet outperforms several stateof-art networks for the RGB semantic segmentation task in terms of efficiency, such as DeepLab <ref type="bibr" target="#b1">[2]</ref> and PSPNet <ref type="bibr" target="#b19">[20]</ref>. Even though LDFNet processes the extra depth information, the entire network has fewer parameters and maintains a faster inference speed. LDFNet can run on the resolution 512?1024 inputs at the speed of 18.4 and 27.7 frames per second (fps) on a single Titan X Maxwell and GTX 1080Ti respectively. Some visual results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this study, we propose a novel information-fused network, LDFNet, to incorporate luminance, depth, and color information for RGB-D semantic segmentation. LDFNet is able to effectively extract the features from both the RGB images and the depth maps to achieve a higher segmentation performance, while it maintains a rather low computational complexity. After conducting a series of experiments, we demonstrate the effectiveness of our design choices. LDFNet successfully outperforms the other state-of-the-art systems on an influential benchmark. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowchart of the proposed semantic segmentation system. Y: luminance information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed LDFNet architecture. The numbers in parentheses represent the number of channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Sample results of LDFNet on the Cityscapes validation set. From left to right: (a) RGB image, (b) depth map, (c) Ground truth, (d) LDFNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results on the Cityscapes validation set, comparing the proposed LDFNet with different design choices.</figDesc><table><row><cell>Method</cell><cell>RGB Inputs</cell><cell>Depth Maps</cell><cell>Y Info.</cell><cell>Shallow Block</cell><cell>Dense Connects</cell><cell>mIoU (%)</cell><cell>Parameters</cell></row><row><cell>ERFNet-Depth</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>47.48</cell><cell>1.97M</cell></row><row><cell>ERFNet-RGB</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65.59</cell><cell>1.97M</cell></row><row><cell>ERFNet-Stack</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>65.06</cell><cell>1.97M</cell></row><row><cell>LDF-non-Dense</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>66.53</cell><cell>2.95 M</cell></row><row><cell>LDF-w/o-Shallow</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>66.54</cell><cell>2.20 M</cell></row><row><cell>LDF-58-w/o-Shallow</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>65.93</cell><cell>2.42 M</cell></row><row><cell>LDF-w/o-Y</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>65.72</cell><cell>2.31M</cell></row><row><cell>LDF-RGB-RGB</cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>67.79</cell><cell>2.31M</cell></row><row><cell>LDFNet</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>68.48</cell><cell>2.31M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on the Cityscapes test set, comparing LDFNet with the other RGB-D methods.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell><cell>Speed (fps)</cell></row><row><cell>MultiBoost</cell><cell>59.3</cell><cell>4.0</cell></row><row><cell>Pixel-level Encoding [16]</cell><cell>64.3</cell><cell>n/a</cell></row><row><cell>Scale invariant CNN+CRF [10]</cell><cell>66.3</cell><cell>n/a</cell></row><row><cell>RGB-D FCN</cell><cell>67.4</cell><cell>n/a</cell></row><row><cell>LDFNet (ours)</cell><cell>71.3</cell><cell>18.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of model efficiency with RGB methods. Sub: the amount of subsampling used by the method at test time.</figDesc><table><row><cell>Method</cell><cell>Parameters Sub</cell><cell>Speed (fps)</cell></row><row><cell>DeepLabv2</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic imae segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">W</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusionbased cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional scale invariance for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Causevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map enhancement on rgbd video captured by kinect v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association (APSIPA) Annual Summit and Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02417</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth map enhancement based on its associated high-resolution rgb video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Advanced Image Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
