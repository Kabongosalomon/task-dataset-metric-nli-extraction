<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">C 4 Net: Contextual Compression and Complementary Combination Network for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazarapet</forename><surname>Tunanyan</surname></persName>
							<email>hazarapet.tunanyan@picsart.com</email>
							<affiliation key="aff0">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">C 4 Net: Contextual Compression and Complementary Combination Network for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning solutions of the salient object detection problem have achieved great results in recent years. The majority of these models are based on encoders and decoders, with a different multi-feature combination. In this paper, we show that feature concatenation works better than other combination methods like multiplication or addition. Also, joint feature learning gives better results, because of the information sharing during their processing. We designed a Complementary Extraction Module (CEM) to extract necessary features with edge preservation. Our proposed Excessiveness Loss (EL) function helps to reduce false-positive predictions and purifies the edges with other weighted loss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding flow (G) makes the prediction more accurate by providing high-level complementary information to shallower layers. Experimental results show that the proposed model outperforms the state-of-theart methods on all benchmark datasets under three evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection (SOD) aims to detect the most visually attractive parts of images and videos, which is widely used in many applications like visual tracking <ref type="bibr" target="#b14">[15]</ref>, image retrieval <ref type="bibr" target="#b6">[7]</ref>, content-aware image editing <ref type="bibr" target="#b2">[3]</ref>, robot navigation <ref type="bibr" target="#b3">[4]</ref>, and many others. For many years, researchers have proposed some solutions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>, which are based on hand-crafted features (e.g. color, contrast, and texture), however, these approaches can not take into account high-level semantic information, which is a big restriction for the solution of the problem. The recent development of deep convolutional neural networks (CNNs) demonstrated powerful capabilities in terms of the high (e.g. class, position) and low (e.g. high-frequency data, detailed information) level feature extraction, which promotes better results of the salient object detection problem. Despite good performance achievements, there is still a large place for improvements. As many modern solutions propose U-Net-like architectures to solve the problem, the feature combination and their processing methods have big potential to improve. Many models observe feature fusing methods with different aggregation functions like multiplication and addition. We find these methods have some drawbacks, because of the choice of aggregation functions. Also, these models process the features of the encoder and decoder separately, which promotes cross-necessary information loss. The processed information of encoders and decoders is complementary to each other and they need to be managed together.</p><p>By analyzing the visual results of different models, we have noticed there are some incorrect predictions like random holes or excessive parts. These types of issues often come from the lack of high-level semantic information. After a couple of comprehensive experiments, we have come up with a modification of the pyramid-pooling module <ref type="bibr" target="#b12">[13]</ref> and called it a pyramid-semantic module, which contains multi-scale context-aware feature representation and channel-wise shift and attention. We proposed a complementary extraction module to combine and process low and high-level information by taking into account the feature representations of the edges. Contextual compression module is made to compress the connections between encoder and decoder, to maintain necessary features, and make the processing faster. Also, our proposed loss function helps to minimize false predictions and is formulated by false-positive values for each pixel. So our contributions are the following.</p><p>? Proposed a family of C 4 Net architectures with different complexities and performance.</p><p>? Proposed the Complementary Extraction Module to combine and process low and high-level information with edge preservation.</p><p>? Proposed Excessiveness Loss function to minimize false-positive predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>During recent decades, researchers developed algorithms for the salient object detection problem, which are based on hand-crafted features (e.g. color or texture) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>. Most of these traditional methods have been surpassed by convolutional neural networks (CNN) in terms of quality and speed. Deep convolutional networks are capable to extract necessary semantic information and combine them properly. By bringing all known approaches together, we can formulate the following common factors. Features Processing Methods. As the majority of the recent solutions are based on U-Net-like architectures, one of the most important parts is the feature processing part at every layer of the decoder. Zuyao Chen et al. <ref type="bibr" target="#b1">[2]</ref> considered separate processing. They applied multiplication operation between the encoder's and decoder's features, then concatenated the results of different branches. Jun Wei et al. <ref type="bibr" target="#b22">[23]</ref> also proposed almost the same approach, where their model separately processes encoder's and decoder's features, multiplies them as a fusing operation then they apply an addition operation on it. They considered that the encoder's features are low-level representations and contain detailed information. In contrast, the decoder's features are high-level representations and contain rich semantic information. By multiplying them, they clean the information and add complementary features for high-quality detection. We find these approaches have some drawbacks, because of the separate processing of high and low-level features, which causes cross-necessary information loss. Edge Preservation Approaches. The salient object detection problem requires a solution with high-quality detection, especially on edges, which need to be purified and exquisite.</p><p>To maintain the high quality of edges, Jiang-Jiang Liu et al. <ref type="bibr" target="#b12">[13]</ref> apply additional edge detection on the middle layers of the decoder. Mengyang Feng et al. <ref type="bibr" target="#b5">[6]</ref> proposed the Boundary-Enhanced Loss function for shallower layers of the network, which is responsible for extracting the features and purifying the edges. All these methods tend to improve the quality of edges by using additional and separated edgespecific processing. We find that the core loss functions also must be adjusted for this manner and proposed weighted losses, where edge pixels have high weights. A similar solution has been done in methods like <ref type="bibr" target="#b22">[23]</ref>. Global Features Extraction. As we have mentioned in the previous sections, one of the advantages of deep learning solutions is the capability of semantic information extraction, but the majority of these approaches do not have direct ties between global semantic modules and shallower layers. Jiang-Jiang Liu et al. <ref type="bibr" target="#b12">[13]</ref> and Zuyao Chen et al. <ref type="bibr" target="#b1">[2]</ref> gave solutions for this manner. They designed a module for high-level semantic information and a global guiding flow to distribute the information in top layers. Our contribution tends to improve global information extraction by channelwise attention and shifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we will describe the architecture of our model and all consisting parts of it. Before going into details, we first need to refer to the feature combination approach. As U-Net-like architectures have shortcut connections between encoder and decoder, it is very important how their feature representations will be combined. Based on the conducted experiments and the design choice of our model, we found out that joint processing of encoder's and decoder's features works better than the branched-separated approach. Also, the concatenation aggregation function for skip connections and feature combinations leads better results compared with other aggregation functions like addition and multiplication. The details about those approaches can be found in the ablation study (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contextual Compression Module</head><p>As we noted in the previous section, shortcut connections between encoder and decoder are crucial for highquality detection and exquisite boundaries. We defined a new term called Compression Factor (CF) to compress half of channels of the network, which starts from the shortcut connections.</p><formula xml:id="formula_0">f l = ?(BN (Conv cf (f e l )))<label>(1)</label></formula><p>where f e l is the feature representation of the encoder, Conv cf is a convolution layer with cf filters ({32, 64, 128} in our experiments), BN is a batch normalization layer, ? is the ReLU activation function. By conducting a couple of comprehensive experiments for shortcut connections, we came up with the following conclusion.</p><p>? Without any additional compression block, the encoder provides noisy information. The experimental results are reported in the fifth and last row of <ref type="table">Table 2</ref>.</p><p>? By using feature compression for half of the network, we have increased the speed of training and testing regimes, also, optimized the memory allocation of it.</p><p>We use the same compression factor for all layers of the CCM module and for the decoder of our network. <ref type="table">Table 1</ref> contains an ablation study for different compression factors and <ref type="table">Table 2</ref> shows the effectiveness of the CCM module. Each next row of <ref type="table">Table 2</ref> either is built on top of the previous row or is replaced with the mentioned module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pyramid-Semantic Module</head><p>We have noted the importance of low-level information, but high-level information is also very important for better detection. We have modified the PPM module proposed in <ref type="bibr" target="#b12">[13]</ref> and designed the Pyramid-Semantic module, which is responsible to handle and maintain high-level information. PSM consists of two main parts, which are visualized in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Encoder The first part is responsible for pyramid-feature extraction, which is made by branched pooling operation and the second part is a channel-wise attention module, which scales and shifts feature representations by channels. For the first part of this module, we use average pooling operation to get multi-resolution pyramid features. Also, It has an identical branch, which lets to maintain high-level semantic information of the same resolution.</p><formula xml:id="formula_1">f (j) h = U p(Conv(P ool k (f l )))<label>(2)</label></formula><p>where f l is the input feature representation of deeper layers provided by the CCM module, P ool k is the average pooling operation with kernel size k. For each branch (j ? {2, 3, 4}) we use different kernel sizes: {10, 5, 2}. Conv is a combination of convolution, batch normalization, and a ReLU activation functions. U p is a bilinear upsampling operation. At the end of the first part, all four branches get concatenated into one feature representation. The second part is the channel-wise attention module, which scales and shifts features. Let f h ? R H?W ?C be the input features. We apply global pooling operation on f h and getf ? R C .</p><formula xml:id="formula_2">w = ?(f c 2 (?(f c 1 (f , W (w) 1 )), W (w) 2 )) v = ?(f c 2 (?(f c 1 (f , W (v) 1 )), W (v) 2 )) (3) f h = w f h ? v, w, v ? R C<label>(4)</label></formula><p>where f c 1 and f c 2 are fully connected layers, W (w) and W (v) are weights matrices for scaling and shifting respectively. ? is the ReLU activation function, and ? is the Sigmoid activation function, is channel-wise multiplication and ? is channel-wise addition function.</p><p>To verify the effectiveness of our modification, we have conducted two other experiments as well with other similar solutions like Pyramid-Pooling Module (PPM) <ref type="bibr" target="#b12">[13]</ref> and Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b0">[1]</ref>. <ref type="table">Table 2</ref> shows that our method outperforms other pyramid-based approaches mainly because of the channel-wise shift and attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Complementary Extraction Module</head><p>As we have referred to the structure of feature processing approach at the beginning of Section 3, it is crucial to choose a right extraction mechanism and a combination function, thus we have decided to choose the joint processing method (PipeMode) with concatenation combination function, as they perform better for the model design we have chosen (see Section 4.4). As a summary, we have designed a module, which is responsible for three different feature extraction. f l ? R H?W ?C is the low-level feature representation, which contains rich details with noise, in contrast to f h ? R H?W ?C , which is a high-level feature and does not contain rich details for exquisite detection, but it is noisy-free. f g ? R H?W ?C , where f g = f 4 h + f 5 h is a global guidance flow using the fourth and fifth layer's features, which helps to complement high-level semantic information in shallow layers. As we seek high-quality detection, we also need to reduce the error on edges, because edges contain the most errors of the detection as shown in <ref type="bibr" target="#b23">[24]</ref>. To adjust the model for multi-level supervision, we apply a convolution layer with the sigmoid function on the output of the previous layer and get S (i) binary mask. We compute edges by dilating and eroding the S (i) mask, then apply pixel-wise multiplication with f l . f edge will be the edge feature representation. We concatenate all four features, f l , f h , f edge and f g and feed into a convolution block with six convolutions, batch normalization, ReLU layers, and a skip connection. <ref type="figure" target="#fig_1">Figure 2</ref> contains the visualization of our proposed CEM module and <ref type="table">Table 2</ref> shows its effectiveness with three different metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Excessiveness Loss Function</head><p>The binary cross-entropy function is one of the most widely used loss functions in salient object detection problem, however, the equal treatment of pixels and the ignorance of global structures make the function not effective, so we have used the approach proposed by Jun Wei et al. <ref type="bibr" target="#b22">[23]</ref>. First, we have used weighted losses by adding pixel-wise weight matrix to increase the importance of the edges.</p><formula xml:id="formula_3">? i,j = 1 +? 1 N k,k ? =1,j=1</formula><p>Gt? ,j ? Gt i,j  <ref type="table">Table 3</ref>. Performance comparison with 11 state-of-the-art methods on 5 benchmark datasets. MAE (smaller is better), mean F-measure (mF larger is better), E-measure (E ? larger is better) metrics used to evaluate the results. All models are based on ResNet backbones. The best and the second best results among models with ResNet50 backbones highlighted in green and blue respectively. The red color indicates the best results of our model with ResNet101 backbone. where Gt is the ground truth mask,? is a hyper parameter, |.| is the absolute value, N = k ? k is the number of pixels of the window with kernel size k.</p><formula xml:id="formula_5">L i,j bce = Gt i,j * log (S i,j ) + (1 ? Gt i,j ) * log (1 ? S i,j ) (6) L wbce = ? W,H i=1,j=1 ? i,j * L i,j bce W,H i=1,j=1 ? i,j<label>(7)</label></formula><p>where S ij is the prediction of i, j-th pixel and W, H is the width and height of the output. As we seek a solution with structural preservation of the objects, we use the following loss.</p><formula xml:id="formula_6">L wiou = 1 ? W,H i=1,j=1 S i,j * Gt i,j * ? i,j W,H i=1,j=1 (S i,j + Gt i,j ? S i,j * Gt i,j ) * ? i,j<label>(8)</label></formula><p>L wiou is the weighted intersection over union loss function, which handles global structures of the foreground object and increases the impact of the pixels near the edges. To improve the detection results, we proposed a new loss function, which is called the Excessiveness Loss function. We observe false positive (F P ), false negative (F N ), and true positive (T P ) predictions for each example. By analyzing the error for these values, we found out that the error mostly accumulates by excessive predictions, which means in most cases, F P is higher than F N . <ref type="table" target="#tab_4">Table 4</ref> contains an ablation study, where mF P and mF N are normalized by the resolution of the output mask. Our proposed weighted excessiveness loss function is</p><formula xml:id="formula_7">?T P = W,H i=1,j=1 S i,j * Gt i,j * ? i,j ?F P = W,H i=1,j=1 ?(S i,j ? Gt i,j ) * ? i,j<label>(9)</label></formula><p>L wel = ?F P ?F P + ??T P <ref type="bibr" target="#b9">(10)</ref> where ?F P , ?T P are weighted false positive and true positive respectively and they are differentiable, ? is the ReLU activation function, ? is a hyper-parameter. The overall loss of our model is a weighted combination of these three loss functions.</p><formula xml:id="formula_8">L = L (1) wel + 5 i=1 1 2 i?1 (L (i) wbce + L (i) wiou )<label>(11)</label></formula><p>where L </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>To evaluate our proposed method, we have used five popular benchmark datasets, including ECSSD <ref type="bibr" target="#b19">[20]</ref> with 1000 images, PASCAL-S <ref type="bibr" target="#b28">[29]</ref> with 850 images, HKU-IS <ref type="bibr" target="#b8">[9]</ref> with 4447 images, DUT-OMRON <ref type="bibr" target="#b27">[28]</ref> with 5168 images, and DUTS <ref type="bibr" target="#b21">[22]</ref> with 15572 images. DUTS is currently the biggest salient object detection dataset and contains 10553 training and 5019 testing examples. We used only the DUTS-Train dataset for training and others for testing. Three metrics are used to evaluate the performance of our model and other state-of-the-art methods. The first metric is the Mean Absolute Error (MAE), which is one of the most commonly used metrics for salient object detection, as shown in Eq. 12. Another widely used metric of the SOD problem is the mean F-measure (mF ) which is formulated with precision and recall as the F ? score, where ? = 0.3. We also use the E-measure (E ? ) <ref type="bibr" target="#b4">[5]</ref> metric, which uses the combination of local pixel values and their means to evaluate the similarity between prediction and ground truth masks.  where P is the prediction and Gt is the ground truth mask.</p><formula xml:id="formula_9">M AE = 1 W ? H W i=1 H j=1 |P i,j ? Gt i,j |<label>(12)</label></formula><p>To show the robustness of our proposed method, we also plot the Precision-Recall (PR) curve, which is calculated by sliding the threshold from 0 to 1. The larger the area under the PR curve, the better is the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use DUTS-Train as the training dataset, with randomly cropping and horizontal flipping augmentation techniques. Different architectures of ResNet <ref type="bibr" target="#b9">[10]</ref> pre-trained on ImageNet are used as the encoder and other parts of the model are initialized randomly from the uniform distribution. We use an adaptive learning rate with a maximum value of 0.005 for the encoder and 0.05 for other parts. The network is trained with stochastic gradient descent (SGD) with 0.9 momentum and 0.0005 weight decay parameter values. The batch size is set to 20 with 50 epochs. Our network is implemented with Pytorch v1.6 and the training and testing processes are conducted on an Nvidia RTX 2080 ti GPU and Intel Core i9-9900k CPU device. The performance report of our proposed models can be found in <ref type="figure" target="#fig_0">Figure 1</ref>, where our fastest model has about 11ms (90 fps) inference time by surpassing other state-of-the-art solutions. All images are resized to 320?320 during training and testing, without any post-processing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Arts</head><p>We compare our proposed algorithm with 11 state-ofthe-art methods, including PiCANet-R <ref type="bibr" target="#b15">[16]</ref>, DGRL <ref type="bibr" target="#b20">[21]</ref>, EGNet <ref type="bibr" target="#b11">[12]</ref>, SCRN <ref type="bibr" target="#b25">[26]</ref>, CPD <ref type="bibr" target="#b24">[25]</ref>, BASNet <ref type="bibr" target="#b18">[19]</ref>, PoolNet <ref type="bibr" target="#b12">[13]</ref>, MINet <ref type="bibr" target="#b16">[17]</ref>, LDF <ref type="bibr" target="#b23">[24]</ref>, GCPANet <ref type="bibr" target="#b1">[2]</ref>   in <ref type="figure" target="#fig_0">Figure 1</ref>. Some other SOTA models like MINet <ref type="bibr" target="#b16">[17]</ref> or BASNet <ref type="bibr" target="#b18">[19]</ref> are either too slow or too complex and are out of the chosen window. Also, the PR and F ? curves show the robustness of our models on the mentioned datasets. Qualitative Comparison. The visual comparison examples are shown in <ref type="figure" target="#fig_5">Figure 5</ref>, where our model has better quality on the edges among all algorithms, because of our weighted loss functions for all layers and the complimentary edge-feature extraction in our CEM module, which helps to get exquisite boundaries. Our model is able to detect narrow parts and recover lost information. Another visible improvement is minimal false predictions, especially false-positive values. Our proposed EL function is able to minimize false-positive areas and maximize the true positive predictions, which is visible in the comparison figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Features Combination Methods. Modern architectures of deep learning solutions for the SOD problem are based on encoders, decoders, and shortcut connections between them. Each layer of the decoder is responsible for combining features of decoder and encoder. There are two main approaches to using that information. The first one is by simply concatenating them, the second one is to fuse them by using other functions like multiplication or addition. Another very important thing is the features processing structure at every layer of the decoder. Methods like <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> pre-fer to process encoder's and decoder's features separately and then fuse them by multiplication or addition. We tried to find out the answers to two main questions ? How the features of the encoder and decoder need to be processed. (joint or separated)</p><p>? How they need to be combined. (fusing or concatenating)</p><p>To answer these questions, we proposed two main structures of a decoder's layer <ref type="figure" target="#fig_6">Figure 6</ref>. We designed a PipeMode layer, which is a joint processing of two feature representations, and BranchedMode layer, which is separated processing of the features. They both contain two aggregation functions R 1 and R 2 . The features of encoding layers, f (i) l contain low-level information like edges, tiny areas, and high-frequency data, which is crucial for high-quality detection, especially on edges and they contain noisy information. The feature representations of decoding layers, f (i) h contain noisy free high-level information like class, position, or shape of the object. These features representations helped Jun Wei et al. <ref type="bibr" target="#b22">[23]</ref> to propose a solution like our BranchedMode, where they used the multiplication function to fuse high and lowlevel features and to clean noisy parts, then they added a skip-like connection as complementary information. We find this approach has some drawbacks, because of the choice of the structure and aggregating functions. To find out the real behavior of these two approaches, we made different experiments with these structures by using different aggregating functions. We designed a simple architecture with ResNet50 backbone, our proposed structures for the decoder, and shortcut connections between them. The BCE loss function was used on top of the decoder. Each model was trained three times with randomly initialized weights and the result was calculated by taking the average of the best performance at each run. <ref type="table">Table 5</ref> contains reports of our experiments. Based on those results and the architecture choice, we can say:</p><p>? In general, PipeMode gives better results than BranchedMode, which shows that the sharing of information about different features leads to better results.</p><p>? The concatenation aggregation function works better for both structures.</p><p>PipeMM results are missing in <ref type="table">Table 5</ref>, because it disturbs the training of the model with activation values pushed to zero <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new solution for the salient object detection problem. Our ablation study contains an investigation about feature combination approaches, where we showed that joint learning with pipe-mode works better than branched-mode. Also, based on our model design, the feature concatenation gives better results for both structures. Our proposed solution is able to extract and preserve feature representations on edges (Complementary Extraction Module, CEM) with high-level semantic information (Pyramid-Semantic Module, PSM), which leads to high-quality detection. Also, the proposed weighted Excessiveness Loss (EL)  <ref type="table">Table 5</ref>. Results of our proposed structures on DUTS-Test dataset with different aggregating functions, where Plus is addition, Mul is multiplication and Cat is concatenation. The green is the overall best result, red is the overall worst result, blue is the best result among BranchedModes, orange is the best result among PipeModes. function helps to minimize false prediction values. Each module leads to significant improvement, which is shown in <ref type="table">Table 2</ref>. The comparison with 11 state-of-the-art methods shows that our approach outperforms other solutions on all benchmark datasets under three evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Performance comparison between our proposed and other state-of-the-art methods by their complexity, latency (ms) and MAE results on DUTS-Test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An overview of our proposed C 4 Net architecture. The model is based on a ResNet [10] encoder with multilevel supervision S (i) . Contextual Compression Module (CCM) is used as compressed shortcut connection between encoder and decoder. Pyramid-Semantic Module (PSM) is used to extract high level semantic information, which is also used in Global Guidance flow (G) and Complementary Extraction Module (CEM) is used to combine three feature representations from the encoder, decoder and guiding flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our Pyramid-Semantic Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of PR curves (the first row), F-measure curves (the second row) on 5 benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>wiou are the corresponding loss functions for i-th layer. To verify the effectiveness of EL loss function, we show the results of two-way experiments in the first and last group ofTable 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison of the proposed model with other state-of-the-art methods. Our model has minimal false prediction and the edges are more exquisite than others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>An overview of our proposed structures for a layer of a decoder. (a) is the joint-features module and we called it PipeMode and (b) is the separated-features module and we called it BranchedMode. R1 and R2 are aggregation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MAE mF E ? MAE mF E ? MAE mF E ? MAE mF E ? PiCANet (CVPR2018) .046 .919 .951 .043 .900 .947 .075 .831 .893 .065 .759 .860 .050 .828 .909 DGRL (CVPR2018) .045 .910 .945 .037 .897 .947 .074 .819 .882 .063 .738 .848 .051 .802 .892 EGNet (ICCV2019) .041 .933 .953 .031 .917 .956 .074 .833 .885 .053 .767 .857 .039 .856 .915 SCRN (ICCV2019) .037 .935 .954 .034 .917 .953 .063 .850 .902 .056 .772 .863 .039 .860 .915 CPD (CVPR2019) .037 .923 .950 .034 .905 .948 .070 .828 .884 .056 .754 .850 .043 .836 .906 BASNet (CVPR2019) .037 .927 .950 .032 .914 .950 .076 .824 .879 .056 .773 .864 .047 .832 .897 PoolNet (CVPR2019) .038 .931 .955 .030 .917 .955 .065 .846 .900 .054 .756 .849 .036 .854 .917 MINet (CVPR2020) .034 .931 .956 .029 .918 .959 .064 .836 .896 .056 .764 .861 .037 .854 .920 LDF (CVPR2020) .034 .930 .926 .027 .914 .954 .060 .848 .866 .052 .773 .862 .034 .855 .910 GCPANet (AAAI2020) .035 .929 .954 .031 .917 .956 .063 .840 .898 .057 .767 .857 .038 .858 .919 F 3 Net (AAAI2020) .033 .925 .930 .028 .912 .954 .062 .834 .886 .053 .770 .862 .035 .842 .903 C 4 Net-C4 (Ours) .029 .939 .957 .027 .925 .956 .056 .854 .903 .051 .776 .863 .030 .875 .929 C 4 Net-C5 (Ours) .030 .939 .956 .025 .931 .961 .055 .861 .904 .047 .788 .865 .029 .886 .937</figDesc><table><row><cell></cell><cell>ECSSD</cell><cell>HKU-IS</cell><cell>PASCAL-S</cell><cell>DUT-OMRON</cell><cell>DUTS-Test</cell></row><row><cell>Algorithms</cell><cell>1,000 images</cell><cell>4,447 images</cell><cell>850 images</cell><cell>5,168 images</cell><cell>5,019 images</cell></row><row><cell></cell><cell>MAE mF E ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of EL contribution of C 4 Net-C3 architecture on DUTS-Test dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>BranchPP Plus Plus 0.0356 0.8514 0.9162 BranchMM Mul Mul 0.0358 0.8496 0.9165 BranchCC Cat Cat 0.0353 0.8535 0.9168 BranchMP Mul Plus 0.0353 0.8509 0.9152</figDesc><table><row><cell>Name</cell><cell>R 1</cell><cell>R 2</cell><cell>M AE</cell><cell>mF</cell><cell>E ?</cell></row><row><cell>PipePP</cell><cell cols="5">Plus Plus 0.0348 0.8528 0.9164</cell></row><row><cell>PipeMM</cell><cell cols="2">Mul Mul</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PipeCC</cell><cell>Cat</cell><cell cols="4">Cat 0.0342 0.8512 0.9171</cell></row><row><cell>PipeCP</cell><cell cols="5">Cat Plus 0.0347 0.8520 0.9159</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global context-aware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Repfinder: Finding approximately repeated scene elements for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Lue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Environment exploration for object-based visual saliency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Craye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goudou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2303" to="2309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3-d object retrieval and recognition with hypergraph analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4290" to="4303" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guanbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yizhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Jia-Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Jiang-Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Deng-Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ju-Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ming-Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for realtime salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency-based discriminant tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1007" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu Nian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Junwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ming-Hsuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Qiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Tiantian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Lihe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Shuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Huchuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruan</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borji</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">F3net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Label decoupling framework for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan Ruan Xiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou</forename><surname>Xiaodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Rehg</forename><surname>Koch Christof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille Alan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
