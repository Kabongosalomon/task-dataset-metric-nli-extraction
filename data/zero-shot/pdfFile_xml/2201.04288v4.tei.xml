<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiview Transformers for Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Yan</surname></persName>
							<email>yanshen6@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research ?</orgName>
								<orgName type="institution">Michigan State University ? Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research ?</orgName>
								<orgName type="institution">Michigan State University ? Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<email>aarnab@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research ?</orgName>
								<orgName type="institution">Michigan State University ? Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research ?</orgName>
								<orgName type="institution">Michigan State University ? Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
							<email>mizhang@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research ?</orgName>
								<orgName type="institution">Michigan State University ? Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
							<email>chensun@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research ?</orgName>
								<orgName type="institution">Michigan State University ? Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<email>cordelias@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research ?</orgName>
								<orgName type="institution">Michigan State University ? Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multiview Transformers for Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video understanding requires reasoning at multiple spatiotemporal resolutions -from short fine-grained motions to events taking place over longer durations. Although transformer architectures have recently advanced the stateof-the-art, they have not explicitly modelled different spatiotemporal resolutions. To this end, we present Multiview Transformers for Video Recognition (MTV). Our model consists of separate encoders to represent different views of the input video with lateral connections to fuse information across views. We present thorough ablation studies of our model and show that MTV consistently performs better than single-view counterparts in terms of accuracy and computational cost across a range of model sizes. Furthermore, we achieve state-of-the-art results on six standard datasets, and improve even further with largescale pretraining. Code and checkpoints are available at: https://github.com/google-research/scenic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision architectures based on convolutional neural networks (CNNs), and now more recently transformers, have made great advances in numerous computer vision tasks. A central idea, that has remained constant across classical methods based on handcrafted features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref> to CNNs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b83">84]</ref> and now transformers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b72">73]</ref>, has been to analyze input signals at multiple resolutions.</p><p>In the image domain, multiscale processing is typically performed with pyramids as the statistics of natural images are isotropic (all orientations are equally likely) and shift invariant <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b65">66]</ref>. To model multiscale temporal information in videos, previous approaches such as SlowFast <ref type="bibr" target="#b22">[23]</ref> have processed videos with two streams, using a "Fast" stream operating at high frame rates and a "Slow" stream at low frame rates, or employed graph neural networks to model long-range interactions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b75">76]</ref>. * This work was done while the first author was an intern at Google.  <ref type="figure">Figure 1</ref>. Overview of our Multiview Transformer. We create multiple input representations, or "views", of the input, by tokenizing the video using tubelets of different sizes (for clarity, we show two views here). These tokens are then processed by separate encoder streams, which include lateral connections and a final global encoder to fuse information from different views. Note that the tokens from each view may have different hidden sizes, and the encoders used to process them can vary in architecture too.</p><p>When creating a pyramidal structure, spatio-temporal information is partially lost due to its pooling or subsampling operations. For example, when constructing the "Slow" stream, SlowFast <ref type="bibr" target="#b22">[23]</ref> subsamples frames, losing temporal information. In this work, we propose a simple transformerbased model without relying on pyramidal structures or subsampling the inputs to capture multi-resolution temporal context. We do so by leveraging multiple input representations, or "views" of the input video. As shown in <ref type="figure">Fig. 1</ref>, we extract tokens from the input video over multiple temporal durations. Intuitively, tokens extracted from long time intervals capture the gist of the scene (such as the background where the activity is taking place), whilst tokens extracted from short segments can capture fine-grained details (such as the gestures performed by a person).</p><p>We propose a multiview transformer ( <ref type="figure">Fig. 1)</ref> to pro-cess these tokens, and it consists of separate transformer encoders specialized for each "view", with lateral connections between them to fuse information from different views to each other. We can use transformer encoders of varying sizes to process each view, and find that it is better (in terms of accuracy/computation trade-offs) to use a smaller encoder (e.g. smaller hidden sizes and fewer layers) to represent the broader view of the video <ref type="figure">(Fig. 1 left)</ref> while an encoder with larger capacity is used to capture the details <ref type="figure">(Fig. 1 right)</ref>. This design therefore poses a clear contrast to pyramid-based approaches where model complexity increases as the spatio-temporal resolution decreases. Our design is verified by our experiments which show clear advantages over the former approach.</p><p>Our proposed method, of processing different "views" of the input video is simple, and in contrast to previous work <ref type="bibr" target="#b22">[23]</ref> generalizes readily to a variable number of views. This is significant, as our experiments show that accuracy increases as the number of views grows. Although our proposed architecture increases the number of tokens processed by the network according to the number of input views, we show that we can consistently achieve superior accuracy/computation trade-offs compared to the current state of the art <ref type="bibr" target="#b2">[3]</ref>, across a spectrum of model sizes, ranging from "Small" to "Huge". We show empirically that this is because processing more views in parallel enables us to achieve larger accuracy improvements than increasing the depth of the transformer network. We perform thorough ablation studies of our design choices, and achieve state-ofthe-art results on six standard video classification datasets. Moreover, we show that these results can be further improved with large-scale pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Evolution of video understanding models.</p><p>Early works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b71">72]</ref> relied on hand-crafted features to encode motion and appearance information. With the emergence of large labelled datasets like ImageNet <ref type="bibr" target="#b15">[16]</ref>, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b38">[39]</ref> showed their superiority over the classic methods. Since AlexNet <ref type="bibr" target="#b35">[36]</ref> won the Ima-geNet challenge by a large margin, CNNs have been quickly adopted to various vision tasks, their architectures have been refined over many generations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b62">63]</ref> and later improved by Neural Architecture Search (NAS) <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b84">85]</ref>. At the same time, CNNs and RNNs have quickly become the de-facto backbones for video understanding tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57]</ref>. Since the release of the Kinetics dataset <ref type="bibr" target="#b32">[33]</ref>, 3D CNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b67">68]</ref> have gained popularity, and many variants <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b77">78]</ref> have been developed to improve the speed and accuracy. Convolution operations can only process one local neighborhood at a time, and consequently, transformer blocks <ref type="bibr" target="#b70">[71]</ref> have been inserted into CNNs as additional layers to improve modeling of long range interactions among spatio-temporal features <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref>. Although achieving great success in natural language <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b52">53]</ref>, pure transformer architectures had not gained the same popularity in computer vision until Vision Transformers (ViT) <ref type="bibr" target="#b17">[18]</ref>. Inspired by ViT, ViViT <ref type="bibr" target="#b2">[3]</ref> and Timesformer <ref type="bibr" target="#b5">[6]</ref> were the first two works that successfully adopted a pure transformer architecture for video classification, advancing the state of the art previously set by 3D CNNs.</p><p>Multiscale processing in computer vision. "Pyramid" structures <ref type="bibr" target="#b0">[1]</ref> are one of the most popular multiscale representations for images and have been key in the early computer vision works, where their use has been widespread in multiple domains including feature descriptors <ref type="bibr" target="#b44">[45]</ref>, feature tracking <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46]</ref>, image compression <ref type="bibr" target="#b8">[9]</ref>, etc. This idea has also been successfully adopted for modern CNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b62">63]</ref> where the spatial dimension of the network is gradually reduced while the network "depth" is gradually increased to encode more semantically rich features. Also, this technique has been used to produce higher resolution output features for downstream tasks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b83">84]</ref>. Multiscale processing is necessary for CNNs because a convolution operation only operates on a sub-region of the input and a hierarchical structure is required to capture the whole view of the image or video. In theory, such a hierarchy is not required for transformers as each token "attends" to all other positions. In practice, due to the limited amount of training data, applying similar multiscale processing in transformers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b72">73]</ref> to reduce complexity of the model has proven to be effective.</p><p>Our model does not follow the pyramid structure but directly takes different views of the video and feeds them into cross-view encoders. As our experiments validate, this alternative multiview architecture has consistently outperformed its single-view counterpart in terms of accuracy/FLOP trade-offs. This is because processing more views in parallel gives us larger accuracy improvements than increasing the depth of the transformer network. Significantly, such improvement persists as we scale the model capacity to over a billion parameters (e.g., our "Huge" model), which has not been shown by the previous pyramidstructured transformers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b72">73]</ref>. Conceptually, our method is most comparable to SlowFast <ref type="bibr" target="#b22">[23]</ref> where a twostream CNN is used to process two views of the same video clip (densely sampled and sparsely sampled frames). Instead of sampling the input video at different frame rates, we obtain different view by linearly projecting spatiotemporal "tubelets" <ref type="bibr" target="#b2">[3]</ref> of varying sizes for each view. Furthermore, we empirically show that our proposed method outperforms <ref type="bibr" target="#b22">[23]</ref> when using transformer backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multiview Transformers for Video</head><p>We begin with an overview of vision transformer, ViT <ref type="bibr" target="#b17">[18]</ref>, and its extension to video, ViViT <ref type="bibr" target="#b2">[3]</ref>, which our model is based on, in Sec. 3.1. As shown in <ref type="figure">Fig. 1</ref>, our model constructs different "views" of the input video by extracting tokens from spatio-temporal tubelets of varying dimensions (Sec. 3.2). These tokens are then processed by a multiview transformer, which incorporates lateral connections to efficiently fuse together information from multiple scales (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries: ViT and ViViT</head><p>We denote our input video as V ? R T ?H?W ?C . Transformer architectures <ref type="bibr" target="#b70">[71]</ref> process inputs by converting inputs into discrete tokens which are subsequently processed by multiple transformer layers sequentially.</p><p>ViT <ref type="bibr" target="#b17">[18]</ref> extracts tokens from images by partitioning an image into non-overlapping patches and linearly projecting them. ViViT <ref type="bibr" target="#b3">[4]</ref> extends this to video by extracting N non-overlapping, spatio-temporal "tubes" <ref type="bibr" target="#b2">[3]</ref> from the input video,</p><formula xml:id="formula_0">x 1 , x 2 , . . . x N ? R t?h?w?c where N = T t ? H h ? W w .</formula><p>Each tube, x i , is then projected into a token, z i ? R d by a linear operator E, as z i = Ex i . All tokens are then concatenated together to form a sequence, which is prepended with a learnable class token z cls ? R d <ref type="bibr" target="#b16">[17]</ref>. As transformers are permutation invariant, a positional embedding p ? R (N +1)?d , is also added to this sequence. Therefore, this tokenization process can be denoted as</p><formula xml:id="formula_1">z 0 = [z cls , Ex 1 , Ex 2 , . . . , Ex N ] + p.<label>(1)</label></formula><p>Note that the linear projection E can also be seen as a 3D convolution with a kernel of size t ? h ? w and stride of (t, h, w) in the time, height and width dimensions respectively. The sequence of tokens z is then processed by a transformer encoder consisting of L layers. Each layer, , is applied sequentially, and consists of the following operations,</p><formula xml:id="formula_2">y = MSA LN z ?1 + z ?1 ,<label>(2)</label></formula><formula xml:id="formula_3">z = MLP LN y + y<label>(3)</label></formula><p>where MSA denotes multi-head self-attention <ref type="bibr" target="#b70">[71]</ref>, LN is layer normalization <ref type="bibr" target="#b4">[5]</ref> and MLP consists of two linear projections separated by GeLU <ref type="bibr" target="#b27">[28]</ref> non-linearity. Finally, a linear classifier, W out ? R d?C maps the encoded classification token, z cls to one of C classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiview tokenization</head><p>In our model, we extract multiple sets of tokens, z 0,(1) , z 0,(2) , . . . , z 0,(V ) from the input video. Here, V is the number of views, and thus z ,(i) denotes tokens after layers of transformer processing for the i th view. We define a view as a video representation expressed by a set of fixed-sized tubelets. A larger view corresponds to a set of larger tubelets (and thus fewer tokens) and a smaller view corresponds to smaller tubelets (and thus more tokens). The 0 th layer corresponds to the tokens that are input to the subsequent transformer. As shown in <ref type="figure">Fig. 1</ref>, we tokenize each view using a 3D convolution, as it was the best tokenization method reported by <ref type="bibr" target="#b2">[3]</ref>. We can use different convolutional kernels, and different hidden sizes, d (i) , for each view. Note that smaller convolutional kernels correspond to smaller spatio-temporal "tubelets", thus resulting in more tokens to be processed for the i th view. Intuitively, fine-grained motions can be captured by smaller tubelets whilst larger tubelets capture slowly-varying semantics of the scene. As each view captures different levels of information, we use transformer encoders of varying capacities for each stream with lateral connections between them to fuse information, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multiview transformer</head><p>After extracting tokens from multiple views, we have Z 0 = [z 0,(1) , z 0,(2) , . . . , z 0,(V ) ] from the input, which are processed with a multiview transformer as shown in <ref type="figure">Fig. 1</ref>. As self-attention has quadratic complexity <ref type="bibr" target="#b70">[71]</ref>, processing tokens from all views jointly is not computationally feasible for video. As a result, we first use a multiview encoder, comprising of separate transformer encoders (consisting of L (i) transformer layers) for the tokens between views, with lateral connections between these encoders to fuse information from each view ( <ref type="figure">Fig. 2</ref>). Finally, we extract a token representation from each view, and process these jointly with a final global encoder to produce the final classification token, which we linearly read-off to obtain the final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Multiview encoder</head><p>Our multiview encoder consists of separate transformer encoders for each view which are connected by lateral connections to fuse cross-view information. Each transformer layer within the encoders follows the same design as the original transformer of Vaswani et al. <ref type="bibr" target="#b70">[71]</ref>, except for the fact that we optionally fuse information from other streams within the layer as described in Sec. 3.3.2. Note that our model is agnostic to the exact type of transformer layer used. Furthermore, within each transformer layer, we compute self-attention only among tokens extracted from the same temporal index, following the Factorised Encoder of <ref type="bibr" target="#b2">[3]</ref>. This significantly reduces the computational cost of the model. Furthermore, self-attention along all spatiotemporal tokens is unnecessary, as we fuse information from other views within the multiview encoder, and also because of the subsequent global encoder which aggregates tokens from all streams.  <ref type="figure">Figure 2</ref>. An illustration of our proposed cross-view fusion methods. In all three subfigures, view i (left) refers to a video representation using larger tubelets, and thus less input tokens and view i + 1 (right) corresponds to the representation with smaller tubelets and more input tokens. "+" denotes summation. Tokens extracted from tubelets are colored red and bottleneck tokens are colored blue. MSA is short for Multihead Self-Attention and CVA stands for Cross View Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Cross-view fusion</head><p>We consider the following three cross-view fusion methods. Note that the hidden dimensions of the tokens, d (i) , can vary between views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-view attention (CVA). A straight-forward method of combining information between different views is to perform self-attention jointly on all</head><formula xml:id="formula_4">i N (i) tokens where N (i)</formula><p>is the number of tokens in the i th view. However, due to the quadratic complexity of self-attention, this is prohibitive computationally for video models, and hence we perform a more efficient alternative. We sequentially fuse information between all pairs of two adjacent views, i and i + 1, where the views are ordered in terms of increasing numbers of tokens (i.e. N (i) ? N (i+1) ). Concretely, to update the tokens from the larger view, z (i) , we compute attention where the queries are z (i) , and the keys and values are z (i+1) (the tokens from the smaller view). As the hidden dimensions of the tokens between the two views can be different, we first project the keys and values to the same dimension, as denoted by</p><formula xml:id="formula_5">z (i) = CVA(z (i) , W proj z (i+1) ),<label>(4)</label></formula><formula xml:id="formula_6">CVA(x, y) = Softmax W Q xW K y ? d k W V y. (5)</formula><p>Note that W Q , W K and W V are the query-, keyand value-projection matrices used in the attention operation <ref type="bibr" target="#b70">[71]</ref>. As shown in <ref type="figure">Fig. 2a</ref>, we also include a residual connection around the cross-view attention operation, and zero-initialize the parameters of this operation, as this helps when using image-pretrained models as is common practice <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. Similar studies on cross stream attention have been done by <ref type="bibr" target="#b10">[11]</ref> for images.</p><p>Bottleneck tokens. An efficient method of transferring in-formation between tokens from two views, z (i) and z (i+1) , is by an intermediate set of B bottleneck tokens. Once again, we sequentially fuse information between all pairs of two adjacent views, i + 1 and i, where the views are ordered in terms of increasing numbers of tokens.</p><p>In more detail, we initialize a sequence of bottleneck tokens, z</p><formula xml:id="formula_7">(i+1) B ? R B (i+1) ?d (i+1)</formula><p>where B (i+1) is the number of bottleneck tokens in the (i + 1) th view and B (i+1) N (i+1) . As shown in <ref type="figure">Fig. 2b (</ref></p><formula xml:id="formula_8">where B = 1), the bottleneck tokens from view i+1, z (i+1) B</formula><p>, are concatenated to the input tokens of the same view, z (i+1) , and processed with selfattention. This effectively transfers information between all tokens from view i + 1. Thereafter, these tokens, z</p><formula xml:id="formula_9">(i+1) B</formula><p>are linearly projected to the depth of view i, and concatenated to z (i) before performing self-attention again. This process is repeated between each pair of adjacent views as shown in <ref type="figure">Fig. 2b</ref>, and allows us to efficiently transfer information from one view to the next.</p><p>As with cross-view attention, we sequentially perform fusion between all pairs of adjacent views, beginning from the view with the largest number of tokens, and proceeding in order of decreasing token numbers. Intuitively, this allows the view with the fewest tokens to aggregate finegrained information from all subsequent views.</p><p>Note that the only parameters introduced into the model from this fusion method are the linear projections of bottleneck tokens from one view to the next, and the bottleneck tokens themselves which are learned from random initialization. We also note that "bottleneck" tokens have also been used by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>MLP fusion. Recall that each transformer encoder layer consists of a multi-head self attention operation (Eq. 2), followed by an MLP block (Eq. 3). A simple method is to fuse before the MLP block within each encoder layer.</p><p>Concretely, as shown in <ref type="figure">Fig. 2c</ref>, tokens from view i + 1, z (i+1) with hidden dimension d (i+1) are concatenated with tokens from view i along the hidden dimension. These tokens are then fed into the MLP block of layer i and linearly projected to the depth d (i) . This process is repeated between adjacent views of the network, where once again, views are ordered by increasing number of tokens per view.</p><p>Fusion locations. We note that it is not necessary to perform cross-view fusion at each layer of the cross-view encoder to transfer information among the different views, since each fusion operation has a global "receptive field" that considers all the tokens from the previous views. Furthermore, it is also possible for the encoders for each individual view to have different depths, meaning that fusion can occur between layer l of view i and layer l of view j where l = l . Therefore, we consider the fusion locations as a design choice which we perform ablation studies on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Global encoder</head><p>Finally, we aggregate the tokens from each of the views with the final global encoder, as shown in <ref type="figure">Fig. 1</ref>, effectively fusing information from all views after the crossview transformer. We extract the classification token from each view, {z</p><formula xml:id="formula_10">(i) cls } V i=1</formula><p>, and process them further with another transformer encoder, following Vaswani et al. <ref type="bibr" target="#b70">[71]</ref>, that aggregates information from all views. The resulting classification token is then mapped to one of C classification outputs, where C is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Model variants. For the backbone of each view, we consider five ViT variants, "Tiny", "Small", "Base", "Large", and "Huge". Their settings strictly follow the ones defined in BERT <ref type="bibr" target="#b16">[17]</ref> and ViT <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b58">59]</ref>, i.e. number of transformer layers, number of attention heads, hidden dimensions. See Appendix A.4 for the detailed settings. For convenience, each model variant is denoted with the following abbreviations indicating the backbone size and tubelet length. For example, B/2+S/4+Ti/8 denotes a three-view model, where a "Base", "Small", and "Tiny" encoders are used to processes tokens from the views with tubelets of sizes 16 ? 16 ? 2, 16 ? 16 ? 4, and 16 ? 16 ? 8, respectively. Note that we omit 16 in our model abbreviations because all our models use 16 ? 16 as the spatial tubelet size except for the "Huge" model, which uses 14 ? 14, following ViT <ref type="bibr" target="#b17">[18]</ref>. All model variants use the same global encoder which follows the "Base" architecture, except that the number of heads is set to 8 instead of 12. The reason is that the hidden dimension of the tokens should be divisible by the number of heads for multi-head attention, and the number of hidden dimensions across all standard transformer architectures (from "Tiny" to "Huge" <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b58">59]</ref>) is divisible by 8.</p><p>Training and inference. We follow the training settings of ViViT reported in the paper and public code <ref type="bibr" target="#b2">[3]</ref>, unless otherwise stated. Namely, all models are trained on 32 frames with a temporal stride of 2. We train our model using synchronous SGD with momentum of 0.9 following a cosine learning rate schedule with a linear warm up. The input frame resolution is set to be 224 ? 224 in both training and inference. We follow <ref type="bibr" target="#b2">[3]</ref> and apply the same data augmentation and regularization schemes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b81">82]</ref>, which were used by <ref type="bibr" target="#b66">[67]</ref> to train vision transformers more effectively. During inference, we adopt the standard evaluation protocol by averaging over multiple spatial and temporal crops. The number of crops is given in the results tables. For reproducibility, we include exhaustive details in Appendix A.3.</p><p>Initialization. Following previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">51]</ref>, we initialize our model from a corresponding ViT model pretrained on large-scale image datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b60">61]</ref> obtained from the public code of <ref type="bibr" target="#b17">[18]</ref>. The initial tubelet embedding operator, E, and positional embeddings, p, have different shapes in the pretrained model and we use the same technique as <ref type="bibr" target="#b2">[3]</ref> to adapt them to initialize each view of our multiview encoder (Sec. 3.3.1). The final global encoder (Sec. 3.3.3) is randomly initialized.</p><p>Datasets. We report the performance of our proposed models on a diverse set of video classification datasets:</p><p>Kinetics <ref type="bibr" target="#b32">[33]</ref> is a collection of large-scale, high-quality datasets of 10s video clips focusing on human actions. We report results on Kinetics 400, 600, and 700, with 400, 600, and 700 classes, respectively. <ref type="bibr" target="#b47">[48]</ref> is a collection of 800,000 labeled 3 second videos, involving people, animals, objects or natural phenomena, that capture the gist of a dynamic scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moments in Time</head><p>Epic-Kitchens-100 <ref type="bibr" target="#b14">[15]</ref> consists of 90,000 egocentric videos, totaling 100 hours, recorded in kitchens. Each video is labeled with a "noun" and a "verb" and therefore we predict both categories using a single network with two "heads". Three accuracy scores ("noun", "verb", and "action") are commonly reported for this dataset with action accuracy being the primary metric. The "action" label is formed by selecting the top-scoring noun and verb pair.</p><p>Something-Something V2 <ref type="bibr" target="#b25">[26]</ref> consists of more than 220,000 short video clips that show humans interacting with everyday objects. Similar objects and backgrounds appear in videos across different classes. Therefore, in contrast to other datasets, this one challenges a model's capability to distinguish classes from motion cues.  <ref type="table">Table 1</ref>. Ablation studies of our method. (a) Assigning larger models to smaller tubelet sizes achieves the highest accuracy. (b) We apply the same "Base" encoder to all views, and show that there is minimal accuracy difference to the alternatives from (a), but a large increase in computation. (c) A comparison of different cross-view fusion methods, shows that Cross-View Attention (CVA) is the best. The "Ensemble" and "late fusion" baselines are detailed in the text. (d) We compare our approach to the alternate temporal multi-resolution method of <ref type="bibr" target="#b22">[23]</ref>, implemented in the context of transformers, and show signficant improvements. (e) We achieve substantial accuracy by adding more views, and this improvement is larger than that obtained by adding more layers to a single encoder. (f) The optimal fusion layers are at the middle and late stages of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>We conduct ablation studies on the Kinetics 400 dataset. In all cases, the largest backbone in the multiview encoder is "Base" for faster experimentation. We report accuracies when averaging predictions across multiple spatio-temporal crops, as standard practice <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>. In particular, we use 4 ? 3 crops, that is 4 temporal crops, with 3 spatial crops for each temporal crop. We used a learning rate of 0.1 for all experiments for 30 epochs, and used no additional regularization as done by <ref type="bibr" target="#b2">[3]</ref>.</p><p>Model-view assignments. Recall that a view is a video representation in terms of tubelets, and that a larger view equates to larger tubelets (and hence fewer transformer tokens) and smaller views correspond to smaller tubelets (and thus more tokens). We considered two model-view assignment strategies: larger models for larger views (e.g., B/8+Ti/2, the larger "Base" model is used to encode 16 ? 16 ? 8 tubelets and the smaller "Tiny" model encodes 16 ? 16 ? 2 tubelets) and smaller models for larger views (e.g., B/2+Ti/8). <ref type="table">Table 1a</ref> shows that assigning a larger model to smaller views is superior. For example, B/2+S/4+Ti/8 scores 81.8% while B/8+S/4+Ti/2 only scores 78.5%. One may argue that this is due to the increase of the FLOPs but B/4+S/8+Ti/16 still outperforms B/8+S/4+Ti/2 by a large margin under similar FLOPs. Our explanation is that larger views capture the gist of the scene, which requires less complexity to learn while the details of the scene are encapsulated by smaller views so a larger-capacity model is needed.</p><p>Another strategy is to assign the same model to all views. <ref type="table">Table 1b</ref> shows that in all three examples there is little difference between assigning a "Base" model and assigning a "Small" or "Tiny" model to larger views. This result is surprising yet beneficial since we can reduce the complexity of the model at almost no cost of accuracy.</p><p>What is the best cross-view fusion method?. <ref type="table">Table 1c</ref> shows the comparison of different fusion methods on a three-view model. We use one late fusion and an ensemble approach as the baselines. "Ensemble" simply sums the probabilities produced from each view, where the models from each view are trained separately. We also tried summing up the logits and majority voting but both obtained worse results. This method actually decreases the performance compared to the B/4 model since "Small" and "Tiny" models perform not comparably well. "Late fusion" concatenates the final embeddings produced by the transformer encoder from each view without any cross-view operations before feeding it into the global encoder. It improves the B/4 model from 78.3% to 80.6%. All of our fusion methods except MLP outperform the baselines while CVA is the best overall. Based on this observation, we choose CVA as the fusion method for all subsequent experiments. MLP fusion is the worst performing method of the three and we think it is because concatenation in the MLP blocks introduces additional channels that have to be randomly initialized, making model optimization more difficult.</p><p>Effect of the number of views.    <ref type="figure" target="#fig_3">Figure 3a</ref> shows that MTV is consistently better and requires less FLOPs than ViViT-FE to achieve higher accuracy across different model scales (shown by the dotted green arrows pointing upper-left). With additional FLOPs, MTV shows larger accuracy gains (shown by the dotted green arrows pointing upper-right).</p><p>Similarly, <ref type="figure" target="#fig_3">Fig. 3b</ref> shows that MTV can have higher throughput than ViVIT-FE, whilst still improving its accuracy, across all model scales. All speed comparisons are measured with the same hardware (Cloud TPU-v4), whilst the accuracy is computed from 4 ? 3 view testing.</p><p>mance on Kinetics-400 as we increase the number of views. With two views we achieve a +2.5% in Top-1 accuracy over the baseline B/4 model. As we increase to three views, the improvement widens to 2.8%. Furthermore, we show that such improvement is non-trivial. For example, we also train a 14-layer and a 17-layer variants of the "Base" model. They share similar FLOPs with our two-view and threeview counterparts but their performance remains similar to that of the baseline.</p><p>Which layers to apply cross-view fusion?. Motivated by Tab. 1c, we fix the fusion method to CVA, and vary the locations and number of layers where we apply CVA, when using a three-view B+S+Ti model (each encoder thus has 12 layers) in Tab. 1f. The choices are in the early-, mid-, and late-stages of the transformer encoders and the number of fusion layers is set to be one and two. When using one fusion layer, the best location for fusion is mid followed by late, then early. Adding more fusion layers in the same stage does not improve the performance but combining mid and late fusion improves the performance. For example, fusion at 5th and 11th layers achieve the best result. Based on this observation, we set the fusion layers to be {11, 23} for L+B+S+Ti and {11, 23, 31} for H+B+S+Ti model variants, respectively, in subsequent experiments.</p><p>Comparison to SlowFast. SlowFast <ref type="bibr" target="#b22">[23]</ref> proposes a twostream CNN architecture that takes frames sampled at two different frame rates. The "Slow" pathway, built with a larger encoder, processes the low frame rate stream to capture the semantics of the scene while the "Fast" pathway that takes in high frame rate inputs is used to capture motion information. To make a fair comparison, we implement <ref type="bibr" target="#b22">[23]</ref> in the context of transformers where we use "Base" and "Tiny" models as the encoders for the Slow and Fast paths respectively and use CVA for lateral connections. The Slow path takes four frames as inputs sampled with a temporal stride of 16 and the Fast path takes 16 frames sampled with a stride of 4. As SlowFast captures multiscale temporal information by varying the frame rate to the two streams, the temporal duration for the tubelets is set to 1 in this case. <ref type="table">Table 1c</ref> shows that our method is significantly more accurate than the SlowFast method whilst also using fewer FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to the state of the art</head><p>We compare to the state-of-the-art across six different datasets. We evaluate models with four temporal-and three spatial-views per video clip, following <ref type="bibr" target="#b2">[3]</ref>. To make the notation more concise, we now use MTV-B to refer to B/2+S/4+Ti/8, MTV-L to refer to L/2+B/4+S/8+Ti/16 and MTV-H to refer to H/2+B/4+S/8+Ti/16. Except for Kinetics, all our models start from a Kinetics 400 checkpoint and then are fine-tuned on the target datasets following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Accuracy/computation trade-offs. <ref type="figure" target="#fig_3">Figure 3</ref> compares our proposed MTV to its single-view counterpart, ViViT Factorized Encoder (FE) <ref type="bibr" target="#b2">[3]</ref> at every model scale on Kinetics 400. We compare to ViViT-FE using tubelets with a temporal dimension of t = 2, as the authors obtained the best performance with this.</p><p>We can control the complexity of MTV by increasing or decreasing t used in each view. For example, increasing t from 2 to 4 for the smallest view (and proportionally <ref type="table">Table 2</ref>. Comparisons to state-of-the-art. For "views", x ? y denotes x temporal views and y spatial views. We report the total TFLOPs to process all spatio-temporal views. We use shorter notation, MTV-B, L, H to denote variants, B/2+S/4+Ti/8, L/2+B/4+S/8+Ti/16, and H/2+B/4+S/8+Ti/16, respectively. Models use a spatial resolution of 224 ? 224, unless explicitly stated by MTV (xp), which refers to a spatial resolution of x ? x. Models are pretrained on ImageNet-21K unless explicitly stated in parenthesis. increasing t for all other views) will roughly reduce the input tokens by half for each view, and thus halve the total FLOPs for processing each input. Our method with t = 4 for the smallest view consistently achieves higher accuracy than ViViT-FE at every complexity level while using fewer FLOPs, indicated by the green arrows pointing to the upperleft in <ref type="figure" target="#fig_3">Fig. 3a</ref>. This further validates that processing more views in parallel enables us to achieve larger accuracy improvements than increasing the number of input tokens. If we set t = 2 as in ViViT-FE, we use additional FLOPs, but increase significantly in accuracy too, as indicated by the green arrow pointing to the upper-right in <ref type="figure" target="#fig_3">Fig. 3a</ref>.</p><p>Furthermore, note how our B/2 model (transformer depth of 12 layers) outperforms ViViT-L/2 (24 layers), whilst using less FLOPs. Similarly, our L/2 model outperforms ViViT-H/2. This shows that we can achieve greater accuracy improvements by processing multiple views in parallel than by increasing the depth for processing a single view.</p><p>Finally, note that <ref type="figure" target="#fig_3">Fig. 3b</ref> shows that our conclusions are also consistent when using the inference time to measure our model's efficiency. Appendix A.1 also shows that these trends also hold when using an unfactorized <ref type="bibr" target="#b2">[3]</ref> backbone architecture of ViViT and MTV.</p><p>Kinetics. We compare to methods that are pretrained on ImageNet-1K, ImageNet-21K <ref type="bibr" target="#b15">[16]</ref> and those that do not utilize pretraining at all in the first part of Tab. 2. In the second part of the tables, we compare to methods that are pretrained on web-scale datasets such as Instagram 65M <ref type="bibr" target="#b24">[25]</ref>, JFT-300M <ref type="bibr" target="#b60">[61]</ref>, JFT-3B <ref type="bibr" target="#b79">[80]</ref>, WTS <ref type="bibr" target="#b59">[60]</ref>, Florence <ref type="bibr" target="#b78">[79]</ref> or HowTo100M <ref type="bibr" target="#b46">[47]</ref>. Observe that we achieve state-of-the-art results both with and without web-scale pretraining.</p><p>On Kinetics 400, our ImageNet-21K pretrained "Base" model improves the "Large" ViViT-FE model <ref type="bibr" target="#b2">[3]</ref>, which corresponds to a deeper, single-view equivalent of our model by 0.1% and 1.2% in Top-1 and Top-5 accuracy, whilst using 40% of the total FLOPs. Our higher resolution version improves further by 0.7% and 1.4% while still using slightly fewer FLOPs. On Kinetics 600, our "Base" model scores second to <ref type="bibr" target="#b34">[35]</ref> whose model structure is derived using architecture search on Kinetics 600 itself. We show significant improvements over <ref type="bibr" target="#b34">[35]</ref> on both Kinetics 400 and 700 for which the architecture of <ref type="bibr" target="#b34">[35]</ref> was not directly optimized for.</p><p>When using additional JFT-300M pretraining, our "Huge" model outperforms other recent transformer models using the same pretraining dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b54">55]</ref>. And when we utilize the Weak Textual Supervision (WTS) dataset of <ref type="bibr" target="#b59">[60]</ref> for pre-training, we substantially advance the best reported results on Kinetics: On Kinetics 400, we achieve a Top-1 accuracy of 89.9%, which improves upon the previous highest result (CoVeR <ref type="bibr" target="#b80">[81]</ref>) by 2.7%. Similarly, on Kinetics 600, we achieve a Top-1 of 90.3%, which is an absolute improvement of 2.4% on <ref type="bibr" target="#b80">[81]</ref>. On Kinetics 700, we achieve 83.4%, which improves even further by 3.6% over <ref type="bibr" target="#b80">[81]</ref>. We also improve upon R3D-RS <ref type="bibr" target="#b18">[19]</ref>, which also used WTS pretraining, by 6.4% and 6.0% on Kinetics-400 and -600.</p><p>Epic-Kitchens-100. Following the standard protocol <ref type="bibr" target="#b14">[15]</ref>, we report Top-1 action-, verb-and noun-accuracies with action accuracy being the primary metric. Our results are averaged over 4 ? 1 crops as additional spatial crops did not help. Both our MTV-B and MTV-B(320p) significantly improve the previous state-of-the-art on noun classes, and MTV-B(320p) achieves a new state-of-the-art of 48.6% on actions. With WTS pretraining and increasing resolution, we improved the results to 50.5%. We found that additional data augmentation (detailed in Appendix A.3) has to be used to achieve good performance (as also observed by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51]</ref>) as this is the smallest dataset of all six with 67,000 training examples.</p><p>Something-Something V2. This dataset consists of class labels such as "move to left" and "pointing to right" <ref type="bibr" target="#b25">[26]</ref>. As the model needs to explicitly reason about direction, we do not perform random horizontal or vertical flipping as data augmentation on this dataset as also done by <ref type="bibr" target="#b20">[21]</ref>. We improve substantially over ViViT-L-FE <ref type="bibr" target="#b2">[3]</ref>, which corresponds to a deeper single-view equivalent of our model by 2.6%, and also improve upon MFormer <ref type="bibr" target="#b50">[51]</ref> by 0.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moments in Time.</head><p>Our MTV-L model significantly improves over the previous state-of-the-art <ref type="bibr" target="#b34">[35]</ref> by 1.5% in Top-1 accuracy. Moreover, our model with ImageNet-21K pretraining even outperforms VATT <ref type="bibr" target="#b1">[2]</ref>, which was pretrained on HowTo100M <ref type="bibr" target="#b46">[47]</ref>, a dataset consisting of around 100M video clips. When using WTS pre-training, we improve our accuracy even further, achieving 47.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a simple method for capturing multiresolution temporal context in transformer architectures, based on processing multiple "views" of the input video in parallel. We have demonstrated that our approach performs better, in terms of accuracy/computation trade-offs than increasing the depth of current single-view architectures. Furthermore, we have achieved state-of-the-art results on six popular video classification datasets. These results were then further improved with large-scale pretraining <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>Limitations and future work. Although we have improved upon the state-of-the-art, there is still a large room for improvement on datasets other than Kinetics. Furthermore, we have relied on models pretrained on large image-or videodatasets for initialization. Reducing this dependence on supervised pretraining is a clear avenue of future research. We have conducted thorough ablations on standard transformer architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>, and will investigate if our approach is complementary to recent, spatial-pyramid based multiscale transformer encoders such as MViT <ref type="bibr" target="#b20">[21]</ref> and Swin <ref type="bibr" target="#b43">[44]</ref>.</p><p>Societal impact. Video classification models can be used in a wide range of applications. We are unaware of all potential applications, but are mindful that each application has its own merits, and that also depends on the intentions of the individuals building and using these systems. We also note that training datasets may contain biases that models trained on them are unsuitable for certain applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional experiments</head><p>In this Appendix, we provide additional experimental details. Section A.1 provides accuracy-FLOPs and accuracythroughput comparison between two model variants of ViViT and MTV. Section A.2 provides the effect of spatial resolution of tubelets. Section A.3 and Section A.4 provides details of our training hyperparameters and model configurations used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Changing transformer encoder architecture</head><p>We present additional results by changing the transformer architecture used within our multiview encoder. Specifically, we use the unfactorized ViViT transformer encoder (Model 1 of <ref type="bibr" target="#b2">[3]</ref>). In this variant, each transformer encoder layer computes self-attention over all spatio-temporal tokens. This makes our multiview transformer encoder cover a wide range of spatial and temporal dimensions across different views. A one-layer MLP with hidden dimension of 3072 is used as the global encoder for our unfactorized MTV model.</p><p>As shown in <ref type="figure">Fig. 4</ref>, MTV (unfactorized) consistently outperforms its single-view counterpart (i.e. ViViT unfactorized) for every scale (see <ref type="figure">Fig. 4a</ref>) and corresponds to a better accuracy-throughput curve as shown in <ref type="figure">Fig. 4b</ref>. Note how MTV can more than double the throughput of ViViT unfactorized, whilst still improving its accuracy, for each model scale. Specifically, MTV (unfactorized) H/4+B/8+S/16+Ti/32 model leads to a significant speed-up by 172% while still keeping a higher accuracy of 0.4% improvement compared to ViViT-H.</p><p>Moreover, we report the accuracy-throughput comparison between MTV and ViViT factorized model (ViViT-FE) in <ref type="figure">Fig. 4d</ref>. Note that the accuracy-FLOPs comparison is already reported in paper Section 4.3. The improvements in accuracy-throughput, and accuracy-FLOPs remain significant in this setting.</p><p>Note that the unfactorized ViViT transformer encoder, which attends to all spatio-temporal tokens, is less efficient than the Factorized Encoder architecture that we used in the main paper. However, we achieve larger relative improvements in accuracy/computation trade-offs compared to the corrsponding single-view ViViT baseline when using this encoder architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Spatial resolution of tubelets</head><p>We study the effect of the spatial resolution of tubelets in Tab. 3. We use our B/4 + Ti/16 model variant, and vary the spatial resolution of the tubelets. Our results indicate that the accuracy is primarily impacted by the spatial resolution of the large encoder. We also note that processing more tokens, and thus using more computation, typically results in higher accuracies.  <ref type="table">Table 4</ref> details the hyperparamters used in all of our experiments. We use synchronous SGD with momentum, a cosine learning rate schedule with linear warmup, and a batch size of 64 for all experiments on the Kinetic datasets. We found that larger batch size and additional regularization are helpful when training on the smaller Epic Kitchens and Something-Something v2 datasets, as also noted by <ref type="bibr" target="#b2">[3]</ref>. <ref type="table">Table 5</ref> summarizes our model configurations of each view for our multiview transformer encoder. For the backbone of each view, we consider five ViT variants, "Tiny", "Small", "Base", "Large", and "Huge". Their settings strictly follow the ones defined in BERT <ref type="bibr" target="#b16">[17]</ref> and ViT <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b58">59]</ref>. For the global encoder, all model variants of MTV use the same global encoder which follows the "Base" architecture, except that the number of heads is set to 8 instead of 12. The reason is that the hidden dimension of the tokens should be divisible by the number of heads for multi-head attention, and the number of hidden dimensions across all backbone sizes is divisible by 8 (as shown in Tab. 5). All model variants of MTV (unfactorized) use a one-layer MLP with the same hidden dimension as the "Base" architecture.    <ref type="figure">Figure 4</ref>. Accuracy/complexity trade-off between ViViT / ViViT-FE <ref type="bibr" target="#b2">[3]</ref> (blue) and our MTV (unfactorized) / MTV (red). MTV (unfactorized) is consistently better and requires less FLOPs (see <ref type="figure">Fig. 4a</ref>) than ViViT to achieve higher accuracy across different model scales (indicated by the dotted green arrows pointing upper-left). With additional FLOPs, MTV shows larger accuracy gains (shown by the dotted green arrows pointing upper-right). The lower number of FLOPs is translated to higher throughput (clips per second), as indicated by the green arrows in <ref type="figure">Fig. 4b</ref>. Note how MTV can more than double the throughput of ViViT unfactorized, whilst still improving its accuracy, across all model scales. Similar findings are also observed by the comparison between ViViT-FE and MTV model in <ref type="figure">Fig. 4c</ref> and <ref type="figure">Fig.  4d</ref>. Note that <ref type="figure">Fig. 4c</ref> appeared as <ref type="figure" target="#fig_3">Figure 3</ref> in the main paper, and is included here for clarity and consistency. All speed comparisons are measured with the same hardware (Cloud TPU-v4). The complexity is for a single 32 ? 224 ? 224 ? 3 input video (denoted as T ? H ? W ? C), and the accuracy is obtained by 4 ? 3 view testing.  <ref type="table" target="#tab_3">Tiny  192  768  3  12  16  Small  384  1536  6  12  16  Base  768  3072  12  12  16  Large  1024  4096  16  24  16  Huge  1280  5120  16  32  14</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Model configurations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>An example of MLP fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Accuracy[%] -Throughput comparison between MTV and ViViT-FE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Accuracy/computation trade-off between ViViT-FE<ref type="bibr" target="#b2">[3]</ref> (blue) and our MTV (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Accuracy[%] -GFLOPs comparison between MTV (unfactorized) andViViT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Accuracy[%] -Throughput comparison between MTV (unfactorized) and ViViT. Accuracy[%] -GFLOPs comparison between MTV and ViViT-FE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Accuracy[%] -Throughput comparison between MTV and ViViT-FE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>)</head><label></label><figDesc>Effects of different model-view assignments. Comparison of different cross-view fusion methods. Effects of increasing number of views.</figDesc><table><row><cell cols="4">Model variants GFLOPs MParams Top-1</cell><cell>(c) Model variants</cell><cell>Method</cell><cell></cell><cell cols="3">GFLOPs MParams Top-1</cell><cell>(e) Model variants</cell><cell>GFLOPs</cell><cell>Top-1</cell></row><row><cell>B/8+Ti/2</cell><cell>81</cell><cell>161</cell><cell>77.3</cell><cell>B/4</cell><cell></cell><cell></cell><cell>145</cell><cell>173</cell><cell>78.3</cell><cell>B/4</cell><cell>145</cell><cell>78.3</cell></row><row><cell>B/2+Ti/8</cell><cell>337</cell><cell>221</cell><cell>81.3</cell><cell>S/8 Ti/16</cell><cell>N/A</cell><cell></cell><cell>20 3</cell><cell>60 13</cell><cell>74.1 67.6</cell><cell>B/4+Ti/16 B/4+S/8+Ti/16</cell><cell>168 195</cell><cell>80.8 (+2.5) 81.1 (+2.8)</cell></row><row><cell>B/8+S/4+Ti/2 B/2+S/4+Ti/8 B/4+S/8+Ti/16</cell><cell>202 384 195</cell><cell>250 310 314</cell><cell>78.5 81.8 81.1</cell><cell>B/4+S/8+Ti/16</cell><cell cols="2">Ensemble MLP Late fusion</cell><cell>168 202 187</cell><cell>246 323 306</cell><cell>77.7 80.6 80.6</cell><cell>B/4 (14) B/4 (17)</cell><cell>168 203</cell><cell>78.1 (-0.2) 78.4 (+0.1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Bottleneck</cell><cell>188</cell><cell>306</cell><cell>81.0</cell><cell></cell><cell></cell></row><row><cell cols="4">(b) Effects of the same model applied to different</cell><cell></cell><cell>CVA</cell><cell></cell><cell>195</cell><cell>314</cell><cell>81.1</cell><cell cols="3">(f) Effects of applying CVA at different layers.</cell></row><row><cell>views.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Fusion layers GFLOPs MParams Top-1</cell></row><row><cell cols="4">Model variants GFLOPs MParams Top-1</cell><cell cols="6">(d) Comparison to SlowFast multi-resolution method.</cell><cell>0</cell><cell></cell><cell>80.96</cell></row><row><cell>B/4+S/8+Ti/16 B/4+B/8+B/16</cell><cell>195 324</cell><cell>314 759</cell><cell>81.1 81.1</cell><cell cols="5">Model variants SlowFast (transformer backbone) GFLOPs MParams Top-1</cell><cell></cell><cell>5 11</cell><cell>195</cell><cell>314</cell><cell>81.08 81.00</cell></row><row><cell>B/2+Ti/8 B/2+B/8</cell><cell>337 448</cell><cell>221 465</cell><cell>81.3 81.5</cell><cell cols="2">Slow-only (B) Fast-only (Ti) Slowfast (B+Ti)</cell><cell>79 63 202</cell><cell>87 6 105</cell><cell>78.0 74.6 79.7</cell><cell></cell><cell>0, 1 5, 6 10, 11</cell><cell>203</cell><cell>323</cell><cell>80.91 80.96 80.81</cell></row><row><cell>B/2+S/4+Ti/8</cell><cell>384</cell><cell>310</cell><cell>81.8</cell><cell cols="2">B/4+Ti/16 (ours)</cell><cell>168</cell><cell>224</cell><cell>80.8</cell><cell></cell><cell>5, 11</cell><cell></cell><cell>81.14</cell></row><row><cell>B/2+B/4+B/8</cell><cell>637</cell><cell>751</cell><cell>81.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0, 5, 11</cell><cell>210</cell><cell>331</cell><cell>80.95</cell></row></table><note>(a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1e</head><label>1e</label><figDesc></figDesc><table><row><cell>Top-1 Accuracy</cell><cell>78 80 82 84 86</cell><cell>S/2 S/4 1.9 2.3 2.7 B/2 3.0 B/2 B/4 L/4 1.6 MTV ViViT-FE</cell><cell>L/2 2.6 L/2</cell><cell>H/4</cell><cell>1.9</cell><cell>H/2</cell><cell>3.1</cell><cell>H/2</cell></row><row><cell></cell><cell>76</cell><cell>S/2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">0 500 1000 1500 2000 2500 3000 3500 4000 GFLOPs</cell></row><row><cell cols="9">(a) Accuracy[%] -GFLOPs comparison between MTV and ViViT-FE.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>shows perfor-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Effect of spatial resolution of tubelets. All experiments are conducted on Kinetics 400 using the model variant B/4+Ti/16. Accuracies are for 4 ? 3 crops.</figDesc><table><row><cell cols="2">Tubelet spatial size</cell><cell cols="2">GFLOPs Top-1</cell></row><row><cell>B</cell><cell>Ti</cell><cell></cell></row><row><cell cols="2">24 ? 24 16 ? 16</cell><cell>68</cell><cell>78.1</cell></row><row><cell cols="2">16 ? 16 24 ? 24</cell><cell>165</cell><cell>80.5</cell></row><row><cell cols="2">16 ? 16 16 ? 16</cell><cell>168</cell><cell>80.5</cell></row><row><cell cols="2">16 ? 16 12 ? 12</cell><cell>169</cell><cell>80.6</cell></row><row><cell cols="2">12 ? 12 16 ? 16</cell><cell>295</cell><cell>81.0</cell></row><row><cell cols="4">A.3. Hyperparameters for each datasets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Training hyperparamters for experiments in the main paper. "-" indicates that the regularisation method was not used at all. Values which are constant across all columns are listed once. Datasets are denoted as follows: K400: Kinetics 400. K600: Kinetics 600. K700: Kinetics 700. MiT: Moments in Time. EK: Epic Kitchens. SSv2: Something-Something v2. Model configurations for each view of MTV.</figDesc><table><row><cell></cell><cell></cell><cell>K400</cell><cell>K600</cell><cell>K700</cell><cell>MiT</cell><cell>EK</cell><cell>SSv2</cell></row><row><cell>Optimization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Synchronous SGD</cell><cell></cell></row><row><cell>Momentum</cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell></cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>256</cell><cell>128</cell><cell>512</cell></row><row><cell cols="2">Learning rate schedule</cell><cell></cell><cell></cell><cell cols="2">cosine with linear warmup</cell><cell></cell></row><row><cell cols="2">Linear warmup epochs</cell><cell></cell><cell></cell><cell>2.5</cell><cell></cell><cell></cell></row><row><cell>Base learning rate</cell><cell></cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>0.5</cell></row><row><cell>Epochs</cell><cell></cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>80</cell><cell>100</cell></row><row><cell cols="2">Data augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Random crop probability</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell cols="2">Random flip probability</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>-</cell></row><row><cell cols="2">Scale jitter probability</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>Maximum scale</cell><cell></cell><cell></cell><cell></cell><cell>1.33</cell><cell></cell><cell></cell></row><row><cell>Minimum scale</cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell cols="2">Colour jitter probability</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Rand augment number of layers [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>1</cell></row><row><cell cols="2">Rand augment magnitude [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10</cell><cell>15</cell></row><row><cell cols="2">Other regularisation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Stochastic droplayer rate [29]</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell></row><row><cell cols="2">Label smoothing [64]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Mixup [82]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.1</cell><cell>0.3</cell></row><row><cell>Model name</cell><cell>Hidden size</cell><cell>MLP dimension</cell><cell></cell><cell>Number of</cell><cell>Number of</cell><cell></cell><cell>Tubelet spatial</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">attention heads</cell><cell>encoder layers</cell><cell></cell><cell>size</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">M</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA engineer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linagzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unified graph structured models for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramidal implementation of the affine lucas kanade feature tracker description of the algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Bouguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel corporation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in computer vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="671" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Revisiting 3d resnets for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01696</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Omni-sourced webly-supervised learning for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistics of natural images and models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinggang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Perceiver io: A general architecture for structured inputs &amp; outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2107.14795</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marsza?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Movinets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra Florian Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tokenlearner: What can 8 learned tokens do for images and videos? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning video representations from textual web supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Statistics of natural image categories. Network: computation in neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Attentionnas: Spatiotemporal attention cell search for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Kaiming He, Christoph Feichtenhofer, and Philipp Krahenbuhl. A multigrid method for efficiently training video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Scaling vision transformers. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Co-training transformer with videos and images improves action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07175</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
