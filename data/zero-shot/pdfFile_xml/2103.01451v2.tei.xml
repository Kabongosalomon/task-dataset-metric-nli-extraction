<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Person Re-Identification with Attribute-guided Metric Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Xiaodong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
							<email>liuwu1@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ping</forename><surname>Zhang</surname></persName>
							<email>xzhang@ee.ryerson.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@live.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Person Re-Identification with Attribute-guided Metric Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the great progress of person re-identification (ReID) with the adoption of Convolutional Neural Networks, current ReID models are opaque and only outputs a scalar distance between two persons. There are few methods providing users semantically understandable explanations for why two persons are the same one or not. In this paper, we propose a post-hoc method, named Attributeguided Metric Distillation (AMD), to explain existing ReID models. This is the first method to explore attributes to answer: 1) what and where the attributes make two persons different, and 2) how much each attribute contributes to the difference. In AMD, we design a pluggable interpreter network for target models to generate quantitative contributions of attributes and visualize accurate attention maps of the most discriminative attributes. To achieve this goal, we propose a metric distillation loss by which the interpreter learns to decompose the distance of two persons into components of attributes with knowledge distilled from the target model. Moreover, we propose an attribute prior loss to make the interpreter generate attribute-guided attention maps and to eliminate biases caused by the imbalanced distribution of attributes. This loss can guide the interpreter to focus on the exclusive and discriminative attributes rather than the large-area but common attributes of two persons. Comprehensive experiments show that the interpreter can generate effective and intuitive explanations for varied models and generalize well under cross-domain settings. As a by-product, the accuracy of target models can be further improved with our interpreter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-identification (ReID), i.e., retrieval of the same person captured by multiple cameras, has attracted tremendous attention from academia and industry <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Although Convolutional Neural Networks (CNNs) have significantly improved the accuracy of person ReID, we still cannot completely trust the results produced by black-box models, especially for critical scenarios <ref type="bibr" target="#b43">[44]</ref>. Therefore, this paper is focused on the interpretation of CNN-based person ReID models which is crucial yet rarely studied. In recent years, there has been a surge of work in discovering how a target CNN processes input images and makes predictions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47]</ref>. These methods usually visualize gradients or salient regions on feature maps w.r.t. the input image and its prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>. Particularly, Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed to explain neural networks semantically and quantitatively by decomposing the prediction made by CNNs into semantic concepts by knowledge distillation. However, these methods mainly consider clas-sification problems. They cannot be directly applied to person ReID, which is an open-set retrieval task and usually solved by metric learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>A CNN-based ReID system usually maps a query image and gallery images into a metric space, then outputs pairwise distances by which a rank list of gallery images is returned, as shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. Although Yang et al. <ref type="bibr" target="#b36">[37]</ref> proposed Ranking Activation Maps which could visualize related regions of two persons, it still cannot semantically explain why they are similar or not. Attributes, e.g., colors and types of clothes, shoes, etc., are semantically understandable for humans and have been exploited as mid-level features for person ReID <ref type="bibr" target="#b17">[18]</ref>, but there is no method using attributes for explanations of person ReID. Therefore, we aim to learn an interpreter with the help of semantic attributes for answering two questions: 1) what attributes make two persons different, and 2) how much impact each attribute contributes to the difference, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b). In real applications, the interpreter not only can help users focus on the most discrepant attributes of two persons but also can assist developers to improve the accuracy of ReID models, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c).</p><p>However, interpretation of ReID models with attributes faces unique challenges. Firstly, since the output of ReID models are distances of pairwise images, it is difficult to use class activation or gradients to visualize salient regions or disentangle semantics as classification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47]</ref>. Moreover, persons in the wild can be described by various fine-grained and imbalanced attributes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>, which may bring biases to ReID results as well as the explanations. For example, attributes with large areas, such as coats and pants, always overwhelm small but discriminative ones like hats and shoes. Furthermore, there are only weakly-annotated image-level or ID-level attribute labels without accurate bounding boxes or masks <ref type="bibr" target="#b17">[18]</ref>, which makes it hard to learn accurate locations and intuitive visualizations for attributes.</p><p>To this end, we propose a post-hoc method, named Attribute-guided Metric Distillation, which explores semantic attributes towards explainable person ReID. Specifically, we design a pluggable interpreter network to utilize the knowledge from a target ReID model by metric distillation. The interpreter is grafted on the target ReID model and directly adopts the parameters of the first several CNN stages to exploit the low-level and mid-level features of the target model. The rest layers of the interpreter are equipped with an attribute decomposition head, by which the interpreter can learn to generate a set of attribute-guided attention maps (AAMs) for a pair of input person images. On the one hand, the generated AAMs can be directly used to visualize discriminative attributes for the image pair. On the other hand, the AAMs can be applied to the visual features of the image pair from the target model. By this means, their features and distance can be decomposed into attribute-guided components to quantify the contribution of each attribute to the overall distance. Thus, the interpreter not only can output the quantitative contributions of attributes to the overall distance of two persons but also can generate intuitively visualizations of attributes for users.</p><p>To guide learning of the interpreter, we design two loss functions. One is a metric distillation loss which can guarantee the consistency between two distance metrics: 1) the decomposed attribute-guided distances from the interpreter, and 2) the overall distance from the target model. The other is the attribute prior loss. It is designed based on the observation that the difference between two persons mainly comes from the exclusive attributes rather than common ones. Thus, the attribute prior loss makes the interpreter pay more attention to exclusive but discriminative attributes of two persons with only weakly-labeled attributes of persons.</p><p>The contributions of this paper are three-fold:</p><p>? This is one of the first attempt toward explainable person ReID by attribute-guided metric distillation that can semantically and quantitatively explain the results of existing ReID models;</p><p>? We design a pluggable interpreter network with an attribute decomposition head to obtain contributions of attributes to the difference of two persons and generate intuitive visualizations for target ReID models;</p><p>? To guide the learning of the interpreter, the metric distillation loss and attribute prior loss are proposed to guarantee consistency during metric distillation and prevent biases of attributes.</p><p>To show the effectiveness and compatibility of the interpreter, we apply it to the state-of-the-art ReID models on different datasets with comprehensive experiments. As a by-product, the performance of the state-of-the-art models is further improved with our interpreter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Interpretation of predictions made by CNNs. Recent studies on post-hoc methods for the interpretation of CNNs usually adopt visualization of salience maps <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47]</ref>, perturbation of input images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>, decision trees <ref type="bibr" target="#b38">[39]</ref>, knowledge distillation <ref type="bibr" target="#b4">[5]</ref>, etc. For example, Simonyan et al. <ref type="bibr" target="#b24">[25]</ref> first proposed two techniques based on computing the gradient of the class score with respect to the input. Zhou et al. <ref type="bibr" target="#b46">[47]</ref> proposed the class activation mapping (CAM) to map the predicted class score back to the convolutional layers in CNNs, which could provide an intuitive way to highlight the discriminative regions for a specific class. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> learned a decision tree to clarify the reasons for predictions made by a CNN via estimating the contributions of object parts. Most recently, Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed to semantically and quantitatively explain CNNs by knowledge distillation, which can decompose the prediction into contributions of a group of semantic concepts. Although these methods cannot be directly applied to the interpretation of person ReID models, they inspire us to design a semantic interpreter based on a set of attributes for explainable person ReID.</p><p>Explainable Person Re-identification. Recent CNNbased person ReID methods concentrate on two aspects: 1) task-specific modules to learn discriminative features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref> and 2) metric-based loss functions to make the features of different persons more separable in the latent space <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref>. Among these methods, various attention modules inspired by the vision system of humans achieve significant performance while making the models more explainable <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>. In particular, Yang et al. <ref type="bibr" target="#b36">[37]</ref> adopted CAM <ref type="bibr" target="#b46">[47]</ref> to discover rich features for person ReID and proposed Ranking Activation Maps (RAMs) to visualize the salient regions based on the similarity a pair of person images. However, RAMs only provide rough and pixel-level visualization on images without any semantic explanations. Therefore, we aims to build a semantic and quantitative interpreter for person ReID models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attribute-guided Metric Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>This subsection first declares necessary notations and definitions for person ReID. The task of person ReID is, given a query person image, to find images of the same person in a gallery set captured by multiple cameras <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43]</ref>. CNN-based ReID methods usually follow a paradigm. 1) We have a dataset S = {(x i , y i )} N where x i and y i are an image and the ID of a person, and N is the number of samples, while S is divided into a training set and a testing set with non-overlapped IDs. The testing set is further split into a query set Q and a gallery set G. 2) In the training stage, a CNN F(?) is trained to embed the image x i into a latent space as f i = F(x i ) by which the features f i of the same person are close and those of different persons are distant. 3) In the testing stage, the trained F(?) takes a pair of images, x q ? Q and x g ? G, as the input to obtain their features (f q , f g ) and normalized distance d q,g = D(f q , f g ) in the latent space. In this paper, Euclidean distance is used as the distance metric unless otherwise specified. By ranking the distances between a query and all images in G, the most similar one can be matched. Through the above paradigm, the prediction of a ReID system is just d q,g for (x q , x g ). Given different distance values, e.g. d q,g &lt; d q,g ? , the system cannot semantically and quantitatively explain why x q and x g are more similar than x q and x g ? , which makes it difficult for users to understand and trust the system.</p><p>In this paper, we assume that the person ReID dataset S is weakly labeled with a set of image-level attributes to obtain a new dataset</p><formula xml:id="formula_0">S A = {(x i , y i , a i )} N . For each image x i ? S A , a i = (a 1 i , a 2 i , ..., a M i ) is a binary vector, where a k i</formula><p>is the k-th attribute denoted by a Boolean value, and M is number of attribute classes. Our approach aims to semantically and quantitatively explain the distance d i,j with the weakly annotated attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attribute-guided Metric Interpreter</head><p>This subsection presents design of the interpreter in Attribute-guided Metric Distillation, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Before that, we give the formulation of the target model.</p><p>The target person ReID model F(?) can be an arbitrary off-the-shelf model (e.g., PCB <ref type="bibr" target="#b27">[28]</ref>, MGN <ref type="bibr" target="#b28">[29]</ref>, BOT <ref type="bibr" target="#b21">[22]</ref>, SBS <ref type="bibr" target="#b11">[12]</ref>, etc) with a CNN (e.g. ResNet <ref type="bibr" target="#b10">[11]</ref>) as the backbone. F(?) is trained on the training data as reviewed in Section 3.1, then we keep it fixed during learning of the interpreter network. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), given a pair of images (x i , x j ), we first extract the feature maps (F i , F j ) from the last convolutional layer. Then, we obtain the feature vectors (f i , f j ) by generalized mean pooling and compute the distance d i,j in the metric space.</p><p>The attribute-guided interpreter network G(?) is the essential module in the AMD framework. As shown in Figure 2 (b), the interpreter network has the same structure as the target model. Since the low-level and mid-level layers of the CNN capture attribute-related features such as texture and color <ref type="bibr" target="#b0">[1]</ref>, the first several CNN stages of G(?) and F(?) are shared to utilize the attribute-related knowledge learned by F(?). The high-level layers in G(?) are learnable to generate the spatial attention maps guided by semantic attributes, which can reflect the contribution of each attribute.</p><p>An Attribute Decomposition Head (ADH) is connected after the last convolutional (conv) layer of G(?). In particular, the ADH contains a C 8 ? 3 ? 3 conv layer, a M ? 1 ? 1 conv layer, and an activation function ?(?), where C is the channel number of the last conv layer of G(?). Given an image pair (x i , x j ), we obtain feature maps from the last conv layer of G(?). Through ADH we can obtain the Attribute-guided Attention Maps (AAMs) A i and A j ? R M ?w?h . After that, A i and A j are sliced into M matrices by channels, i.e., (</p><formula xml:id="formula_1">A 1 i , A 2 i , ..., A M i ) and (A 1 j , A 2 j , ..., A M j )</formula><p>, where A k i and A k j ? R h?w are the attention maps of k-th attribute, where h and w are the height and width of the attention maps.</p><p>To this end, we can apply A k i and A k j of attribute k to the feature maps F i and F j from the target model by</p><formula xml:id="formula_2">F k i = F i ? A k , F k j = F j ? A k ,<label>(1)</label></formula><p>where ? is the element-wise multiplication. By this means, each input image can obtain M attribute-guided feature maps in which the pixels activated by attribute k will be highlighted, while other pixels will be depressed. After that, the attribute-guided feature vectors f k i and f k j can be calculated from F k i and F k j by generalized mean pooling. Finally, similar to compute d i,j for (x i , x j ), we can obtain their attribute-guided distances</p><formula xml:id="formula_3">(d 1 i,j , d 2 i,j , ..., d M i,j )</formula><p>. It is noteworthy that the activation function ?(?) in ADH is important for the generation of AAMs. For existing attention modules in ReID methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>, the attention maps are usually normalized by a sigmoid activation function to constrain the values in [0, 1]. However, the sigmoid function will make attention values close to either 0 or 1, which can cause gradient vanishing and failure of model convergence. Besides, when multiplying A k to F , the large-area parts such as upper clothes and pant will dominant the attributeguided feature vector f k and make the interpreter learn biased representation for attribute decomposition. Therefore, we design a Positive Exponential Power Unit (PePU):</p><formula xml:id="formula_4">?(x) = ? ? (x + 1) ? , x &gt; 0, ? ? e x , x &lt;= 0,<label>(2)</label></formula><p>where x is the output of the last conv layer in ADH, ? and ? are growth factor in (0, 1) to smooth attention values and improve propagation of gradients. With PePU, the interpreter network can effectively decompose the salience regions for various attributes by eliminating biases of imbalanced attribute distribution. In summary, the interpreter network aims to 1) learn the importance of each attribute for the prediction made by a target model via attribute-guided attention maps, and 2) decompose the distance of two persons into a set of attributeguided distances based on their contributions to the distance, by which it can provide semantically and quantitatively explanations for person ReID. To achieve this goal, we elaborately design two types of loss functions in the next section for learning the interpreter by attribute-guided metric distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>To make the interpreter generate effective and reasonable explanation, we propose two types of objective functions: the metric distillation loss and the attribute prior loss.</p><p>Loss function of metric distillation. The main task of G(?) is to decompose the distance d i,j given by the target model F(?) into the contributions of attributes, which can be formulated as:</p><formula xml:id="formula_5">d i,j ?d i,j = M k=1 d k i,j ,<label>(3)</label></formula><p>where M is the number of attributes, d k i,j is the attributeguided distance between x i and x j for attribute k, andd i,j is reconstructed distance by the interpreter. Therefore, we define the metric distillation loss as</p><formula xml:id="formula_6">L d = |d i,j ? M k=1 d k i,j |.<label>(4)</label></formula><p>Different from conventional knowledge distillation for classification, the metric distillation loss can guarantee the consistency between the distance metrics from the target model and the decomposed components from the interpreter. Loss function of attribute prior. Only based on the metric distillation loss, the interpreter still cannot decompose the distance in a human understandable way. As discussed in <ref type="bibr" target="#b4">[5]</ref>, without any prior knowledge, the explainer tends to suffer from the biased representations, which makes the network tend to approximate the overall distance only by a few dominant attributes instead of discriminative attributes. To overcome this problem, we define two groups of constraints for the attribute-guided distances.</p><p>The prior constraints are based on the observation that differences of two persons are mainly caused by exclusive attributes like different belongings, rather than common attributes like similar pants. Therefore, given a pair of input images with attributes, i.e., (x i , y i , a i ) and (x j , y j , a j ), the pairwise attribute vector a i,j is computed by</p><formula xml:id="formula_7">a i,j = a i ? a j ,<label>(5)</label></formula><p>where ? is Exclusive OR. With a i,j , we can obtain the common attributes that both x i and x j contain or lack, and the exclusive attributes that only one image contains. Based on a i,j , the first group of constraints are applied to the total contribution of exclusive attributes and that of common attributes, which is formulated by:</p><formula xml:id="formula_8">d i,j ? e ? 1 ? ( M E M ) ? M ? M E .<label>(8)</label></formula><p>Here, we let the above upper bound and lower bound be equal, so the value of ? can be solved by</p><formula xml:id="formula_9">? = 1 2 ln M ? M E ( M E M ) ? M E (1 ? ( M E M ) ? ) .<label>(9)</label></formula><p>From Equation 9, we can see that the upper bound and lower bound are related to the ratios of exclusive attributes and common attributes to all attributes. Based on these priors, we define the second part of the attribute prior loss as:</p><formula xml:id="formula_10">L p2 = M E e=1 max(0, e ?? ( M E M ) ? M E ? d e i? d ij ) + M ?M E c=1 max(0, d c i? d ij ? e ? 1 ? ( M E M ) ? M ? M E ).<label>(10)</label></formula><p>Through the two attributes prior losses, the interpreter will be more focused on the exclusive attributes that make more contribution to the overall difference of two persons. Finally, the interpreter is optimized by the total loss function:</p><formula xml:id="formula_11">L = L d + ?L p1 + ?L p2 ,<label>(11)</label></formula><p>where ? and ? are the balance factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Inference</head><p>Training. Firstly, a ReID model F(?) trained on person ReID data is used as the target model and fixed during learning the interpreter G(?). In each training iteration, we take P ? S images as a mini-batch where P is the number of IDs and S is sample number per ID. In a mini-batch, we can obtain P 2 ? S 2 pairs of images to train the interpreter with Equation 11. For each pair (x i , x j ), we use the distance d i,j generated by F(?) and the attribute vector a i,j as the supervision for training G(?).</p><p>Inference. During testing, given a query image x q and a gallery image x g , The interpreter G(?) can generate the attention maps of attributes </p><formula xml:id="formula_12">(A 1 q , A 2 q , ..., A M q ) and (A 1 g , A 2 g , ..., A M g ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To show the effectiveness and compatibility of the interpreter learned by AMD, we first evaluate our method for different target models on individual datasets. Then the cross-domain experiments are conducted to demonstrate the generalization of the interpreter. At last, we incorporate the interpreter with several state-of-the-art ReID models to achieve superior accuracy on different benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Our experiments is performed on two large-scale person ReID datasets: Market-1501 <ref type="bibr" target="#b42">[43]</ref> and DukeMTMC-ReID <ref type="bibr" target="#b45">[46]</ref>. For convenience, we directly use the ID-level attributes labeled by Lin et al. <ref type="bibr" target="#b17">[18]</ref> to train our interpreter.</p><p>Market-1501 contains 751 training IDs with 19,732 images and 750 testing IDs with 13,328 images. We select 26 attributes labeled by <ref type="bibr" target="#b17">[18]</ref> including: gender (female/male), hair length (long/short), sleeve length (long/short), length of lower clothing (long/short), type of lower clothing (pants/dress), wearing hat (yes/no), carrying backpack (yes/no), carrying handbag (yes/no), carrying other bags (yes/no), 8 colors of upper clothing, and 9 colors of lower clothing. The statistics of attributes in Market1501 is shown in <ref type="figure" target="#fig_3">Figure 3</ref>, which reflects the imbalance of attributes.</p><p>DukeMTMC-ReID contains 702 training IDs with 16,522 images and 702 testing IDs with 19,889 images. For DukeMTMC-ReID, we select 23 attributes for the interpreter. For more details on the attributes of DukeMTMC-ReID, please refer to the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>This subsection presents the implementation details of our framework and training strategy of the interpreter.</p><p>Network Structure. The ReID model F(?) and the interpreter G(?) are built as in Section 3.2. For the target models F(?), we use one of the state-of-the-art ReID models, i.e., Stronger-Baseline (SBS) <ref type="bibr" target="#b11">[12]</ref>, with different backbones, e.g., ResNet-18/34/50/101. The interpreter G(?) uses the same backbone with F(?) and shares the first three CNN stages, i.e., Conv1 to Conv3, from F(?). The rest stages are initialized by parameters pretrained on ImageNet <ref type="bibr" target="#b5">[6]</ref>.</p><p>Networks Training. The interpreter G(?) is trained on the training sets of Market-1501 and DukeMTMC-ReID with the attribute number M = 26 and 23, respectively. We adopt the Adam <ref type="bibr" target="#b13">[14]</ref> optimizer to train G(?) for 30 epochs with the basic learning rate lr = 10 ?4 . The warm-up strategy is used for the first 10 epochs with the initial lr = 10 ?6 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>Although existing interpretation for CNNs are usually demonstrated by visualization and evaluated by subjective observation, we define a group of objective metrics to evaluate the correctness of our interpreter for person ReID.</p><p>Metric for Distillation. Since the interpreter aims to decompose d p,q of (x q , x g ) generated by a target model into a set of components, we first measure the information loss during metric distillation from F(?) to G(?). Given the attribute-guided distances {d k q,g } M from G(?), we sum all items to obtain a reconstructed distanced q,g = M k=1 d k q,g . We report the Average Distance Reconstruction Error (ADRE) over all query-gallery pairs by . Moreover, we use the reconstructedd q,g to perform the ReID task as using d q,g . Thus, we can observe the information loss by comparing the ReID performance, e.g., Rank-1 accuracy (Rank-1) and mean Average Precision (mAP), of F(?) and G(?). Metric for Attribute Decomposition. As we expect the interpreter to find the most discrepant attributes of x q and x g , we measure the ability of G(?) based on whether it can assign more contributions to the exclusive attributes rather than the common attributes. In traditional explainability literature, measures such as the pointing game <ref type="bibr" target="#b37">[38]</ref> or insertion/deletion <ref type="bibr" target="#b22">[23]</ref> are not suitable for our task. The "Point Game" requires the bounding boxes for attributes, while ReID datasets only have image-level labels. The "Insertion/Deletion" and the "Blur Integrated Gradients" <ref type="bibr" target="#b35">[36]</ref> are mainly designed for the classification task. Therefore, we design two metrics X-mAP e and X-mAP c for exclusive attributes and common attributes, respectively.</p><p>Given input (x q , x g ), pairwise attribute vector a q,g , and the attribute-guided distances (d 1 q,g , d 2 q,g , ..., d M q,g , ), we rank the distances in an descending order as the larger value means more difference. Then we compute the Average Precision (AP) of the ranked list like the retrieval task to measure whether exclusive attributes are ranked at the top positions in the list. The X-mAP e is the mean value of the AP values over all query and gallery pairs. Similarly, the X-mAP c is calculated by ranking the distances in an ascending order to measure whether common attributes are ranked the top positions in the list. In our experiments, we evaluate the X-mAP e and X-mAP c on the testing set for all query and gallery pairs except the pairs from the same ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental Results of Different Models</head><p>This subsection presents experiments for interpreting the SBS models with different backbones (ResNet-34/50/101) on Market-1501 and DukeMTMC-ReID. The metrics including the ReID accuracy of the target model and the interpreter, and X-mAP e , X-mAP c , and ADRE of the interpreter are listed in <ref type="table" target="#tab_1">Table 1</ref>. Examples of attribute-guided attention maps (AAMs) and contributions of top-3 attributes generated for SBS (ResNet-50) are shown in <ref type="figure" target="#fig_7">Figure 4</ref>.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, the target model and the corresponding interpreter are grouped for comparison. From ReID accuracy, we can see that interpreters achieve very close accuracy to target ReID models, meanwhile the ADRE between each interpreter and ReID model is also very small. This means that the information loss is very minor during decomposing the distance from the target model into attribute-guided components by distillation. The minor loss is acceptable and reasonable because the attribute-guided interpreter sacrifices some discriminative representations but obtains more    explainable representations. Moreover, the good performance of X-mAP e and X-mAP c objectively reflects the rationality and correctness of interpreter. Furthermore, we can find that the metrics of the interpreters are consistent with the target models on different datasets, which reflects the generalization of the interpreter.</p><p>In <ref type="figure" target="#fig_7">Figure 4</ref>, we show several pairwise images and their explanations for SBS (ResNet-50) models trained on Market-1501 and DukeMTMC-ReID, respectively. For each pair of images, we visualize the attribute-guided attention maps (AAMs) of the top-3 attributes and list the contributions of top-3 attributes to the overall distance. From the AAMs, we can observe that for the positive attributes (labeled as 1), the interpreter can effectively focus on corresponding regions, while for the negative attributes (labeled as 0), the attentions usually spread around persons. Particu-larly, some small objects like backpacks and hats can also be attended to by the interpreter even though they are partially occluded, which shows the effectiveness of the interpreter.</p><p>The effectiveness of the interpreter can also be demonstrated by the top-3 contributory attributes. First of all, the more different the attribute is, the more proportion it contributes to the distance. Taking the first pair of images as an example, they are the same person captured by different cameras. Due to varied viewpoints and illuminations, this person looks different especially on the backpack and the color of upper clothes. The interpreter can effectively assign larger contributions to these discriminative attributes, which can also be observed on other examples.</p><p>To further illustrate attentions learned by the interpreter, we show the average attention maps of individual attributes on two datasets in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Rank-   <ref type="table">Table 3</ref>. Comparison between results of the SOTA methods and refined results by re-weighted distances on Market-1501 and DukeMTMC-ReID. For all compared models, the results are further boosted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Cross-domain Evaluation</head><p>To validate the generalization of our interpreter, we conduct experiments under the cross-domain setting on Market-1501 and DukeMTMC-ReID. Here we use the SBS (ResNet-50) models trained on Market-1501 and on DukeMTMC-ReID as two target models, then learn two interpreters for these two models. During the evaluation, we apply the SBS model and the interpreter trained on Market-1501 to the testing set of DukeMTMC-ReID (M ? D) and vise vase (D ? M). The experimental results are listed in <ref type="table" target="#tab_3">Table 2</ref>. From the cross-domain results, we can find that the ReID metrics of the interpreters are still consistent with those of the target models, meanwhile the ADRE values are also very small. This also reflects that the interpreters can learn consistent knowledge from the target models under the cross-domain setting. Interestingly, we can find that the results of interpreters are better than those of the target models. This may be because that the generalization of the attribute-based representations is stronger than the visual features learned by the target models for cross-domain ReID. Therefore, attributes may have the potential to bridge the domain gap between different datasets. See the supplementary material for more examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Distance Re-weighting by Interpreter</head><p>As a by-product, we try a straightforward method to improve the performance of the state-of-the-art models with the explanations generated by the interpreter. As in Section 3.4, our interpreter can output the contributions of exclusive attributes to the overall distance, i.e., {d e p,q } M E for each pair of query and gallery images. Then we can simply amplify the component of the most contributed attribute to obtain an updated distance. Given an original distance d p,q , the updated distance can be computed by linear reweighting as:</p><formula xml:id="formula_13">d ? i,j = d i,j + ? ? max({d e p,q } M E ),<label>(12)</label></formula><p>where ? is a hyper-parameter and set to 1.0 in experiments.</p><p>Comparison between results of the state-of-the-art (SOTA) methods and refined results by re-weighted distances from interpreters are listed in Tabled 3. On both Market-1501 and DukeMTMC-ReID datasets, the performance of all models is improved. Especially on DukeMTMC-ReID, we obtain 0.9% and 0.7% increases in rank-1 and mAP accuracy for the powerful SBS (ResNet-101). These results demonstrate great potential to explore attributes for further improvement of person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents an Attribute-guided Metric Distillation (AMD) method to use semantic attributes for explainable person ReID. The AMD learns a pluggable interpreter that can be grafted on any target CNN-based ReID model. With the metric distillation guided by attribute priors, the learned interpreter can decompose the distance of two person images into quantitative contributions of attributes by which users can know what attributes make two persons different. Meanwhile, the interpreter can visualize attention maps of discriminative and exclusive attributes to tell users where the most significant attributes are. With such quantitative explanations and intuitive visualizations, the interpreter can help users make decisions more effectively. In future work, the proposed AMD framework can be applied for explanation of other metric-based computer vision tasks like content-based image retrieval <ref type="bibr" target="#b7">[8]</ref>, vehicle ReID <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>In this supplementary material, we first introduce the attributes of the DukeMTMC-ReID <ref type="bibr" target="#b45">[46]</ref> dataset. Then we provide additional experiments for 1) more visualizations of the learned attribute-guided attention maps (AAMs) on Market-1501 <ref type="bibr" target="#b42">[43]</ref> and DukeMTMC-ReID <ref type="bibr" target="#b45">[46]</ref> datasets, 2) visualizations of AAMs under the cross-domain setting, and 3) analysis on different designs of the interpreter networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Details of the DukeMTMC-ReID Dataset</head><p>For DukeMTMC-ReID, we select 23 attributes for the interpreter, i.e., gender (female/male), shoe type (boots/other shoes), wearing a hat (yes/no), carrying a backpack (yes/no), carrying handbag(yes/no), carrying other bags (yes/no), the color of shoes (dark/bright), length of upper clothing (long/short), 8 colors of upper clothing (black, white, red, purple, gray, blue, green, and brown) and 7 colors of lower clothing (black, white, red, gray, blue, green, and brown). The statistics of attributes on DukeMTMC-ReID <ref type="bibr" target="#b45">[46]</ref> annotated by <ref type="bibr" target="#b17">[18]</ref> are shown in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Visualizations of AAMs on Different Datasets</head><p>In this subsection, we exploit an interpreter learned for the target model, i.e., SBS (ResNet-50) <ref type="bibr" target="#b11">[12]</ref> to further study the attention maps learned by the interpreter. Here we generate the average attention maps of individual attributes on the testing set of Market-1501 <ref type="bibr" target="#b42">[43]</ref> and DukeMTMC-ReID <ref type="bibr" target="#b45">[46]</ref>, as shown in <ref type="figure" target="#fig_9">Figure 6</ref> (a) and (b), respectively. In <ref type="figure" target="#fig_9">Figure 6</ref> (a) or (b), the left part shows the positive average attention maps which are obtained from all images that contain a certain attribute. The right part shows the negative average attention maps obtained from all images that do not contain that attribute. From the average attention maps, we can observe that: 1) Overall we can find that the interpreter can effectively focus on the salient regions of most attributes, which is consistent with the observation of humans. 2) For the large-area attributes such as the colors of upper and lower clothes, the interpreter can accurately focus on the corresponding regions. However, there are some worse examples, such as "lowgreen". This may be because these attributes have very few samples in the dataset, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. 3) For those small but discriminative attributes like bag, hair, and handbag, the interpreter can also attend to the areas where the objects are most likely to appear. With this observation, we can figure out how the interpreter captures the differences between different attributes through attention.</p><p>Moreover, we have several interesting findings: 1) The salient region of "gender" is mainly focused on the head and upper body of a person, which is similar to the attention of humans. 2) For DukeMTMC-ReID, the activation of "boots" is focused on both head and feet while the attention of "shoes" is only on the feet. This reflects that the ReID model learns biased knowledge about "boots" since the correlation among "boots", "hair", and "female" is very high as discussed in <ref type="bibr" target="#b17">[18]</ref>. Therefore, the interpreter can help researchers and users find the biases in the datasets and improve the ReID models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Interpretation for Cross-domain Setting</head><p>In <ref type="figure">Figure 7</ref>, we show several pairwise images and their explanations for SBS (ResNet-50) models trained on Market-1501 and DukeMTMC-ReID under the crossdomain setting. For each pair of images, we visualize the attribute-guided attention maps (AAMs) of the top-3 attributes and list the contributions of top-3 attributes to the overall distance.</p><p>From the AAMs, we can observe that for the attributes with similar distributions in two datasets, such as "up-red" and "backpack", the interpreter can effectively focus on corresponding regions under the cross-domain setting and make appropriate explanations. However, for the unique attributes of a certain dataset, e.g., "boots" that only exist in DukeMTMC-ReID, the interpreter would learn nothing about these attributes and generate incorrect explanations.  To solve this problem, a straightforward strategy is to exploit a more comprehensive dataset with more diverse attributes, such as MTMS17 <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Study on Different Designs of Interpreter</head><p>We explore different designs of the interpreter by comparison of interpreter networks that share different numbers of stages from the target ReID model. <ref type="table" target="#tab_4">Table 4</ref> lists the results of interpreters sharing from only one stage to sharing all five stages (N = 1 ? 5). We visualize several AAMs generated by different interpreters (N = 1, 3, 5) in <ref type="figure">Figure 8</ref>.</p><p>From <ref type="table" target="#tab_4">Table 4</ref>, we can find that the quantitative metrics of all variants are very close. This means that all stages of the target ReID model may implicitly learn the knowledge to distinguish attributes. Only based on this observation, we might conclude that sharing five stages and only training the ADH module is the best choice since it needs to train much fewer parameters. However, as shown in <ref type="figure">Figure 8</ref>, the attention maps generated by different interpreters are of great difference. When N = 1, the attention maps are more scattered since the receptive field of the lower stage is relatively small which only focuses on local details. If N = 5, the attention maps are more focused on the centers of objects because the higher stages learn more high-level semantic concepts. By sharing parameters from the middle stage, i.e., N = 3, it can reach an equilibrium point between the low-level patterns and high-level semantics, which is more suitable for the representation of attributes. Therefore, our interpreter shares three stages from the target models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The motivation of attribute-guided metric distillation. (a) Given a query, the ReID model returns a rank list of gallery images based on pairwise metrics. (b) The learned Interpreter can visualize intuitive attention maps of attributes to tell users what attributes make two persons different, and generate contributions of attributes to reflect the impact of each attribute. (c) Refined results by re-weighted distances from Interpreter. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of the attribute-guided metric distillation framework for person ReID. (a) The target ReID model that generates the pairwise distance for an image pair. (b) The interpret network that learns to decompose the pairwise distance into components of attributes and generates attention-guided attention maps for individual attributes. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and output the attribute-guided distances {d k p,q } M by forward propagation. The contribution ratio of attribute k is computed by r k p,q = d k p,q / M k=1 d k p,q .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The statistics of attributes on the Market-1501 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 .</head><label>5</label><figDesc>For the hyper-parameters in Equation 11, ? and ? are set to 10.0 and 50.0, respectively. The ? in Equation 6 is set to 0.The ? and ? of PePU in Equation 2 is set to 1/M and 0.5, respectively. The mini-batch size is 6 ? 4, et al., 6 IDs and 4 samples per ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>DatasetsSBS Models Rank-1 (%) Rank-5 (%) mAP (%) X-mAPe (% ?) X-mAPc (% ?) ADRE (% ?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Pairwise examples and explanations for SBS (ResNet-50) on two datasets. For each pair of images, the upper part visualizes the AAMs of the top-3 attributes, which shows that the AAMs are attended to the discriminative attributes. The lower part shows the overall distance and contributions of the top-3 attributes. These figures show the most contributed attributes discovered by the interpreter. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>The attribute distribution of DukeMTMC-ReID<ref type="bibr" target="#b45">[46]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>up-white up-red up-yellow up-white up-red up-purple up-green up-gray up-purple up-green up-gray up-blue up-black long-upper up-blue up-black longlow-pink low-green low-purple low-pink low-green low-gray low-brown low-blue low-gray low-brown low-up-red up-purple up-blue up-black long-upper up-blue up-black long-Visualization of average attention maps from the interpreter for the target model, SBS (ResNet-50) [12] trained on (a) Market-1501 [43] and (b) DukeMTMC-ReID [46]. In each sub-figure, the left part shows the average attention maps of each attribute which is obtained from all images that contain a certain attribute. The right part shows the average attention maps of each attribute obtained from all images that do not contain that attribute. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Pairwise examples and explanations for SBS (ResNet-50) under the cross-domain setting: (a) The interpreter is learned on DukeMTMC-ReID and applied to Market-1501 (Duke ? Market); (b) The interpreter is trained on Market-1501 and tested on DukeMTMC-ReID (Market ? Duke). For each pair of images, the upper part visualizes the AAMs of the top-3 attributes, which shows that the AAMs are attended to the discriminative attributes. The lower part provides the overall distance and contributions of the top-3 attributes to show the most contributed attributes discovered by the interpreter. (Best viewed in color.) Attention maps from interpreters sharing different stages N from target models. (a) N = 1. (b) N = 3. (c) N = 5. The visualizations reflect the difference of interpreters using different levels of features from the target model. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of interpreters for different backbone models on Market-1501 and DukeMTMC-ReID. Each target model and the corresponding interpreter are grouped for comparison. The results show that the interpreters learn consistent knowledge to the target models for effective explanations.</figDesc><table><row><cell></cell><cell>ResNet-34</cell><cell>93.94</cell><cell>97.74</cell><cell>83.95</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Interpreter</cell><cell>94.21</cell><cell>97.80</cell><cell>84.12</cell><cell>73.71</cell><cell>96.39</cell><cell>2.31</cell></row><row><cell>Market-1501</cell><cell>ResNet-50</cell><cell>94.77</cell><cell>98.13</cell><cell>87.15</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Interpreter</cell><cell>94.74</cell><cell>98.16</cell><cell>87.11</cell><cell>74.29</cell><cell>96.59</cell><cell>1.99</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>95.94</cell><cell>98.40</cell><cell>88.64</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Interpreter</cell><cell>95.55</cell><cell>98.52</cell><cell>88.29</cell><cell>75.40</cell><cell>96.73</cell><cell>1.87</cell></row><row><cell></cell><cell>ResNet-34</cell><cell>86.67</cell><cell>92.77</cell><cell>71.71</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Interpreter</cell><cell>86.13</cell><cell>92.91</cell><cell>72.00</cell><cell>69.58</cell><cell>95.79</cell><cell>1.93</cell></row><row><cell>DukeMTMC-reID</cell><cell>ResNet-50</cell><cell>88.24</cell><cell>94.17</cell><cell>75.54</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Interpreter</cell><cell>87.84</cell><cell>94.34</cell><cell>75.27</cell><cell>70.30</cell><cell>96.03</cell><cell>1.73</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>89.33</cell><cell>95.20</cell><cell>78.41</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Interpreter</cell><cell>89.21</cell><cell>95.15</cell><cell>78.26</cell><cell>70.52</cell><cell>96.11</cell><cell>1.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of the interpreters for SBS (ResNet-50) under the cross-domain setting. M ? D means the SBS models and interpreters are trained on Market-1501 and tested on DukeMTMC-ReID, and D ? M means the reverse setting. The results demonstrate that the information loss of interpreters is very minor under the cross-domain setting.</figDesc><table><row><cell>Models</cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-ReID</cell></row><row><cell></cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>OSNet [48]</cell><cell>94.7</cell><cell>85.7</cell><cell>87.9</cell><cell>74.1</cell></row><row><cell>+ Re-weighting</cell><cell>95.0(+0.3)</cell><cell>86.1(+0.4)</cell><cell>88.5(+0.6)</cell><cell>74.9(+0.8)</cell></row><row><cell>BOT [22] (R50)</cell><cell>93.8</cell><cell>84.7</cell><cell>86.9</cell><cell>74.3</cell></row><row><cell>+ Re-weighting</cell><cell>94.4(+0.6)</cell><cell>86.1(+1.4)</cell><cell>88.6(+1.7)</cell><cell>75.6(+1.3)</cell></row><row><cell>CL [27] (R50)</cell><cell>94.9</cell><cell>85.7</cell><cell>87.1</cell><cell>71.9</cell></row><row><cell>+ Re-weighting</cell><cell>95.2 (+0.3)</cell><cell>86.4 (+0.7)</cell><cell>88.3(+1.2)</cell><cell>73.1(+1.2)</cell></row><row><cell>SBS [12] (R50)</cell><cell>94.8</cell><cell>87.2</cell><cell>88.2</cell><cell>75.5</cell></row><row><cell>+ Re-weighting</cell><cell>95.2(+0.4)</cell><cell>87.9(+0.7)</cell><cell>89.1(+0.9)</cell><cell>75.6(+0.1)</cell></row><row><cell>SBS [12] (R101)</cell><cell>95.9</cell><cell>88.6</cell><cell>89.3</cell><cell>78.4</cell></row><row><cell>+ Re-weighting</cell><cell>96.1(+0.2)</cell><cell>88.8(+0.2)</cell><cell>90.2(+0.9)</cell><cell>79.1(+0.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results of interpreters sharing different numbers N of stages from the target model. The results show that the interpreters have close performance for distance distillation.</figDesc><table><row><cell>Model</cell><cell cols="5">N Rank-1 mAP X-mAPe X-mAPc</cell></row><row><cell>ResNet-50</cell><cell>-</cell><cell>94.77</cell><cell>87.15</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>1</cell><cell>94.76</cell><cell>86.54</cell><cell>73.79</cell><cell>96.53</cell></row><row><cell></cell><cell>2</cell><cell>93.98</cell><cell>86.27</cell><cell>74.31</cell><cell>96.36</cell></row><row><cell>Interpreter</cell><cell>3</cell><cell>94.74</cell><cell>87.11</cell><cell>74.29</cell><cell>96.59</cell></row><row><cell></cell><cell>4</cell><cell>94.52</cell><cell>86.91</cell><cell>74.47</cell><cell>96.79</cell></row><row><cell></cell><cell>5</cell><cell>94.89</cell><cell>87.01</cell><cell>74.24</cell><cell>96.12</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3759" to="3768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adapting grad-cam for embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2783" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining neural networks semantically and quantitatively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="9186" to="9195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian attribute recognition at far distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="789" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explainability for content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hoogs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="95" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3449" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining explanations: An overview of interpretability of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leilani</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayesha</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalana</forename><surname>Kagal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE DSAA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fastreid: A pytorch toolbox for general instance re-identification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A richly annotated dataset for pedestrian attribute recognition. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1603.07054</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving person reidentification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno>abs/2104.11536, 2021. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Largescale vehicle re-identification in urban surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PROVID: progressive and multimodal vehicle reidentification for large-scale urban surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="658" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Black-box explanation of object detectors via saliency maps. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitali</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing deep similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2029" to="2037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="6397" to="6406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and A strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metasearch: Incremental product search via deep meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7549" to="7564" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person transfer GAN to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel deep model with multi-loss and efficient training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si-Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ping (</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-An</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Shuang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="69" to="75" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learningbased methods for person re-identification: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si-Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ping ;</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-An</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Shuang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">337</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="354" to="371" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards global explanations of convolutional neural networks with concept attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weibin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenglin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Black re-id: A head-shoulder descriptor for the challenging problem of person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="673" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attribution in scale and space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9677" to="9686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards rich feature discovery with class activation maps augmentation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6261" to="6270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3239" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Group-aware label transfer for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5310" to="5319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno>abs/1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3701" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
