<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Point Cloud Representation Learning via Separating Mixed Shapes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Self-supervised Point Cloud Representation Learning via Separating Mixed Shapes</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Point cloud</term>
					<term>Pre-training</term>
					<term>Self-supervised learn- ing</term>
					<term>Graph Neural Network</term>
					<term>Representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The manual annotation for large-scale point clouds costs a lot of time and is usually unavailable in harsh realworld scenarios. Inspired by the great success of the pre-training and fine-tuning paradigm in both vision and language tasks, we argue that pre-training is one potential solution for obtaining a scalable model to 3D point cloud downstream tasks as well. In this paper, we, therefore, explore a new self-supervised learning method, called Mixing and Disentangling (MD), for 3D point cloud representation learning. As the name implies, we mix two input shapes and demand the model learning to separate the inputs from the mixed shape. We leverage this reconstruction task as the pretext optimization objective for self-supervised learning. There are two primary advantages: 1) Compared to prevailing image datasets, e.g., ImageNet, point cloud datasets are de facto small. The mixing process can provide a much larger online training sample pool. 2) On the other hand, the disentangling process motivates the model to mine the geometric prior knowledge, e.g., key points. To verify the effectiveness of the proposed pretext task, we build one baseline network, which is composed of one encoder and one decoder. During pre-training, we mix two original shapes and obtain the geometry-aware embedding from the encoder, then an instance-adaptive decoder is applied to recover the original shapes from the embedding. Albeit simple, the pre-trained encoder can capture the key points of an unseen point cloud and surpasses the encoder trained from scratch on downstream tasks. The proposed method has improved the empirical performance on both ModelNet-40 and ShapeNet-Part datasets in terms of point cloud classification and segmentation tasks. We further conduct ablation studies to explore the effect of each component and verify the generalization of our proposed strategy by harnessing different backbones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Visualized results of mixed point clouds. We select seven types of point clouds from ShapeNet-Part <ref type="bibr" target="#b0">[1]</ref>. Each row and column corresponds to the original point clouds and the intersection corresponds to the mixed point cloud. M denotes the number of samples in the training set. In theory, we can generate O(M ?M ) different point clouds by sampling various cloud pairs, resulting in a much larger online generated training sample pool.</p><p>unaffordable expenses <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. In this work, we argue that pre-training is one potential way to relieve data limitation. We are inspired by successes in image recognition, where the pre-training model on ImageNet can efficiently adapt to various computer vision tasks, including image segmentation <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref> and image retrieval <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b21">[22]</ref>. We also note that pretraining on point cloud is still under-explored. To fill this gap, in this work, we resort to model pre-training via selfsupervised learning to reduce the demand for annotated data.</p><p>Most existing works <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> focus on designing pretext tasks by exploring the spatial characteristics of the single point cloud, which does not solve the problem of data limitation in current open-source datasets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>. To address this limitation, we introduce a simple solution Mixing to combine two point clouds as a new mixed point cloud. Benefiting from the batch training of deeply-learned models, each point cloud can be mixed with a large number of point clouds during the whole training process to enlarge the training data pool. The proposed approach is memory and resource-efficient, which can directly process point clouds in a single pass. Compared with texture, which is important in 2D images, shape information is critical for point cloud representation. The human can easily figure out the two 3D models, i.e., chair and airplane, in mixed point clouds, according to the prior knowledge of shape (see <ref type="figure">Fig. 1</ref>). Inspired by the work on 2D clothes changing <ref type="bibr" target="#b24">[25]</ref>, we propose a novel self-supervised task, Mixing and Disentangling (MD) for point clouds. As arXiv:2109.00452v3 [cs.CV] 23 Sep 2022 the name implies, our intuition underpinning the proposed MD is encouraging the network to mine geometric knowledge, i.e., shape-aware features, and such knowledge can be easily transferred to various tasks. As shown in <ref type="figure">Fig. 2</ref>, the pretext task of self-supervised learning is designed to separate original point clouds from the mixed one. Given two input point clouds A of N points and B of N points, the mixing progress outputs mixed point cloud C of N points sampled from A and B. In particular, we adopt a random sampling strategy in each input point cloud and select N 2 points in A and B respectively. After the sampling stage, we concatenate 2 sampled N 2 points together as a new point cloud C with N points, and we also disrupt the indices of these points again to prevent over-fitting the point order. The disentangling process demands the model to mine the key points of both two original point clouds from the mixed point cloud. The decoder output 2 tensors shape of N ? 3, and each reconstructs one input point cloud. In particular, the decoder disentangles a specified point cloud based on its conditional coordinates shown in <ref type="figure">Fig. 3</ref> as the decoder input. For instance, two generated point clouds shown in <ref type="figure">Fig. 2</ref> demands two decoding process (the model should forward twice while the sum loss of two results backward once), one for the plane, and another for the chair. Briefly, our contributions are as follows:</p><p>? Different from most existing pre-training works on image recognition, there do not exist large-scale datasets like ImageNet <ref type="bibr" target="#b25">[26]</ref> for 3D point cloud pre-training. To address this problem, we leverage the mixing process to generate large-scale mixed data for 3D point cloud training; ? Inspired by the human ability to figure out two shapes from one mixed object, we propose a new self-supervised learning method on the point cloud, called Mixing and Disentangling (MD) to learn the geometric prior knowledge without the requisite of annotations; ? As one minor contribution, we implement one basic pipeline to verify the effectiveness of the proposed selfsupervised learning strategy. It contains one encoder with the learnable aggregation function and one instanceadaptive decoder, to learn from the mixed point cloud and conduct the disentanglement. ? Albeit simple, experimental results on two benchmarks, i.e., ModelNet-40 <ref type="bibr" target="#b10">[11]</ref> and ShapeNet-Part <ref type="bibr" target="#b0">[1]</ref>, show that the self-supervised learning model can effectively and efficiently improve the accuracy of classification and segmentation tasks by the pre-training and fine-tuning paradigm. Self-supervised learning on the point cloud also can reduce the network dependence on labeled data. The rest is organized as follows. We introduce existing works in Section II. Section III describes the proposed method with details. Quantitative and qualitative experiments verify the effectiveness of MD pre-training in Section IV, followed by the conclusion in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. 3D Point Cloud Processing</head><p>In recent years, deep learning-based approaches <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref> facilitate the development of point cloud processing. Due to <ref type="bibr">Fig. 2</ref>. A schematic overview of our method. The two original point clouds are mixed and input into encoder E to obtain the embedding vector of the mixed point cloud. A coordinate extracting operation, Erase, is used to extract partial information from the original point cloud for instance disentanglement. Then the embedding vector and instance information are input to the decoder D and the denoising module to generate the original point cloud, as shown in the red line and the blue line, respectively. The reconstruction loss, i.e., L Chamf er , between the generated point cloud and the original point cloud is used as the self-supervision during training. the irregular and disordered characteristics of point clouds, the traditional neural networks in the 2D field, i.e., convolutional neural network (CNN), cannot be directly applied to point clouds. Therefore, researchers resort to various methods as follows. <ref type="bibr" target="#b0">(1)</ref>. Some methods are voxel-based. The voxel-based methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b30">[31]</ref> voxelize the point cloud to obtain a cube with a grid structure, which can be directly processed by 3DCNN. However, the accuracy of this method is limited by the resolution during voxelization. <ref type="bibr" target="#b1">(2)</ref>. Some methods are projection-based. The projection-based methods <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> project the point cloud on multiple planes. The existing 2D CNN is used to extract the projected point cloud features on different planes. After fusing multiple features, the descriptor of the point cloud is obtained, which can be used in downstream tasks. <ref type="bibr" target="#b2">(3)</ref>. Some methods are point-based. PointNet <ref type="bibr" target="#b34">[35]</ref> proposed a neural network structure that directly processes the raw point cloud data. PointNet++ <ref type="bibr" target="#b35">[36]</ref> further considers both the global feature and the local feature. Pointbased approaches can be divided into the graph-based method and the convolution-based method. The graphed-based method treats the point set as a graph <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. The convolution-based method <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> introduces the concept of the convolution kernel in 2D CNN to the point cloud. Taking one step further, recent methods <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> also explore attention and transformer structures, yielding competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-Supervised Learning</head><p>Self-supervised learning is a machine learning paradigm that uses the structural information of the data itself to generate labels required for training. <ref type="bibr" target="#b0">(1)</ref>. Some methods are based on the transformation invariance. In particular, image rotations <ref type="bibr" target="#b43">[44]</ref>, image jigsaw puzzle <ref type="bibr" target="#b44">[45]</ref>, random erasing <ref type="bibr" target="#b45">[46]</ref> , adaptive exploration <ref type="bibr" target="#b46">[47]</ref> have been shown to be helpful. <ref type="bibr" target="#b1">(2)</ref>. Some methods are based on the context of data. This type of method learns the feature by image generation, e.g., image colorizing <ref type="bibr" target="#b47">[48]</ref>, image inpainting <ref type="bibr" target="#b48">[49]</ref> and super-resolution methods <ref type="bibr" target="#b49">[50]</ref>. <ref type="bibr" target="#b50">[51]</ref> adopts a context reconstruction loss for self-supervised temporal modeling. Generative Adversarial Networks (GANs) are used for image generation <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. <ref type="bibr" target="#b2">(3)</ref>. Other methods are based on contrastive learning. The type of method mines the structure information from data by designing loss functions. These methods define positive and negative samples firstly and then conduct metric learning by narrowing the distance between positive pairs and widening the distance between negative pairs. For instance, CPC <ref type="bibr" target="#b53">[54]</ref> introduces anti-noise estimation, while InfoNCE <ref type="bibr" target="#b53">[54]</ref> proposes a loss function based on mutual information. Following the spirits of these works, researchers <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b58">[59]</ref> further design various model architectures and loss functions. Even worse than 2D image datasets, the annotation of 3D point clouds is usually unavailable due to the annotation time costs and human expenses. In recent years, there are several self-supervised learning methods <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b62">[63]</ref>, which explore the point cloud pre-training. For instance, Sauder et al. <ref type="bibr" target="#b22">[23]</ref> propose space reconstruction, PointContrast <ref type="bibr" target="#b23">[24]</ref> is based on multi-view contrastive learning, and DepthContrast <ref type="bibr" target="#b63">[64]</ref> leverages contrastive learning which can handle different input data formats. However, these methods usually require a relatively large dataset, such as Scannet <ref type="bibr" target="#b12">[13]</ref>, and do not solve the data limitation in real-world tasks. To fill this gap, we adopt a simple point cloud mixing strategy to enlarge the small dataset. Some works have already applied the mixing progress to supervised learning as a data augmentation strategy. For example, PointMixup <ref type="bibr" target="#b29">[30]</ref> proposes the shortest path interpolation with EMD (Earth Move Distance), while PointCutMix <ref type="bibr" target="#b64">[65]</ref> searches the one-toone correspondence by EMD. Besides, RSMix <ref type="bibr" target="#b65">[66]</ref> proposes to replace some points by extracting subsets from another point cloud. Differently, our method focuses on self-supervised training of point clouds with the disentangling task while these works only apply the mixing progress as a data augmentation in supervised learning. From another point of view, our method is complementary to the existing work. We can apply our method to conduct self-supervised pre-training on the unlabeled dataset and fine-tune the model on the downstream task with the above data augmentation strategies. The main difference between the proposed method and existing works are: 1). We generate more "within-distribution" training point clouds via Mixing, which have the same mean and variance with the original data. This process largely increases the training pool and lets the model "see" more inlier variants during training. 2). The proposed pretext task is more challenging. Separating original objects from mixed point clouds is more difficult than extracting key points of a single object. 3). We show that the learned models are competitive in various downstream tasks and settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we illustrate the pipeline and the basic model, which simply contains two modules, i.e., encoder, and decoder. Then we explain the training strategy of Mixing and Disentangling, followed by the discussion on components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Our model is to recover two 3D input point clouds according to the corresponding 2D projections from the mixed point cloud. This challenging task motivates the model to learn shape-related geometric knowledge, which <ref type="figure">Fig. 3</ref>. The left part represents a point cloud of N points, and each point has 3D coordinates. After erasing, the random one-dimensional coordinate of each point is set to zero which intends to make the pre-training process more challenging. The right part is the visualization of an erased point cloud of the plane and its three-view drawing after erasing. The erasing process equals randomly set about <ref type="bibr" target="#b0">1</ref> 3 N points to the corresponding projection surface.</p><p>benefits downstream tasks. As shown in <ref type="figure">Fig. 2</ref>, the structure is mainly composed of one encoder and one decoder. The encoder is to extract the embedding vector of the mixed point cloud. Given the output features from the encoder, the decoder is used to restore the original point clouds based on the embedding vector and the conditional input from partial coordinates of the original point cloud (see <ref type="figure">Fig. 3</ref>). After pre-training, the encoder can be utilized to extract discriminative features for subsequent tasks, e.g., classification, and segmentation. As shown in <ref type="figure" target="#fig_0">Fig. 4</ref>, the black arrows denote the fine-tuning workflow for recognition, while the blue arrows are the segmentation workflow. We denote the input point cloud as a set of points S = {s 1 , s 2 , ..., s n }, s i ? R 3 and each point has 3D coordinates. However, the point cloud does not have a regular spatial structure as 2D images, we can not apply the convolutional neural network (CNN) directly. Therefore, we apply K-Nearest Neighbor (KNN) algorithm to construct the graph structure G in the feature space. The KNN algorithm based on the feature inputs can efficiently and effectively find two points with the most similar semantics, i.e., the points on the two legs of the chair with similar semantics. The graph can be expressed as G = (V, E), where V represents the set of vertices and E ? V ? V represents the edge set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 4</ref>, the backbone of the encoder is composed of three EdgeConv layers and three learnable aggregation (LA) layers. Specifically, we deploy EdgeConv layers to leverage the graph information. Given the input of N ? H in and the corresponding graph G, the EdgeConv layer performs feature transformation on the N points and outputs the updated point features of N ? H out . The input features can be represented as X = {x 1 , x 2 , ..., x n }, x i ? R Hin where x i represents the feature of point i. The relation feature r k i ? R N ?Hout can be formulated via an EdgeConv layer as:</p><formula xml:id="formula_0">r k i = M LP (concat(x i , x i ? x k )), k : (i, k) ? E, k = i,<label>(1)</label></formula><p>explicitly encodes the relationship between x i and x k , where x k is one neighbor of x i and M LP notes a linear function with ReLU <ref type="bibr" target="#b66">[67]</ref>. To consider all the neighbor of x i , we apply Learnable Aggregation (LA) to the local neighbor relation r k i . We notice that the current widely-used aggregation methods applied in the point cloud include both max pooling and average pooling. It is usually challenging to decide which pooling function should be used. As one minor contribution, we involve Learnable Aggregation (LA), which is a learnable pooling layer, and let the model learn the adaptive weight:</p><formula xml:id="formula_1">LA(x i ) = ?? max (i,k) ?E,k =i (r k i )+(1??)? avg (i,k) ?E,k =i (r k i ),<label>(2)</label></formula><p>where the interpolation ratio ? is a learnable parameter of the network. Finally, the encoder backbone outputs the point cloud feature map by combining intermediate features from multiple layers as <ref type="bibr" target="#b67">[68]</ref> to enhance the representation capability.For the classification downstream tasks, we add the max pooling layer to compress the feature map to the vector, which is sent to the decoder. When fine-tuning the downstream classification, we simply add one linear classifier to this learned vector. As for the segmentation downstream task, which is a dense point-level task, the vectorization compromises the spatial representation. Therefore, we concatenate the intermediate feature maps to keep the same structure for fine-tuning. We also add the extra category vector to specify part predictions, because different objects contain different semantic parts. For instance, the plane contains the aircraft nose, plane body, plane tail, and wings while the chair has legs, the seat, and the back. When fine-tuning, similarly, we only need to add one linear classifier to the learned feature map.</p><p>Discussion. By using the proposed mixing mechanism, we can generate more "within-distribution" point cloud samples, which largely enriches training samples. The previous methods <ref type="bibr" target="#b22">[23]</ref> use ShapeNet-Part for pre-training and it can only The encoder can mine more prior knowledge from data, which improves downstream tasks. Besides, to extract the features more efficiently, we propose a new feature aggregation method LA in the point cloud. Most existing works <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref> apply max pooling to aggregate the neighboring features, which leads to much information loss. LA leverages a learnable parameter to dynamically adjust the aggregation method, which can effectively retain the local structure. More ablation studies on LA are provided in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Decoder</head><p>We propose an instance-adaptive decoder, which can restore the input point cloud from the embedding vector according to the conditional coordinates adaptively. The decoder treats the embedding vector as the structure representation of the mixed point cloud and aims to disentangle key points from different objects. In our network, the decoder is utilized as a selective filter, which gradually enhances the feature of points belonging to one object and weakens the feature of the rest points of another object. After several transformations, the feature mainly contains the key points of one object, which can be transformed to the recovered point cloud by the final linear layers. As shown in <ref type="figure" target="#fig_0">Fig. 4</ref>, the decoder is composed of Instance-aware Residual Block (InsResBlock) <ref type="bibr" target="#b68">[69]</ref>, 1 ? 1 Convolution Layers and one denoising module i.e., Denoise-Block. There are two inputs to the decoder: the mixed point cloud embedding vector f , the conditional coordinates C coord of size N ? 3. To recover the original point cloud from the mixed feature, the 2D projection C coord of each point cloud is given during training. We use the random two-dimensional coordinates of the original point cloud as partial information. As shown in <ref type="figure">Fig. 3</ref>, we randomly erase <ref type="bibr" target="#b45">[46]</ref> one of the three coordinates of each point cloud. For each point in a point cloud, the erased dimensions are randomly generated instead of taking a certain dimension. Since the direct erasing of a certain dimension will cause the point cloud to change from N ?3 to N ? 2, the neural network cannot distinguish the specific dimensions of the remaining two coordinates. For instance, the model can not foreknow the two coordinates are (X,Y) or (X,Z) or (Y,Z). We choose to set the erasing coordinates to zero instead of directly deleting the dimension. Instanceaware Residual Block (InsResBlock) is a basic module used to disentangle the mixed feature from f according to the conditional coordinates C coord . The structure of InsResBlock is shown in <ref type="figure" target="#fig_0">Fig. 4 (a)</ref>, which is composed of two convolution layers and instance normalization <ref type="bibr" target="#b69">[70]</ref>. A pair of InsResBlock and 1?1 convolution layer is regarded as a basic decoding unit. Our decoder D is built with three sequential basic decoding units, which are connected sequentially to reduce the feature dimension layer by layer and finally output the generated point cloud of 3 coordinate channels. For each basic encoding unit, we concatenate the conditional coordinates and the feature map from the previous unit to specify the reconstruction target. To refine the generated point cloud, we further introduce a denoising module as the last unit of decoder. As shown in <ref type="figure" target="#fig_0">Fig. 4 (b)</ref>, the denoising module is implemented via a selfattention manner. The module leverages the context information between neighbor points to assign different weights for adjacent points and noisy points, which can ensure each point will be optimized with the global information. Given the noisy point cloud s, the denoising module outputs the denoising point cloud? = Denoise( s), s = D(f, C coord ), where D represents the decoder. In particular, given the tensor s of N ? 3, we perform average pooling in the dimension of N to obtain an initial weight of N ? 1 for every point. We apply two convolution layers to aggregate the global information from all points and normalize the weight of each key point via the sigmoid function. Then the weight is multiplied by the original point cloud to obtain the denoised point cloud. The multiplication allows every point to refer to the neighbor points and ignore outlier points with low scores.</p><p>Discussion. The choice of conditional disentanglement.</p><p>The choice of C coord has a large impact on the reconstruction results and downstream tasks. If the decoder can only obtain little original point cloud information from C coord , the reconstruction effect of disentanglement will be poor. On the contrary, if the decoder obtains too much information from the original point cloud, the decoder will restore the original point cloud based on the conditional information directly. Then the encoder cannot be well trained to fully mine the geometric information. We think this choice is still open and in this paper, we observe that using random two-dimensional coordinates is a balanced choice, which guarantees the completeness of the disentanglement and the challenge of the pre-training task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head><p>As the target of the proposed Mixing and Disentangling task (MD), we supervise the model training via reconstruction. Besides, we add the intermediate embedding loss for selfsupervision. There are many candidates for embedding loss. In this work, without loss of generality, we select contrastive loss to verify the compatibility of the proposed method instead of pursuing the best loss. Next, we describe the two objectives.</p><p>Reconstruction loss. We apply Chamfer distance to measure the distance between the original point clouds s and the reconstructed one?. As Eq. 3 shows, the distance is a symmetric function. Since we could not know the exact matching between two point sets, the Chamfer distance accumulates one sub-optimal but effective distance between the closest points in both the original and reconstructed point cloud. The reconstruction loss can be formulated as:</p><formula xml:id="formula_2">L Chamf er = 1 |?| p?? min p?s ||p ?p|| 2 + 1 |s| p?s min p?? ||p ? p|| 2 ,<label>(3)</label></formula><p>where p denotes the point position in s, andp denote the points in?. || ? || 2 denotes the L2 distance between two points and | ? | denotes the number of points. Specifically, we apply the Chamfer distance as the reconstruction loss on s and?.</p><p>Embedding loss. Inspired by contrastive loss <ref type="bibr" target="#b70">[71]</ref>, we introduce it to our total loss function, which intends to widen the distance between different categories. In particular, since we apply a mix strategy, we view every sample as one single category. Following the existing practise, e.g., Instance loss <ref type="bibr" target="#b71">[72]</ref> and MoCo <ref type="bibr" target="#b55">[56]</ref>, we encourage that these point clouds should have different embeddings and deploy metric as an optimization objective. Given a mini-batch of B different mixed point clouds, we construct B ? B pairs of samples by calculating the distance G ij between every pair (i and j). The basic contrastive loss can be formulated as:</p><formula xml:id="formula_3">LCon = 1 B 2 B i=1 B j=1 [yGij 2 + (1 ? y)max(0, 1 ? Gij) 2 ],<label>(4)</label></formula><p>We can transfer the distance into the similarity format. Considering Q ij = 1 ? G ij , Eq. 4 can be rewritten as:</p><formula xml:id="formula_4">LCon = 1 B 2 B i=1 B j=1 [y(1 ? Qij) 2 + (1 ? y)max(0, Qij) 2 ],<label>(5)</label></formula><p>where y = 1 only if i == j, otherwise y = 0. In practise, we adopt the cosine similarity Q ij = fi?fj fi fj . Following the previous self-supervised work <ref type="bibr" target="#b55">[56]</ref>, we ignore the rare case that i and j belong to the cloud of the same mixed category. Moreover, we transfer Eq. 5 to the matric format for better efficiency, and adopt L1 loss, which is relatively stable. Considering Q ij ? [?1, 1], we optimize the distance between and an identity matrix I, which can be formulated as:</p><formula xml:id="formula_5">L Con = | Q + 1 2 ? I|.<label>(6)</label></formula><p>Finally, to optimize parameters in both encoder and decoder, we deploy the total loss as follows:</p><formula xml:id="formula_6">L total = L Chamf er + ?L Con ,<label>(7)</label></formula><p>where ? is the weight to control Contrastive loss. Actually, Contrastive loss is an optional choice. In the ablation studies, we also study the effect of Contrastive loss by setting ? = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Datasets.</p><p>(1) ModelNet-40 <ref type="bibr" target="#b10">[11]</ref> is sampled from the mesh surfaces of CAD models, containing 40 categories, 12,311 models. 9,843 models are used for training and 2468 are reversed for testing. Each point cloud contains 2048 points and the coordinates of all points are normalized into the unit sphere. Following existing works <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b75">[76]</ref>, we sample 1024 points from each object and augment the data by randomly scaling objects and perturbing point locations. (2) ShapeNet-Part <ref type="bibr" target="#b76">[77]</ref> is sampled on the CAD models, having a total of 16,881 point clouds, of which 12,137 point clouds are used for training, 1870 point clouds are used for verifying, and 2874 point clouds are used for testing. Each point cloud is composed of 2048 points, and the coordinates of all points are normalized into the unit sphere. We sample 1024 points from each object. Each point has 4 attributes including the 3D coordinates of each point and the category label of each point.</p><p>(3) S3DIS <ref type="bibr" target="#b77">[78]</ref> is a real-scan dataset composed of six large scale indoor areas with 271 rooms. Each room is split with 1m ? 1m area into blocks. We sample 4096 points from each block. Each point has 9 attributes including 3D coordinates, RGB channels and normalized 3D coordinates of each point.</p><p>Implementation. The pre-trained model is trained with Adam optimizer (? 1 = 0.9, ? 2 = 0.99) of a minibatch of 12 for 200 epochs. The initial learning rate is set to 1e-4. We gradually decrease the learning rate via the cosine policy <ref type="bibr" target="#b78">[79]</ref>. Following existing works <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b75">[76]</ref>, we adopt position jittering as data augmentation and set k = 20 for the KNN algorithm to build the dynamic graph. In the classification network, to fairly compare with other works, we deploy four EdgeConv layers and the channel number is {64, 64, 128, 256} by default. In the segmentation network, we adopt three EdgeConv layers and the channel number is {64, 64, 64}. Two dropouts with 0.5 drop rate are used when fusing the conditional coordinates and the embedding vector. We deploy one Nvidia RTX 3090 by default. The pre-training time is  It is worth noting that the complexity of our model mainly depends on the encoder structure. Our method is open to different encoder choices, which can be selected according to the computing resource. The proposed method can converge smoothly as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. In the finetuning stage, we add a linear classifier for both classification and segmentation tasks. The segmentation task demands extra category vector (see <ref type="figure" target="#fig_0">Fig. 4</ref>). We keep a random category vector during pre-training to hold the position and provide the real category vector during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative &amp; Qualitative Results</head><p>The pre-training improves the baseline. We evaluate how the pre-training affects the performance in Tab. I. For the classification task, the model is pre-trained and fine-tuned on ModelNet-40. We note that the pre-trained model gets an overall accuracy of 93.39%, which have +0.65% than the baseline (92.74%). Similarly, the mean class accuracy also increases from 89.88% to 90.26% with +0.38% accuracy improvement. <ref type="figure">Fig. 7</ref>. Visualization of the distance between input point clouds P A and P B in 3 layers of the decoder. We observe that the distance between P A and P B becomes larger from the shallow layer to the deep layer in the decoder. <ref type="figure">Fig. 8</ref>. Visualization of the key points of the mixed point cloud. a) is the point cloud mixed by b) and c), and the key points of a) is indicated by red points. The result shows that our encoder can successfully capture the key points of two original point clouds after pre-training.</p><p>For the segmentation task, the model is pre-trained and finetuned on ShapeNet-Part. The pre-trained model achieves a competitive mean Intersection-over-Union (mIoU) of 85.50%. Experimental results show that the accuracy of our baseline surpasses three existing works, and pre-training can still bring gains to the accuracy both in classification and segmentation tasks, which verifies the effectiveness of our pretext task.</p><p>Visualization of the reconstructed point cloud.</p><p>(1) As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we visualize original point clouds, mixed point clouds, and generated point clouds on ShapeNet-Part. Our method achieves good reconstruction results, recovering the key geometry. (2) We further track the decoder activation changes during reconstruction. Given one mixed embedding (extracted from mixed P A and P B ), we visualize the disentangle process via Chamfer distance (see <ref type="figure">Fig. 7</ref>). There are three layers in our decoder and the channel of each layer output is 128, 64, and 3. We normalize the output feature in the channel dimension and divide the Chamfer distance by the channel dimension for distance calculation. We observe that the distance between P A and P B becomes larger from the shallow layer to the deep layer in the decoder. It indicates that the decoder follows the conditional information and chooses different features for reconstructing P A and P B respectively.</p><p>Visualization of key points. We visualize the top 25% key points of the mixed point cloud, which has max activation after the last pooling layer in the encoder (see <ref type="figure">Fig. 8</ref>). The result shows that the key points of a) include the key points of both b) and c), which verifies that the pre-trained embedding learns the salient geometric information of both input point clouds. Visualization of embeddings. To verify the scalability of learned embeddings, we extract embeddings of ShapeNet-Part via the pre-trained model on ModelNet-40. We apply the T-SNE <ref type="bibr" target="#b79">[80]</ref> to reduce the dimension of the embedding vector to R 2 for plotting. As shown in <ref type="figure" target="#fig_5">Fig. 9</ref>, the distance between intra-class samples is small and the distance between interclass is large. The above result supports the generalization of the pre-trained encoder to unseen point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies and Further Discussion</head><p>Performance on the real-scan dataset. We further conduct our pre-training process on S3DIS, a real-scan dataset. S3DIS has 6 areas and we follow existing works <ref type="bibr" target="#b42">[43]</ref> using area 5 for testing and others for training. We pre-train our model in the training part and then we fine-tune the pre-trained model on ModelNet-40 for the classification task, on ShapeNetpart for the part segmentation task and on S3DIS for the segmentation task. As shown in Tab. II, our method benefits the performance both on ModelNet-40 and ShapeNet-part. It verifies the scalability of the model pre-trained on the large dataset. Furthermore, in the real-scan dataset, our model with pre-training achieves better performance 51.74% mIoU than the model trained from scratch (50.78%) by a clear margin.</p><p>Illustration of "within-distribution" data. The "withindistribution" means that generated point clouds have similar characteristic (e.g., mean and variance) with the original point clouds. Some existing works apply GAN to generate 3D point clouds. This line of works may change the data distribution (mean and variance), since 3D point clouds are typically generated from random Gaussian noise <ref type="bibr" target="#b80">[81]</ref>- <ref type="bibr" target="#b82">[83]</ref>. Compared with these GAN-based methods, new point clouds generated by our method are more similar to the original ones from a statistics view. We sample 1000 point clouds from ModelNet-40 and plot the distance between each point to its center point. In <ref type="figure">Fig. 10</ref>, we observe that the distribution between the original data and mixed 3D point clouds is almost identical.  show that the distance between intra-class samples is small while the distance between inter-class data is large, which verifies that our encoder learns a robust and general structure embedding vector.</p><p>Comparisons under the semi-supervised setting. In some real scenarios, labeled data is inadequate. To simulate this harsh situation, we divide the original dataset into two parts A and B, and regard A as unlabeled data. We use A + B to pre-train the model and use B to fine-tune the model.</p><p>(1) We set the percentages of labeled data as 10%, 25%, and 50% respectively. We verify the effect of pre-training by comparing the encoder trained from scratch with the encoder pre-trained on ShapeNet-Part in Tab. III. The results show that as the amount of labeled data increases, the accuracy of segmentation gradually increases. The performance of the pretrained model generally surpasses that of the model trained from scratch, especially when the labeled data is extremely limited. This verifies that our method can successfully leverage unlabeled data to improve the accuracy of the model with limited annotated data. (2) Although our work is not designed for the semi-supervised setting, it can nevertheless be coupled with other semi-supervised learning methods. To verify this point, we build a preliminary semi-supervised baseline based on predicted pseudo labels. We adopt 10% labeled data to train the baseline model. Pseudo labels for the rest 90% unlabeled data are then predicted by this baseline model. We select the unlabeled data with the pseudo label confidence greater than 0.7 and the original 10% labeled data to form the new training set, and then fine-tune the segmentation model. In Tab. IV, the model trained on the pseudo-labeled data improves the mIoU performance from 81.59% to 82.50%, but is suffer from the label noise, which compromises average accuracy. Our pre-training method can be coupled with the pseudo label to relieve the negative impact from noisy annotations. Specifically, initializing the model with our pre-training weight <ref type="figure">Fig. 10</ref>. We sample 1000 point clouds and count the distance from each point to the center of its point cloud. The X-axel means the distance from each point to the center point and the Y-axel means the number of points belonging to each distance interval. We observe that both the mean and std by mixing 3D point clouds are identical to the original data distribution. We compare the performance of the model initialized randomly (baseline) and initialized with our pre-training method in Tab. V. The accuracy of our method surpasses the model trained from scratch by a clear margin. Scalability of learned embeddings. We apply a simple linear classifier to directly classify learned embeddings. The classifier consists of a linear layer, a batch norm layer and a linear layer. We arrive at a competitive accuracy of 89.63% with the fine-tuning result of 93.39% on ModelNet-40. Similar to the observation in the large pre-training model CLIP <ref type="bibr" target="#b83">[84]</ref>, fine-tuning helps the pre-trained model to achieve better performance on specific sub-tasks, while directly using learned features also is a sub-optimal but efficient choice.</p><p>Effect of the learnable aggregation. We deploy LA to aggregate neighbor point features. We apply the same network architecture to compare the accuracy of LA, max pooling, and attentive pooling <ref type="bibr" target="#b37">[38]</ref>. As shown in Tab. VI, regardless of pretraining, the accuracy of using LA is higher than that of using max pooling or attentive pooling. The observation verifies that LA can better preserve the local context of the point cloud. There are 3 LA modules in our model and concrete numbers of learned ? in each layer are 0.6996, 0.6503, and 0.5500 from shallow layer to deep layer in the encoder. It means that in shadow layers, max pooling plays an important role to reduce the redundant points, and in deep layers, average pooling is as important as max pooling to aggregate the feature.</p><p>Effect of the contrastive loss. We apply the contrastive loss to narrow the distance between similar samples and widen the <ref type="figure">Fig. 11</ref>. Visualization results of generated point clouds. The original point cloud is shown in a). b) is the generated point cloud without denoising and c) is the point cloud after denoising. The result shows that the denoise module can help to remove the noise point circled in b). distance between different samples. The encoder can extract more robust embedding from data, which can improve downstream tasks. We train two pre-trained models with or without the contrastive loss and then fine-tune the two models on ShapeNet-Part, and finally, compare the segmentation accuracy of the two models in Tab. VI. We observe that the contrastive loss can further improve the segmentation performance. Effect of DenoiseBlock. The visualization results are shown in <ref type="figure">Fig. 11</ref>. a) is the original point cloud, and b) is the generated point cloud obtained by the network without the denoising module. c) is the generated point cloud obtained by the network with the denoising module. We can find that there is a noise point circled in the red circle of b). The denoising module successfully pulls the noise point back to the point cloud by considering the global feature.</p><p>Compatibility with different backbone structures. To verify that our pretext task is free from the choice of backbones, we further explore leveraging several widely-adopted architecture as the backbone of the point cloud encoder, such as PointNet++ <ref type="bibr" target="#b35">[36]</ref>, OGNet <ref type="bibr" target="#b75">[76]</ref>, PAConv <ref type="bibr" target="#b40">[41]</ref>, Point Transformer <ref type="bibr" target="#b42">[43]</ref> and Point Cloud Transformer <ref type="bibr" target="#b41">[42]</ref>. We compare the accuracy of the model trained from scratch and initialized with the pre-trained model. We carry out experiments of classification respectively and results are shown in Tab. VII. We observe consistent improvements with these backbones which verifies the generality of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a new self-supervised learning method, called Mixing and Disentangling (MD), for point cloud pre-training. Different from existing works, we propose to mix the original point clouds in the training set to form "new" data and then demand the model to "separate" the mixed point cloud. In this way, the model is asked to mine Overall Accuracy; mA: Mean Class Accuracy; mIoU: mean IoU; * : We re-implement the model, which achieves a slightly different performance. the geometric knowledge, e.g., the shape-related key points for reconstruction. To verify the effectiveness of the proposed method, we build a simple baseline to implement our method. We use an encoder to obtain the embedding of the mixed point cloud and then an instance-adaptive decoder is harnessed to separate the original point clouds. During the self-supervised training, the encoder learns the prior knowledge of point cloud structure, which is scalable and can improve the downstream tasks. Experiments show consistent performance improvement through classification and segmentation tasks and verify the effectiveness of our method especially when the number of labeled data is limited. We hope that our approach can benefit the future point cloud works, and take one closer step to the harsh real-world setting, i.e., limited annotations. In the future, we will continue to study the point cloud pre-training method on large-scale datasets, and focus on finding an efficient way to take advantage of the large-scale point data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Pipeline. The model is mainly composed of one encoder (left) and one decoder (right). Encoder: The encoder takes N points as input and aggregates the special features of each point and its corresponding points in the neighborhood at an EdgeConv layer. The classification network (top branch) contains three EgdeConv layers and three learnable aggregation (LA) layers. The output of the encoder is the embedding vector f of the input point cloud. In the segmentation network, to concatenate with the point-wise feature from the shallow layers, the corresponding embedding vector f is repeated N times for point segmentation, where N is the number of points. The feature maps from shallow layers are concatenated with the embedding vector to obtain the multiple-scale feature map as the output of the encoder. Different from the classification branch, the category vector representing the type of point cloud is fused into the embedding vector for the part segmentation. Decoder: The decoder is composed of Instance-aware Residual Block (InsResBlock), 1 ? 1 Convolution Layers and one DenoiseBlock to refine the final reconstructed point cloud. The structure of InsResBlock and DenoiseBlock is shown in a) and b). The random two-dimensional coordinates of the original point cloud are used as conditional information to be fused with the backbone features in three different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>generate O(M ) training samples, where M is the number of point clouds. In theory, our method can generate O(M ? M ) different point clouds by sampling various cloud pairs, resulting in a much larger online generated training sample pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The loss curve on ModelNet-40 test set during pre-training. We observe that the proposed method can converge smoothly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of recovered point clouds after disentangling. The experiment is performed on the Shapenet-Part. We observe that the proposed method can successfully disentangle the mixed point cloud into two separate point clouds. In line 5, the chair and the table have a similar structure. There is a hole in the middle of the table and the model tends to fill this hole which makes the disentangling more challenging. about one day. FLOPs (Floating Point Operations) of the whole model is 2.798 GFLOPs and the number of parameters is 2.070 M. The test time is about 0.0019 seconds per point cloud. FLOPs and the number of parameters of the decoder are 0.729 GFLOPs and 0.712 M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>The visualization of embeddings on ShapeNet-Part in the semantic space via T-SNE<ref type="bibr" target="#b79">[80]</ref>. The encoder is pre-trained on ModelNet-40 and is applied to encode the point clouds on ShapeNet-Part directly. The results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc>FOR OUR BASELINE AND BASELINE + PRE-TRAINING. WE CARRY OUT TWO EXPERIMENTS OF CLASSIFICATION AND SEGMENTATION ON MODELNET-40 AND SHAPENET-PART RESPECTIVELY AND COMPARE THE ACCURACY WITH SEVERAL COMPETITIVE APPROACHES. THE BASELINE MODEL WITH PRE-TRAINING ACHIEVES THE HIGHEST ACCURACY.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">ModelNet-40 ShapeNet-Part</cell></row><row><cell>Methods</cell><cell>Publication</cell><cell cols="3">Classification Segmentation</cell></row><row><cell></cell><cell></cell><cell cols="3">OA (%) mA (%) mIoU (%)</cell></row><row><cell>3DShapeNets [11]</cell><cell>CVPR'15</cell><cell>84.7</cell><cell>77.3</cell><cell>-</cell></row><row><cell>VoxNet [31]</cell><cell>IROS'15</cell><cell>85.9</cell><cell>83.0</cell><cell>-</cell></row><row><cell>PointNet [35]</cell><cell>CVPR'17</cell><cell>89.2</cell><cell>86.0</cell><cell>83.7</cell></row><row><cell>PointNet++ [36]</cell><cell>NeurIPS'17</cell><cell>91.9</cell><cell>-</cell><cell>85.1</cell></row><row><cell>SpecGCN [73]</cell><cell>ECCV'18</cell><cell>91.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">PCNN by Ext [74] SIGGRAPH'18 92.2</cell><cell>-</cell><cell>85.1</cell></row><row><cell>DGCNN [39]</cell><cell>TOG'19</cell><cell>92.9</cell><cell>90.2</cell><cell>85.1</cell></row><row><cell>Point Trans. [43]</cell><cell>ICCV'21</cell><cell>93.7</cell><cell>90.6</cell><cell>86.6</cell></row><row><cell>PAConv [41]</cell><cell>CVPR'21</cell><cell>93.9</cell><cell>-</cell><cell>86.1</cell></row><row><cell>CurveNet [75]</cell><cell>ICCV'21</cell><cell>94.2</cell><cell>-</cell><cell>86.8</cell></row><row><cell>Ours (from scratch)</cell><cell>-</cell><cell cols="2">92.74 89.88</cell><cell>85.27</cell></row><row><cell>Ours (pre-training)</cell><cell>-</cell><cell cols="2">93.39 90.26</cell><cell>85.50</cell></row></table><note>OA: Overall Accuracy; mA: Mean Class Accuracy; mIoU: mean IoU</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>PERFORMANCE OF OUR METHOD IN THE REAL-SCAN DATASET. WE PRE-TRAIN THE MODEL ON S3DIS AND FINE-TUNE THE MODEL IN 3 DIFFERENT TASKS. THE RESULTS SUGGEST A CONSISTENT IMPROVEMENT.</figDesc><table><row><cell></cell><cell cols="2">ModelNet-40</cell><cell>ShapeNet-Part</cell><cell>S3DIS</cell></row><row><cell>Methods</cell><cell cols="2">Classification</cell><cell cols="2">Segmentation Segmentation</cell></row><row><cell></cell><cell cols="2">OA (%) mA (%)</cell><cell>mIoU (%)</cell><cell>mIoU (%)</cell></row><row><cell cols="2">Ours (from scratch) 92.74</cell><cell>89.88</cell><cell>85.27</cell><cell>50.78</cell></row><row><cell>Ours  *</cell><cell>92.78</cell><cell>90.06</cell><cell>85.37</cell><cell>51.74</cell></row><row><cell cols="5">OA: Overall Accuracy; mA: Mean Class Accuracy; mIoU: mean IoU;</cell></row><row><cell></cell><cell cols="3">*  means pre-training on S3DIS</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="5">SEGMENTATION RESULTS ON PARTIAL LABELED DATA. THE</cell></row><row><cell cols="5">PERFORMANCE BOOST IS MORE SIGNIFICANT WHEN LABELED DATA IS</cell></row><row><cell cols="5">LIMITED, VERIFYING OUR INTUITION TO BENEFIT THE REAL-WORLD</cell></row><row><cell cols="5">MODEL TRAINING UNDER THE DATA LIMITATION.</cell></row><row><cell cols="5">Labeled Data Ratio (%) Pre-trained Average Accuracy (%) mIoU (%)</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell>74.02</cell><cell>81.59</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell>77.04</cell><cell>82.23</cell></row><row><cell>25</cell><cell></cell><cell></cell><cell>80.39</cell><cell>83.76</cell></row><row><cell>25</cell><cell></cell><cell></cell><cell>81.11</cell><cell>84.49</cell></row><row><cell>50</cell><cell></cell><cell></cell><cell>81.46</cell><cell>84.71</cell></row><row><cell>50</cell><cell></cell><cell></cell><cell>83.31</cell><cell>85.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV SEMI</head><label>IV</label><figDesc>-SUPERVISED SEGMENTATION RESULTS ON 10% LABELED DATA. WE CAN OBSERVE TWO POINTS: (1) BOTH PSEUDO LABEL-BASED APPROACH AND OUR METHOD CAN IMPROVE THE PERFORMANCE. (2) OUR METHOD CAN BE COUPLED WITH OTHER SEMI-SUPERVISED LEARNING METHODS TO IMPROVE THE PERFORMANCE.</figDesc><table><row><cell>Methods</cell><cell cols="2">Pre-trained Average Accuracy (%) mIoU (%)</cell></row><row><cell>baseline</cell><cell>74.02</cell><cell>81.59</cell></row><row><cell>baseline</cell><cell>77.04</cell><cell>82.23</cell></row><row><cell>pseudo label</cell><cell>72.39</cell><cell>82.50</cell></row><row><cell>pseudo label</cell><cell>73.57</cell><cell>82.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V WE</head><label>V</label><figDesc>SELECT 20 CLASSES OF 40 CATEGORIES ON MODELNET-40 FOR PRE-TRAINING, AND THE REST 20 CLASSES FOR FINE-TUNING AND TESTING. THE ACCURACY OF OUR METHOD SURPASSES THE MODEL TRAINED FROM SCRATCH BY A CLEAR MARGIN.can help to predict more robust pseudo labels. Therefore, semisupervised methods with our pre-training can arrive at better average accuracy from 72.39% to 73.57% and mIoU from 82.50% to 82.81%. (3) We further study category-wise semisupervised setting. In particular, we select 20 classes of 40 categories on ModelNet-40 for pre-training, and the rest 20 classes for fine-tuning and testing, while the compared baseline model is training and testing only on the remaining 20 classes.</figDesc><table><row><cell>Methods</cell><cell>OA (%)</cell><cell>mA (%)</cell></row><row><cell>baseline</cell><cell>95.07</cell><cell>93.82</cell></row><row><cell>pre-training</cell><cell>96.65</cell><cell>94.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDY ON DIFFERENT COMPONENTS DURING TRAINING. WE VERIFY THE EFFECTS OF LA MODULE AND CONTRASTIVE LOSS.</figDesc><table><row><cell>Pooling Method</cell><cell>Pre-trained Contrastive loss mIoU (%)</cell></row><row><cell>Max Pooling</cell><cell>85.17</cell></row><row><cell>Max Pooling</cell><cell>85.34</cell></row><row><cell>Attentive Pooling [38]</cell><cell>85.12</cell></row><row><cell>Attentive Pooling</cell><cell>85.36</cell></row><row><cell>LA</cell><cell>85.27</cell></row><row><cell>LA</cell><cell>85.40</cell></row><row><cell>LA</cell><cell>85.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII RESULTS</head><label>VII</label><figDesc>OF USING DIFFERENT BACKBONES. WE CAN OBTAIN ONE CONSISTENT RESULT THAT OUR METHOD IS COMPATIBLE WITH DIFFERENT NETWORK STRUCTURES AND CAN STILL IMPROVE THE ACCURACY OF CLASSIFICATION AND SEGMENTATION TASKS.PT donates Point Transformer. PCT donates Point Cloud Transformer. PAConv (PN) donates using PointNet as the backbone (without voting). OA:</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ModelNet-40</cell></row><row><cell>Methods</cell><cell>Pre-trained</cell><cell cols="2">Classification</cell></row><row><cell></cell><cell></cell><cell>OA (%)</cell><cell>mA (%)</cell></row><row><cell>Ours</cell><cell>-</cell><cell>92.74</cell><cell>89.88</cell></row><row><cell>Ours</cell><cell>ShapeNet-Part</cell><cell>92.79</cell><cell>90.10</cell></row><row><cell>Ours</cell><cell>ModelNet-40</cell><cell>93.39</cell><cell>90.26</cell></row><row><cell>PointNet++ [36]  *</cell><cell>-</cell><cell>92.07</cell><cell>88.89</cell></row><row><cell>PointNet++  *  + Ours</cell><cell>ShapeNet-Part</cell><cell>92.19</cell><cell>89.63</cell></row><row><cell>PointNet++  *  + Ours</cell><cell>ModelNet-40</cell><cell>92.57</cell><cell>89.96</cell></row><row><cell>OGNet  *  [76]</cell><cell>-</cell><cell>93.23</cell><cell>89.82</cell></row><row><cell>OGNet  *  + Ours</cell><cell>ShapeNet-Part</cell><cell>93.35</cell><cell>90.51</cell></row><row><cell>OGNet  *  + Ours</cell><cell>ModelNet-40</cell><cell>93.31</cell><cell>90.71</cell></row><row><cell>PAConv  *  (PN) [41]</cell><cell>-</cell><cell>92.50</cell><cell>-</cell></row><row><cell>PAConv  *  (PN) + Ours</cell><cell>ShapeNet-Part</cell><cell>92.70</cell><cell>-</cell></row><row><cell>PAConv  *  (PN) + Ours</cell><cell>ModelNet-40</cell><cell>92.79</cell><cell>-</cell></row><row><cell>PT  *  [43]</cell><cell>-</cell><cell>91.47</cell><cell>89.32</cell></row><row><cell>PT  *  + Ours</cell><cell>ShapeNet-Part</cell><cell>92.03</cell><cell>89.50</cell></row><row><cell>PT  *  + Ours</cell><cell>ModelNet-40</cell><cell>92.07</cell><cell>89.58</cell></row><row><cell>PCT  *  [42]</cell><cell>-</cell><cell>92.71</cell><cell>89.36</cell></row><row><cell>PCT  *  + Ours</cell><cell>ShapeNet-Part</cell><cell>93.07</cell><cell>90.27</cell></row><row><cell>PCT  *  + Ours</cell><cell>ModelNet-40</cell><cell>93.15</cell><cell>90.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for image and point cloud fusion in autonomous driving: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for lidar point clouds in autonomous driving: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of end-to-end driving: Architectures and training methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Semikin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rgb-d point cloud registration based on salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boost 3-d object detection via point clouds segmentation and fused 3-d giou-l loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning semantic segmentation of large-scale point clouds with random sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A comprehensive survey on point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02690</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale person re-identification as retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep top-k ranking for image-sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1109/TMM.2019.2931352</idno>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="775" to="785" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vehiclenet: Learning robust visual representation for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1109/TMM.2020.3014488</idno>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2683" to="2693" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pre-training for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge,&quot; IJCV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d shape segmentation via shape fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="182" to="192" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Large-scale 3d shape reconstruction and segmentation from shapenet core55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06104</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards a weakly supervised framework for 3d point cloud object detection and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointmixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Auto-mvcnn: Neural architecture search for multi-view 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05493</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive exploration for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1145/3369393</idno>
	</analytic>
	<monogr>
		<title level="j">ACM TOMM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Temporal cross-layer correlation mining for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1109/TMM.2021.3057503</idno>
		<idno>doi:10. 1109/TMM.2021.3057503</idno>
	</analytic>
	<monogr>
		<title level="m">TMM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning for point cloud understanding by contrasting and clustering using graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-angle point cloudvae: Unsupervised feature learning for 3d point clouds from multiple angles by joint self-reconstruction and half-to-half prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Unsupervised 3D learning for shape analysis via multiresolution instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-supervised pretraining of 3d features on any point-cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pointcutmix: Regularization strategy for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">505</biblScope>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Regularization strategy for point cloud via rigidly mixed sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multiple knowledge representation for big data artificial intelligence: framework, applications, and case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1631/FITEE.2100463</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1551" to="1558" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dualpath convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1145/3383184</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Walk in the cloud: Learning curves for point clouds shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Parameter-efficient person re-identification in the 3d space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04569</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An informationrich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Unsupervised 3d shape completion through gan inversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">3d point cloud generative adversarial network based on tree structured graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Pu-gan: a point cloud upsampling adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021. Chao Sun received the B.E. degree in software engineering from Zhejiang University</title>
		<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Science at Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a master student with the School of. His current research interests include the 3D point cloud and the generative models</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">He is currently a postdoctoral research fellow at Sea-NExT joint lab, School of Computing, National University of Singapore. He was an intern at</title>
	</analytic>
	<monogr>
		<title level="j">Nvidia Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Zhedong Zheng received the Ph.D. degree from the University of Technology Sydney, Australia, in 2021 and the B.S. degree from Fudan University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include robust learning for image retrieval, generative learning for data augmentation. and unsupervised domain adaptation</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">His research interest includes video analysis, egocentric vision and multi-modal understanding</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2021</biblScope>
			<pubPlace>Zhejiang University, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xiaohan Wang received the Ph.D. degree in computer science from University of Technology Sydney, Australia</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a postdoctoral researcher with the College of Computer Science and Technology</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">He received his Ph.D. degree in computer science and technology from the State Key Lab of</title>
		<editor>B.S. and M.S</editor>
		<imprint>
			<pubPlace>Hangzhou, China; Zhengzhou, China, respectively</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Information Engineering of Zhengzhou University, China ; CAD&amp;CG at Zhejiang University ; degrees from the Computer Science Department, Zhengzhou University</orgName>
		</respStmt>
	</monogr>
	<note>Mingliang Xu is a professor in the</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">He is currently a professor with University of Technology Sydney, Australia. He was a Post-Doctoral Research with the School of Computer Science</title>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Hangzhou, China; Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Yi Yang received the Ph.D. degree in computer science from Zhejiang University ; Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>His current research interest includes machine learning and its applications to multimedia content analysis and computer vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
